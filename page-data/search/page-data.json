{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"개요 Edge function을 열심히 개발하고 있었는데, 코드 자동완성이나 줄바꿈이나 이런것들이 적용되지 않았습니다.  Edge function은 Deno를 사용하는데, 이것때문인 것 같긴 했는데 짬이 안나 살펴보지 못하다가 이제서야 살펴보니, 뭔가를 하지 않아서였습니다.  Deno 설치 정말 당연하게도 Deno를 먼저 설치해야 합니다. 이런것도 하지 …","fields":{"slug":"/Supabase-EdgeFunction---Deno-개발환경-꾸미기/"},"frontmatter":{"date":"March 14, 2024","title":"Supabase EdgeFunction  - Deno 개발환경 꾸미기","tags":["Supabase","Deno"]},"rawMarkdownBody":"![](image1.png)\n## 개요\n\nEdge function을 열심히 개발하고 있었는데, 코드 자동완성이나 줄바꿈이나 이런것들이 적용되지 않았습니다. \n\nEdge function은 Deno를 사용하는데, 이것때문인 것 같긴 했는데 짬이 안나 살펴보지 못하다가 이제서야 살펴보니, 뭔가를 하지 않아서였습니다. \n\n## Deno 설치\n\n정말 당연하게도 Deno를 먼저 설치해야 합니다. 이런것도 하지 않고 개발을 하고 있었습니다.. 맥 기준으로 설치 방법을 정리하겠습니다. \n\n### Deno 설치\n\n다음의 명령어를 통해 설치 여부를 먼저 확인해줍니다. \n\n```bash\ndeno --version\n```\n\n설치가 되어있지 않다면 설치를 진행합니다. [공식사이트](https://docs.deno.com/runtime/manual)에서 자세한 설치법을 확인할 수 있습니다. \n\n```bash\ncurl -fsSL https://deno.land/install.sh | sh\n```\n\n를 이용해 설치를 해줍니다. \n\n그 다음, ~/.zshrc 를 수정해 환경변수에 deno 실행 명령어를 등록합니다. \n\n### Deno VS Code 확장 설치\n\nvscode를 사용하고 있으므로 deno 확장을 설치해줍니다. \n\n### Deno 확장 활성화 및 설정\n\n확장 설치 후, Deno를 사용할 프로젝트 폴더에 대해 Deno 확장을 활성화해야 합니다. 프로젝트의 루트 디렉토리에 `.vscode` 폴더를 생성하고, 그 안에 `settings.json` 파일을 만들어 다음 설정을 추가합니다.\n\n```json\n{\n    \"deno.enable\": true,\n    \"deno.lint\": true,\n    \"deno.unstable\": true\n}\n```\n\n\n\n여기까지 해주고 vs code를 재실행하면 이제 개발환경이 잘 꾸며진 것을 확인할 수 있습니다. \n\n## 오류\n\n`Uncached or missing remote URL` 오류가 발생할때가 있습니다. 이 오류는 지정된 URL에서 모듈을 캐시할 수 없거나 URL이 잘못되었을 때 발생합니다. 이 오류는 Deno가 모듈을 다운로드하고 캐싱하는 방식과 관련이 있습니다. 오류 메시지는 `https://esm.sh/@supabase/supabase-js@2`로 지정된 모듈을 Deno가 찾지 못했거나 캐시하지 못했다는 것을 나타냅니다. \n\n### 해결법\n\nDeno는 모듈을 한 번 다운로드하면 로컬에 캐시합니다. 캐시된 모듈이 문제를 일으킬 수 있으므로, 모듈 캐시를 강제로 업데이트해 볼 수 있습니다. 다음 명령어를 사용하여 모듈을 강제로 캐시에서 업데이트하고 다시 로드할 수 있습니다.\n\n```bash\ndeno cache --reload https://esm.sh/@supabase/supabase-js@2\n```\n\n### 캐시 초기화\n\n외부 모듈이 자동으로 캐시되지 않거나 오래된 버전의 캐시가 문제를 일으킬 수 있습니다. Deno 캐시를 갱신하려면, 터미널에서 `deno cache --reload` 명령어를 사용할 수 있습니다.\n\n\n\n"},{"excerpt":"개요 지난 시간에 Egde Function을 만들어서 배포하는 것까지 진행해봤습니다. 하지만 저기서 끝낸다면 로컬에 DB가 없으므로 DB에 접근을 필요로 하는 Edge Function은 로컬에서의 정상적인 테스트가 불가능합니다.  따라서 이번에는 테스트에 필요한 환경을 구성하는 방법을 정리합니다.  사전 작업 테스트를 한다는 것은 실제 개발 또는 Prod…","fields":{"slug":"/Supabase-Local-개발-환경-꾸미기---DB-세팅/"},"frontmatter":{"date":"March 13, 2024","title":"Supabase Local 개발 환경 꾸미기 - DB 세팅","tags":["Supabase"]},"rawMarkdownBody":"![](image1.png)\n## 개요\n\n[지난 시간](https://sharknia.github.io/Supabase-Local-Dev-환경-꾸미기)에 Egde Function을 만들어서 배포하는 것까지 진행해봤습니다. 하지만 저기서 끝낸다면 로컬에 DB가 없으므로 DB에 접근을 필요로 하는 Edge Function은 로컬에서의 정상적인 테스트가 불가능합니다. \n\n따라서 이번에는 테스트에 필요한 환경을 구성하는 방법을 정리합니다. \n\n## 사전 작업\n\n테스트를 한다는 것은 실제 개발 또는 Production 서버와 같은 환경을 꾸며야 한다는 것이고, 이는 데이터나 스키마가 동일해야 한다는 것으로, 현재로서는 개발 환경 프로젝트와 동일한 DB 환경을 꾸미는 것을 목표로 합니다. 따라서, `supabase link ..` 작업까지는 완료가 되어있어야 합니다. \n\n## DB 스키마 맞추기\n\n### supabase db pull\n\n다음의 명령어를 통해 연결된 프로젝트의 스키마를 모두 생성하는 쿼리문을 생성할 수 있습니다. \n\n```bash\nsupabase db pull --linked\n```\n\n이렇게 하면 supbase/migrations 디렉토리 안에 `<timestamp>_remote_schema.sql` 꼴의 쿼리문 파일이 생성됩니다. 해당 쿼리문은 자동으로 실행되지는 않으며, 직접 쿼리문을 실행해줘야 합니다. \n\n초기 설정 이후에는 `supabase db pull --linked`를 실행하면 변경된 사항만 자동으로 쿼리문을 생성합니다. 스키마를 지정해 특정 스키마만 pull 할 수도 있습니다. \n\n로컬에서 DB가 돌게 되므로, 대시보드는 없지만 pgAdmin등을 통해 로컬 DB에도 연결할 수 있습니다. 연결 정보가 생각나지 않는다면 `supabase status` 명령어를 통해 다시 확인할 수 있습니다. \n\n### supabase migration\n\n버전 관리 기능도 갖추고 있습니다. \n\n- supabase migration list : 마이그레이션 상태를 확인합니다. \n\n- supabase migration up : 보류 중인 마이그레이션을 로컬 데이터베이스에 적용합니다. \n\n### supabase db reset\n\n여기까지 해두면, supabase db reset을 할 때마다 데이터베이스를 지웠다가 스키마를 다시 생성합니다. 언제든지 초기 상태로 돌아올 수 있습니다. \n\n## 테스트 데이터 맞추기\n\n원활한 테스트를 위해서는 스키마는 물론이고 테스트 데이터도 들어있어야 합니다. db reset를 할 때마다 테스트 데이터를 넣기가 여간 번거로운 일이 아닐텐데, seed.sql 파일에 insert 구문을 미리 작성해두면 db reset에서 데이터베이스 삭제 후, migration 쿼리문을 통해 스키마를 맞춘 후 seed.sql의 insert 구문을 실행해 테스트 데이터까지 삽입한 깔끔한 테스팅 상태로 맞춰줍니다. \n\n## 참고\n\n[https://supabase.com/docs/guides/cli/local-development](https://supabase.com/docs/guides/cli/local-development)\n\n[https://supabase.com/docs/reference/cli/supabase-db-pull](https://supabase.com/docs/reference/cli/supabase-db-pull)\n\n[https://supabase.com/docs/guides/cli/seeding-your-database](https://supabase.com/docs/guides/cli/seeding-your-database)\n\n"},{"excerpt":"Row-Level Security (RLS) 란? Row-Level Security (RLS)는 데이터베이스 시스템에서 제공하는 보안 기능 중 하나로, 데이터베이스의 각 행(row)에 대한 접근을 제어하는 세밀한 방법을 제공합니다. RLS를 사용하면 데이터베이스 사용자나 응용 프로그램이 특정 조건을 만족하는 행에만 접근하거나 그러한 행을 변경할 수 있도록…","fields":{"slug":"/Supabase와-Row-Level-Security-RLS/"},"frontmatter":{"date":"March 12, 2024","title":"Supabase와 Row-Level Security (RLS) ","tags":["Supabase","Postgresql","RLS"]},"rawMarkdownBody":"![](image1.png)\n## Row-Level Security (RLS) 란?\n\nRow-Level Security (RLS)는 데이터베이스 시스템에서 제공하는 보안 기능 중 하나로, 데이터베이스의 각 행(row)에 대한 접근을 제어하는 세밀한 방법을 제공합니다. RLS를 사용하면 데이터베이스 사용자나 응용 프로그램이 특정 조건을 만족하는 행에만 접근하거나 그러한 행을 변경할 수 있도록 정책을 설정할 수 있습니다. 이는 사용자별 데이터 접근 제한, 다중 사용자 환경에서의 데이터 격리, 그리고 민감한 정보의 보호와 같은 목적을 위해 중요합니다.\n\n### 특징\n\n#### 세밀한 접근 제어\n\nRLS는 사용자 또는 사용자 그룹별로 데이터베이스 행에 대한 접근 권한을 설정할 수 있게 해줍니다. 이를 통해 특정 데이터에 대한 접근을 정밀하게 제한할 수 있습니다.\n\n#### 정책 기반 보안\n\nRLS는 데이터베이스 레벨에서 정책(policy)을 정의하여 작동합니다. 이 정책들은 데이터에 대한 `SELECT`, `INSERT`, `UPDATE`, `DELETE`와 같은 작업 수행 시 적용되어, 해당 작업이 정책에 정의된 규칙을 충족시킬 때만 수행될 수 있습니다.\n\n#### 투명성\n\nRLS는 데이터베이스 시스템 내에서 자동으로 적용되므로, 응용 프로그램 코드를 변경하지 않고도 데이터 접근 제어를 강화할 수 있습니다. 사용자는 자신이 접근 권한을 가진 데이터만 볼 수 있으며, RLS 정책에 의해 접근이 제한된 데이터는 *존재하지 않는 것처럼* 처리됩니다.\n\n## 예제 파헤쳐보기\n\n먼저 RLS를 하나 설정해보고, 이 구문이 뭘 의미하는지 한 번 살펴보겠습니다. \n\n우선, 특정 테이블에 대해 RLS를 활성화 해줍니다. \n\n```sql\nALTER TABLE my_table ENABLE ROW LEVEL SECURITY;\n```\n\n특정 조건에 따라 데이터 접근을 제어하는 정책을 추가합니다. 예를 들어, 사용자가 자신의 데이터에만 접근할 수 있도록 하는 정책을 추가할 수 있습니다.\n\n```sql\nCREATE POLICY my_policy ON my_table FOR SELECT\nUSING (user_id = current_user_id());\n```\n\n이러한 방식으로 RLS를 구성함으로써, 데이터베이스 관리자는 데이터의 보안과 무결성을 유지하면서도 필요한 사용자에게 필요한 데이터에 대한 접근을 허용할 수 있습니다.\n\n이는 모든 쿼리에 WHERE 절을 추가하는 것으로 생각하면 좀 더 쉽습니다. 예를 들어 위 정책의 경우, \n\n```sql\nselect * from my_table\nwhere current_user_id = my_table.user_id;\n-- Policy is implicitly added.\n```\n\n이렇게 정책이 where 절에 암묵적으로 추가된다고 생각하면 이해하기 쉽습니다. \n\n### Supabase에서의 RLS\n\n좀 더 복잡하고, Supabase에 특화된 예제를 보면 다음과 같습니다. \n\n```python\ncreate policy \"Allow authenticated uploads\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n  (storage.foldername(name))[1] = auth.uid()::text\n);\n```\n\n여기서 설정한 정책은 \"Allow authenticated uploads\"라는 이름으로, `storage.objects` 테이블에 대한 `insert` 작업에 적용됩니다. 이 정책은 다음과 같은 조건을 충족시키는 경우에만 삽입 작업을 허용합니다:\n\n\n\n객체의 이름에서 파생된 폴더 이름 (`storage.foldername(name)` 함수를 통해 추출)의 첫 번째 요소가 현재 인증된 사용자의 ID (`auth.uid()`)와 일치해야 합니다. 이는 사용자가 자신의 고유 폴더에만 파일을 업로드할 수 있도록 제한하는 조건입니다.\n\n\n\n요약하자면, 이 RLS 정책은 인증된 사용자가 'profile\\_images' 버킷 내에 자신의 사용자 ID에 해당하는 폴더에만 파일을 업로드할 수 있게 합니다.\n\n## Using과 Check\n\n위의 두 예제를 살펴보면 각각 Using과 Check를 사용한 것을 볼 수 있습니다. \n\nPostgreSQL의 행 수준 보안(Row-Level Security, RLS) 정책을 사용할 때, `WITH CHECK`와 `USING` 절은 서로 다른 목적으로 사용됩니다. 이들의 주요 차이점은 정책이 적용되는 시점과 데이터 조작 작업(DML) 유형에 따라 달라집니다.\n\n### `WITH CHECK`\n\n- `WITH CHECK` 절은 `INSERT`와 `UPDATE` 작업에 대해 사용됩니다.\n\n- 이 절은 새로운 데이터가 삽입되거나 기존 데이터가 업데이트될 때 적용되어야 하는 조건을 정의합니다.\n\n- `WITH CHECK`은 해당 작업이 실행된 후 최종적으로 데이터가 정책에 정의된 조건을 만족하는지 검증합니다. 만약 조건을 만족하지 않는다면, 작업은 거부됩니다.\n\n- 예를 들어, 사용자가 자신의 폴더에만 파일을 삽입하거나 업데이트할 수 있도록 제한하는 정책을 설정할 때 사용됩니다.\n\n### `USING`\n\n- `USING` 절은 `SELECT`와 `DELETE` 작업에 대해 사용됩니다.\n\n- 이 절은 해당 작업을 수행할 때 접근이 허용되는 행을 결정하는 조건을 정의합니다.\n\n- `SELECT` 작업에 대해, `USING` 조건을 만족하는 행만 조회될 수 있습니다.\n\n- `DELETE` 작업에 대해, `USING` 조건을 만족하는 행만 삭제될 수 있습니다.\n\n- 예를 들어, 사용자가 자신의 폴더에 있는 파일만 조회하거나 삭제할 수 있도록 제한하는 정책을 설정할 때 사용됩니다.\n\n### 예시\n\n결론적으로, `WITH CHECK`와 `USING`의 차이는 적용되는 데이터 조작 작업의 유형과 해당 조건이 검증되는 시점에 있습니다.\n\n## Roles\n\n### to authenticated\n\n`to authenticated` 구문은 해당 정책이 인증된 사용자, 즉 로그인한 사용자에게만 적용됨을 의미합니다. Supabase 및 PostgreSQL의 Row-Level Security (RLS) 정책에서는, 특정 데이터베이스 작업(예: 삽입, 조회, 수정, 삭제)에 대한 접근 권한을 세밀하게 제어할 수 있습니다. 여기서 `to authenticated`는 인증 과정을 성공적으로 마친 사용자, 즉 시스템이 식별하고 인증한 사용자에게만 해당 작업을 허용한다는 것을 명시합니다.\n\n### to anon\n\n이와 반대되는 개념으로는 `to anon`이 있습니다. Supabase Auth는 모든 요청을 `authenticated` 또는 `anon` 에 매핑합니다. `authenticated` 이 로그인한 유저라면, anon은 로그인 하지 않은 유저를 의미합니다. \n\n### 예제\n\n```sql\ncreate policy \"Profiles are viewable by everyone\"\non profiles for select\nto authenticated, anon\nusing ( true );\n\n-- OR\n\ncreate policy \"Public profiles are viewable only by authenticated users\"\non profiles for select\nto authenticated\nusing ( true );\n\n```\n\n“Profiles are viewable by everyone” 정책은 인증된 유저와 인증되지 않은 유저 모두에게 읽기를 허용하며, \"Public profiles are viewable only by authenticated users\" 정책은 오직 인증된 유저에게만 읽기를 허용합니다. \n\n### service\\_role\n\nSupabase는 `service_role` 같은 특별한 \"서비스\" 키를 제공하는데, 이는 RLS를 우회할 수 있어 관리 작업에 유용하나, 클라이언트 사이드나 고객에게 노출되어서는 안 됩니다. 이는 반드시 백엔드 단에서만 사용해야 합니다. \n\n\n\n**Supbase는 클라이언트 라이브러리가 Service Key로 초기화된 경우에도 서명한 사용자의 RLS 정책을 준수합니다.**\n\n## auth 모듈\n\n### auth.uid()\n\n`auth.uid()`는 현재 인증된 사용자의 고유 식별자(ID)를 반환하는 함수입니다. Supabase는 내부적으로 사용자 인증 시스템을 갖추고 있으며, 사용자가 로그인할 때 각 사용자에게 고유한 ID를 할당합니다. 이 ID는 사용자의 세션 및 인증 정보와 연결되어 있으며, `auth.uid()`를 통해 현재 세션의 사용자 ID를 쉽게 조회할 수 있습니다. 이를 통해 데이터베이스 내에서 사용자별 데이터 접근을 제어하거나 사용자 식별 정보를 기반으로 한 작업을 수행할 수 있습니다.\n\n예를 들어, 사용자가 파일을 업로드할 때, `auth.uid()`를 사용하여 해당 파일이 특정 사용자에게 속함을 식별하거나, 사용자별로 데이터를 격리하여 보안을 강화하는 데 활용할 수 있습니다. 사용자가 로그인 상태가 아니라면, `auth.uid()`는 `null`이나 유효하지 않은 값을 반환할 수 있으므로, `to authenticated` 구문과 함께 사용하여 인증된 사용자에게만 특정 작업을 허용하는 방식으로 보안을 관리합니다.\n\n### auth.jwt()\n\n현재 요청을 하는 사용자의 JWT(Json Web Token)를 반환합니다. JWT 내에는 사용자의 `app_metadata` 및 `user_metadata` 컬럼에 저장된 정보가 포함될 수 있습니다.\n\n- `user_metadata`: 사용자가 `supabase.auth.update()` 함수를 통해 업데이트할 수 있는 메타데이터입니다. 사용자 관련 정보(예: 선호 설정)를 저장하는 데 적합하지만, 인증 데이터를 저장하기에는 적합하지 않습니다.\n\n- `app_metadata`: 사용자가 업데이트할 수 없기 때문에 인증 데이터를 저장하는 데 적합한 메타데이터입니다. 예를 들어, 사용자가 특정 팀에 속해 있는지 여부를 판단하는 데 사용할 수 있는 팀 데이터를 여기에 저장할 수 있습니다.\n\n```sql\ncreate policy \"User is in team\"\non my_table\nto authenticated\nusing ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));\n```\n\n예를 들어, 사용자가 어떤 팀에 속해 있는지 확인하는 데 `app_metadata` 내에 저장된 팀 데이터(팀 ID 배열 등)를 사용할 수 있습니다. \n\n## 결론\n\nSupabase에서는 항상 RLS를 사용할 것을 권장하고 있습니다. 만약 모두의 접근이 필요한 경우에도, RLS를 활성화하고 인증된 유저와 인증되지 않은 유저에게 권한을 부여하는 방식을 사용할 것을 권장합니다. \n\n## 참고\n\n[https://supabase.com/docs/guides/database/postgres/row-level-security](https://supabase.com/docs/guides/database/postgres/row-level-security)\n\n"},{"excerpt":"Supabase CLI 설치 우선 supabase CLI를 설치해야 합니다.  제대로 설치가 되면 다음의 명령어를 통해 설치가 된 것을 확인합니다.  login을 진행합니다. 터미널의 다음의 명령어를 입력한 후, 로그인은 웹에서 이뤄집니다.  Project를 연결해야 합니다. 먼저 다음의 명령어를 통해 프로젝트 리스트를 확인합니다.  다음과 같은 결과가 …","fields":{"slug":"/Supabase-Local-Dev-환경-꾸미기/"},"frontmatter":{"date":"March 11, 2024","title":"Supabase Local Dev 환경 꾸미기","tags":["Supabase","Postgresql","Edge-Function"]},"rawMarkdownBody":"![](image1.png)\n## Supabase CLI 설치\n\n우선 supabase CLI를 설치해야 합니다. \n\n```shell\nbrew install supabase/tap/supabase\n```\n\n제대로 설치가 되면 다음의 명령어를 통해 설치가 된 것을 확인합니다. \n\n```shell\nsupabase help\n```\n\nlogin을 진행합니다. 터미널의 다음의 명령어를 입력한 후, 로그인은 웹에서 이뤄집니다. \n\n```shell\nsupabase login\n```\n\nProject를 연결해야 합니다. 먼저 다음의 명령어를 통해 프로젝트 리스트를 확인합니다. \n\n```shell\nsupabase projects list\n```\n\n다음과 같은 결과가 출력됩니다. \n\n```shell\n    LINKED │        ORG ID        │     REFERENCE ID     │   NAME   │         REGION         │  CREATED AT (UTC)\n  ─────────┼──────────────────────┼──────────────────────┼──────────┼────────────────────────┼──────────────────────\n           │ [ORG ID        ]     │ [REFERENCE ID     ]  │ [NAME]   │ Northeast Asia (Seoul) │ 2024-02-17 04:10:12\n```\n\n이 중에서 REFERENCE ID를 사용해 project에 연결할 수 있습니다. \n\n```shell\nsupabase link --project-ref [REFERENCE ID]\n```\n\n#### 참고\n\n## Edge Function 로컬에서 개발하기\n\n로컬에 supabase project를 생성합니다. \n\n```shell\nsupabase init\n```\n\n다음의 명령어를 사용하면 예제 Edge function을 만들 수 있습니다. 이 명령어는 함수를 로컬에 생성합니다. \n\n```shell\nsupabase functions new hello-world\n```\n\n다음과 같은 구조로 Edge Function이 생성됩니다. \n\n```shell\n└── supabase\n    ├── functions\n    │   └── hello-world\n    │   │   └── index.ts ## Your function code\n    └── config.toml\n```\n\n### 예제 코드\n\n```typescript\nDeno.serve(async (req) => {\n  const { name } = await req.json()\n  const data = {\n    message: `Hello ${name}!`,\n  }\n\n  return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })\n})\n```\n\nEdge Function은 native Deno.serve를 사용합니다. \n\n## Edge Function 로컬에서 실행하기\n\nstart 명령어를 사용하면, 도커를 이용해서 로컬에서 supabase service를 실행할 수 있습니다. \n\n```shell\nsupabase start\n```\n\n성공적으로 실행되면 다음과 같은 내용이 출력됩니다. 해당 내용은 `supabase status` 명령어를 사용해 다시 볼 수 있습니다.\n\n```shell\nStarted supabase local development setup.\n\n         API URL: http://localhost:54321\n          DB URL: postgresql://postgres:postgres@localhost:54322/postgres\n      Studio URL: http://localhost:54323\n    Inbucket URL: http://localhost:54324\n        anon key: eyJh......\nservice_role key: eyJh......\n```\n\n`supabase stop` 명령을 사용하여 로컬 데이터베이스를 재설정하지 않고 언제든지 모든 서비스를 중지할 수 있습니다. 또한 `supabase stop --no-backup`을 사용하여 모든 서비스를 중지하고 로컬 데이터베이스를 재설정할 수 있습니다.\n\n\n\n다음의 명령어를 사용해 Function Watcher를 실행합니다. \n\n```shell\nsupabase functions serve\n```\n\n이 명령어는 hot-reloading 기능을 가지고 있어 코드에 수정이 생기면 Deno server를 자동으로 재시작합니다. \n\n\n\n여기까지 마친 후, 다음의 명령어를 통해 방금 만든 Edge Function을 실행해볼 수 있습니다. \n\n```shell\ncurl --request POST 'http://localhost:54321/functions/v1/hello-world' \\\n  --header 'Authorization: Bearer SUPABASE_ANON_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{ \"name\":\"Functions\" }'\n```\n\nSUPABASE\\_ANON\\_KEY는 start 이후 생성된 anon key를 사용하면 됩니다. 성공적으로 실행되면 다음과 같은 문구를 확인할 수 있습니다. \n\n```shell\n{\"message\":\"Hello Functions!\"}\n```\n\n## Edge Function 배포하기\n\n기본적인 배포는 다음의 명령어를 사용합니다. 모든 Edge Function을 배포합니다. \n\n```shell\nsupabase functions deploy\n```\n\n특정 Edge function만 배포할 수도 있습니다. \n\n```shell\nsupabase functions deploy hello-world\n```\n\n기본적으로 Edge Function은 JWT 토큰이 필수입니다. 토큰이 필요없는 Edge function을 만들기 위해서는 `--no-verify-jwt` 옵션을 사용합니다. \n\n```shell\nsupabase functions deploy hello-world --no-verify-jwt\n```\n\n배포가 성공적으로 이뤄지면 supabase 대쉬보드에서도 이를 확인할 수 있습니다. \n\n\n\n다음의 명령어를 사용해 방금 배포한 Edge Function을 테스트 해볼 수 있습니다. \n\n```shell\ncurl --request POST 'https://[REFERENCE ID].supabase.co/functions/v1/hello-world' \\\n  --header 'Authorization: Bearer ANON_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{ \"name\":\"Functions\" }'\n```\n\n`--no-verify-jwt` 옵션을 사용한 경우에는 ANON\\_KEY가 필요 없으며, ANON\\_KEY는 Supabase 대시보드에서 확인할 수 있습니다. \n\n## Edge Function 삭제\n\n다음의 명령어를 통해 Edge Function을 삭제할 수 있습니다.\n\n```shell\nsupabase functions delete hello-world --project-ref dcrdrguovpqchlptvlqy\n```\n\n단 이 명령어는 로컬에서는 작동하지 않습니다. 로컬의 함수는 수동으로 삭제하면 됩니다. \n\n## 기타 Edge Function 관련 CLI 명령어\n\nfunction의 리스트를 확인합니다. 이 코드는 연결된 프로젝트의 함수 리스트를 가져옵니다. \n\n```shell\nsupabase functions list\n```\n\n특정 function을 다운로드 합니다. \n\n```shell\nsupabase functions download <Function name>\n```\n\n공통적으로 `--project-ref` **옵션을 사용해 특정 project를 명시할 수 있습니다.** \n\n\n\n## Deno 환경에서 supabase client 사용하기\n\nDeno 환경에서는 npm을 사용하지 않으므로, supabase client가 필요할 경우 다음의 방법을 사용해 import 합니다. \n\n```typescript\n  import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n```\n\n또는 import map을 사용할 수 있습니다. \n\nsupabase/functions/import\\_map.json 파일을 생성 후, 다음의 내용을 입력해줍니다. \n\n```json\n{\n    \"imports\": {\n        \"supabase\": \"https://esm.sh/@supabase/supabase-js@2\"\n    }\n}\n```\n\nimport map을 사용하면 다음과 같이 쉽게 import 할 수 있습니다. \n\n```typescript\nimport { createClient } from 'supabase';\n```\n\n## Deno 환경에서 환경변수 사용하기\n\n다음의 코드를 사용해 환경변수를 사용할 수 있습니다. \n\n```typescript\nDeno.env.get(MY_SECRET_NAME)\n```\n\n현재 설정되어있는 환경 변수들은 다음의 명령어를 통해 확인할 수 있습니다. \n\n```shell\nsupabase secrets list\n```\n\n기본적으로 supabase는 4개의 환경 변수를 설정하지 않아도 지원합니다. \n\n- SUPABASE\\_URL\n\n- SUPABASE\\_ANON\\_KEY\n\n- SUPABASE\\_SERVICE\\_ROLE\\_KEY\n\n- SUPABASE\\_DB\\_URL\n\n### 환경변수 추가하기 - 로컬\n\n추가적으로 환경변수를 설정해야 할 때가 있습니다. 이 때 다음의 방법을 사용합니다. \n\n먼저 환경 변수를 생성합니다. \n\n```shell\necho \"MY_NAME=Yoda\" >> ./supabase/.env.local\n```\n\n그리고 로컬에서 serve 시에 다음의 옵션값을 함께 줍니다. \n\n```shell\nsupabase functions serve --env-file ./supabase/.env.local\n```\n\n이렇게 하면 로컬에서 MY\\_NAME 환경 변수를 사용할 수 있습니다. \n\n### 환경변수 추가하기 - 실서버\n\n우선 실서버에 적용할 환경변수 파일을 생성합니다. \n\n```shell\ncp ./supabase/.env.local ./supabase/.env\n```\n\n그리고 다음의 명령어를 사용해 환경변수를 등록합니다. \n\n```shell\nsupabase secrets set --env-file ./supabase/.env\n\n# You can also set secrets individually using:\nsupabase secrets set MY_NAME=Chewbacca\n```\n\n#### 참고\n\n[https://supabase.com/docs/guides/functions/secrets](https://supabase.com/docs/guides/functions/secrets)\n\n"},{"excerpt":"unique_constraints란? 테이블의 특정 컬럼 또는 컬럼의 조합에 대해 유니크 제약조건을 설정하는 데 사용됩니다. 이는 데이터베이스에 동일한 값을 가진 중복 레코드가 없도록 보장하는 데 유용합니다. unique_constraints 설정하기 클래스 내부에  속성을 정의하고, unique_constraints 튜플을 이 속성에 할당합니다. 이 튜…","fields":{"slug":"/SQLModel에서-unique_constraints-설정하기/"},"frontmatter":{"date":"March 05, 2024","title":"SQLModel에서 unique_constraints 설정하기","tags":["SqlAlchemy","SQLModel"]},"rawMarkdownBody":"## unique\\_constraints란? \n\n테이블의 특정 컬럼 또는 컬럼의 조합에 대해 유니크 제약조건을 설정하는 데 사용됩니다. 이는 데이터베이스에 동일한 값을 가진 중복 레코드가 없도록 보장하는 데 유용합니다.\n\n## unique\\_constraints 설정하기\n\n클래스 내부에 `__table_args__` 속성을 정의하고, unique\\_constraints 튜플을 이 속성에 할당합니다. 이 튜플 내에서, 각 유니크 제약조건은 컬럼 이름을 담은 튜플로 표현됩니다.\n\n### 예시 코드\n\n다음은 User 모델에 대해 `email`과 `username` 컬럼 조합에 유니크 제약조건을 설정하는 예시입니다.\n\n```python\nfrom sqlmodel import Field, SQLModel\n\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    username: str\n    email: str\n    \n    __table_args__ = (\n        UniqueConstraint('username', 'email'),\n    )\n\n```\n\n위 코드는 같은 `username`과 `email` 값을 가진 두 개의 `User` 레코드가 데이터베이스에 존재할 수 없음을 의미합니다.\n\n### 주의 사항\n\n`__table_args__` 는 SQLAlchemy의 기능을 직접 사용합니다. 따라서, SQLModel을 사용할 때 SQLAlchemy에 대한 기본적인 이해가 필요할 수 있습니다.\n\n\n\n"},{"excerpt":"서론 JSON은 이제는 아주 익숙한 데이터 저장 형식입니다. Postgresql에서는 JSON 타입을 지원해 이 데이터를 효과적으로 다룰 수 있도록 하는데, JSONB 타입을 또 지원합니다.  이 두 타입은 저장 방식과 처리 속도, 기능성 측면에서 차이가 있습니다. 각각의 타입은 특정 사용 사례에 더 적합할 수 있으므로, 차이점을 이해하는 것이 중요합니다…","fields":{"slug":"/Postgresql의-JSON과-JSONB/"},"frontmatter":{"date":"March 04, 2024","title":"Postgresql의 JSON과 JSONB","tags":["Postgresql"]},"rawMarkdownBody":"## 서론\n\nJSON은 이제는 아주 익숙한 데이터 저장 형식입니다. Postgresql에서는 JSON 타입을 지원해 이 데이터를 효과적으로 다룰 수 있도록 하는데, JSONB 타입을 또 지원합니다. \n\n이 두 타입은 저장 방식과 처리 속도, 기능성 측면에서 차이가 있습니다. 각각의 타입은 특정 사용 사례에 더 적합할 수 있으므로, 차이점을 이해하는 것이 중요합니다.\n\n## **JSON**\n\n- `JSON` 데이터 타입은 입력된 JSON 데이터를 텍스트 기반의 형식으로 저장합니다.\n\n- 데이터는 그대로의 형태로 저장되므로, 입력 시점과 동일한 형태로 데이터를 검색할 수 있습니다.\n\n- `JSON` 타입은 JSON 데이터를 분석하거나 조작할 때 마다 원본 텍스트를 해석해야 하므로, `JSONB`에 비해 연산이 느릴 수 있습니다.\n\n- `JSON` 형식은 원본 데이터의 형식을 보존하는 것이 중요할 때 유용합니다.\n\n## **JSONB**\n\n- `JSONB` 데이터 타입은 JSON 데이터를 바이너리 형식으로 저장합니다. 이는 데이터를 저장하기 전에 처리(분석 및 재구성)하여 저장 공간을 최적화하고, 검색 및 조작 속도를 향상시킵니다.\n\n- `JSONB`는 내부적으로 인덱싱이 가능하므로, 큰 데이터 세트에서 검색과 조작이 훨씬 빠릅니다.\n\n- 바이너리 형식으로 인해, 데이터를 저장할 때 순서가 변경되거나 중복 키가 제거될 수 있습니다. 따라서, 원본 데이터의 정확한 형식과 순서를 보존하지 않습니다.\n\n- `JSONB`는 연산자와 함수를 지원하여, 데이터에 대한 복잡한 쿼리를 수행할 수 있게 해줍니다.\n\n## 주요 차이점 요약 및 결론\n\n- **저장 방식:**  `JSON`은 텍스트 기반으로, `JSONB`는 바이너리 형식으로 데이터를 저장합니다.\n\n- **처리 속도:**  `JSONB`는 데이터 처리와 쿼리 수행이 더 빠릅니다, 특히 인덱스를 사용할 때.\n\n- **공간 효율:**  `JSONB`는 데이터를 최적화하여 저장하기 때문에, 같은 데이터를 `JSON`보다 적은 공간을 사용해 저장할 수 있습니다.\n\n- **기능성:**  `JSONB`는 `JSON`보다 더 많은 연산자와 함수를 지원합니다.\n\n사용 사례에 따라, 원본 JSON 데이터의 형식과 순서를 정확히 유지할 필요가 있다면 JSON을, 데이터에 대한 빠른 검색과 조작을 필요로 한다면 JSONB를 선택하는 것이 좋습니다.\n\n\n\n"},{"excerpt":"Geocoder와 Reverse Geocoder란? 간단하게 이야기해 주소로 위도와 경도를 얻거나(Geocoder) 위도와 경도로 주소를 얻는(Reverse Geocoder) 것을 의미합니다.  왜 필요할까? 주소로는 서로간의 거리를 계산하기 힘들기 떄문에 변환이 필요하고, 위도와 경도는 사람이 읽기 힘들기 때문에 다시 주소로 변환이 필요합니다. 이 때 …","fields":{"slug":"/Geocoder와-Reverse-Geocoder/"},"frontmatter":{"date":"February 27, 2024","title":"Geocoder와 Reverse Geocoder","tags":["ETC","Python"]},"rawMarkdownBody":"![](image1.png)\n## Geocoder와 Reverse Geocoder란? \n\n간단하게 이야기해 주소로 위도와 경도를 얻거나(Geocoder) 위도와 경도로 주소를 얻는(Reverse Geocoder) 것을 의미합니다. \n\n### 왜 필요할까? \n\n주소로는 서로간의 거리를 계산하기 힘들기 떄문에 변환이 필요하고, 위도와 경도는 사람이 읽기 힘들기 때문에 다시 주소로 변환이 필요합니다. 이 때 사용하는 기술입니다. \n\n## geopy\n\n파이썬에서 geopy를 사용하면 간단하게 Reverse Geocoder와 Geocoder를 구현해볼 수 있습니다. \n\n테스트에 앞서 우선 geopy를 설치해줍니다.\n\n```bash\npip install geopy\n```\n\n### Geocoder 예제\n\n```bash\nfrom geopy.geocoders import Nominatim\n\n# Nominatim 서비스 초기화 (user_agent 설정 필요)\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n\n# 변환하고자 하는 주소\naddress = \"서울특별시 중구 세종대로 110\"\n\n# 주소를 위도와 경도로 변환\nlocation = geolocator.geocode(address)\n\nif location:\n    print((location.latitude, location.longitude))\nelse:\n    print(\"좌표를 찾을 수 없습니다.\")\n\n```\n\n위의 코드를 실행하면 해당 주소에 해당하는 좌표(위도와 경도) 값을 얻을 수 있습니다. \n\n### Reverse Geocoder 예제\n\n```python\nfrom geopy.geocoders import Nominatim\n\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n# 예시 좌표, language 인자를 reverse 메소드 호출 시 사용\nlocation = geolocator.reverse(\"37.541578, 126.840436\", exactly_one=True, language=\"ko\")\n\nif location:\n    print(location.address)\nelse:\n    print(\"주소를 찾을 수 없습니다. \")\n\n```\n\n변환된 주소를 얻을 수 있습니다. \n\n```bash\n강서로, 화곡1동, 강서구, 서울특별시, 07726, 대한민국\n```\n\nreerse method에 language를 함께 넣어주면 한글 주소를 얻을 수 있습니다. \n\n### 이외에는?\n\n`geopy`는 지오코딩과 역 지오코딩 외에도 다양한 기능을 제공합니다:\n\n1. 거리 계산: `geopy.distance`를 사용하면 두 지점(위도와 경도 좌표) 사이의 거리를 계산할 수 있습니다. 여러 가지 거리 계산 방법(예: Vincenty, Great Circle)을 제공합니다.\n\n1. 다양한 지오코더 지원: `Nominatim` 외에도 `GoogleV3`, `Bing`, `OpenCage`, `ArcGIS` 등 다양한 지오코딩 서비스를 지원합니다. 각 서비스마다 특징과 사용 방법이 다를 수 있습니다.\n\n1. 유연한 API 사용: `geopy`는 사용자가 다양한 외부 지오코딩 서비스에 쉽게 접근할 수 있도록 돕습니다. 이를 통해 개발자는 특정 서비스의 API 제한이나 사용 제한에 구애받지 않고 서비스를 선택할 수 있습니다.\n\n### Nominatim?\n\n위의 예시에서는 `Nominatim` 을 사용했습니다. `Nominatim`은 OpenStreetMap 데이터를 기반으로 하는 지리적 위치 조회 서비스로, 공개된 무료 API 이지만 공정한 사용 정책이 걸려있어 실제 서비스에서 사용하기에는 무리가 있습니다. \n\n## 그 밖에 다른 서비스는? \n\n그렇다면 상용화를 위해서는 어떤 서비스를 이용해야 할까요? 간략하게 찾아서 정리해봤습니다. 국내에서는 크게 네이버/카카오/구글을 사용할 수 있습니다. \n\n### Naver Map API\n\n- 월 300만건 무료, REST API 지원, 300만개 초과시 건당 0.5원\n\n- [https://www.ncloud.com/product/applicationService/maps](https://www.ncloud.com/product/applicationService/maps)\n\n- [https://guide.ncloud-docs.com/docs/maps-reversegeocoding-api](https://guide.ncloud-docs.com/docs/maps-reversegeocoding-api)\n\n### Kakao Map API\n\n- 월 300만건, 하루 30만건, 초과 시 협의 필요\n\n- [https://developers.kakao.com/docs/latest/ko/getting-started/quota](https://developers.kakao.com/docs/latest/ko/getting-started/quota)\n\n- [https://apis.map.kakao.com/web/documentation/#services\\_Geocoder](https://apis.map.kakao.com/web/documentation/#services_Geocoder)\n\n### Google Maps Platform\n\n- 10만회 이하 : 1000회당 5달러\n\n- 10만회 ~ 50만회 : 1000회당 4달러\n\n- 50만회 이상 : 협의\n\n- \n\n- [월간 200달러의 무료 크레딧을 제공합니다.](https://mapsplatform.google.com/pricing/?hl=ko) \n\n[https://mapsplatform.google.com/pricing/?hl=ko](https://mapsplatform.google.com/pricing/?hl=ko)\n\n[https://developers.google.com/maps/documentation/geocoding/usage-and-billing?hl=ko](https://developers.google.com/maps/documentation/geocoding/usage-and-billing?hl=ko)\n\n\n\n"},{"excerpt":"서론 새로운 프로젝트의 백엔드 언어가 파이썬으로 확정되었습니다. 파이썬 버전은 추후 머신러닝 사용을 감안하여 현재 라이브러리와 가장 호환성이 좋을 것으로 생각되는 3.9버전을 사용하려고 합니다. 다만, 파이썬 3.9는 그다지 최신 버전은 아닙니다. 최근에 타입스크립트를 만지면서 nvm을 좋게 사용한 경험도 있고, 이번에 내친김에 파이썬에서도 버전관리를 해…","fields":{"slug":"/Apple-Silicon과-pyenv/"},"frontmatter":{"date":"February 26, 2024","title":"Apple Silicon과 pyenv","tags":["Python","Homebrew","Pyenv"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n새로운 프로젝트의 백엔드 언어가 파이썬으로 확정되었습니다. 파이썬 버전은 추후 머신러닝 사용을 감안하여 현재 라이브러리와 가장 호환성이 좋을 것으로 생각되는 3.9버전을 사용하려고 합니다. 다만, 파이썬 3.9는 그다지 최신 버전은 아닙니다. 최근에 타입스크립트를 만지면서 nvm을 좋게 사용한 경험도 있고, 이번에 내친김에 파이썬에서도 버전관리를 해주는 툴인 pyenv를 사용해서 환경을 꾸며보기로 했습니다. \n\n## 설치\n\n설치는 아주 간단합니다. homebrew는 미리 설치되어 있어야 합니다. \n\n```bash\nbrew install pyenv\n```\n\npyenv를 설치해주고,\n\n```bash\npyenv install --list | grep 3.9\n```\n\n이 명령어를 사용하면 현재 설치 가능한 파이썬 3.9버전의 리스트를 확인할 수 있습니다. 현재 3.9의 최신 버전은 3.9.18로 보입니다. 해당 버전을 설치해줍니다. \n\n```bash\npyenv install 3.9.18\n```\n\n간단하게 파이썬이 설치.. 되지 않았습니다! 오류가 납니다. \n\n![](image2.png)\n## 해결\n\n챗지피티한테 물어보거나 검색을 해보는 등 여러가지 방법을 써보면 많은 방법이 나옵니다. zshrc에 여러 명령어를 export 해주거나 특정 라이브러리를 설치해주거나.. 또는 파이썬 버전을 바꾸던가 \n\n여러가지 방법을 시도했지만 잘 되지 않았습니다. 여전히 같은 오류가 발생합니다. \n\n문제는 홈브루에 있었습니다. \n\n### Homebrew 버전 확인 및 최신 버전 설치\n\n#### Homebrew 버전 확인\n\n아래 코드로 내 homebrew를 확인할 수 있습니다. \n\n```bash\nwhich brew\n```\n\n이 코드를 실행했을 때에,\n\n```bash\n/usr/local/bin/brew\n```\n\n가 출력된다면 구버전의 Homebrew가 설치되어 있는 것입니다. \n\n#### Homebrew 구버전 삭제\n\n다음의 명령어로 Homebrew를 삭제할 수 있습니다. \n\n```bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/uninstall.sh)\"\n```\n\n해당 코드를 실행한 뒤에도 `which brew` 를 실행해보면 여전히 위치가 나옵니다. 이 경우에는 삭제 코드를 다시 실행해주면 됩니다. 정상적으로 삭제 된다면 `command not found` 가 발생합니다. \n\n#### Homebrew 최신 버전 설치\n\n아래의 명령어로 최신 버전을 설치할 수 있습니다. \n\n```bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\n설치가 완료되면 다음과 같은 문구가 나타납니다. 버전에 따라 다를 수 있습니다. \n\n```bash\n==> Installation successful!\n\n==> Homebrew has enabled anonymous aggregate formulae and cask analytics.\nRead the analytics documentation (and how to opt-out) here:\n  https://docs.brew.sh/Analytics\nNo analytics data has been sent yet (nor will any be during this install run).\n\n==> Homebrew is run entirely by unpaid volunteers. Please consider donating:\n  https://github.com/Homebrew/brew#donations\n\n==> Next steps:\n- Run these two commands in your terminal to add Homebrew to your PATH:\n    (echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') >> /Users/UserName/.zprofile\n    eval \"$(/opt/homebrew/bin/brew shellenv)\"\n- Run brew help to get started\n- Further documentation:\n    https://docs.brew.sh\n\n```\n\nNext steps를 보니, 다음의 명령어를 실행하라고 합니다. 위는 제 예시로, 터미널의 명령어를 복사하면 됩니다. 줄의 명령어를 실행하고 `which brew` 를 다시 실행해보면\n\n```bash\n/opt/homebrew/bin/brew\n```\n\n이렇게 표시되면 최신 버전의 brew가 설치된 것입니다. \n\n### 다시 pyenv, python 설치\n\n다시 위로 돌아가서, pyenv과 python을 설치해주면 이번에는 오류가 발생하지 않습니다. 해~결~\n\n## pyenv 설치 후\n\n설치된 파이썬 버전 리스트를 확인할 수 있습니다. \n\n```bash\npyenv versions\n```\n\n현재 사용중인 파이썬 버전을 확인할 수 있습니다. \n\n```bash\npyenv version\n```\n\n사용중인 파이썬을 설치한 버전으로 바꿔줍니다. \n\n```bash\npyenv global 3.9.18\n```\n\n현재 디렉토리에서만 사용할 파이썬 버전을 지정할 수 있습니다.\n\n```bash\npyenv local 3.9.18\n```\n\n쉘에만 적용할수도 있습니다. \n\n```bash\npyenv shell 3.9.9\n```\n\n## 참고\n\n[https://www.lainyzine.com/ko/article/how-to-install-homebrew-for-m1-apple-silicon/](https://www.lainyzine.com/ko/article/how-to-install-homebrew-for-m1-apple-silicon/)\n\n"},{"excerpt":"서론 FastAPI-Python을 새로운 프로젝트의 백엔드 언어로 선정하였습니다. 프로젝트 초기화 과정을 기록합니다.  poetry 환경 구성은 이미 끝났다고 가정하고, 해당 라이브러리를 활용하여 환경을 꾸미겠습니다.  필수라고 생각되는 라이브러리 설치 FastAPI FastAPI를 웹 프레임워크로 선정하였으니, 이를 먼저 설치해줍니다.  Uvicorn …","fields":{"slug":"/FastAPI-프로젝트의-시작/"},"frontmatter":{"date":"February 26, 2024","title":"FastAPI 프로젝트의 시작","tags":["FastAPI","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nFastAPI-Python을 새로운 프로젝트의 백엔드 언어로 선정하였습니다. 프로젝트 초기화 과정을 기록합니다. \n\n[poetry](https://sharknia.github.io/Poetry) 환경 구성은 이미 끝났다고 가정하고, 해당 라이브러리를 활용하여 환경을 꾸미겠습니다. \n\n## 필수라고 생각되는 라이브러리 설치\n\n### FastAPI\n\nFastAPI를 웹 프레임워크로 선정하였으니, 이를 먼저 설치해줍니다. \n\n```bash\npoetry add fastapi\n```\n\n### Uvicorn\n\nASGI 서버로 FastAPI 애플리케이션을 호스팅하는 데 사용됩니다. FastAPI 공식 문서에서도 Uvicorn을 사용하는 것을 권장하고 있습니다.\n\n```bash\npoetry add uvicorn\n```\n\n### SQLAlchemy 2.0\n\n이미 직전 회사에서 많은 일을 함께 겪었던 ~~(^^)~~ 든든한 동지네요. 이번에도 2.0을 설치는 하지만, 비동기 엔진은 절대 사용하지 않을 예정입니다. 아무튼 설치해줍니다. \n\n```bash\npoetry add sqlalchemy@^2.0\n```\n\n### Alembic\n\nSQLAlchemy를 위한 데이터베이스 마이그레이션 도구입니다. 데이터베이스 스키마 변경을 버전 관리할 수 있게 해줍니다. 지난 회사에서는 이 부분을 적용하지 못했었는데, 이번에는 잘 해보려고 합니다. \n\n```bash\npoetry add alembic\n```\n\n### HTTPX\n\n비동기 HTTP 클라이언트 라이브러리입니다. FastAPI 애플리케이션 내부에서 비동기적으로 외부 API 호출을 수행할 때 사용할 수 있습니다.\n\n```bash\npoetry add httpx\n```\n\n### Passlib & python-jose\n\n보안 및 인증을 위한 라이브러리입니다. Passlib는 비밀번호 해싱을, python-jose는 JWT(JSON Web Tokens) 생성 및 검증을 위해 사용됩니다. 기본적으로 JWT 토큰을 사용하게 될 것 같으므로 설치해줍니다. Zsh가 괄호 `[]`를 특별한 패턴으로 해석하기 때문에 위와는 설치 명령어가 조금 다릅니다. \n\n```bash\npoetry add 'passlib[bcrypt]' 'python-jose[cryptography]'\n```\n\n## 테스트 코드를 위한 라이브러리 설치\n\n이번에도 프로젝트 작업시간이 굉장히 빡빡한데, 가능하면 TDD를 지켜보고 싶습니다. 가능할지 모르겠지만, 일단 라이브러리 설치는 해두려고 합니다. 해당 라이브러리들은 실서버 환경에서는 필요하지 않으므로 `--dev` 옵션을 붙여주겠습니다. \n\n### pytest\n\nPython에서 가장 널리 사용되는 테스트 프레임워크 중 하나로, 강력한 기능과 플러그인 시스템을 제공합니다. \n\n```bash\npoetry add --dev pytest\n```\n\n### pytest-asyncio\n\n비동기 코드를 위한 pytest 플러그인으로, asyncio를 사용하는 비동기 함수를 테스트할 수 있게 해줍니다. FastAPI는 비동기 프레임워크이므로, 이 플러그인은 비동기 요청 및 비동기 데이터베이스 작업을 테스트하는 데 유용합니다.\n\n```bash\npoetry add --dev pytest-asyncio\n```\n\n### pytest-cov\n\n코드 커버리지를 측정하기 위한 pytest 플러그인입니다. 테스트가 얼마나 많은 코드를 실행하는지를 분석하고, 테스트가 누락된 영역을 식별하는 데 도움을 줍니다.\n\n```bash\npoetry add --dev pytest-cov\n```\n\n\n\n"},{"excerpt":"서론 작업을 하다보면 항상 뭔가 찾게 됩니다. 좀 더 편한 것, 좀 덜 귀찮은 것, 좀 더 새로운 것 등등..  항상 새로운 것에서 자극을 많이 받는 것 같습니다. 매일 하던 것만 하면 재미 없잖아요.  접한지는 조금 되었지만 사용하지 않고 있었는데, 이제 새롭게 사용해보려고 하는 건 arc 브라우저 입니다.  업무용으로는 크롬을 사용하고, 개인용 윈도우…","fields":{"slug":"/arc-browser/"},"frontmatter":{"date":"February 22, 2024","title":"arc browser","tags":["ETC"]},"rawMarkdownBody":"## 서론\n\n작업을 하다보면 항상 뭔가 찾게 됩니다. 좀 더 편한 것, 좀 덜 귀찮은 것, 좀 더 새로운 것 등등.. \n\n항상 새로운 것에서 자극을 많이 받는 것 같습니다. 매일 하던 것만 하면 재미 없잖아요. \n\n접한지는 조금 되었지만 사용하지 않고 있었는데, 이제 새롭게 사용해보려고 하는 건 arc 브라우저 입니다. \n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n저는 지금까지는 개인적인 용도로는 사파리를 주로 사용하고 있었습니다. 아이폰-아이패드-맥을 모두 사용하고 있기 때문이기도 하고 딱~히 불편함을 못느끼고 통합성이 너무 좋아서 불편 없이 사용하고 있었습니다. \n\n업무용으로는 크롬을 사용하고, 개인용 윈도우 데스크탑에서는 엣지를 사용하고 있었는데 왜 그렇냐? 하면 사실 딱히 이유는 없습니다.. 다만 업무용 환경과 개인용 환경이 합쳐지는 걸 좋아하지 않아 구분해서 사용하고 있었다, 정도겠네요. \n\n## 그런데 왜 arc 브라우저를 갑자기? \n\n앞에서 언급한대로 새로운게 좋기 때문입니다. 이번에 또 이직을 하게 되면서 환경이 확 바뀌게 되었는데, 뭔가 같이 변화를 가져가면 좋잖아요. \n\n주변 개발자들이 많이 사용하고 있었기도 합니다. 추천도 받았구요. 자세한 장점은 좀 더 써보면 정리할 수 있지 않을까 합니다. \n\n### 첫인상?\n\n일단 디자인이 유려합니다. 주변에서 추천해주신 분들 중에서는 arc 브라우저를 사용하다가 크롬이나 엣지를 보면 촌스러워 보인다고 말씀해주신 분도 있습니다. ~~(사실 엣지는 요즘.. 코파일럿이 지저분하게 붙었다는 느낌이어서 번잡하다는 느낌이 들긴 합니다.)~~ \n\n## 어떤 기능들을 사용하나요? \n\n### Little Arc\n\n외부 어플리케이션에서 링크를 클릭하여 브라우저를 여는 경우가 있습니다. 독특하게도 이 때 아크 브라우저에서는 해당 링크가 Little Arc 창에서 열립니다. \n\n다른 열려있는 탭들에 영향을 주지 않고 모달처럼 더 작은 브라우저 창에서 새로운 링크가 열리게 됩니다. 따라서 산만하지 않게 원래의 작업을 유지할 수 있습니다. 물론 메인 탭에 리틀 아크에서 열린 탭을 포함시킬 수도 있습니다. \n\n외부 링크가 아니고 직접 실행할 수도 있는데, 브라우저 상에서 뿐만 아니라 시스템 전역 어디에서든 `Cmd` + `Option` + `N` **** 단축키를 통해 리틀 아크를 실행할 수 있습니다. \n\n### Space\n\nArc 브라우저에서는 Space라는 분리된 공간을 제공합니다. 이는 사파리의 신규 기능인 프로필 기능과 비슷하지만, 더 유려한 UI와 편의성을 제공합니다. \n\n사용자는 Space를 통해 작업 탭과 개인 또는 프로젝트 별로 관련된 탭을 그룹화 하여 분리할 수 있습니다. 각 Space는 자체적인 탭, 테마, 아이콘 등을 가지며 그러면서도 즐겨찾기는 모든 Space에서 공유되어 편리하게 이용할 수 있습니다. \n\n### 이외에도..\n\n이외에도 많은 독특한 기능을 제공합니다. 개인적으로는 Split 기능도 열심히 사용을 해보려고 합니다. \n\n개인적으로는 한글이 지원이 아직 안된다는 점, 윈도우에서 지원이 아직 제공되지 않는다는 점이 아쉽습니다. \n\n자세한 내용은 더욱 써보고 정리해보려고 합니다. 해당 페이지에서는 개인 확인 용도로 기록을 늘려가려고 합니다. \n\n## 같이 보기\n\n[https://www.hongkiat.com/blog/arc-browser-keyboard-shortcuts/](https://www.hongkiat.com/blog/arc-browser-keyboard-shortcuts/)\n\n"},{"excerpt":"서론 새로 개발할 서비스의 DB 설계 작업이 시작되었습니다. DB를 설계하는 것은 많이 해본 일이어서 저도 모르게 무결성이나 관계성 등 중요한 내용을 챙기고 있지만, 이런것들은 말로 설명하라고 하면 어려울 때도 많습니다.  그래서 이번 기회에 해당 개념들에 대해서 기회가 날때마다 정리하는 습관을 들이려고 합니다.  DB가 문서도 없고, 관계성도 정의되어 …","fields":{"slug":"/식별-관계와-비식별-관계-그리고-CASCADE-옵션의-이해/"},"frontmatter":{"date":"February 18, 2024","title":"식별 관계와 비식별 관계, 그리고 CASCADE 옵션의 이해","tags":["DataBase","DataBase Design"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n새로 개발할 서비스의 DB 설계 작업이 시작되었습니다. DB를 설계하는 것은 많이 해본 일이어서 저도 모르게 무결성이나 관계성 등 중요한 내용을 챙기고 있지만, 이런것들은 말로 설명하라고 하면 어려울 때도 많습니다. \n\n그래서 이번 기회에 해당 개념들에 대해서 기회가 날때마다 정리하는 습관을 들이려고 합니다. \n\n### DB가 문서도 없고, 관계성도 정의되어 있지 않다면?\n\n전에 있던 회사에서는 정규화는 고사하고 아예  테이블들의 관계를 설정하지 않았었고, 관계성의 정의에 대한 문서화도 되어있지않아 어느 테이블의 어느 컬럼이 다른 어느 테이블과의 관계를 가지는지를 확인하기 위해서는 “감”을 사용하거나 코드를 읽고 파악해야 해 그야말로 전쟁터를 방불케했습니다. 무언가 하나를 수정할 경우 데이터 무결성이 깨지고 오류가 발생하는 것을 막는 일은 전쟁터에서 많아도, 신경을 곤두세워도 피하기 쉽지 않았습니다. \n\n어떻게 생각하면 이렇게 문서도 없고 관계성도 정의되지 않은데다가 하나의 프로젝트를 여러 개발자가 손 댔을 경우 얼마나 유지보수가 어려워지는지, 개발 생산성이 얼마나 떨어지는지 깨닫게 되는 좋은 경험을 했구나, 하는 생각도 합니다. 그런 아비규환을 겪기 전에는 무결성이나 관계성 정의가 중요하다고 공부를 해도 와닿지 않았었거든요. ~~(역시 사람은 맞아봐야 깨닫습니다)~~  \n\n특히 이번 프로젝트에서는 개인적으로 많은 목적을 담고 있기 때문에 저도 함께 등한시 했던 작업들에 더 주의를 기울이려고 합니다. 오늘은 체계적인 문서화를 위해 DB 설계를 [ERD-Cloud](https://www.erdcloud.com/)로 진행했고, 그 과정에서 새롭게 정리된 개념들에 대해 정리하고 넘어가겠습니다. \n\n## 관계 유형\n\n먼저 관계 유형에 대해 간단하게 정리하고 넘어가겠습니다. 관계 유형은 테이블 간의 데이터가 어떻게 연결되는지를 정의합니다. 기본적으로 세 가지 관계 유형이 있습니다. \n\n### 1:1 관계\n\n이 관계는 한 테이블의 레코드가 다른 테이블의 단 하나의 레코드와만 관련될 때 발생합니다. 예를 들어 사용자 테이블과 사용자 상세 정보 테이블 간의 관계를 생각하면 쉽습니다. 상세 정보는 한 유저 당 하나만 존재합니다. \n\n종종 데이터를 논리적으로 분리하거나, 특정 정보에 대한 보안 요구 사항이 다를 때 사용됩니다. \n\n### 1:N 관계\n\n가장 흔한 관계유형입니다. 외래키를 사용하여 부모 테이블과 자식 테이블 간의 연결을 구현합니다. 한 테이블의 레코드가 다른 테이블의 여러 레코드와 관련될 수 있습니다. 예를 들면 사용자 테이블과 주문 리스트 테이블을 생각하면 쉽습니다. 한 사용자가 주문을 여러번 할 수 있기 때문에, 유저 테이블에는 레코드가 하나지만 주문 테이블에는 여러 레코드가 존재할 수 있습니다. \n\n### N:N 관계\n\n두 테이블 간에 서로 다수의 레코드가 관련될 수 있습니다. 이 관계는 직접적으로 데이터베이스에 표현될 수 없으며, 일반적으로 두 테이블간의 다대다 관계를 1:N으로 분해하는 연결 테이블을 사용하여 구현됩니다. 예를 들어, 학생 테이블과 수업 간의 관계에서 한 학생이 여러 수업을 듣고 하나의 수업에 여러 학생이 등록될 수 있습니다. \n\n## 식별 관계와 비식별 관계\n\n식별 관계와 비식별 관계는 데이터베이스 설계에서 테이블 간의 연결 방식을 정의하는 두 가지 방법입니다. \n\n### 식별 관계(Identifying Relationship)\n\n이 관계에서는 자식 테이블의 기본키에 부모 테이블의 기본키가 포함됩니다. 이는 자식 테이블의 레코드가 부모 테이블의 레코드 없이는 존재할 수 없음을 의미합니다. \n\n이 관계는 강한 의존성이 있는 테이블 간에 사용됩니다. 예를 들어 자식 테이블이 부모 테이블의 생명주기에 밀접하게 결합되어 있을 때 적합합니다. \n\n### 비식별 관계 (Non-Identifying Relationship)\n\n비식별 관계에서는 자식 테이블이 부모 테이블을 참조하는 외래키를 가지지만 이 외래키가 자식 테이블의 기본 키에는 속하지 않습니다. 자식 테이블의 레코드는 부모 테이블의 레코드와 독립적으로 존재할 수 있습니다. 예를 들어, 직원 테이블에서 부서 테이블을 참조하는 경우 직원은 부서에 속하지 않더라도 독립적으로 존재할 수 있습니다. \n\n즉 테이블간 서로 관련은 있지만, 서로의 존재에 필수적이지 않을 때 적합합니다. \n\n### 무조건 식별관계로 묶어두는게 정답일까?\n\n식별 관계는 테이블간의 강한 연결과 무결성을 보장하는 데 유용합니다. 반면 비식별 관계는 데이터 모델의 확장성과 유연성을 높이는 데 도움이 됩니다. \n\n그냥 단순하게 생각하면 반드시 가능할때마다 식별관계로 묶어두는게 좋지 않을까? 라고 쉽게 생각이 들기도 합니다. 하지만 식별 관계를 과도하게 사용하면 여러가지 문제가 발생할 수 있습니다. \n\n#### 1. 유연성 감소\n\n식별 관계는 자식 테이블이 부모 테이블의 존재에 종속되는 것이나 똑같습니다. 이렇게 만들어진 강한 결합도는 요구 사항이 변경될 때 테이블간의 관계를 수정하기 어렵게 만듭니다. \n\n만약 부모 테이블의 기본 키 구조가 변경되면 이를 참조하는 모든 자식 테이블의 구조가 함께 변경되어야 합니다. 이는 유지보수에 문제를 초래할 수 있습니다. \n\n#### 2. 확장성 문제\n\n식별 관계는 데이터 모델을 특정 구조에 굳건히 묶어 놓음으로써 시스템의 확장성을 제한할 수 있습니다. 특히 대규모 시스템에서는 요구 사항이 시간에 따라 변할 수 있으므로 유연하게 대응할 수 있는 비식별 관계가 더 유리할 수 있습니다. \n\n#### 3. 데이터 모델의 복잡성 증가\n\n과도하게 증가한 복잡성은 데이터 모델을 이해하고 관리하는 데 더 많은 노력이 필요함을 의미합니다. \n\n### 그렇다면 싹 다 비식별 관계로 묶으면 되지 않을까?\n\n식별 관계를 사용했을 때 발생하는 문제점들은 유연함과 단순함이 너무 없어 어지럽기도 합니다. 그렇다면 비식별 관계만으로 정의를 하면 이 모든 문제들이 해결될까요? 물론 비식별 관계만을 고집하면 발생할 수 있는 문제점들도 있습니다. \n\n#### 1. 데이터 무결성의 약화, 데이터 관리의 어려움\n\n비식별 관계는 자식 테이블이 부모 테이블에 느슨하게 연결되어 있으므로 데이터의 무결성을 유지하기 위한 추가적인 제약 조건이나 규칙을 구현해야 할 수 있습니다. 즉, 데이터의 일관성과 정확성을 보장하는 데 있어 추가적인 노력이 필요할 수도 있음을 의미합니다. \n\n이는 데이터 관리의 어려움으로 이어집니다. 예를 들어 부모 테이블이 삭제될 때 자식 테이블에 대한 처리를 비식별관계에서는 수동으로 관리해야 할 수 있습니다. \n\n#### 2. 관계의 명확성 감소\n\n테이블간의 관계가 덜 명확해지면, 복잡한 데이터 모델에서 테이블간의 관계를 이해하고 추적하기 어렵게 만들 수 있습니다. \n\n#### 3. 과도한 유연성의 함정\n\n과도한 유연성은 결국 모델의 명확성을 흐리게 하고 미래의 확장성에 대한 고려 없이 단기적인 해결에 치중하게 만들 수 있습니다. 대충 지어진 모래성이 무너지는 느낌이 이런게 아닐까요? 큰 집일수록 기초가 튼튼해야 함을 전 회사에서 느꼈습니다. \n\n### 결론\n\n데이터베이스 설계에 있어서 식별 관계와 비식별 관계를 적절히 혼합하여 사용하는 것이 중요합니다. 각각의 관계 유형이 가진 장단점을 이해하고 프로젝트의 요구 사항과 목표에 따라 최적의 접근 방식을 선택해야 합니다. 따라서, 단일한 접근 방식에만 의존하기보다는 유연하게 접근하여, 각 상황에 가장 적합한 데이터 모델링 전략을 개발하는 것이 필요합니다.\n\n## CASCADE\n\nCASCADE는 데이터베이스 관리 시스템에서 외래 키 제약 조건의 일환으로 사용할 수 있는 옵션입니다. 이 옵션은 부모 테이블의 레코드가 업데이트 되거나 삭제될 때 관련된 자식 테이블의 레코드에 대해 자동으로 같은 작업을 수행하도록 설정합니다. \n\n### CASCADE의 주요 기능\n\n#### CASCADE DELETE\n\n부모 테이블의 레코드가 삭제될 경우 해당 레코드와 연결된 자식 테이블의 레코드도 자동으로 삭제됩니다. 이는 참조 무결성을 유지합니다. \n\n#### CASCADE UPDATE\n\n부모 테이블의 기본 키 값이 변경될 경우 해당 변경이 자식 테이블의 관련 외래키 값에도 자동으로 반영됩니다. \n\n### 비식별 관계에서의 CASCADE 사용\n\nCASCADE 설정은 식별/비식별 관련없이 모두 적용할 수 있습니다. 따라서 이 설정을 활용해 위에서 살펴봤던 비식별 관계의 약점을 보완할 수 있습니다. \n\n#### 비식별 관계의 약점 보완\n\n비식별 관계에서 CASCADE를 사용하면 참조하는 부모 테이블이 변경될 때 자식 테이블의 데이터를 자동으로 갱신하여 데이터를 수동으로 관리하는 번거로움을 줄일 수 있으며, 일관성을 유지할 수 있습니다. \n\n#### 반드시 CASCADE + 비식별이 식별보다 유리하지 않을까? \n\n이 방법을 사용하면 유연성, 무결성 유지, 고아 레코드 방지 등에 있어서 비식별 관계의 약점을 모두 보완하는 것처럼 보이기도 합니다. 따라서 단순히 생각하면 식별 관계를 사용하는 것보다 우월하다고 생각될 수 있습니다. 하지만 데이터 모델의 명확성과 이해도를 유지하는 것 역시 중요한 고려 사항입니다. 따라서, 식별 관계와 비식별 관계의 선택은 각각의 상황에 따라 그 장단점을 잘 이해하고, 프로젝트의 목표와 요구 사항에 가장 잘 맞는 방식을 선택하는 것이 필요합니다.\n\n### CASCADE 사용시 고려 사항\n\nCASCADE도 만능은 아닙니다. 사용시 다음과 같은 점들을 고려해야 합니다. \n\n#### 데이터 수정/삭제에서 오는 부작용\n\nCASCADE 옵션은 데이터베이스의 무결성을 유지하는 데 유용하지만 데이터를 삭제하거나 업데이트 할 때 예상치 못한 부작용이 발생할 수 있으므로 주의가 필요합니다. \n\n항상 데이터모델과 비즈니스 로직을 신중하게 고려해야 합니다. 무분별한 사용은 데이터의 실수를 일으킬 수 있습니다. \n\n#### 성능 영향\n\n대량의 데이터가 관련되어 있는 경우 CASCADE 동작은 데이터베이스의 성능에 영향을 줄 수 있습니다.\n\n## 개인적인 오늘의 결론\n\n첫 번째 회사에 오래 있었고, 두 번째 회사에서 일한지는 얼마 되지 않았지만 첫번째 회사에서 “엉망으로 일할 때 발생하는 문제” 들을 몸으로 겪었다면, 두번쨰 회사에서는 그것들을 해결하는 방법들에 대해 고민하는 시간을 많이 가졌던 것 같습니다. \n\n오늘 살펴본 것도 몸으로는 아는 내용들이었고 자연스럽게 구현하고 중요하게 여기던 내용들입니다. 이런것들을 몰라도 된다고 무심결에 무시하던 때도 있었는데, 이런 내용들을 정리된 이론으로 머릿속에 다시 집어넣는게 중요하다고 요즘에 크게 느낍니다. \n\n남한테 설명할 때에도 그렇고 결국 내가 꺠달은 것들을 미리 깨달은 사람들이 잘 정리해 둔 것들을 굳이 피해가는 것도 멍청한 일이라는 생각이 듭니다.\n\n다만, 아예 백지 상태에서 배우는 것보다는 (너무 많은 시간을 고생했지만) 잘못된 반례들을 겪고 공부하니 훨씬 더 잘 다가오는 감이 있습니다. \n\n앞으로도 여태까지와는 다른 방향으로 더 성장하는 개발자가 되려고 합니다. \n\n"},{"excerpt":"서론 현재 개발을 준비중인 서비스에는 인앱결제가 포함되어있습니다. 여태까지 운이 좋았던 것인지, 운이 나빴던 것인지 아무튼 인앱결제를 구현한 적이 없어서 구현 전에 역시 미리 서치해두려고 합니다.  개발 예정 서비스에서는 내부 재화를 인앱결제로 구매하고 기타 아이템들은 내부 재화로 구매할 수 있게 만들 예정입니다. 내부 재화 구매 프로세스 구매는 결국 어…","fields":{"slug":"/인앱결제에서-백엔드는-무엇을-준비해야-할까/"},"frontmatter":{"date":"February 17, 2024","title":"인앱결제에서 백엔드는 무엇을 준비해야 할까?","tags":["ETC"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n현재 개발을 준비중인 서비스에는 인앱결제가 포함되어있습니다. 여태까지 운이 좋았던 것인지, 운이 나빴던 것인지 아무튼 인앱결제를 구현한 적이 없어서 구현 전에 역시 미리 서치해두려고 합니다. \n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n이 과정은 크게 각 플랫폼별 인앱 결제 API 연동, 결제 요청 및 검증, 결제 성공 시 내부 재화 충전, 그리고 결제 실패 및 취소 처리로 나눌 수 있습니다.\n\n개발 예정 서비스에서는 내부 재화를 인앱결제로 구매하고 기타 아이템들은 내부 재화로 구매할 수 있게 만들 예정입니다.\n\n## 내부 재화 구매 프로세스\n\n구매는 결국 어떻게 이뤄질까? 앱 사용자가 상품 구매를 한다고 하고 어떤 흐름으로 결제가 이뤄져 어떻게 사용자가 내부 재화를 가지게 되는지를 먼저 그려보겠습니다. \n\n### 1. 사용자가 앱에서 내부 재화 구매를 선택\n\n#### FE\n\n사용자가 앱을 열고, 상점 페이지로 이동해 내부 재화를 구매하고자 하는 상품을 선택합니다. \n\n앱은 사용자가 선택한 상품의 정보를 보여주고 구매 버튼을 노출합니다. \n\n#### BE\n\n백엔드는 상품 정보를 제공합니다. \n\n### 2. 사용자가 구매 버튼을 누름\n\n#### FE\n\n구매 버튼이 눌러지면 앱은 해당 플랫폼의 인앱 결제 프로세스를 시작합니다. (iOS : StoreKit, Android : Google Play Billing Libarary)\n\n사용자는 구매 확인, 결제 방법 선택(등록된 결제 수단 사용 등) 등의 단계를 수행합니다. \n\n### 3. 결제 프로세스 진행\n\n#### FE\n\n사용자는 결제 과정을 진행하며 이 과정은 앱스토어세서 직접 관리됩니다. \n\n과정이 종료된 후, 결제 성공/실패/취소 등의 결과가 어플리케이션으로 전달됩니다.\n\n#### BE\n\n결제가 성공한 경우, 어플리케이션에서 결제 성공 정보와 결제 영수증을 백엔드로 전송합니다. \n\n백엔드는 영수증 정보를 플랫폼의 서버(애플 혹은 구글)에 검증 요청을 합니다. \n\n검증이 통과하면 백엔드는 사용자의 계정에 해당하는 내부 재화를 충전하고 충전 결과를 앱에 알립니다. \n\n### 4 구매 완료 및 내부 재화 충전\n\n#### FE\n\n백엔드로부터 내부 재화 충전 성공 응답을 받으면 앱은 사용자 인터페이스를 업데이트하여 새로운 재화 잔액을 표시합니다. \n\n사용자에게 구매 성공 알림을 제공할수도 있습니다. \n\n#### BE\n\n내부 재화 충전과 관련된 모든 정보를 데이터베이스에 기록합니다. \n\n구매 내역을 로깅하여 추후 문제 해결이나 사용자 문의에 대응하도록 할 수도 있습니다. \n\n## 어플리케이션 안에서의 환불 요청 프로세스\n\n환불에도 대응을 해야 할 것이므로, 환불에 관해서도 프로세스를 미리 서치했습니다. \n\n### 1. 사용자가 환불을 요청\n\n#### FE\n\n사용자는 앱 내에서 환불을 요청할 수 있는 UI를 통해 환불을 신청합니다. 앱은 사용자의 환불 요청 정보를 백엔드로 전송합니다. \n\n### 2. 환불 요청 검증\n\n#### BE\n\n사용자의 구매 내역과 환불 요청을 검증합니다. 구매 영수증, 환불 정책 준수 등을 검증합니다. \n\n환불 가능 여부를 결정합니다. \n\n### 3.  환불 처리\n\n#### BE\n\n환불이 가능할 경우, 백엔드는 환불 처리를 진행합니다. 내부 재화를 차감합니다. \n\n이후 실제 결제가 이루어진 플랫폼(애플, 구글 등)을 통해 환불을 진행해야 합니다. 결제 플랫폼에 환불 요청을 전송합니다. \n\n### 4. 환불 결과 통지\n\n#### FE\n\n백엔드로부터 환불 처리 결과를 전송받습니다. 사용자에게 이를 통지합니다. \n\n## 앱스토어에서 직접 환불 처리 시 프로세스\n\n환불에는 앱 내에서 진행하는 경우와 플랫폼을 통해 환불이 진행되는 경우 두 가지가 있습니다. 위에서 알아본 방법은 첫번째 방법이며, 여기서 알아볼 방법은 두 번째 방법입니다. 두 번째 방법의 경우에는 앱 개발자의 개입이 불가능하게 진행이 됩니다.\n\n### 1. 앱스토어에서 환불 처리\n\n사용자가 앱스토어의 고객 지원을 통해 환불을 요청합니다. 앱스토어는 이를 진행합니다. \n\n### 2. 환불 통지 수신\n\n#### BE\n\n앱스토어는 환불 처리 결과를 앱의 백엔드에 통지합니다. 이는 대부분 웹훅 형태로 제공됩니다. \n\n백엔드는 앱스토어로부터 환불 통지를 받고 해당 정보를 검증합니다. 검증 과정에는 통지의 유효성 확인, 환불된 결제와 연관된 사용자 및 내부 재화 찾기 등이 포함됩니다. \n\n### 3. 내부 재화 상태 업데이트\n\n#### BE\n\n환불 통지가 검증된다면, 사용자의 내부 재화 상태를 업데이트합니다. \n\n## 결론\n\n길게 적게 되었지만 사실 제가 궁금했던 부분을 “실제 결제 과정을 어디에서 담당하냐” 였습니다. 결제 과정은 앱스토어에서 직접 담당하므로 결국 인앱결제에 있어서 백엔드가 해야 할 가장 핵심 프로세스는 영수증 검증인것으로 보입니다.\n\n물론, 금전을 직접 다루는 중요한 과정이므로 보안 등의 사항에 특별히 유념해서 더 신경써야 하겟습니다. \n\n오늘은 이론적인 부분만 미리 살펴봤습니다. 언제나 그렇지만 실재 구현에 들어가면 항상 예외적인 상황이 발생합니다. 나중에 구현을 할 경우에 그 부분에 대해서도 자새히 기록을 해보려고 합니다. \n\n"},{"excerpt":"서론 DB 설계를 하다보면 index를 정의해야 하는 경우가 많습니다. 만약 Sqlalchemy의 create_all() 메소드를 사용해 테이블을 생성하고 있다면 Sqlalchemy에서 동시에 index를 정의할 수 있습니다.  Sqlalchemy에서의 인덱스 정의 단일 컬럼 인덱스 Sqlalchemy의  객체를 생성할 때에   플래그를 설정하면 간단하게…","fields":{"slug":"/Sqlalchemy의-index/"},"frontmatter":{"date":"February 13, 2024","title":"Sqlalchemy의 index","tags":["SqlAlchemy"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nDB 설계를 하다보면 index를 정의해야 하는 경우가 많습니다.\n\n만약 Sqlalchemy의 [create\\_all()](https://sharknia.github.io/SQLAlchemy-create_all-메소드로-데이터베이스-테이블-자동-생성하기) 메소드를 사용해 테이블을 생성하고 있다면 Sqlalchemy에서 동시에 index를 정의할 수 있습니다. \n\n## Sqlalchemy에서의 인덱스 정의\n\n### 단일 컬럼 인덱스\n\nSqlalchemy의 `Column` 객체를 생성할 때에 `index=True`  플래그를 설정하면 간단하게 단일 컬럼에 대한 인덱스를 생성할 수 있습니다. \n\n```python\nColumn('user_id', Integer, index=True)\n```\n\n### 복합 인덱스\n\n두 개 이상의 컬럼을 포함하는 복합 인덱스는 `Index` 객체를 사용하여 생성할 수 있습니다. \n\n```python\nfrom sqlalchemy import Index\nIndex('my_index', MyModel.column1, MyModel.column2.desc())\n```\n\n여기서 `desc()` 메소드는 해당 컬럼을 내림차순으로 정렬하도록 지시합니다. 오름차순은 기본값이므로 `asc()`는 일반적으로 생략됩니다.\n\n### Unique 인덱스\n\nUnique 인덱스는 값의 중복을 방지하고 해당 컬럼의 각 값이 유일하다는 것을 보장합니다. \n\n```python\nColumn('email', String, unique=True)\n```\n\n### 복합 Unique 인덱스\n\nSqlAlchemy에서는 복합 인덱스에 대해서도 유니크 제약 조건을 적용할 수 있습니다. 이를 통해 테이블 내에서 특정 컬럼 조합의 유니크성을 강제할 수 있습니다. \n\nSqlAlchemy에서 복합 유니크 인덱스를 정의하는 방법은 두 가지입니다. \n\n1. `UniqueConstraint` 사용\n\n    `UniqueConstraint`는 테이블의 컬럼에 대한 유니크 제약 조건을 정의하는 데 사용됩니다. 이는 `__table_args__` 속성 내부에서 사용될 수 있습니다.\n\n    ```python\n    from sqlalchemy import Column, Integer, String, create_engine\n    from sqlalchemy.ext.declarative import declarative_base\n    from sqlalchemy.schema import UniqueConstraint\n    \n    Base = declarative_base()\n    \n    class MyModel(Base):\n        __tablename__ = 'my_model'\n        id = Column(Integer, primary_key=True)\n        column1 = Column(Integer)\n        column2 = Column(String)\n        __table_args__ = (UniqueConstraint('column1', 'column2', name='my_unique_constraint'),)\n    ```\n\n1. `Index` 사용 시 `unique=True` 설정\n\n    `Index` 객체를 생성할 때 `unique=True` 플래그를 설정하여 복합 유니크 인덱스를 생성할 수 있습니다. 이 방법은 인덱스 생성과 동시에 컬럼 조합의 유니크성도 보장합니다.\n\n    ```python\n    from sqlalchemy import Column, Integer, String, MetaData, Table\n    from sqlalchemy.schema import Index\n    \n    metadata = MetaData()\n    my_table = Table('my_table', metadata,\n        Column('id', Integer, primary_key=True),\n        Column('column1', Integer),\n        Column('column2', String),\n    )\n    \n    # 복합 유니크 인덱스 생성\n    my_unique_index = Index('my_unique_index', my_table.c.column1, my_table.c.column2, unique=True)\n    ```\n\n두 방법 모두 데이터베이스에서 해당 컬럼 조합의 유니크성을 강제합니다. `UniqueConstraint`는 주로 제약 조건을 명시적으로 표현할 때 사용되며, `Index`의 `unique=True` 설정은 검색 최적화와 유니크 제약 조건을 동시에 적용하고자 할 때 유용합니다.\n\n### **_\\_table\\_args\\__**\n\n  `__table_args__` 클래스 속성은 튜플이나 사전 형태로 제공될 수 있으며, 테이블 생성 시 추가적인 SQL 제약 조건이나 인덱스를 정의하는데 사용됩니다.  \n\n예를 들어 복합 인덱스를 `__table_args__` 클래스 속성을 사용해 모델 내부에서 정의한 예제 코드는 다음과 같습니다. \n\n```python\nclass Event(Base):\n    __tablename__ = 'events'\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    timestamp = Column(DateTime)\n    user_id = Column(Integer)\n    is_active = Column(Boolean)\n\n    # 복합 인덱스 정의\n    __table_args__ = (\n        Index('ix_events_user_id_timestamp', 'user_id', timestamp.desc()),\n    )\n```\n\n이 방식으로 정의된 인덱스는 데이터베이스에 해당 모델을 반영할 때 자동으로 생성됩니다. \n\n\n\n"},{"excerpt":"서론 새로운 서비스의 개발을 준비하고 있습니다. 그 서비스를 위해서 위치 기반 검색이 가능해야 할 것으로 생각하고 있습니다.  아직 해당 기능에 대해서 구현 해본 적도 없고, 어떻게 구현 해야 할지 감도 잡히지 않아 아직 개발단계에는 이르지 않았지만 이에 대해 미리 서치한 내용을 기록하려고 합니다.  Postgresql을 사용하게 될 것이므로, Postg…","fields":{"slug":"/위치-기반-검색-기능의-구현/"},"frontmatter":{"date":"February 13, 2024","title":"위치 기반 검색 기능의 구현","tags":["Postgresql","DataBase","ETC"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n새로운 서비스의 개발을 준비하고 있습니다. 그 서비스를 위해서 위치 기반 검색이 가능해야 할 것으로 생각하고 있습니다. \n\n아직 해당 기능에 대해서 구현 해본 적도 없고, 어떻게 구현 해야 할지 감도 잡히지 않아 아직 개발단계에는 이르지 않았지만 이에 대해 미리 서치한 내용을 기록하려고 합니다. \n\nPostgresql을 사용하게 될 것이므로, Postgreslql에서의 구현 방법도 미리 기록합니다. \n\n위치 기반 검색 기능은 사용자의 위치 정보(위도와 경도)를 데이터베이스에 저장하고, 이를 기반으로 특정 거리 내에 있는 다른 사용자나 장소를 찾는 것입니다. \n\n## 위치 기반 검색 기능의 구현\n\n### 위치 정보의 저장\n\n사용자의 위치 정보(위도, 경도)를 저장합니다. 위치 정보는 주소 변환(Geocoding)이나 모바일 기기의 GPS를 통해 직접 얻을 수 있습니다.  또 외부 API를 사용해 위도,경도 정보를 다시 주소로 변환할 수도 있습니다. \n\n### 거리 계산\n\n데이터베이스에 저장된 위치 정보를 사용하여 사용자 간의 거리를 계산합니다. 이 때 대표적으로 사용되는 알고리즘은 하버사인 공식 입니다. 하버사인 공식은 두 지점 사이의 거리를 계산할 때 지구의 구형을 고려하여 보다 정확한 결과를 제공합니다. \n\n### 거리 기반 검색\n\n사용자의 위치에서 특정 거리 내에 있는 사용자나 장소를 찾기 위해 SQL 쿼리 내에서 거리 계산 로직을 사용합니다. 예를 들어 SQL의 HAVING 절을 사용하여 계산된 거리가 특정 버위 내에 있는 레코드만 선택할 수 있습니다. \n\n### 인덱싱과 최적화 \n\n위치 기반 검색의 효율을 높이기 위해 공간 데이터를 효율적으로 쿼리할 수 있는 공간 인덱스 (Spatial Index)를 사용할 수 있습니다. \n\n## 공간 인덱스(Spatial Index)\n\n### 공간 인덱스란? \n\n공간 인덱스(Spatial Index)는 공간 데이터(위치 정보를 포함한 데이터)를 효율적으로 쿼리하기 위한 데이터베이스 인덱스입니다. 공간 데이터는 위도와 경도로 표현되는 지점(point), 선(line), 면(area) 등 다양한 형태가 있을 수 있습니다. 이러한 데이터를 빠르게 검색하고, 공간적 관계(예: 교차, 인접, 포함 등)를 효율적으로 분석하기 위해 공간 인덱스를 사용합니다.\n\n### 공간 인덱스의 종류\n\n- R-트리(R-tree): 계층적 구조를 가진 트리 기반 인덱스로, 사각형 영역을 노드로 사용하여 공간 데이터를 구분합니다. 각 노드는 자식 노드를 포함하는 최소한의 경계 사각형을 가집니다. R-트리는 공간 검색과 범위 쿼리에 효율적입니다.\n\n- 쿼드트리(Quadtree): 공간을 사각형 영역으로 재귀적으로 분할하는 트리 기반 인덱스입니다. 각 노드는 최대 네 개의 자식(북동, 북서, 남동, 남서)을 가질 수 있으며, 공간을 균등하게 분할합니다.\n\n- GiST(Generalized Search Tree): 다양한 검색 트리를 일반화한 구조로, R-트리, B-트리 등 여러 종류의 인덱스를 하나의 프레임워크 내에서 지원합니다. PostgreSQL에서 사용됩니다.\n\n- SP-GiST(Space-Partitioned Generalized Search Tree): 공간을 분할하는 기법에 기반한 인덱스로, GiST의 확장형입니다. 공간을 비균등하게 분할하여 더 효율적인 검색을 가능하게 합니다.\n\n### 공간 인덱스의 사용\n\n공간 인덱스는 공간 쿼리를 실행할 때 데이터베이스가 전체 데이터를 순차적으로 검색하는 대신, 인덱스를 활용하여 필요한 데이터를 빠르게 찾아낼 수 있도록 합니다. 예를 들어, 특정 지역 내의 모든 관심 지점을 찾거나, 두 지점 사이의 거리를 계산할 때 공간 인덱스를 활용할 수 있습니다.\n\n### 공간 인덱스의 장점\n\n- 쿼리 성능 향상: 공간 데이터에 대한 쿼리를 실행할 때 검색 시간을 크게 줄일 수 있습니다.\n\n- 공간 관계 분석 용이: 데이터 간의 공간적 관계를 빠르게 분석할 수 있어, 인접한 데이터 찾기, 영역 내 데이터 검색 등이 용이합니다.\n\n### 공간 인덱스를 지원하는 데이터베이스 예\n\n- PostgreSQL의 PostGIS 확장: 공간 데이터를 위한 강력한 지원과 함께 R-트리 기반의 GiST 인덱스를 제공합니다.\n\n- MySQL: R-트리 기반의 공간 인덱스를 지원합니다.\n\n- SQLite의 SpatiaLite 확장: 공간 데이터를 위한 기능과 함께 R-트리 인덱스를 지원합니다.\n\n- MongoDB: 2D 및 2DSphere 인덱스를 통해 공간 데이터를 위한 인덱싱을 제공합니다.\n\n## Postgresql에서의 공간 인덱스\n\nPostgresql에는 지리적 객체를 저장, 쿼리 및 조작할 수 있는 기능을 추가하는 PostGIS 확장이 존재합니다. \n\n### PostGIS 확장 설치\n\n다음의 명령어를 통해 PostGIS 확장을 설치할 수 있습니다. \n\n```sql\nCREATE EXTENSION IF NOT EXISTS postgis;\n```\n\n### 공간 데이터 저장을 위한 테이블\n\n위치 데이터를 저장할 테이블을 생성하고 공간 정보를 저장할 수 있는 `GEOMETRY` 또는 `GEOGRAPHY` 타입의 컬럼을 포함시킵니다.\n\n```sql\nCREATE TABLE places (\n  id SERIAL PRIMARY KEY,\n  name VARCHAR(255),\n  location GEOGRAPHY(POINT, 4326) -- 위도와 경도를 사용하는 점 타입\n);\n```\n\n#### `GEOMETRY` 와 `GEOGRAPHY`  \n\n`GEOGRAPHY`와 `GEOMETRY`는 PostgreSQL의 PostGIS 확장에서 제공하는 두 가지 공간 데이터 타입입니다. \n\n- `GEOGRAPHY`\n\n    - 목적: 지구의 곡률을 고려하여 실제 지리적 위치를 더 정확하게 모델링합니다.\n\n    - 사용 케이스: 큰 거리를 다루거나, 극지방 같이 지구의 곡률이 중요한 계산에 영향을 미치는 지역에서 주로 사용됩니다.\n\n    - 계산 방식: 지구를 타원체로 가정하고, 위도와 경도를 사용하여 거리와 면적을 계산합니다. 이로 인해 `GEOMETRY`에 비해 계산이 더 복잡하고 느릴 수 있습니다.\n\n    - 좌표 체계: 기본적으로 WGS 84 (EPSG:4326) 좌표 체계를 사용합니다.\n\n- `GEOMETRY`\n\n    - 목적: 평면 상의 점들을 사용하여 공간 데이터를 모델링합니다. 지구의 곡률을 고려하지 않고, 모든 계산을 유클리드(평면) 기하학을 사용하여 수행합니다.\n\n    - 사용 케이스: 소규모 지역 또는 곡률이 중요하지 않은 계산에 주로 사용됩니다. 예를 들어, 도시 또는 건물 내부와 같은 상대적으로 작은 지역에서 사용됩니다.\n\n    - 계산 방식: 유클리드 기하학을 바탕으로 거리, 면적 등을 계산합니다. 이는 `GEOGRAPHY`에 비해 계산이 더 간단하고 빠릅니다.\n\n    - 좌표 체계: 다양한 좌표 체계를 지원하며, 사용자가 필요에 따라 선택할 수 있습니다.\n\n- 주요 차이점\n\n    - 계산 정확도와 복잡성: `GEOGRAPHY`는 지구의 곡률을 고려하기 때문에 더 복잡하고, 대규모 거리 계산에 적합합니다. 반면, `GEOMETRY`는 간단한 평면 계산에 적합합니다.\n\n    - 성능: `GEOMETRY`는 일반적으로 `GEOGRAPHY`보다 계산이 더 빠르지만, 이는 계산의 복잡성과 정확도에 따라 달라질 수 있습니다.\n\n    - 적용 범위: `GEOGRAPHY`는 전 세계적인 지리적 데이터 처리에 적합하고, `GEOMETRY`는 지역적 또는 평면적 공간 데이터 처리에 더 적합합니다.\n\n    - 대규모, 정확한 지리적 계산이 필요한 경우 `GEOGRAPHY`를, 소규모 또는 빠른 계산이 필요한 경우 `GEOMETRY`를 사용하는 것이 좋습니다.\n\n#### 결론\n\n제가 개발할 서비스의 경우, 대부분의 상호 작용이 도시 수준에서 일어나고, 성능과 응답 시간이 중요한 요소라고 생각되기 때문에 `GEOMETRY`가 더 유리해 보입니다. \n\n### 공간 인덱스 생성\n\n공간 인덱스도 생성할 수 있습니다. \n\n```sql\nCREATE INDEX idx_places_location ON places USING GIST (location);\n```\n\n### 위치 기반 검색 쿼리의 작성법\n\n사용자의 위치를 기반으로 가까운 장소를 찾아주는 `ST_DWithin` 같은 공간 함수를 사용하는 쿼리를 작성합니다.\n\n```sql\n-- 사용자 위치 (위도 37, 경도 127) 주변 10km 이내의 장소 검색\nSELECT name FROM places\nWHERE ST_DWithin(\n  location,\n  ST_MakePoint(127, 37)::geography,\n  10000 -- 거리(미터 단위)\n);\n```\n\n#### 공간함수(Spatial functions)란? \n\n공간 함수는 PostGIS, PostgreSQL의 확장 기능 중 하나입니다. 공간 데이터를 처리하기 위한 함수로, 지리적 위치의 관계, 형태, 거리 등을 계산하고 분석하는 데 사용됩니다.\n\n#### PostGIS에서 제공하는 주요 공간 함수\n\n1. 기하학적 관계 판단 함수:\n\n    - `ST_Contains(A, B)`: A가 B를 포함하는지 여부를 반환합니다.\n\n    - `ST_Intersects(A, B)`: A와 B가 교차하는지 여부를 반환합니다.\n\n    - `ST_Within(A, B)`: A가 B 내부에 있는지 여부를 반환합니다.\n\n    - `ST_Touches(A, B)`: A와 B가 접하는지 여부를 반환합니다.\n\n    - `ST_DWithin(A, B, distance)`: A와 B가 지정된 거리 `distance` 이내에 있는지 여부를 반환합니다.\n\n1. 기하학적 변환 및 생성 함수:\n\n    - `ST_Buffer(geometry, radius)`: 주어진 지오메트리 주변에 지정된 반경의 버퍼를 생성합니다.\n\n    - `ST_Centroid(geometry)`: 지오메트리의 중심점(centroid)을 계산합니다.\n\n    - `ST_MakePoint(x, y)`: x, y 좌표로 점을 생성합니다.\n\n1. 거리 및 길이 측정 함수:\n\n    - `ST_Distance(A, B)`: A와 B 사이의 최소 거리를 계산합니다.\n\n    - `ST_Length(geometry)`: 선형 지오메트리의 길이를 계산합니다.\n\n1. 공간 분석 함수:\n\n    - `ST_Area(geometry)`: 면적을 계산합니다.\n\n    - `ST_Union(A, B)`: 두 지오메트리의 합집합을 생성합니다.\n\n    - `ST_Intersection(A, B)`: 두 지오메트리의 교집합 영역을 계산합니다.\n\n1. 공간 색인 및 최적화를 위한 함수:\n\n    - `ST_GeomFromText('POINT(0 0)')`, `ST_GeomFromEWKT(...)`: Well-Known Text(EWKT)로부터 지오메트리를 생성합니다.\n\n    - `ST_SetSRID(geometry, srid)`: 지오메트리에 공간 참조 시스템(Spatial Reference System Identifier, SRID)을 설정합니다.\n\n## 같이 보기\n\n[PostGIS 3.0.0 사용자 지침서](https://postgis.net/docs/manual-3.0/postgis-ko_KR.html)\n\n"},{"excerpt":"서론 현재 회사에서는 Alembic 같은 데이터마이그레이션 도구를 사용하고 있지 않습니다. 따라서 스키마 버전 관리등은 별도로 하고 있지 않으며, 다만 SqlAlchemy의 create_all() 메소드를 이용해 프로덕션 환경이나 dev 환경에서의 테이블의 누락은 안생기게끔만 간단하게 관리하고 있습니다.  이 방법은 테이블의 수정 또는 삭제를 반영할 수는…","fields":{"slug":"/SQLAlchemy-create_all-메소드로-데이터베이스-테이블-자동-생성하기/"},"frontmatter":{"date":"February 12, 2024","title":"SQLAlchemy create_all() 메소드로 데이터베이스 테이블 자동 생성하기","tags":["SqlAlchemy","Work","Python","DataBase","FastAPI"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n현재 회사에서는 Alembic 같은 데이터마이그레이션 도구를 사용하고 있지 않습니다. 따라서 스키마 버전 관리등은 별도로 하고 있지 않으며, 다만 SqlAlchemy의 create\\_all() 메소드를 이용해 프로덕션 환경이나 dev 환경에서의 테이블의 누락은 안생기게끔만 간단하게 관리하고 있습니다. \n\n이 방법은 테이블의 수정 또는 삭제를 반영할 수는 없는 방법이지만, 아직 RDB 기반으로 옮긴지 얼마 되지 않아 상대적으로 DB의 크기가 작아 현재는 이 방법으로 충분한 상황입니다. \n\ncreate\\_all() 메소드를 사용하면서 알게 된 점을 기록해두려고 합니다. \n\n## create\\_all() 메소드란? \n\nSqlAlchemy의 create\\_all() 메소드는 `Metadata` 객체에 등록된 모든 테이블 정의를 바탕으로 해당 데이터베이스 엔진에 테이블을 생성하는데 사용됩니다. 이 메소드는 이미 생성되어있는 테이블은 생성하지 않으며, 새로 정의된(DB에 없는) 테이블만 새롭게 데이터베이스에 생성합니다. \n\n따라서, 스키마가 달라도 뭔가 수정이 되었어도 DB에 존재만 한다면 생성하거나 덮어쓰지 않으므로 만약 수정 사항을 반영하는걸 해당 메소드로 진행하려면 테이블을 drop한 뒤 생성해야 합니다. 물론 데이터가 많지 않거나 데이터를 날려도 상관없는 dev 환경에서만 유효한 방법입니다. \n\n### 기본적인 사용 방법\n\n```python\nfrom sqlalchemy import create_engine, MetaData\n\n# 데이터베이스 엔진 생성\nengine = create_engine('sqlite:///example.db', echo=True)\n\n# MetaData 인스턴스 생성\nmetadata = MetaData()\n\n# 테이블 정의들...\n# 예: users_table = Table('users', metadata, Column('id', Integer, primary_key=True), ...)\n\n# create_all 호출하여 모든 테이블 생성\nmetadata.create_all(engine)\n```\n\n`create_engine` 함수는 데이터베이스와의 연결을 설정합니다. `echo=True`는 SQLAlchemy가 실행하는 SQL 명령을 콘솔에 출력하도록 설정하는 옵션입니다.\n\n`create_all()` 메소드는 첫 번째 인자로 `Engine` 객체를 받습니다. 이 `Engine` 객체는 SQLAlchemy가 데이터베이스와 통신하기 위해 사용하는 기본 구성 요소입니다.\n\n### 비슷한 메소드들? \n\n#### drop\\_all()\n\n`drop_all()` 메소드는 `create_all()`과 반대로 작동합니다. `MetaData` 객체에 등록된 모든 테이블을 데이터베이스에서 삭제합니다. 테이블에 데이터가 있을 경우, 데이터도 함께 삭제되므로 사용할 때 주의가 필요합니다.\n\n```python\nmetadata.drop_all(engine)\n```\n\n#### Table.create() 및 Table.drop()\n\n`create_all()` 또는 `drop_all()` 메소드 대신, 개별 `Table` 객체에 대해 `create()` 또는 `drop()` 메소드를 호출할 수 있습니다. 이 방법을 사용하면 특정 테이블에 대한 작업을 더 세밀하게 제어할 수 있습니다.\n\n```python\n# 개별 테이블 생성\nusers_table.create(engine)\n\n# 개별 테이블 삭제\nusers_table.drop(engine)\n```\n\n#### reflect()\n\n`reflect()` 메소드는 데이터베이스의 기존 스키마 정보를 `MetaData` 객체로 로드합니다. 이 메소드는 기존 데이터베이스 구조를 SQLAlchemy 모델로 반영할 때 유용합니다.\n\n```python\nmetadata.reflect(engine)\n```\n\n## 나는 어떻게 사용하고 있나? \n\ndev 또는 로컬 환경에서 테스트를 할 경우 적극적으로 사용하고 있습니다. 주로 처음 테이블을 생성할 때에 굳이 create 쿼리문을 별도로 쓰지 않고 모델만 생성 후 해당 메소드를 이용해 테이블을 생성하거나, 테이블에 수정 사항이 생긴 경우 drop 후 create\\_all() 메소드를 생성해 다시 DB 테이블을 생성합니다. \n\n물론, 그때 그때 create 함수를 쓰기는 너무나 귀찮은 일이기 때문에, FastAPI의 lifespan을 활용해서 FastAPI 어플리케이션이 시작될 때에 자동으로 create\\_all() 메소드를 호출하도록 세팅을 해두었습니다. \n\n```python\nfrom sqlalchemy.orm import declarative_base\n\nBase = declarative_base()\n_db_connection: Engine = Optional[Engine]\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    await supabase_on_startup()\n    settings = Settings()\n    logger.info(f\"[SERVER_ENVIRONMENT] {settings.desc.SERVER_ENVIRONMENT}\")\n    yield\n    await supabase_on_shutdown()\n\n\nasync def on_startup():\n    global _db_connection\n    DATABASE_URL = f\"postgresql+asyncpg://postgres.{settings.postgresql_setting.projectid}:{settings.postgresql_setting.password}@aws-0-ap-northeast-2.pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n\n    # pool_size와 max_overflow의 초기값 설정\n    pool_size = 5  # 기본값\n    max_overflow = 10  # 기본값\n    echo = False\n\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        pool_size=pool_size,\n        max_overflow=max_overflow,\n        echo=echo,\n    )\n    # 비동기 엔진을 사용하여 테이블 생성\n    async with _db_connection.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n```\n\n이런식으로 구현해두었습니다. 이렇게 하면 어플리케이션이 수정될 때마다 존재하지 않는 테이블을 생성하게 됩니다. 다만, 이렇게 구현했을 때에 주의해야 할 점이 있습니다. \n\n### 주의\n\n위에서 언급했듯이, create\\_all() 메소드는 `Metadata`에 등록된 테이블 정보를 가지고 테이블을 생성합니다. 뒤집어 말하면 `Metadata`에 테이블이 등록되지 않는다면 생성되지 않는 다는 말인데, 따라서  `Metadata`에 테이블 정보가 등록되는 시점을 정확히 아는게 중요합니다. \n\n### Metadata에 테이블 정보가 등록되는 시점\n\n`MetaData` 객체에 테이블 정보가 등록되는 시점은 해당 `Table` 객체가 생성될 때입니다. SqlAlchemy에서는 `Table` 객체를 `MetaData` 객체에 직접 연결하여 테이블을 정의합니다. 클래스 정의 내에서 `Column`과 같은 필드를 사용하여 `Base` 클래스를 상속받는 모델을 정의하면, 이러한 모델 클래스가 생성될 때 내부적으로 `Table` 객체가 생성되고 `MetaData` 객체에 자동으로 등록됩니다. `declarative_base()`를 호출하여 생성된 `Base` 클래스는 내부적으로 `MetaData` 인스턴스를 포함하고 있으며, 이 `MetaData` 인스턴스는 모델 클래스를 통해 정의된 모든 테이블의 메타데이터를 관리합니다.\n\n따라서, 모델 클래스를 정의하는 순간 해당 클래스에 연결된 `Table` 객체가 생성되고, 이 객체는 자동으로 `Base` 클래스에 내장된 `MetaData` 객체에 등록됩니다. 이후 `Base.metadata.create_all(engine)`와 같은 방식으로 호출하면, `MetaData`에 등록된 모든 테이블에 대한 생성 명령이 데이터베이스 엔진으로 전송됩니다.\n\n#### FastAPI에서 모델 클래스가 정의되는 순간은? \n\nFastAPI 어플리케이션에서 “모델 클래스가 정의되는 순간”은 Python 스크립트가 실행되어 해당 모델 클래스가 메모리에 로드되는 시점을 의미합니다. \n\nFastAPI 어플리케이션을 실행하면(예를 들어 `uvicorn main:app` 등의 명령어를 사용하여) Python 인터프리터는 main.py를 로드하고 실행합니다. 이 과정에서 main.py에서 직접적으로, 또는 임포트된 모듈 내에서 다시 임포트된 SqlAlchemy의ㅏ 모델 클래스들도 함께 로드되고 메모리에 생성됩니다. 바로 이 시점에 각 모델 클래스에 해당하는 `Table` 객체가 `Metadata` 객체에 등록됩니다. \n\n다시 쉽게 이야기하자면, app.py 또는 main.py 같은 파일과 이 파일에 임포트 된 파일들, 다시 임포트 된 파일들 중에 모델 객체가 있어야만 합니다. 외딴섬처럼 덩그러니 model 파일을 생성해두고 아무곳에도 import 하지 않는다면(메인 py 파일과 연결되어 있지 않다면) 테이블은 생성되지 않습니다. 어떻게 생각하면 당연한 일인데, 이 당연한 부분을 놓쳐 길게 헤맸던 기억이 있습니다. 이 기억 때문에 해당 문서를 작성한 셈입니다. \n\n"},{"excerpt":"nolog 소개 많은 개발자들이 한 번쯤은 \"나만의 블로그를 운영해야지\"라고 생각해본 적이 있을 겁니다. 하지만 실제로 블로그를 시작하고 유지하는 일은 상당한 도전과제입니다. 공부하고 개발하는 것만으로도 바쁜데, 여기에 글을 쓰고 관리하는 일은 때때로, 아니 항상 부담스러운 일입니다. 노션으로 공부를 하는 개발자가 있습니다. 일을 하면서일수도 있고, 따로…","fields":{"slug":"/Readme/"},"frontmatter":{"date":"February 08, 2024","title":"Readme","tags":["Notion-API","Typescript","Blogging","Hobby"]},"rawMarkdownBody":"## nolog\n\n### 소개\n\n많은 개발자들이 한 번쯤은 \"나만의 블로그를 운영해야지\"라고 생각해본 적이 있을 겁니다. 하지만 실제로 블로그를 시작하고 유지하는 일은 상당한 도전과제입니다. 공부하고 개발하는 것만으로도 바쁜데, 여기에 글을 쓰고 관리하는 일은 때때로, 아니 항상 부담스러운 일입니다.\n\n노션으로 공부를 하는 개발자가 있습니다. 일을 하면서일수도 있고, 따로 짬을 내어 공부를 하면서일수도 있습니다. 그러다가 한 번 쯤은 생각하게 됩니다. 노션으로 한 공부나 작업 기록이 블로그에 자동으로 업로드된다면 좋을텐데!\n\n저 역시 마찬가지입니다. 저는 귀찮은 일이 너무 귀찮습니다. 공부나 블로그나 어차피 한 번에 할 수 있는거 아닌가? 공부만 하면 포스팅이 되면 안되나? 왜 똑같은 일을 두 번 세 번 해야 하는걸까?\n\n#### 그래서 만들었습니다.\n\nnolog는 노션에 집중해서 작업하거나 공부해서 기록하는 것만으로도 그 내용이 자동으로 마크다운으로 변환되어 GitHub 기반의 블로그에 포스팅될 수 있도록 해주는 프로그램입니다. 취업 준비, 지식 공유, 개인 브랜딩 등 여러 목적으로 블로그를 운영하고자 하는 분들에게 nolog는 시간과 노력을 크게 절약해 줄 것입니다.\n\n### 시작하기\n\n#### Notion 준비\n\n1. 노션 계정을 준비합니다.\n\n1. [Base Template](https://www.notion.so/248d5b9bf2a644b4b25485c828d5b04f)을 내 노션으로 복제합니다.\n\n    ![](image1.png)\n1. 복제된 페이지에서 공유 - 링크 복사를 눌러 페이지의 주소를 얻습니다. 해당 주소는 `username.notion.site/NOTION_PAGE_ID` 꼴입니다. `NOTION_PAGE_ID`를 기록합니다.\n\n1. [Notion API 관리 페이지](https://www.notion.so/my-integrations)에서 `새 API 통합 만들기` 를 선택해 API를 생성해줍니다. 워크스페이스별로 권한을 관리하므로, 정확한 워크스페이스를 선택하여 진행합니다.\n\n1. `프라이빗 API 통합 시크릿키`를 기록합니다.\n\n1. 콘텐츠 기능은 읽기/업데이트/입력을 필수로 선택해야 합니다.\n\n    ![](image2.png)\n1. 노션에서 연결된 워크스페이스를 선택하고, 위에서 만든 통합 API를 연결해줍니다.\n\n    ![](image3.png)\n#### 프로젝트 준비\n\n1. 이 저장소의 별표를 누르고 여러분의 프로필로 포크합니다. 그 후 로컬로 클론합니다.\n\n1. (Optional) [github.io](http://github.io/) 기반의 블로그 저장소를 준비합니다.\n\n1. `.env.example` 파일의 이름을 `.env`로 변경하고 설정값들을 세팅합니다.\n\n    - NOTION-KEY : 프라이빗 API 통합 시크릿키를 입력합니다.\n\n    - NOTION_DATABASE_ID : NOTION_PAGE_ID를 입력합니다.\n\n    - BLOG_URL : 깃허브 블로그의 URL을 입력합니다. 노션 페이지 링크를 실제 블로그 링크로 변경하는데 사용됩니다.\n\n    - SAVE_DIR : 콘텐츠들이 저장될 디렉토리입니다. [github.io](http://github.io/) 블로그의 저장소에서 포스팅될 마크다운들이 저장될 디렉토리입니다.\n\n    - SAVE_SUB_DIR (Optional) : 포스트들이 저장될 하위 디렉토리입니다. 노션 페이지의 프로퍼티들로 정의합니다. 예를 들어 date/tag/로 입력한 경우, date와 tag가 노션 페이지의 속성에 존재해야 합니다.\n\n    - BLOG_REPO : 블로그 레포지토리의 경로입니다.\n\n    - GIT_USER_NAME : 깃허브 User Name 입니다.\n\n    - GIT_USER_EMAIL : 깃허브 User Email 입니다.\n\n1. 저장소를 내려 받은 곳에서 다음의 명령어를 사용합니다.\n\n    - nvm use\n\n    - npm install\n\n    - npm install -g ts-node\n여기까지 세팅 후 ts-node index.ts를 실행합니다. 메타데이터 파일이 초기화됩니다. 초기화된 메타데이터를 저장소에 커밋/푸쉬합니다.\n\n### 사용법\n\n#### 노션 페이지 마크다운 변환\n\n변환은 상태값을 기준으로 작동합니다. 상태값의 정의는 다음과 같습니다.\n\n- Writting : 작성중인 글입니다. 블로그에 반영되지 않습니다.\n\n- Ready : Ready 상태의 글은 프로그램이 실행되면 마크다운으로 변환됩니다. 변환 된 후, updated 상태로 바뀝니다.\n\n- Updated : 마크다운 변환이 완료된 글입니다.\n\n- ToBeDeleted : 삭제할 글입니다. 타겟 디렉토리 안의 해당하는 마크다운 파일을 삭제합니다.\n\n- Deleted : 삭제된 글입니다.\n\n노션에 원하는대로 글을 작성하고, 적당한 상태값을 설정한 뒤 `ts-node index.ts`로 프로그램을 실행하면 지정한 디렉토리에 마크다운 파일이 저장됩니다.\n\n#### 호환 노션 블록\n\n노션의 다양한 기본 블록을 모두 지원합니다.\n또한, `미디어 - 이미지`, `미디어 - 코드`, `미디어 - 북마크` 블록들을 지원합니다.\n\n#### 페이지 링크 기능\n\n같은 데이터베이스(블로그 데이터베이스)에 속한 노션 페이지라면 해당 페이지 링크를 걸면 그 페이지에 해당하는 블로그 주소로 링크로 변환되는 기능을 제공합니다.\n\n### [Optional] 블로그 자동 배포\n\n로컬에서만 사용해도 노션 글을 마크다운으로 바꿔주는 기능을 갖고 있지만, [github.io](http://github.io/) 기반의 블로그 자동 배포도 가능합니다. 이를 위해서 깃허브 액션을 사용합니다.\n\n#### Git Token 발행\n\n깃허브의  `Settings` - `Developer Settings` - `Personal Access Token` 메뉴에서 토큰을 발행해야 합니다.\n\n#### 변수 설정\n\n레포지토리에 Secrets와 Variables를 설정해줘야 합니다.\n\n![](image4.png)\n저장소의 `Settings` - `Secrets and varaibles` - `Actions`에서 깃허브 액션에 필요한 환경변수를 설정할 수 있습니다.\n\n- Secrtes\nGITTOKEN, NOTION_DATABASE_ID, NOTION_KEY를 생성 해줍니다. GITTOKEN은 방금 발행한 Git Token 값이며, 나머지는 환경변수에 입력한 것과 동일한 값입니다.\n\n- Varaiables\nBLOG_REPO, BLOG_URL, GIT_USER_EMAIL, GIT_USER_NAME, SAVE_DIR를 생성합니다. 환경변수 입력과 동일한 값입니다.\n\n#### 워크플로우 설정\n\n깃허브 기반의 블로그 배포는 사용하는 테마에 따라 방법이 완전히 다릅니다. 따라서 깃허브 액션 워크플로우를 자동 배포까지 하는 경우, 자동 배포는 별도로 구성하여 블로그 저장소에 커밋/푸쉬까지만 하면 되는 케이스별로 구분해두었습니다.\n\n포크를 한 경우에는 기본적으로 워크플로우가 비활성화 되어있습니다. 원하는 워크플로우 고른 후 활성화 해주면 됩니다.\n\n#### 블로그 저장소에 커밋/푸쉬까지만 자동화하는 워크플로우\n\n`workflow_2.yml.disabled` 워크플로우를 사용하면 됩니다. 파일 이름에서 .disabled를 제거해야 합니다.\n노션에서 글을 가져와 깃허브 블로그 레포지토리에 마크다운 파일을 집어넣고 커밋/푸쉬 하는 것까지 자동화 되어있습니다.\n\n#### 블로그 배포까지 자동화하는 워크플로우\n\n`main.yml` 파일의 `blog-deploy` Job을 참조해서 워크플로우를 각자 블로그 배포 형식에 맞게 수정하면 됩니다. 해당 파일은 제가 실제로 배포에 사용하는 파일입니다.\n\n### 개발과정\n\n[개발과정 몰아보기](https://sharknia.github.io/series/GitHub-Pages%EC%99%80-Notion-API-%EC%97%B0%EB%8F%99/)\n\n### 문의\n\n[zel@kakao.com](mailto:zel@kakao.com)\n\n"},{"excerpt":"서론 개인적으로 이것저것 만들려고 시도한 적은 많았는데, 완성한 적은 이번이 처음입니다. (nolog 프로젝트) 라이센스도 등록해보고 릴리즈도 처음 해봤는데, 릴리즈를 하려다 보니 다음과 같은 문구가 있었습니다.  “버전 관리” 에 대해서는 추상적으로 알고 있었지만, 시맨틱 버전 관리라는 개념의 이름에 대해서는 처음 알았습니다. 부끄러운 일이기도 하고, …","fields":{"slug":"/시맨틱-버전-관리/"},"frontmatter":{"date":"February 08, 2024","title":"시맨틱 버전 관리","tags":["ETC"]},"rawMarkdownBody":"## 서론\n\n개인적으로 이것저것 만들려고 시도한 적은 많았는데, 완성한 적은 이번이 처음입니다. ([nolog 프로젝트](https://sharknia.github.io/series/GitHub-Pages와-Notion-API-연동/)) 라이센스도 등록해보고 릴리즈도 처음 해봤는데, 릴리즈를 하려다 보니 다음과 같은 문구가 있었습니다. \n\n\n        If you’re new to releasing software, we highly recommend to learn more about semantic versioning.\n\n“버전 관리” 에 대해서는 추상적으로 알고 있었지만, 시맨틱 버전 관리라는 개념의 이름에 대해서는 처음 알았습니다. 부끄러운 일이기도 하고, 또 이번 기회에 잘 알아둬야겠다고 생각이 들어서 한 번 정리해두려고 합니다. \n\n## 시맨틱 버전 관리란? \n\n시맨틱 버전 관리는 소프트웨어 버전 번호를 할당하고 증가시키는 규칙 시스템입니다. 이 시스템은 소프트웨어의 버전을 명확하게 해줘 개발자가 소프트웨어의 변경 사항을 쉽게 이해할 수 있게 돕습니다. 시맨틱 버전 관리는 주로 세 부분으로 구성된 버전 번호를 사용합니다. \n\n### Major Version\n\n이전 버전과 호환되지 않는 새로운 변경사항을 도입했을 때 증가합니다.\n\n### Minor Version\n\n이전 버전과 호환되면서 새로운 기능이 추가되었을 때 증가합니다. \n\n### Patch Version\n\n이전 버전과 호환되는 버그 수정이 이루어졌을 때 증가합니다. \n\n버전 번호는 `Major.Minor.Patch` 형식으로 표현됩니다. 예를 들어 `2.3.1` 과 같은 형태로 나타날 수 있습니다. \n\n## 목적\n\n소프트웨어 어떤 버전이 다른 버전과 어떻게 다른지를 명확하게 명시하여 소프트웨어를 사용하는 개발자들이 해당 변경사항을 쉽게 파악하고 대응할 수 있도록 합니다. \n\n예를 들어 어떤 라이브러리의 Major 버전이 변경되었다면 이는 기존 코드와 호환되지 않을 수 있는 중대한 변경이 있었음을 의미합니다. 따라서 개발자는 코드를 업데이트 하기 전에 해당 변경 사항을 주의 깊게 검토할 필요가 있습니다. \n\n"},{"excerpt":"서론 파이썬에서 잠깐 테스트코드를 맛본적이 있습니다. 문법 자체는 조금 복잡했지만(익숙하지 않았지만) 설치 자체는 그다지 어렵지 않았던 것으로 기억합니다.  그래서 타입스크립트에서도 그럴 줄 알았습니다.  설치 일단, 설치가 한 두개가 아니었습니다. 또, 설치하는 패키지를 package.json 파일의 devDependencies에 추가했습니다. 이는 개…","fields":{"slug":"/Typescript의-Testcode-맛보기/"},"frontmatter":{"date":"February 06, 2024","title":"Typescript의 Testcode 맛보기","tags":["Typescript"]},"rawMarkdownBody":"## 서론\n\n파이썬에서 잠깐 테스트코드를 맛본적이 있습니다. 문법 자체는 조금 복잡했지만(익숙하지 않았지만) 설치 자체는 그다지 어렵지 않았던 것으로 기억합니다. \n\n그래서 타입스크립트에서도 그럴 줄 알았습니다. \n\n## 설치 \n\n일단, 설치가 한 두개가 아니었습니다. 또, 설치하는 패키지를 package.json 파일의 devDependencies에 추가했습니다. 이는 개발 시에만 필요한 의존성을 나타냅니다. 처음에는 멋모르고 다음의 패키지 하나만 설치해주었습니다. \n\n### @types/jest 설치\n\n```shell\nnpm i --save-dev @types/jest\n```\n\n#### `--save-dev`\n\n설치하는 패키지를 package.json 파일의 devDependencies에 추가하는 옵션이 바로 이 옵션입니다. 프로덕션에서는 필요하지 않은 테스트 라이브러리, 빌드 도구 등을 설치할 때에 해당 옵션을 붙여 설치합니다. 이 옵션을 붙여 설치할 경우, 다음과 같이 추가됩니다. \n\n```yaml\n\"devDependencies\": {\n    \"@types/jest\": \"^26.0.0\"\n}\n```\n\n이후 빌드 시스템이나 배포 파이프라인에서 `npm install --production` 명령어를 사용하면 이 곳에 정의된 패키지들은 설치를 건너뛰게 됩니다. \n\n#### `@types/jest`\n\nJest는 자바스크립트 테스팅 프레임워크입니다. `@types/jest` **** 는 Jest의 타입 정의를 포함하는 타입스크립트 패키지입니다. 타입스크립트 프로젝트에서 Jest를 사용할 때, Jest의 함수와 객체에 대한 자동 완성, 타입 체킹 등의 이점을 제공합니다. \n\n그렇습니다. Jest는 별도로 설치해야 합니다. \n\n### Jest 설치\n\nJest 역시 설치해줍니다. 역시 개발 환경에서만 사용할 라이브러리 이므로 —save-dev 옵션을 사용합니다.\n\n```bash\nnpm install --save-dev jest\n```\n\nJest 설치 후 `package.json`  scripts 섹션에 다음의 내용을 추가합니다. \n\n```json\n\"scripts\": {\n    \"test\": \"jest\"\n}\n```\n\n이렇게 하면 `npm test` 명령어가 Jest를 실행합니다. \n\n### TypeScript를 위한 Jest 설정\n\nJest는 기본적으로 JavaScript 파일을 지원하지만 TypeScript 파일을 처리하기 위해서는 추가적인 설정이 필요합니다. \n\n#### `ts-jest` 설치\n\nJest가 TypeScript 파일을 이해할 수 있도록 ts-jest를 설치해야 합니다. \n\n```json\nnpm install --save-dev ts-jest\n```\n\n#### jest.config.js\n\n프로젝트 루트에 jest.config.js 파일을 생성 또는 수정하여 ts-jest를 사용하도록 Jest를 구성합니다. \n\n```json\n// jest.config.js\nmodule.exports = {\n  preset: 'ts-jest',\n  testEnvironment: 'node',\n};\n```\n\n### `npm test`\n\n모든 설정을 마친 후 npm test를 실행하면 `__test__` 를 자동으로 탐색하여 `.test.js` 또는 `.spec.js` 접미사를 가진 파일의 테스트를 실행합니다. \n\n## 테스트 파일의 경로\n\n테스트 파일을 어디에 둘 것인가? 는 몇가지 패턴이 있고, 가장 널리 사용되는 방식은 다음의 두가지입니다. \n\n### 동일 디렉토리 내에 테스트 파일 배치 \n\n코드 파일과 동일한 디렉토리에 그에 해당하는 테스트 파일을 둡니다. 테스트 파일과 관련된 코드 파일이 가까이에 있어서 관리하기가 쉬운것이 장점입니다. 파일 이름은 대체로 테스트되는 파일의 이름을 따르며, `.test` 또는 `.spec` 접미사를 추가하여 구분합니다. \n\n### 별도의 테스트 디렉토리 사용\n\n프로젝트 루트 또는 각 기능별 디렉토리에 `__tests__` 디렉토리를 만들고, 그 안에 관련 테스트 파일을 모으는 것입니다. 이 구조는 테스트 파일을 소스 코드에서 분리하여, 소스 코드 디렉토리를 더 깔끔하게 유지할 수 있게 해줍니다.\n\n또는 프로젝트 루트에 `tests` 디렉토리를 만들고 그 안에 모든 테스트 파일을 분류하는 방법도 있습니다.\n\n## 오늘의 마무리\n\n여기까지 하면 일단 test 코드를 실행할 수는 있습니다. 코파일럿의 도움을 받아 테스트코들 작성하고, 코드를 읽는데까지는 성공했습니다(?)\n\n예를 들어 완성한 테스트 코드 하나는 다음과 같습니다. \n\n<details>\n<summary>envConfig.test.ts</summary>\n\n```python\nimport { EnvConfig } from '../envConfig';\n\ndescribe('EnvConfig', () => {\n    let envConfig: EnvConfig;\n    beforeAll(() => {\n        // 환경 변수 설정\n        process.env.BLOG_URL = 'https://example.com/';\n        process.env.SAVE_DIR = '/path/to/save';\n        process.env.SAVE_SUB_DIR = 'subdir';\n\n        // EnvConfig 인스턴스 생성\n        envConfig = EnvConfig.create();\n    });\n\n    afterAll(() => {\n        // 환경 변수 정리\n        delete process.env.BLOG_URL;\n        delete process.env.SAVE_DIR;\n        delete process.env.SAVE_SUB_DIR;\n    });\n\n    it('should have the correct notionKey value', () => {\n        expect(envConfig.notionKey).toEqual(process.env.NOTION_KEY || '');\n    });\n\n    it('should have the correct databaseid value', () => {\n        expect(envConfig.databaseid).toEqual(\n            process.env.NOTION_DATABASE_ID || '',\n        );\n    });\n\n    it('should correctly handle trailing slashes', () => {\n        expect(envConfig.blogUrl).toEqual('https://example.com'); // 끝 슬래시 제거 확인\n        expect(envConfig.saveDir).toEqual('/path/to/save/'); // 끝에 슬래시 추가 확인\n        expect(envConfig.saveSubDir).toEqual('subdir/'); // 끝에 슬래시 추가 확인\n    });\n});\n```\n\n\n</details>\n\n테스트코드를 작성하면서 느낀점은, 역시 테스트코드는 사전에 짜는게 의미가 있는 것 같습니다. 또 역시 문법이 상당히 어색합니다. 추가적인 노력이 많이 필요하다고 느끼고, 가능하면 다음에는 프로젝트 처음부터 테스트 코드를 먼저 짜보려고 합니다. \n\n오히려 좀 더 필요성을 느끼게 되었습니다. 살짝 고생했지만 좋은 시간이었습니다. \n\n"},{"excerpt":"목적 깃허브 액션에서 Job들은 서로 다른 독립된 환경을 가집니다. 각 작업은 독립적으로 실행되며, 각기 다른 러너 환경에서 다른 버전의 도구를 사용할 수 있습니다. 이것은 아주 강점이지만, 때로 다른 Job끼리 변수를 공유해야 하는 상황이 있을 수 있습니다.  이때 사용하는 것이 Output 입니다.  선언하기 기본적인 선언 방법은  의 꼴입니다. 예전…","fields":{"slug":"/Github-Action-Output/"},"frontmatter":{"date":"February 04, 2024","title":"Github Action Output","tags":["Github Actions"]},"rawMarkdownBody":"## 목적\n\n[깃허브 액션](https://sharknia.github.io/Github-Actions)에서 Job들은 서로 다른 독립된 환경을 가집니다. 각 작업은 독립적으로 실행되며, 각기 다른 러너 환경에서 다른 버전의 도구를 사용할 수 있습니다. 이것은 아주 강점이지만, 때로 다른 Job끼리 변수를 공유해야 하는 상황이 있을 수 있습니다. \n\n이때 사용하는 것이 Output 입니다. \n\n## 선언하기\n\n```yaml\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    outputs: # 다른 job에서 사용하기 위해 선언\n      output1: ${{ steps.step1.outputs.test }}\n      output2: ${{ steps.step2.outputs.test }}\n    steps:\n      - id: step1 # 해당 output을 특정하기 위한 id 선언\n        run: echo \"test=hello\" >> \"$GITHUB_OUTPUT\"\n      - id: step2\n        run: echo \"test=world\" >> \"$GITHUB_OUTPUT\"\n```\n\n기본적인 선언 방법은 `echo \"변수=값\" >> \"$GITHUB_OUTPUT”` 의 꼴입니다. 예전에는 set_output을 사용했던 것 같은데, 이 방법도 지금 작동은 제대로 하지만 해당 코드로 실행을 해보면 보안상의 문제로 곧 depacreted 될 거라는 warning이 발생합니다. \n\n반드시 id를 지정해줘야 합니다. 해당 id를 사용해 나중에 원하는 변수를 참조할 수 있습니다. \n\noutput은 동일한 job이 아니라 job2에서 사용을 하고 싶을 경우에 선언해야 합니다. \n\n## 사용하기 - 동일한 job\n\n```yaml\n- name: use output same job\n        run: |\n          echo ${{ steps.step1.outputs.test}}\n          echo ${{ steps.step2.outputs.test}}\n```\n\n같은 job 안에서 사용할때에는 `${{ steps.”id”.outputs.”변수명”}}` 의 꼴로 사용하면 됩니다. \n\n## 사용하기 - 다른 job\n\n```yaml\n\n  job2:\n    runs-on: ubuntu-latest\n    needs: job1\n    steps:\n      - env:\n          OUTPUT1: ${{needs.job1.outputs.output1}}\n          OUTPUT2: ${{needs.job1.outputs.output2}}\n        run: echo \"$OUTPUT1 $OUTPUT2\"\n```\n\n다른 job에서 해당 변수를 사용할 경우에도 [needs](https://sharknia.github.io/Github-Actions-Job---needs) 종속성 선언을 해줘야 합니다. job1에서 선언한 output을 사용할 것이므로 needs 설정을 해주고 `${{ needs.”job-이름”.outputs.”output변수명”}}` 의 꼴로 참조할 수 있습니다. \n\n또한, 변수 명의 경우 대소문자는 구분하지 않습니다. \n\n## 참조\n\n[https://docs.github.com/ko/actions/using-jobs/defining-outputs-for-jobs#예-작업-출력-정의](https://docs.github.com/ko/actions/using-jobs/defining-outputs-for-jobs#예-작업-출력-정의)\n\n"},{"excerpt":"소개 Github Actions는 깃허브에서 직접 소프트웨어 개발 워크플로우를 자동화할 수 있는 기능입니다. 개발자는 이를 사용해서 소프트웨어 빌드, 테스트, 배포와 같은 과정을 자동화하여 특정 트리거가 발생했을 경우 자동으로 실행되는 워크플로우를 작성할 수 있습니다.  강점 및 특징 깃허브와의 통합 깃허브와 깊게 통합되어 있어 깃허브 리포지토리 내에서 …","fields":{"slug":"/Github-Actions/"},"frontmatter":{"date":"February 04, 2024","title":"Github Actions","tags":["Github Actions"]},"rawMarkdownBody":"## 소개\n\nGithub Actions는 깃허브에서 직접 소프트웨어 개발 워크플로우를 자동화할 수 있는 기능입니다. 개발자는 이를 사용해서 소프트웨어 빌드, 테스트, 배포와 같은 과정을 자동화하여 특정 트리거가 발생했을 경우 자동으로 실행되는 워크플로우를 작성할 수 있습니다. \n\n## 강점 및 특징\n\n### 깃허브와의 통합\n\n깃허브와 깊게 통합되어 있어 깃허브 리포지토리 내에서 직접 워크플로우를 관리하고 실행할 수 있습니다. 따라서 코드 변경, PR, 이슈 생성 등 깃허브 이벤트에 기반한 자동화를 간편하게 설정할 수 있습니다. \n\n### 언어와 프레임워크에 대한 광범위한 지원\n\n다양한 프레임워크와 프로그래밍 언어를 지원합니다.이로 인해 거의 모든 소프트웨어 프로젝트에 적용할 수 있는 유연성을 제공합니다. \n\n### 재사용 가능한 컴포넌트\n\n마켓플레이스에서 다른 개발자가 만든 액션을 재사용하거나 자신의 커스텀 액션을 만들어 공유할 수 있습니다. \n\n### 유연한 트리거 옵션\n\n다양한 깃허브 이벤트에 대한 트리거 옵션을 제공합니다. 또한 cron 기반의 스케쥴을 통해서도 실행이 가능합니다. 이를 이용해 자유롭게 필요에 따라 자동화 작업을 실행할 수 있습니다. \n\n## Github Actions의 이해\n\n### Workflow\n\n워크플로우는 하나 이상의 작업을 실행하는 자동화된 프로세스입니다. 워크플로우는 리포지토리의 `.github/workflows` 디렉토리 아래 YAML 파일에 의해 정의되며 리포지토리의 이벤트 또는 스케쥴러에 따라 실행됩니다. \n\n### Jobs\n\n크게 뭉뚱그려서 이야기하면 워크플로우는 결국 Job들의 집합입니다.\n\n Job들은 서로 다른 독립된 환경을 가집니다. 각 작업은 독립적으로 실행되며, 각기 다른 러너 환경에서 다른 버전의 도구를 사용할 수 있습니다. 이 방법을 사용하면, 하나의 워크플로우 안에서 서로 다른 환경, 서로 다른 패키지 세트를 사용하여 작업을 수행할 수 있습니다. \n\n#### 여러 Job의 선언\n\n예를 들어, 첫번째 잡과 두번째 잡이 각각 다른 node 버전을 사용해야 하는 경우 다음과 같이 구성할 수 있습니다. \n\n```yaml\nname: job1-job2\n\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Use Node.js (Version for Build)\n      uses: actions/setup-node@v2\n      with:\n        node-version: '12' # 첫 번째 작업에 필요한 노드 버전\n\n    - name: Install Dependencies for Build\n      run: npm install\n\n    - name: Build\n      run: npm run build\n\n    - name: Additional Build Steps\n      run: # 필요한 추가 빌드 스텝\n\n  job2:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Use Node.js (Version for Deploy)\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18' # 두 번째 작업에 필요한 노드 버전\n\n    - name: Install Dependencies for Build\n      run: npm install\n\n    - name: Build\n      run: npm run build\n\n    - name: Additional Build Steps\n      run: # 필요한 추가 빌드 스텝\n\n```\n\n### Step\n\nJob은 Step들의 집합입니다. Step은 실행될 셀 스크립트 또는 실행될 action들이이며, Step들은 정의된 순서대로 실행됩니다.\n\n각 Step은 동일한 runner에서 실행되므로(Job이 가진 환경을 공유하므로) step 마다 데이터를 공유할 수 있다. 즉 한 job 안에서 빌드하는 step, 이를 테스트 하는 step을 한 번에 가질 수 있습니다.\n\n\n\n## 참조\n\n[https://docs.github.com/ko/actions/learn-github-actions/understanding-github-actions](https://docs.github.com/ko/actions/learn-github-actions/understanding-github-actions)\n\n\n\n"},{"excerpt":"개요 기본적으로 아무 설정이 없다면 각 Job들은 병렬적으로 실행됩니다.  예를 들어 job1과 job2를 선언한 경우, 이 두 job은 워크플로우 실행 시 동시에 시작이 됩니다.  이 때,  를 사용해 Job들 사이에 종속성을 주어 Job 하나가 끝나야 다음 Job이 실행되도록 설정할 수 있습니다. 예제 순차적으로 실행하기 다음과 같이 needs에 다른…","fields":{"slug":"/Github-Actions-Job---needs/"},"frontmatter":{"date":"February 04, 2024","title":"Github Actions Job - needs","tags":["Github Actions"]},"rawMarkdownBody":"## 개요\n\n기본적으로 아무 설정이 없다면 각 Job들은 병렬적으로 실행됩니다.  예를 들어 job1과 job2를 선언한 경우, 이 두 job은 워크플로우 실행 시 동시에 시작이 됩니다. \n\n이 때, `needs` 를 사용해 Job들 사이에 종속성을 주어 Job 하나가 끝나야 다음 Job이 실행되도록 설정할 수 있습니다.\n\n## 예제\n\n### 순차적으로 실행하기\n\n다음과 같이 needs에 다른 job의 이름을 지정해주면, 해당 job이 끝나야 이 job이 실행되도록 할 수 있습니다. \n\n```yaml\nname: job1-job2\n\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    steps:\n\t\t\t# 첫번째 작업 실행...\n  job2:\n\t\tneeds: job1 # 'job1' 작업이 성공적으로 완료된 후에 실행\n    runs-on: ubuntu-latest\n    steps:\n\t\t\t# 두번째 작업 실행...\n```\n\n이 경우에는 job1이 실패할 경우, job2의 작업도 건너뛰게 됩니다. \n\n### 여러개의 needs 선언하기\n\n```yaml\njobs:\n  job1:\n  job2:\n    needs: job1\n  job3:\n    needs: [job1, job2]\n```\n\n위와 같이 선언되어있다고 할 때에, job1이 반드시 성공해야 job2가 실행이 되며, job1, job2가 모두 성공적으로 완료되어야 job3이 실행됩니다. \n\n### 성공-실패 여부 상관없이 순차적으로 실행하기\n\njob의 실행에도 조건문을 걸 수 있습니다. \n\n```yaml\njobs:\n  job1:\n  job2:\n    needs: job1\n  job3:\n    if: ${{ always() }}\n    needs: [job1, job2]\n```\n\n예를 들어 이 조건문은 job1, job2의 성공 여부와는 상관없이 단지 순차적으로 작업을 실행합니다. 즉, job2가 실패하더라도 job3은 job1, job2 완료 이후에 실행됩니다. \n\n## 참조\n\n[https://docs.github.com/ko/actions/using-jobs/using-jobs-in-a-workflow](https://docs.github.com/ko/actions/using-jobs/using-jobs-in-a-workflow)\n\n\n\n"},{"excerpt":"왜 필요한가? 깃허브 액션 워크플로우를 로컬에서 테스트하기가 쉽지 않습니다. 결국 깃허브 콘솔 또는 깃허브 CLI를 이용해서 테스트 하게 되는 경우가 대부분입니다.  단순히 Re-Run을 실행하면, 레포에 변경된 내용이 반영되지 않고 그 당시 환경에 대해서만 재실행이 되어 정상적인 테스트가 불가능합니다.  결국 트리거에 Push 조건을 걸어놓고, 변경 사…","fields":{"slug":"/Github-Actions-Workflow-수동으로-실행하기/"},"frontmatter":{"date":"February 04, 2024","title":"Github Actions Workflow 수동으로 실행하기","tags":["Github Actions"]},"rawMarkdownBody":"## 왜 필요한가?\n\n깃허브 액션 워크플로우를 로컬에서 테스트하기가 쉽지 않습니다. 결국 깃허브 콘솔 또는 깃허브 CLI를 이용해서 테스트 하게 되는 경우가 대부분입니다. \n\n단순히 Re-Run을 실행하면, 레포에 변경된 내용이 반영되지 않고 그 당시 환경에 대해서만 재실행이 되어 정상적인 테스트가 불가능합니다. \n\n결국 트리거에 Push 조건을 걸어놓고, 변경 사항을 만들어서 Push를 해서 워크플로우를 테스트 하고 있었습니다. \n\n그런데, 수동으로 깃허브 웹 콘솔에서 수동으로 워크플로우를 테스트 할 수 있게 해주는 방법이 있습니다. \n\n## 워크플로우 파일 수정\n\n트리거에 `workflow_dispatch` 이벤트를 추가해줍니다. \n\n```yaml\non:\n    workflow_dispatch:\n```\n\n## 수동으로 워크플로우 수동으로 실행\n\n위의 이벤트를 트리거에 추가해주고, 웹 콘솔에서 다음의 화면으로 이동합니다. \n\n![](image1.png)\nActions 탭 - 수정된 워크플로우로 이동하면 Run Workflow 버튼이 생긴 것을 볼 수 있습니다. \n\n특이사항으로, 혹시 워크플로우를 비활성화 한다면 해당 버튼도 활성화되지 않습니다. \n\n## 참조\n\n[https://docs.github.com/ko/actions/using-workflows/manually-running-a-workflow?tool=webui](https://docs.github.com/ko/actions/using-workflows/manually-running-a-workflow?tool=webui)\n\n"},{"excerpt":"요약 짬짬이 틈을 내서 하는 작업이다 보니 문서화 작업을 제대로 못했습니다. 짜잘짜잘한 오류 수정이 있었으며, 비교적 큰 수정도 있었고 완성이 됐다고 판단해 버전을 정의하고 릴리즈도 해봤습니다.  버그 수정 코드 블록의 들여쓰기가 제대로 적용되지 않는 문제가 수정되었습니다.    스타일 내에서 언더바가 제대로 인식되지 않는 문제가 수정되었습니다.  타이틀…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅11/"},"frontmatter":{"date":"February 04, 2024","title":"NotionAPI를 활용한 자동 포스팅(11)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"## 요약\n\n짬짬이 틈을 내서 하는 작업이다 보니 문서화 작업을 제대로 못했습니다. 짜잘짜잘한 오류 수정이 있었으며, 비교적 큰 수정도 있었고 완성이 됐다고 판단해 버전을 정의하고 릴리즈도 해봤습니다. \n\n## 버그 수정\n\n- 코드 블록의 들여쓰기가 제대로 적용되지 않는 문제가 수정되었습니다. \n\n- `코드`  스타일 내에서 언더바가 제대로 인식되지 않는 문제가 수정되었습니다. \n\n- 타이틀에 스타일이 포함된 경우(노션에서 확인하기 어려움) 타이틀이 짤리던 문제가 수정되었습니다. \n\n- 기타 일부 스타일이 적용되지 않던 문제가 해결됐습니다. \n\n- API 호출 시 타임아웃이 될 경우 대책없이 프로그램이 중단되던 문제가 해결됐습니다. \n\n## 업데이트\n\n- 이제 토글 블록이 지원됩니다.\n\n- 블로그 자동 배포가 워크플로우에 포함되었습니다. 이를 위해 깃허브 계정을 미리 설정할 수 있게 되었습니다. \n\n- 메타데이터 초기화 기능이 추가되었습니다. 이제 새로운 계정으로 세팅 시 메타데이터 파일이 초기화됩니다.\n\n- 코드 리팩토링의 일부로 일부 코드의 디렉토리가 분리되었습니다. \n\n- `v1.0.0-beta`가 릴리즈 되었습니다. Readme가 완성되었습니다. \n\n- 프로젝트 이름이 `nolog`로 확정되었습니다.\n\n- 라이센스가 명시되었습니다. \n\n## 타임아웃 문제의 해결\n\n기타 다른 버그는 짜잘짜잘한 수정이었지만 타임아웃 문제 해결은 나름대로 머리를 써서 구현했으므로 해당 과정을 별도로 기록합니다. \n\n### 문제 파악\n\n노션 API를 사용하고 있고, 해당 API를 사용하기 위해서 notionClient 라이브러리를 사용하고 있습니다. 그리고 주로 데이터베이스 정보를 읽어올 때, 페이지 안의 내용을 읽어올 때, 블록 안의 내용을 읽어올 때 해당 라이브러리를 사용하여 통신합니다. \n\n하지만 가끔 가다 간헐적으로 규칙성 없이 timeout이 발생하는 문제가 있었습니다. 이를 해결하기 위해 Notion Client 라이브러리를 상속한 NotionClientWithRetry 클래스를 생성하고, 세가지 메소드에 대해 재시도 로직을 하는 공통 메소드를 생성하고 코드상에서 Client 직접 호출 대신 해당 메소드를 이용해 Client를 호출하도록 수정해주었습니다. \n\n<details>\n<summary>NotionClientWithRetry.ts</summary>\n\n```typescript\nimport { Client } from '@notionhq/client';\n\nexport class NotionClientWithRetry extends Client {\n    private maxRetries: number;\n    private retryDelay: number;\n\n    constructor(options: {\n        auth: string;\n        maxRetries?: number;\n        retryDelay?: number;\n    }) {\n        super({ auth: options.auth });\n        this.maxRetries = options.maxRetries || 3;\n        this.retryDelay = options.retryDelay || 1000; // 기본 1초 대기\n    }\n\n    private async retryAPI<T>(\n        operation: () => Promise<T>,\n        retries: number = this.maxRetries,\n    ): Promise<T> {\n        try {\n            return await operation();\n        } catch (error) {\n            if (retries <= 0) throw error;\n            console.log(\n                `[retryAPI] ${this.maxRetries - retries} retrying.....`,\n            );\n            await new Promise((resolve) =>\n                setTimeout(resolve, this.retryDelay),\n            );\n            return this.retryAPI(operation, retries - 1);\n        }\n    }\n\n    public async blocksRetrieve(args: { block_id: string }) {\n        return this.retryAPI(() => this.blocks.retrieve(args));\n    }\n\n    public async databasesQuery(args: {\n        database_id: string;\n        filter?: any;\n        sorts?: any;\n    }) {\n        return this.retryAPI(() => this.databases.query(args));\n    }\n\n    public async pagesRetrieve(args: { page_id: string }) {\n        return this.retryAPI(() => this.pages.retrieve(args));\n    }\n}\n```\n\n\n</details>\n\n따로 사용자가 설정할 수 있지만 기본적으로는 1초에 한 번씩 최대 3번까지 재시도를 하도록 해주었습니다. \n\n해당 코드를 적용한 이후로는 API 호출에 실패해서 프로그램이 중단되는 일은 겪지 못했습니다. \n\n## 블로그 자동 배포가 포함된 워크플로우 작성\n\n블로그 자동 배포 워크플로우는 사실 [github.io](http://github.io/) 쪽 레포에 이미 구현이 되어 있어 추가 구현이 필요 없었으며, 깃허브 블로그 테마마다 배포 방식이 다를 수 있어 사실 꼭 구현이 필수인 영역은 아니었습니다. \n\n하지만 깃허브 액션에 대해 이해도가 깊어질 수 있고, 또 프로젝트의 목적 상 가능하면 하나로 합쳐지는 예시를 제공하는 것도 좋아보여 공부 겸 진행했습니다. \n\n\n\n"},{"excerpt":"메타데이터 구조 확정 지난시간, 메타데이터를 관리하기 위한 클래스 초안을 작성했다. 시간이 없어서 급하게 만들었는데, 제대로 매개변수 이름을 지어주고 메타데이터 구조를 확정지었다.  크게 변하지는 않았고, 이름을 정해주는 수준이었다.  그리고 추가로 저장된 디렉토리 명을 사용해 포스팅된 마크다운 디렉토리를 삭제하는 메소드를 추가해주었다.  Posting …","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅10/"},"frontmatter":{"date":"February 03, 2024","title":"NotionAPI를 활용한 자동 포스팅(10)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"## 메타데이터 구조 확정\n\n지난시간, [메타데이터를 관리하기 위한 클래스 초안](https://sharknia.github.io/NotionAPI를-활용한-자동-포스팅9)을 작성했다. 시간이 없어서 급하게 만들었는데, 제대로 매개변수 이름을 지어주고 메타데이터 구조를 확정지었다. \n\n크게 변하지는 않았고, 이름을 정해주는 수준이었다. \n\n그리고 추가로 저장된 디렉토리 명을 사용해 포스팅된 마크다운 디렉토리를 삭제하는 메소드를 추가해주었다. \n\n<details>\n<summary>수정된 코드 전문</summary>\n\n```typescript\nimport { promises as fs } from 'fs';\nimport { join } from 'path';\nimport { EnvConfig } from './envConfig';\n\nconst METADATA_FILE_PATH = './pageMetadata.json';\n\ninterface PageMetadata {\n    path: string;\n}\n\ninterface Metadata {\n    [pageIdx: string]: PageMetadata;\n}\n\nexport class MetadataManager {\n    private static instance: MetadataManager;\n    private metadata: Metadata | null;\n    private envConfig: EnvConfig;\n\n    private constructor() {\n        this.metadata = null;\n        this.envConfig = EnvConfig.create();\n    }\n\n    /**\n     * 인스턴스를 반환하는 메서드입니다.\n     * @returns {MetadataManager} MetadataManager 인스턴스\n     */\n    public static async getInstance(): Promise<MetadataManager> {\n        if (!this.instance) {\n            this.instance = new MetadataManager();\n            await this.instance.loadMetadata();\n        }\n        return this.instance;\n    }\n\n    /**\n     * 메타데이터를 로드합니다.\n     * @returns {Promise<void>} Promise 객체\n     */\n    public async loadMetadata(): Promise<void> {\n        try {\n            const data = await fs.readFile(METADATA_FILE_PATH, 'utf8');\n            this.metadata = JSON.parse(data) as Metadata;\n            console.log('메타데이터 파일 읽기 성공:', this.metadata);\n        } catch (error) {\n            console.error('메타데이터 파일 읽기 오류:', error);\n            this.metadata = {};\n        }\n    }\n\n    /**\n     * 메타데이터를 반환합니다.\n     * @returns 메타데이터 객체 또는 null\n     */\n    public getMetadata(): Metadata | null {\n        return this.metadata;\n    }\n\n    /**\n     * 페이지 메타데이터를 업데이트합니다.\n     *\n     * @param pageIdx 페이지 식별자\n     * @param pageData 페이지 메타데이터\n     */\n    public updatePageMetadata(pageIdx: string, pageData: PageMetadata): void {\n        if (!this.metadata) {\n            this.metadata = {};\n        }\n        this.metadata[pageIdx] = pageData;\n        console.log(`메타 데이터 업데이트 [${pageIdx}]`);\n    }\n\n    /**\n     * 페이지 메타데이터를 삭제합니다.\n     * @param pageIdx 삭제할 페이지의 ID\n     */\n    public deletePageMetadata(pageIdx: string): void {\n        if (this.metadata && this.metadata[pageIdx]) {\n            delete this.metadata[pageIdx];\n        }\n    }\n\n    /**\n     * 메타데이터를 파일에 저장합니다.\n     * @returns 메타데이터가 성공적으로 저장될 때 해결되는 Promise입니다.\n     */\n    public async saveMetadata(): Promise<void> {\n        if (this.metadata) {\n            try {\n                await fs.writeFile(\n                    METADATA_FILE_PATH,\n                    JSON.stringify(this.metadata, null, 2),\n                );\n                console;\n            } catch (error) {\n                console.error('메타데이터 파일 저장 오류:', error);\n            }\n        }\n    }\n\n    /**\n     * 지정된 페이지 인덱스에 대한 메타데이터를 삭제합니다.\n     * @param pageIdx 삭제할 페이지 인덱스\n     * @returns 삭제 작업이 완료된 후에는 아무 값도 반환하지 않습니다.\n     */\n    public async deleteFromMetadata(pageIdx: string): Promise<void> {\n        if (this.metadata && this.metadata[pageIdx]) {\n            let dir = join(\n                this.envConfig.saveDir!,\n                this.metadata[pageIdx].path,\n            );\n            try {\n                await fs.unlink(dir);\n                console.log('파일 삭제 성공:', dir);\n            } catch (error) {\n                console.error('파일 삭제 오류:', error);\n            }\n        }\n    }\n}\n```\n\n\n</details>\n\n## Posting 클래스 수정\n\n메타데이터 정보는 모아두었다가, 프로그램 종료시에 한 번에 파일에 저장하도록 종료되는 부분에서 saveMetadata 메소드를 호출해주었다. \n\n<details>\n<summary>수정된 Posting 클래스 코드</summary>\n\n```typescript\npublic async start(): Promise<void> {\n        console.log('[posting.ts] start!');\n        try {\n            this.metadataManager = await MetadataManager.getInstance();\n            this.EnvConfig = EnvConfig.create();\n            const notionkey: string = this.EnvConfig.notionKey || '';\n            const databaseid: string = this.EnvConfig.databaseid || '';\n            this.notionApi = await NotionAPI.create(notionkey);\n            this.dbInstance = await DataBase.create(databaseid, '');\n\n            console.log('[posting.ts] page 순회 시작');\n            for (const item of this.dbInstance.pageIds) {\n                const page: Page = await Page.create(item.pageId);\n            }\n            this.metadataManager.saveMetadata();\n        } catch (error) {\n            console.error('Error creating database instance:', error);\n        }\n    }\n```\n\n\n\n\n</details>\n\n## Database 클래스 수정\n\n기존에는 상태값이 Ready 인 것만 쿼리하고 있었는데, 이제는 삭제도 진행해야 하므로 상태가 ToBeDeleted인 것도 쿼리하도록 수정해주었다. \n\n<details>\n<summary>수정된 코드</summary>\n\n```typescript\npublic async queryDatabase(): Promise<QueryDatabaseResponse> {\n        try {\n            const response: QueryDatabaseResponse =\n                await this.notion.databasesQuery({\n                    database_id: this.databaseId,\n                    filter: {\n                        or: [\n                            {\n                                property: '상태',\n                                select: {\n                                    equals: PageStatus.Ready,\n                                },\n                            },\n                            {\n                                property: '상태',\n                                select: {\n                                    equals: PageStatus.ToBeDeleted,\n                                },\n                            },\n                        ],\n                    },\n                });\n            // pageId 리스트 업데이트\n            this.pageIds = response.results.map((page) => ({\n                pageId: page.id,\n            }));\n            return response;\n        } catch (error) {\n            console.error('Error querying the database:', error);\n            throw error;\n        }\n    }\n```\n\n\n</details>\n\n## Page 클래스 수정\n\n### init() 메소드 수정\n\n- pageIdx 속성을 추가하고 init() 메소드에서 해당 속성을 초기화해준다.\n\n### create() 메소드 수정\n\n- create 메소드에서 상태값에 따른 분기처리를 추가했다. \n\n    기존에는 ready상태만 있었으므로 일관된 처리를 진행하면 됐지만, 이제는 삭제 대기 상태가 추가되었으므로 상태값에 따른 분기처리를 추가하고 각각 다르게 작동하도록 해주었다. \n\n    1. Ready, ToBeDeleted 두 상태 모두 일단 해당하는 파일을 삭제한다. Ready 인데도 삭제하는 이유는 타이틀이 바뀐 경우에는 파일을 삭제해야 하기 때문이다. \n\n    1. Ready 상태인 경우에는 기존의 로직을 실행하고, Metadata를 업데이트 해준다. \n\n    1. ToBeDeleted 상태인 경우에는 메타데이터에서 해당하는 내용을 삭제한다. \n\n<details>\n<summary>create() 메소드 안쪽 수정된 내용 코드</summary>\n\n    ```typescript\n    public static async create(pageId: string) {\n            const notionApi: NotionAPI = await NotionAPI.create();\n            const page: Page = new Page(pageId, notionApi.client);\n            MarkdownConverter.imageCounter = 0;\n            await page.init(page);\n            const status = page.properties!['상태'];\n            console.log(\n                `[page.ts] start - pageTitle : (${status})${page.pageTitle}`,\n            );\n            // 저장하기 전에도 기존 파일을 삭제한다. 타이틀이 달라진 update 일 수 있기 때문이다.\n            await page.metadataManager?.deleteFromMetadata(page.pageIdx!);\n            if (status === PageStatus.ToBeDeleted) {\n                // 페이지가 삭제될 예정인 경우\n                await page.metadataManager?.deletePageMetadata(page.pageIdx!);\n                await page.updatePageStatus(PageStatus.Deleted);\n                return page;\n            } else if (status === PageStatus.Ready) {\n                // 포스팅이 준비된 경우\n                page.contentMarkdown = await page.fetchAndProcessBlocks();\n                await page.printMarkDown();\n                await page.metadataManager?.updatePageMetadata(page.pageIdx!, {\n                    path: page.pageUrl!,\n                });\n                await page.updatePageStatus(PageStatus.Updated);\n                return page;\n            } else {\n                console.error(`[page.ts] start - status : ${status}`);\n                throw new Error(`[page.ts] start - status : ${status}`);\n            }\n        }\n    ```\n\n\n</details>\n\n## 기타 변경 내용\n\n- 열거형 클래스를 새로 만들어 상태값 정의를 미리 해주었다. \n\n- 또한, 메타데이터 파일도 레포의 일부인데 깃허브 액션 워크플로우 작동 후 이 파일도 수정될 것이므로 한 번 더 커밋/푸시가 필요하다. 해당 내용을 워크플로우의 Run Script 바로 다음에 추가해주었다. \n\n<details>\n<summary>추가된 워크플로우</summary>\n\n    ```yaml\n    - name: Commit and Push Changes to Current Repository\n          run: |\n            git config --global user.name 'name'\n            git config --global user.email 'mail'\n            git add .\n            git commit -m \"Update contents\" || echo \"No changes to commit in current repo\"\n            git push\n    ```\n\n\n</details>\n\n## 주의할 점\n\n일단, 로컬에서 실행 시 권한 문제로 파일 삭제가 제대로 되지 않았다. 그래서 깃허브 액션에서도 파일 권한 문제가 생길 우려가 있어 권한을 조정하는 방법이 있나 찾아보았는데, 깃허브 액션에서는 외부를 컨트롤 하려는게 아닌 이상 별도의 권한 문제가 발생하지 않는다고 한다. \n\n따라서 로컬에서 발생하는 문제는 따로 수정하지 않았다. \n\n\n\n"},{"excerpt":"타입스크립트에서  을 사용해 특정 값들의 집합을 정의하고 각 멤버에 문자열 값을 할당할 수 있다. 이는 열거형이라고 하며 관련된 상수 값들의 집합에 이름을 부여하여 코드의 가독성을 높이고 오류 가능성을 줄여준다.  임의의 열거형을 정의하면 다음과 같다.  이렇게 정의하면 PageStatus 타입의 변수를 사용할 때 PageStatus.Deleted 로 사…","fields":{"slug":"/Typescript의-열거형/"},"frontmatter":{"date":"February 03, 2024","title":"Typescript의 열거형","tags":["Typescript"]},"rawMarkdownBody":"타입스크립트에서 `enum` 을 사용해 특정 값들의 집합을 정의하고 각 멤버에 문자열 값을 할당할 수 있다. 이는 열거형이라고 하며 관련된 상수 값들의 집합에 이름을 부여하여 코드의 가독성을 높이고 오류 가능성을 줄여준다. \n\n임의의 열거형을 정의하면 다음과 같다. \n\n```typescript\nenum PageStatus {\n    Deleted = \"deleted\",\n    Writing = \"writing\"\n}\n```\n\n이렇게 정의하면 PageStatus 타입의 변수를 사용할 때 PageStatus.Deleted 로 사용해서 열거형 멤버게 접근할 수 있다. \n\n이렇게 얻어진 값은 각각에 할당된 문자열과 같다. \n\n사용법 예시 코드는 다음과 같다. \n\n```typescript\nlet currentPageStatus: PageStatus = PageStatus.Writing;\n\nif (currentPageStatus === PageStatus.Deleted) {\n    console.log(\"The page is deleted.\");\n} else if (currentPageStatus === PageStatus.Writing) {\n    console.log(\"The page is in writing status.\");\n}\n```\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n만약, 열거형을 정의할 때 값을 명시하지 않으면 열거형의 값은 기본적으로 0에서 시작해 순차적으로 증가한다.  예를 들어 다음과 같다. \n\n```typescript\nenum PageStatus {\n    Deleted, // 0\n    Writing  // 1\n}\n```\n\n물론 임의로 숫자를 지정할 수도 있다. \n\n```typescript\nenum StatusCode {\n    Success = 200,\n    NotFound = 404,\n    ServerError = 500\n}\n```\n\n또한 숫자 열거형에서 일부 멤버에만 값을 지정하고 다른 멤버에는 값을 지정하지 않는다면 타입스크립트는 자동으로 이전 멤버의 값에서 1을 더해 계산한다. \n\n```typescript\nenum Example {\n    Start = 1,\n    Middle, // 값이 2로 자동 할당됩니다.\n    End // 값이 3으로 자동 할당됩니다.\n}\n```\n\n\n\n"},{"excerpt":"지난시간 지난시간, 신나게 자동 배포를 만들었다.  이제 새로 생성된 문서들에 대해서 자동으로 한시간마다 배포가 되어 글이 포스팅된다.  근데, 이런 경우 글을 어떻게 삭제하지? 현재 타이틀이 키 값인데 타이틀 명이 바뀌면 내용은 똑같은데 제목만 다른 글이 두 개?  그렇다, 삭제 및 수정 기능이 구현되어야 하는 것이다.  설계 어떤걸 키 값으로 해야 하…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅9/"},"frontmatter":{"date":"February 02, 2024","title":"NotionAPI를 활용한 자동 포스팅(9)","tags":["Blogging","Typescript","Hobby","Notion-API"]},"rawMarkdownBody":"## 지난시간\n\n지난시간, 신나게 자동 배포를 만들었다. \n\n이제 새로 생성된 문서들에 대해서 자동으로 한시간마다 배포가 되어 글이 포스팅된다. \n\n\n\n**근데, 이런 경우 글을 어떻게 삭제하지?**\n\n**현재 타이틀이 키 값인데 타이틀 명이 바뀌면 내용은 똑같은데 제목만 다른 글이 두 개?** \n\n\n\n그렇다, 삭제 및 수정 기능이 구현되어야 하는 것이다. \n\n## 설계\n\n어떤걸 키 값으로 해야 하나 고민이 많았는데, 답은 간단했다. 노션에 ID를 추가하는 기능이 있었다. 이렇게 되면, 내 노션 블로그를 위한 필수 속성은 타이틀, 상태, ID 세 가지가 된다. 이런게 너무 많이 생기는걸 원하지 않지만 어쩔 수 없는 부분이라고 생각이 든다. \n\n계획은 다음과 같다. \n\n- 노션 글들에 ID 속성을 추가해 키값으로 삼는다. \n\n- 상태값에 ToBeDeleted, Deleted를 추가한다. \n\n- 키 값 - 저장된 디렉토리가 매핑된 메타데이터 json 파일을 생성한다. \n\n- ToBeDeleted를 쿼리해와서, 매핑 테이블을 참조해 데이터를 삭제 후 매핑 정보도 삭제하고 Deleted 상태로 바꿔준다. \n\n- Ready를 쿼리해와서 매핑 테이블을 참조해 기존 데이터를 삭제 후 매핑 정보를 갱신하고 마크다운을 다시 저장한다. \n\n이렇게 하면 수정과 삭제에 모두 대응이 가능해보인다!\n\n## 구현\n\n우선, 전역적으로 메타데이터를 읽어오고, 메모리에 데이터를 저장한 다음 프로그램 종료 직전에 메타 데이터를 한 번에 수정해주는 역할을 할 클래스를 생성했다. \n\n<details>\n<summary>metadataManager.ts</summary>\n\n```typescript\nimport { promises as fs } from 'fs';\n\nconst METADATA_FILE_PATH = './pageMetadata.json';\n\ninterface PageMetadata {\n    url: string;\n}\n\ninterface Metadata {\n    [pageId: string]: PageMetadata;\n}\n\nexport class MetadataManager {\n    private static instance: MetadataManager;\n    private metadata: Metadata | null;\n\n    private constructor() {\n        this.metadata = null;\n    }\n\n    /**\n     * 인스턴스를 반환하는 메서드입니다.\n     * @returns {MetadataManager} MetadataManager 인스턴스\n     */\n    public static getInstance(): MetadataManager {\n        if (!this.instance) {\n            this.instance = new MetadataManager();\n        }\n        return this.instance;\n    }\n\n    /**\n     * 메타데이터를 로드합니다.\n     * @returns {Promise<void>} Promise 객체\n     */\n    public async loadMetadata(): Promise<void> {\n        try {\n            const data = await fs.readFile(METADATA_FILE_PATH, 'utf8');\n            this.metadata = JSON.parse(data) as Metadata;\n        } catch (error) {\n            console.error('메타데이터 파일 읽기 오류:', error);\n            this.metadata = {};\n        }\n    }\n\n    /**\n     * 메타데이터를 반환합니다.\n     * @returns 메타데이터 객체 또는 null\n     */\n    public getMetadata(): Metadata | null {\n        return this.metadata;\n    }\n\n    /**\n     * 페이지 메타데이터를 업데이트합니다.\n     *\n     * @param pageId 페이지 식별자\n     * @param pageData 페이지 메타데이터\n     */\n    public updatePageMetadata(pageId: string, pageData: PageMetadata): void {\n        if (!this.metadata) {\n            this.metadata = {};\n        }\n        this.metadata[pageId] = pageData;\n    }\n\n    /**\n     * 페이지 메타데이터를 삭제합니다.\n     * @param pageId 삭제할 페이지의 ID\n     */\n    public deletePageMetadata(pageId: string): void {\n        if (this.metadata && this.metadata[pageId]) {\n            delete this.metadata[pageId];\n        }\n    }\n\n    /**\n     * 메타데이터를 파일에 저장합니다.\n     * @returns 메타데이터가 성공적으로 저장될 때 해결되는 Promise입니다.\n     */\n    public async saveMetadata(): Promise<void> {\n        if (this.metadata) {\n            try {\n                await fs.writeFile(\n                    METADATA_FILE_PATH,\n                    JSON.stringify(this.metadata, null, 2),\n                );\n            } catch (error) {\n                console.error('메타데이터 파일 저장 오류:', error);\n            }\n        }\n    }\n}\n```\n\n\n</details>\n\n이 클래스는 프로그램 시작과 동시에 초기화되며, 끝날 때 메타데이터 파일을 갱신하는 역할을 할 것이다. \n\n정확한 매개변수는 아직 정하지 않았으며, 초안만 짜뒀다. \n\n이제 내일, 이 클래스를 명확히 정의하고 이 클래스를 사용해 수정/삭제를 구현하면 된다. \n\n"},{"excerpt":"목표 단계적으로 GitHub Action을 사용해 블로그 자동 배포를 하려고 한다.  일단, 구상한 깃허브 액션에서 실행할 플로우는 다음과 같다.  구상 블로그 레포지토리를 받아온다.  내 프로그램을 돌려 블로그 레포지토리를 업데이트한다.  블로그 레포지토리를 커밋/푸시한다.  해당 작업은 특정 시간에, 또는 특정 시간마다 이뤄져야 한다.  구현 Noti…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅8/"},"frontmatter":{"date":"February 01, 2024","title":"NotionAPI를 활용한 자동 포스팅(8)","tags":["Notion-API","Blogging","Hobby","GitHub","Typescript"]},"rawMarkdownBody":"## 목표\n\n단계적으로 GitHub Action을 사용해 블로그 자동 배포를 하려고 한다. \n\n일단, 구상한 깃허브 액션에서 실행할 플로우는 다음과 같다. \n\n## 구상\n\n1. 블로그 레포지토리를 받아온다. \n\n1. 내 프로그램을 돌려 블로그 레포지토리를 업데이트한다. \n\n1. 블로그 레포지토리를 커밋/푸시한다. \n\n1. 해당 작업은 특정 시간에, 또는 특정 시간마다 이뤄져야 한다. \n\n## 구현\n\n### Notion to Markdown 프로그램 실행\n\n#### 워크플로우 파일 생성\n\n단계적으로 구현해보려고 한다. 일단은 내 프로그램을 돌려보자. \n\n일단 대충 다음과 같이 초안을 짜주었다. \n\n```yaml\n# 이름을 정의한다. \nname: Run TypeScript\n# 실행 시기입니다. 이번엔 push, pull_request 될 때 이다. \non: [push, pull_request]\n\n# 깃허브 액션에서 실행할 작업을 정의하는 섹션이다.\njobs:\n\t# 단일 작업의 이름을 정의합니다. 이 경우에는 이름이 run 이다.\n  run:\n\t\t# 작업이 어떤 환경에서 실행될지 정의한다. \n    runs-on: ubuntu-latest\n\t\t# 각 작업에서 실행할 순차적인 단계들을 정의한다. \n    steps:\n\t\t# uses : 특정 액션을 사용할 때 사용한다.\n\t\t# name : step의 이름을 정의한다.\n\t\t# with : user로 지정된 액션에 추가적인 매개변수를 전달한다. \n\t\t# run : 쉘 명령어를 실행한다. \n\t\t# env : 환경변수를 설정한다. \n\t\t# if : 조건문\n    - uses: actions/checkout@v2\n    - name: Use Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18'\n\n    - name: Install Dependencies\n      run: npm install\n\n    - name: Install TypeScript\n      run: npm install --save-dev typescript\n\n    - name: Build\n      run: npx tsc\n\n    - name: Run Script\n      run: node ./index.js\n```\n\n이 파일을 프로젝트의 `.github/workflows/main.yml` 로 저장해준다.\n\n그리고 커밋-푸시를 하면 \n\n  \n\n![](image1.png)\n이렇게 실행이 된다. 그리고\n\n![](image2.png)\nenv 파일이 없기 때문에 바로 설정값 오류가 났다. \n\n#### Github secret을 이용한 환경변수 설정\n\n환경변수 설정을 워크플로우 파일 내에서 직접 설정하거나, 리포지토리 설정 또는 깃허브 환경에서 설정할 수 있다. 환경변수가 노출되지 않기를 원하지만 깃허브 전체에 적용될 필요는 없으므로 리포지토리 Setting에서 설정해주려고 한다. \n\n![](image3.png)\nSecrets는 로그에도 노출되지 않으며, variables는 로그에서 노출이 된다. 적절하게 선택하자. \n\nNOTION_KEY와 NOTION_DATABASE_ID는 시크릿에, 나머지는 변수들에 저장해주었다. \n\n다음과 같이 워크플로우 파일을 수정해주면 정상 작동된다. \n\n```yaml\nname: Run TypeScript\n\non: [push, pull_request]\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Use Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18'\n\n    - name: Install Dependencies\n      run: npm install\n\n    - name: Install TypeScript\n      run: npm install --save-dev typescript\n\n    - name: Build\n      run: npx tsc\n\n    - name: Run Script\n      run: node ./index.js\n\t\t\t# 환경변수 불러오기 추가 \n      env:\n        NOTION_KEY: ${{ secrets.NOTION_KEY }}\n        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}\n        BLOG_URL: ${{ var.BLOG_URL }}\n        SAVE_DIR: ${{ var.SAVE_DIR }}\n        SAVE_SUB_DIR: ${{ var.SAVE_SUB_DIR }}\n```\n\n### 블로그 Repository 추가 \n\n1, 3번 작업을 위해 블로그 레포를 체크아웃 하는 과정을 추가하자. \n\n이 전에 깃 권한을 얻기 위해 토큰을 발급해주었다. Setting - Develper 설정에 가서 토큰을 발급해주자. classic token으로 발행하고, 적당히 권한을 수정해주었다. \n\n그리고 해당 토큰도 레포의 Secret에 추가해주었다. \n\n그리고 나서 워크플로우를 아래와 같이 수정해주었다. \n\n```yaml\nname: Run TypeScript\n\non: [push, pull_request]\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n\t\t# 블로그 레포 체크아웃\n    - name: Checkout Blog Repository\n      uses: actions/checkout@v2\n      with:\n        repository: 'Sharknia/Sharknia.github.io'\n        token: ${{ secrets.GITTOKEN }}\n        path: 'blog-repo'\n\n    - name: Use Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18'\n\n    - name: Install Dependencies\n      run: npm install\n\n    - name: Install TypeScript\n      run: npm install --save-dev typescript\n\n    - name: Build\n      run: npx tsc\n\n    - name: Run Script\n      run: node ./index.js\n      env:\n        NOTION_KEY: ${{ secrets.NOTION_KEY }}\n        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}\n        BLOG_URL: ${{ var.BLOG_URL }}\n        SAVE_DIR: ${{ var.SAVE_DIR }}\n        SAVE_SUB_DIR: ${{ var.SAVE_SUB_DIR }}\n\t\t# 블로그 레포 커밋/푸쉬\n    - name: Set Git remote URL with token and Commit and Push Changes\n\t\t\t# 다른 저장소이므로 git remote set-url을 실행한다. \n      run: |\n        cd blog-repo\n        git remote set-url origin https://${{ secrets.GITTOKEN }}@github.com/name/name.github.io\n        git config user.name 'name'\n        git config user.email 'mail'\n        git add .\n        git commit -m \"Update blog contents\" || echo \"No changes to commit\"\n        git push\n```\n\n여기까지 해두면, 제대로 깃허브 블로그에 새로운 글이 커밋/푸쉬 되었다!\n\n## 마무리\n\n마지막으로, 테스트를 종료하고 자동 배포가 매 시각 되게 하기 위해 on 부분을 다음과 같이 수정해주었다. \n\n```yaml\non: \n  push:\n  pull_request:\n  schedule:\n    - cron: '0 * * * *'\n```\n\n### 다음 계획\n\n이 방법으로는 삭제가 되지 않는다는 사실을 깨달았다. \n\n당분간은 수동삭제를 ~~ㅠㅠ~~ 하기로 하고 삭제를 어떻게 할 것인지에 대해서는 구상을 해두었다. \n\n다음 시간은 삭제의 구현이다. \n\n\n\n"},{"excerpt":"editor.quickSuggestions 설정은 VSCode에서 코드를 작성하는 동안 자동완성을 어떻게 표시할지를 결정한다.  다음 세가지의 옵션이 있다.  string 문자열 내에서 자동 완성 제안을 활성화/비활성화 한다.  comments 주석 내에서 자동 완성 제안을 활성화/비활성화 한다.  other 코드(문자열이나 주석이 아닌 부분) 내에서 자…","fields":{"slug":"/vscode-quick-Suggestions/"},"frontmatter":{"date":"January 31, 2024","title":"vscode-quick Suggestions","tags":["VSCode"]},"rawMarkdownBody":"editor.quickSuggestions 설정은 VSCode에서 코드를 작성하는 동안 자동완성을 어떻게 표시할지를 결정한다. \n\n다음 세가지의 옵션이 있다. \n\n- string\n\n    문자열 내에서 자동 완성 제안을 활성화/비활성화 한다. \n\n- comments\n\n    주석 내에서 자동 완성 제안을 활성화/비활성화 한다. \n\n- other\n\n    코드(문자열이나 주석이 아닌 부분) 내에서 자동 완성 제안을 활성화/비활성화 한다. \n\nVSCode의 설정을 json으로 열어 다음의 내용을 입력하면 된다. \n\n```json\n\"editor.quickSuggestions\": {\n    \"strings\": true,\n    \"comments\": true,\n    \"other\": true\n}\n```\n\n\n\n"},{"excerpt":"퇴근을 하고 고치다보니 시간이 부족하다.. 어쩔 수 없다.  돌아보기 어제 상당수의 디버깅을 진행하고 기존 블로그에서 새 블로그로 글을 이전했다. 깃허브 블로그에 업로드까지 마치고, 오늘 다시 보니 짜잘짜잘한 오류가 다시 또(!!) 발견되었다.  유지보수 Callout 수정 내가 선택한 블로그의 기능인지, 원래 마크다운 기능인지 모르겠다. 굳이 태그를 넣…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅7/"},"frontmatter":{"date":"January 30, 2024","title":"NotionAPI를 활용한 자동 포스팅(7)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"퇴근을 하고 고치다보니 시간이 부족하다.. 어쩔 수 없다. \n\n## 돌아보기\n\n어제 상당수의 디버깅을 진행하고 기존 블로그에서 새 블로그로 글을 이전했다. 깃허브 블로그에 업로드까지 마치고, 오늘 다시 보니 짜잘짜잘한 오류가 다시 또(!!) 발견되었다. \n\n## 유지보수\n\n### Callout 수정\n\n내가 선택한 블로그의 기능인지, 원래 마크다운 기능인지 모르겠다. 굳이 태그를 넣을 필요가 없어서, 태그를 제거해주었다. \n\n### 이미지 캡션 수정\n\n이미지 캡션도 마찬가지였다. 혹시 하는 노파심에 따로 p태그를 넣어주고 있었는데, 아무 문제 없이 정상 작동 되었다. 아무래도 깃허브 블로그도 마크다운 기반으로 움직이다보니 마크다운에서 좀 잘 나오면 잘 이쁘게 지원하는 것 같다. \n\n### 넘버링 숫자 안올라감 & 들여쓰기 문제\n\n넘버링이 모두 1로 나오고, 글머리 기호와 넘버링 리스트를 제외한 나머지 부분에 들여쓰기를 넣지 않았다는 사실을 깨달았다. \n\n넘버링부터 고치려다가 문득 들여쓰기가 제대로 되면 숫자가 제대로 나오지 않을까? 해서 들여쓰기부터 수정했는데, 정답이었다. \n\n들여쓰기를 고치고 나서 마크다운에 1. 이라고 숫자가 고정되어 있어도 제대로 숫자가 증가되면서 출력된다. \n\n### 일부 부분 [objcet Promise] 출력\n\n이건 await을 까먹은거다. 추가해주었다. \n\n### 이미지 깨짐\n\n어제 디버깅을 하면서 이미지 카운터를 static에서 인스턴스로 변경해주었는데, 어제 내가 멍청하다고 썼는데 사실은 오늘의 내가 멍청한 것이었다. \n\n이미지 카운터를 컨버터에 넣어놓을거면 거기서 인스턴스 변수로 선언하면 안됐다. 이미지가 거의 모든 경우에 항상 1번으로 저장된다.. \n\n페이지에 이미지 카운터 변수를 심고 블록-컨버터까지 내려줄 생각을 하니 아득해졌는데 생각해보니 그냥 컨버터 클래스에 넣어놓고 static 변수로 변경한 다음 페이지를 새로 만들 때마다 해당 값을 초기화 해주면 되지 않나 싶어서 그렇게 구현 해보려고 한다. \n\n### 언더바 또는 *가 포함된 경우 스타일 깨짐\n\n예를 들어 _db_connection_serializable 라고 입력된 텍스트가 *db*connection*serializable* 로 마음대로 스타일이 변경되고 있었다. 이를 예방하기 위해 일단 \n\n```typescript\n// 언더스코어 이스케이프 처리 함수\n    private escapeMarkdownUnderscores(text: string): string {\n        return text.replace(/(\\w)_(\\w)/g, '$1\\\\_$2');\n    }\n```\n\n해당 메소드를 사용해 언더바 앞에 백슬래시를 붙여주었다. \n\n잘 될지는 배포 후 테스트를 해봐야 한다. \n\n## 신규 기능\n\n### 테이블 지원\n\n이제 테이블을 지원한다.. 지금 고치느라 기록할 시간이 없는데 많은 것을 고쳤다.. \n\n근본적인 설계가 아쉽다. 굳이 block을 별도의 클래스로 뺄 필요도 없었을 것 같고, MarkdownConverter 클래스가 굳이 인스턴스 메소드가 이렇게 많아야 할 이유도 모르겠다. \n\n여유가 된다면 전체적으로 코드 리팩토링을 (다시 만드는 수준으로) 진행해야 할 것 같다. \n\n\n\n"},{"excerpt":"서론 동시성 제어 문제를 해결하기 위한 여러가지 방법을 고민 끝에 PostgreSQL Advisory Locks를 사용한 방법으로 구현하기로 했다.  구현을 하면서 겪은 과정을 여기에 기록한다.  구현 목표 현재 회사에서는 FastAPI와 SqlAlchemy 2.0, PostgreSQL을 사용하고 있다. 아주 짧은 텀으로 중복으로 온 API 요청에 대해 …","fields":{"slug":"/PostgreSQL-Advisory-Locks-트랜잭션-레벨에서-구현/"},"frontmatter":{"date":"January 30, 2024","title":"PostgreSQL Advisory Locks 트랜잭션 레벨에서 구현","tags":["Postgresql","SqlAlchemy","Python","DataBase","Work"]},"rawMarkdownBody":"## 서론\n\n[동시성 제어 문제를 해결하기 위한 여러가지 방법을 고민](https://sharknia.github.io/동시성-제어문제-해결) 끝에 PostgreSQL Advisory Locks를 사용한 방법으로 구현하기로 했다. \n\n구현을 하면서 겪은 과정을 여기에 기록한다. \n\n## 구현 목표\n\n현재 회사에서는 FastAPI와 SqlAlchemy 2.0, PostgreSQL을 사용하고 있다. 아주 짧은 텀으로 중복으로 온 API 요청에 대해 중복 구매가 되지 않도록 동시성 제어를 하려고 한다. \n\n유저의 아이디 값 또는 API 고유의 값 + 유저의 아이디 값을 사용해 트랜잭션 레벨에서 PostgreSQL Advisory Locks를 생성하고, 동일한 유저가 PostgreSQL Advisory Locks에 걸린 경우에는 중복 요청이라고 판단하고 두번째 요청은 중단 하거나, 또는 반드시 유효성 검사를 하도록 해 중복 구매가 되는 일이 없게 사전에 차단하려고 한다. \n\n## 구현 사전단계\n\n[공식문서](https://www.postgresql.org/docs/9.1/functions-admin.html)를 살펴보니 트랜잭션 레벨에서의 락을 위해 내가 사용할 수 있는 함수는 두 가지가 있었다. 트랜잭션 레벨의 락은 공통적으로 명시적인 언락이 불가능하다. \n\n아래의 두가지 함수는 모두 트랜잭션 레벨의 함수로 Advisory Lock을 획득할 수 있다는 점은 같지만, Advisory Lock을 바로 획득하지 못했을 때의 반응이 다르다. \n\n### pg\\_advisory\\_xact\\_lock\n\n지정된 키에 대한 Advisory Lock을 획득할 때까지 호출을 블로킹(대기)한다. 만약 다른 세션이 이미 해당 키에 대한 Advisory Lock을 갖고 있다면 그 Advisory Lock이 해제될 때까지 현재 세션에서의 처리가 중단된다. \n\nAdvisory Lock을 반드시 획득해야 해서 대기해야 할 경우에 적합하다.\n\n### pg\\_try\\_advisory\\_xact\\_lock\n\n이 함수는 즉시 Advisory Lock을 시도하고 성공하면 True, 실패하면 False를 반환한다. 이 함수는 잠금 획득을 위해 대기하지 않으며, 잠금이 이미 다른 세션에 의해 보유되고 있는 경우 즉시 실패한다.\n\nAdvisory Lock을 반드시 획득할 필요가 없고 대기하지 않고 다른 작업을 수행해야 할 경우에 적합하다. \n\n### 결정\n\n우리의 경우에는 쿠폰 중복 발급을 막으려고 하는 것이므로 만약 이미 잠금이 걸린 경우에는 굳이 나머지 DB 작업을 실행할 필요가 없다. 따라서 pg\\_try\\_advisory\\_xact\\_lock를 사용하는 것이 적절해보인다. \n\n## 구현\n\n따라서, 서비스 로직의 시작 지점에 다음의 코드를 넣어 락을 얻거나, 락을 얻지 못한 경우 해당 요청은 실행을 중단하려고 한다. \n\n```python\n# Advisory Lock 획득\n    result = await db.execute(f\"SELECT pg_try_advisory_xact_lock({lock})\")\n    # Advisory Lock을 획득하지 못한 경우에는 이미 선제 작업이 있는 것이므로 DB 작업을 계속할 필요가 없으므로\n    # 바로 중단한다.\n    if not result.scalar():\n        raise Exception(\"Unable to acquire lock\")\n```\n\nSQL 인젝션과 같은 보안 문제를 줄이고 쿼리 구성과 관련된 오류를 예방하기 위해 다음과 같이 코드를 수정했다. \n\n```python\n# Advisory Lock 획득\n    result = await db.execute(func.pg_try_advisory_xact_lock(100, lock))\n    # Advisory Lock을 획득하지 못한 경우에는 이미 선제 작업이 있는 것이므로 DB 작업을 계속할 필요가 없으므로\n    # 바로 중단한다.\n    if not result.scalar():\n        raise Exception(\"Unable to acquire lock\")\n```\n\n## 잠금 해제 지점은 어떻게 될까? \n\n막연히 트랜잭션이 커밋되거나 롤백될 때라고 알아둬도 충분할 것 같다(세션 주입 방식을 통해 에러가 발생하거나 요청이 정상정이 끝나지 않는 경우에 세션을 롤백하며, 정상적으로 요청이 정리된 경우에는 세션을 커밋하는 과정이 포함되어 있을 것이므로 충분할 것이다).\n\n다만, 트랜잭션 레벨에서의 락은 명시적으로 언락이 불가능하므로 정확한 언락 지점을 알아둬야 할 필요성이 있다고 느꼈다. \n\n- 트랜잭션 커밋 또는 롤백\n\n    함수 내부에서 명시적으로 await db.commit() 또는 await db.rollback()이 호출되는 경우에 같은 세션이라고 하더라도 언락된다. \n\n- 비동기 세션 컨텍스트 종료\n\n    AsyncSession 인스턴스가 컨텍스트 매니저(async with) 내에서 사용되고, 해당 컨텍스트 블록이 종료되는 경우\n\n- 예외 발생\n\n    함수 실행 중 예외가 발생하여 처리 흐름이 중단되는 경우. SQLAlchemy의 세션 관리는 트랜잭션이 커밋되지 않으면 자동으로 롤백을 수행하므로 사실상 트랜잭션이 롤백되는 시점과 일치한다. \n\n\n\n"},{"excerpt":"func란? SQL 함수를 생성하고 호출하는 데 사용되는 기능이다. SQL 표준 함수 뿐 아니라 데이터베이스 별 특정 함수까지 다룰 수 있다.  Sqlalchemy의 유연한 기능으로 다양한 데이터베이스 작업을 보다 Pythonic한 방식으로 작성할 수 있게 해준다.  func의 특징 함수 생성기 func는 데이터베이스의 내장 함수나 사용자 정의 함수를 파…","fields":{"slug":"/Sqlalchemy의-func/"},"frontmatter":{"date":"January 30, 2024","title":"Sqlalchemy의 func","tags":["SqlAlchemy","Python","DataBase"]},"rawMarkdownBody":"## func란? \n\nSQL 함수를 생성하고 호출하는 데 사용되는 기능이다. SQL 표준 함수 뿐 아니라 데이터베이스 별 특정 함수까지 다룰 수 있다. \n\nSqlalchemy의 유연한 기능으로 다양한 데이터베이스 작업을 보다 Pythonic한 방식으로 작성할 수 있게 해준다. \n\n## func의 특징\n\n- 함수 생성기\n\n    func는 데이터베이스의 내장 함수나 사용자 정의 함수를 파이썬 코드 내에서 호출하기 위한 함수 생성기이다. \n\n- 동적 생성\n\n    func 객체에 접근할 때 Python의 속성 접근 매커니즘을 통해 동적으로 SQL 함수 호출을 생성한다. 예를 들어 func.count() 는 SQL의 COUNT() 함수 호출을 생성한다. \n\n- 데이터베이스 독립성\n\n    다양한 데이터베이스 시스템에서 사용할 수 있는 표준 SQL 함수를 추상화한다. 따라서 특정 데이터베이스에 종속적이지 않은 코드를 작성할 수 있다. \n\n- 다양한 함수 지원\n\n    func는 거의 모든 종류의 SQL 함수를 호출할 수 있도록 지원한다. \n\n## `__getattr__` method\n\nfunc 객체의 __getattr__ 메소드는 특정 속성에 접근할 때 호출된다. 이 메소드는 동적으로 SQL 함수 호출을 생성한다. \n\n예를 들어, func.pg_try_advisory_lock에 접근하면 pg_try_advisory_lock 이름으로 _FunctionGenerator 객체를 생성한다. 이 객체는 최종적으로 SQL 쿼리 내에서 해당 함수 호출을 나타낸다. \n\n## 사용방법\n\n### 기본 사용\n\n함수 이름을 func 객체의 속성으로 접근하여 사용한다. (__getattr__ 메소드로 연결된다.)\n\n```python\nfrom sqlalchemy import func\n\n# COUNT 함수 호출 예시\nquery = select([func.count()]).select_from(my_table)\n```\n\n### 함수 매개변수 전달\n\n함수 호출 시 매개변수를 전달할 수 있다. \n\n```python\n# LENGTH 함수에 문자열 전달\nquery = select([func.length('some string')])\n```\n\n### 복잡한 표현식\n\n복잡한 SQL 표현식을 만드는 데도 사용할 수 있다. \n\n```python\n# 조건부 SQL 함수 호출\nquery = select([func.coalesce(my_table.column, 'default value')])\n```\n\n\n\n"},{"excerpt":"문제 가~끔 네트워크 문제 때문인지 클라이언트에서 같은 요청이 0.01초 미만의 간격으로 두 번씩 들어오는 경우가 있다. 사실상 요청이 동시에 들어오는 것과 같다. 대부분의 경우에는 문제가 되지 않지만, 쿠폰 구매 같은 민감한 요청에 대해서는 회사의 손해 또는 사용자의 불편과 예민하게 직결되므로 문제가 커질 수 있다.  따라서 해당 문제를 완벽히 해결하고…","fields":{"slug":"/동시성-제어문제-해결/"},"frontmatter":{"date":"January 30, 2024","title":"동시성 제어문제 해결","tags":["DataBase","Postgresql","Work"]},"rawMarkdownBody":"## 문제\n\n가~끔 네트워크 문제 때문인지 클라이언트에서 같은 요청이 0.01초 미만의 간격으로 두 번씩 들어오는 경우가 있다. 사실상 요청이 동시에 들어오는 것과 같다. 대부분의 경우에는 문제가 되지 않지만, 쿠폰 구매 같은 민감한 요청에 대해서는 회사의 손해 또는 사용자의 불편과 예민하게 직결되므로 문제가 커질 수 있다. \n\n따라서 해당 문제를 완벽히 해결하고자 한다. \n\n현재 별다른 조치가 없는 상황에서 발생하는 문제 상황은 다음과 같다. \n\n```plain text\n1번 요청과 2번 요청이 0.005초 차이이고(사실상 동시),\n\n1번 요청 - 트랜잭션 시작\n2번 요청 - 트랜잭션 시작\n1번 요청 - 트랜잭션 완료, 커밋. 새로운 구매 내역이 테이블에 추가됨.\n2번 요청 - 트랜잭션 완료, 커밋. 새로운 구매 내역이 테이블에 추가됨. \n\n따라서 동일한 유저가 두 번 쿠폰을 구매한 것 처럼 처리되었다. \n```\n\n### 제약조건 확인\n\n현재 테이블의 키값이 user\\_id가 아니며, 난수 발생을 통해 생성된 값을 키 값으로 하고 있다. 또한 Redis와 같은 외부 라이브러리를 도입할 여유는 없다. 따라서 가능한 한 현재 시스템(Sqlalchemy, Postgresql)내에서 해결책을 찾고자 한다. \n\n## 해결방안 고민\n\n### 최근 구매 내역 확인(기각)\n\n가장 쉽게 생각할 수 있는 방법은 쿠폰 구매 로직에서 이 유저의 최근 구매 로직을 확인하는 것이다. 하지만 현재 이 상황에서는 사실상 요청이 동시에 들어오고 있으므로, 2번 요청이 시작됐을 때에는 해당 유저의 구매 내역이 아직 DB에 없을 것이므로, 이 방법은 사용할 수 없다. \n\n### Database-Level Optimistic Locking(기각)\n\n각 트랜잭션에 버전 번호를 추가하고 트랜잭션이 커밋되기 전에 해당 버전 번호가 여전히 유효한지 확인한다. 하지만 이 방법도 여전히 2번 요청이 1번 요청의 결과를 감지하지 못할 수 있다. \n\n### Application-Level Locking(기각)\n\n코드 내에서 직접 잠금 로직을 구현하는 방법이다. 데이터베이스가 아닌 애플리케이션의 메모리 내에서 잠금을 관리한다. 주로 멀티스레드 환경에서 특정 자원에 대한 동시 접근을 제어하는데 사용된다. 현재 FastAPI를 사용하고 있고, FastAPI는 기본적으로 비동기 방식으로 단일 스레드 환경에서 실행된다는 점을 고려하면 전통적인 스레드 기반의 잠금 매커니즘보다 async 라이브러리에 들어있는 비동기 프로그래밍에 적합한 잠금 메커니즘인 asyncio.Lock을 사용하는 것이 이상적이다.\n\n#### FastAPI에서의 Application-Level Locking\n\n```python\nimport asyncio\n\nlocks = {}\n\nasync def purchase_coupon(user_id: int):\n    lock = locks.setdefault(user_id, asyncio.Lock())\n\n    async with lock:\n        # 여기에 쿠폰 구매 로직을 구현합니다.\n        # 이 블록 내의 코드는 동시에 하나의 요청만 처리합니다.\n        pass\n\n# FastAPI 엔드포인트에서 이 함수를 호출합니다.\n```\n\n이 코드는 user\\_id에 대해 별도의 asyncio.Lock을 생성하고 관리한다. 해당 user\\_id에 대한 작업이 진행중인 경우 다른 요청들은 해당 블록이 해제될 때까지 대기한다. \n\n예를 들어 다음 코드를 가정하자. 예를 들기 위해 아주 러프하게 작성했다. \n\n```python\nasync def test_function(a: int, b: int):\n    lock = locks.setdefault(a, asyncio.Lock())\n\n    async with lock:\n        await asyncio.sleep(1)\n        print(a)\n```\n\n이 코드는 다음과 같이 호출된다. \n\n```plain text\ntest_function(1, 2) 호출 -> a 값이 1인 잠금 획득 -> 처리 시작\ntest_function(1, 2) 호출 -> a 값이 1인 동일한 잠금으로 인해 대기\ntest_function(2, 2) 호출 -> a 값이 2인 새로운 잠금 획득 -> 처리 시작\ntest_function(2, 2) 처리 완료 -> a 값이 2인 잠금 해제\ntest_function(1, 2)의 첫 번째 호출 처리 완료 -> a 값이 1인 잠금 해제\ntest_function(1, 2)의 두 번째 호출이 대기 상태에서 해제 -> 처리 시작\n\n따라서 print는 1, 2, 1의 순서가 된다. \n```\n\n다만, 비동기 프로그래밍, 특히 Python의 [asyncio](https://sharknia.github.io/FastAPI와-asyncio)를 사용할 때 코드의 실행순서는 이벤트 루프에 의해 관리된다. 작업의 일시 중단과 다른 작업의 실행이 이벤트 루프에 의해 어떻게 스케줄링 될지는 여러 요인에 따라 달라질 수 있다. \n\n### PostgreSQL Advisory Locks(당장 채용)\n\nAdvisory Locks는 애플리케이션에서 데이터베이스 레벨의 잠금을 관리할 수 있도록 해준다. 이를 통해 특정 user\\_id 에 대한 동시 요청을 제어할 수 있다. \n\nAdvisory Locks는 테이블이나 행에 대한 접근을 잠그는 개념이 아니다. 특정 값 (정수)에 대한 잠금을 제공한다. 즉, 작업 자체에 1번이라는 이름을 붙이고 1번을 잠근다면, 다시 요청되는 1번 작업은 잠금이 된다. \n\n#### **Advisory Locks의 작동 방식**\n\n- 잠금 설정 : 애플리케이션이 pg\\_advisory\\_lock(key) 함수를 호출하여 특정 키에 대한 잠금을 요청한다.\n\n- 동일 키 사용 시 대기 : 같은 키를 사용하는 다른 데이터베이스 작업이 잠금을 보유하고 있다면, 새로운 잠금 요청은 해당 잠금이 해제될 때까지 대기한다. \n\n- 잠금 해제 : 원래의 작업이 완료되면 pg\\_advisory\\_unlock(key) 함수를 호출하여 잠금을 해제한다. 이후 대기 중이던 다른 작업이 잠금을 획득하고 진행될 수 있다. \n\n#### **Advisory Locks의 잠금 레벨**\n\nAdvisory Locks는 두 가지 레벨에서 사용할 수 있다. \n\n- 세션 레벨 : 이 잠금은 데이터베이스 세션과 연결되어있다. 세션이 종료되면 자동으로 해제된다. 세션 레벨 잠금은 장시간 유지되어야 하는 경우에 유용하다. \n\n    ```python\n    async def use_session_level_lock(db: AsyncSession, lock_key: int):\n        # 세션 레벨 잠금 획득\n        await db.execute(f\"SELECT pg_advisory_lock({lock_key})\")\n    \n        # 데이터베이스 작업 수행\n        # ...\n    \n        # 필요한 경우, 명시적으로 잠금 해제\n        # await db.execute(f\"SELECT pg_advisory_unlock({lock_key})\")\n    \n        # 세션 종료 시 잠금이 자동으로 해제됩니다.\n    ```\n\n- 트랜잭션 레벨 잠금 : 트랜잭션 레벨 잠금은 현재 트랜잭션과 연결되어 있으며 트랜잭션이 커밋되거나 롤백 될 때 자동으로 해제된다. \n\n    ```python\n    from sqlalchemy.ext.asyncio import AsyncSession\n    \n    async def use_transaction_level_lock(db: AsyncSession, lock_key: int):\n        async with db.begin():\n            # 트랜잭션 레벨 잠금 획득\n            await db.execute(f\"SELECT pg_advisory_xact_lock({lock_key})\")\n            \n            # 데이터베이스 작업 수행\n            # ...\n    \n        # 트랜잭션이 종료되면 잠금이 자동으로 해제됩니다.\n    ```\n\n쿠폰 구매 서비스 로직에서 트랜잭션 시작 전에  user\\_id를 기반으로 Advisory Lock을 요청하면 1번 요청이 처리되는 동안 동일한 user\\_id 에 대한 다른 요청은 대기하게 된다. 쿠폰 구매 처리가 완료되면, Advisory Lock을 해제하면 된다. \n\n## 결론 - PostgreSQL Advisory Locks 채용\n\n이 방법을 사용하면 매우 짧은 간격의 요청도 효과적으로 처리할 수 있다. 특히 트랜잭션 레벨의 잠금은 각 트랜잭션이 커밋되거나 롤백될 때까지만 유지되므로 0.005초와 같은 짧은 간격의 요청을 동기화하는데 매우 적합하다. \n\n백엔드 팀에서 협의를 거쳐 해당 방법을 사용해 구현하기로 최종 결정하였다. \n\n\n\n"},{"excerpt":"기능 구현이 거의 다 되었기 때문에, 오늘은 일단 짜잘짜잘한 오류 수정을 진행했다.  유지보수 내역 Front Matter-프로퍼티 연계 수정 프로퍼티가 Front Matter에 추가되는데, 값이 빈 것도 추가되어서 제대로 필터링 되지 않는 문제가 있었다. 값이 존재하는 프로퍼티만 Front Matter에 추가되도록 수정해주었다.  추가로, 프로퍼티가 고…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅6/"},"frontmatter":{"date":"January 29, 2024","title":"NotionAPI를 활용한 자동 포스팅(6)","tags":["Blogging","Typescript","Hobby","Notion-API"]},"rawMarkdownBody":"기능 구현이 거의 다 되었기 때문에, 오늘은 일단 짜잘짜잘한 오류 수정을 진행했다. \n\n## 유지보수 내역\n\n### Front Matter-프로퍼티 연계 수정\n\n프로퍼티가 Front Matter에 추가되는데, 값이 빈 것도 추가되어서 제대로 필터링 되지 않는 문제가 있었다. 값이 존재하는 프로퍼티만 Front Matter에 추가되도록 수정해주었다. \n\n추가로, 프로퍼티가 고정값이 아니라 커스텀에 따라서 유연하게 대응해 Front Matter에 추가되도록 해주었다. \n\n### 줄바꿈 주의!\n\n마크다운은 기본적으로 엔터를 두 번씩 쳐야 줄바꿈이 되는 모양이다. 리스트 컨버터 일부에 줄바꿈이 하나만 들어가있던 것을 두 개가 들어가도록 수정해주었다. \n\n### 이미지 다운로드 안되는 현상\n\n분명 동기적으로 작동하게끔 짜놓았는데도 불구하고 이미지가 제대로 들어가지 않는 현상이 있었다.  axios 재시도 패키지를 설치하고, 재시도 로직을 추가하고 타임아웃 10초 설정도 해주었다. 뭐가 문제였는지는 정확히 파악이 안됐는데, (오류 로그조차 없었다) 일단 이 이후로는 이미지 다운로드에 실패하는 현상은 없다. \n\n추가로, 이미지 다운로드 시에도 디렉토리가 없다면 만들도록 해주었다. \n\n### 스타일이 두 개 적용된 경우 \n\n스타일이 두 개가 적용된 경우는 더 띄어쓰기에 민감했다. \n\n```plain text\n*~~안녕하세요 ~~*\n```\n\n처럼 애매하게 띄어쓰기가 들어가면 하나만 적용된다. \n\n그래서 스타일이 있는 경우에는 공백을 제거 한 후에, 스타일을 붙이고 다시 스타일 뒤로 공백을 붙여서 자연스럽게 포맷이 유지되도록 수정해주었다. \n\n### ImageCounter 변수 수정\n\n이미지가 디렉토리가 변경되어도 숫자가 계속 늘어나는 현상을 겪었는데… 멍청하게 내가 이미지 카운터를 static 변수로 선언해서 모든 인스턴스가 값을 공유하고 있는 것 뿐이었다. 바로 인스턴스 변수로 변경 해주었다. \n\n### 멘션 링크 이후 줄바꿈 문제 해결\n\n멘션 링크를 북마크로 바꿔주는 메소드에 줄바꿈이 고정되어있어서, paragraph 안에 들어가 있는 멘션의 경우에도 줄바꿈이 들어가버렸다. 번거롭지만, 메소드를 수정해서 리스트로 처리되는 경우에만 줄바꿈이 들어가도록 수정해주었다. \n\n### notion url만 주어지는 경우\n\n노션의 페이지 링크를 따온 다음에, 아무 글자 또는 문장을 드래그 하고 붙여넣기 하면 해당 글자 또는 문장에 페이지 링크로 연결되는 북마크가 생성된다. \n\n![이렇게, 이 하이퍼링크는 노션 내부의 모 페이지로 연결된다. /xxxx… 만의 주소를 갖고 있다. ](image1.png)\n근데 애석하게도 이 링크는 단순히 32자리 문자열만 가지고 있다. (페이지의 아이디가 아니다). 따라서 해당 경우에는 북마크를 연결할 수가 없었다… \n\n인 줄 알았는데, 32자리인게 이상해서 혹시나 해서 UUID 형식으로 변환해봤더니 그게 내부 페이지의 아이디였다. 그러니까, 각 블록의 아이디가 UUID로 주어지는데 거기에서 하이픈만 제외하고 내부 URL로 사용중이었던 것이다. \n\n얼씨구나~ 하고 바로 해당 케이스도 북마크 기능으로 연결해주었다. \n\n### URL 형식 변경\n\n예를 들어 `노션 소개 페이지` 라는 타이틀을 `노션소개페이지` 로 변환해서 URL로 사용하고 있었는데, 공백을 완전히 제거하는 대신 하이픈을 넣도록 수정해주었다. \n\n## 다음 단계는? \n\n이제 다음 단계는 자동화가 목적이다. 어느정도 기능 구현은 90프로 이상 됐다고 생각하고, 당장 내가 블로그 글을 쓰는데 큰 문제가 없어보인다. 이제 github.io와 합쳐서 실질적인 사용이 가능하도록 해보려고 한다!\n\n"},{"excerpt":"구현 목적 상품 구매에 관련된 API를 구현하려고 한다. DynamoDB를 사용할 때에 동시성 이슈로 쿠폰 중복 구매 이슈가 있었으므로 이번에 RDS로 옮긴 김에 해당 문제를 완벽하게 해결하기 위해 다각도로 방법을 고민했다.  그 방안 중 하나가 트랜잭션 격리 수준(Transaction Isolation Level) 을 이용한 것이다.  SqlAlchem…","fields":{"slug":"/Sqlalchemy에서의-트랜잭션-격리-수준-구현/"},"frontmatter":{"date":"January 29, 2024","title":"Sqlalchemy에서의 트랜잭션 격리 수준 구현","tags":["SqlAlchemy","DataBase","Python","Work"]},"rawMarkdownBody":"## 구현 목적\n\n상품 구매에 관련된 API를 구현하려고 한다. DynamoDB를 사용할 때에 동시성 이슈로 쿠폰 중복 구매 이슈가 있었으므로 이번에 RDS로 옮긴 김에 해당 문제를 완벽하게 해결하기 위해 다각도로 방법을 고민했다. \n\n그 방안 중 하나가 [트랜잭션 격리 수준(Transaction Isolation Level)](https://sharknia.github.io/트랜잭션-격리-수준Transaction-Isolation-Level) 을 이용한 것이다. \n\nSqlAlchemy - Postgresql을 사용하고 있는데, 이 라이브러리에서 트랜잭션 격리 수준을 어떻게 구현했는지 기록한다. \n\n## 현재\n\n현재에는 기본 격리 수준을 사용한(별다른 옵션값이 없는) 엔진만 사용하고 있다. API 별로 별도의 세션을 사용하기 위해 의존성 주입 방식을 사용하며, 이를 위해 `AsyncIterable[AsyncSession]`을 생성한다. 대략적인 코드는 다음과 같다. \n\n```python\n_db_connection: AsyncEngine\n\n...\n\nasync def on_startup():\n\t\t....\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        pool_size=pool_size,\n        max_overflow=max_overflow,\n        echo=echo,\n    )\n\t\t....\n\n....\n\nasync def get_db_connection() -> AsyncEngine:\n    assert _db_connection is not None\n    return _db_connection\n\n....\n\nasync def get_db_session(\n    db_conn: AsyncEngine = Depends(get_db_connection),\n) -> AsyncIterable[AsyncSession]:\n    session = None\n    try:\n        async with sessionmaker(\n            db_conn,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )() as session:\n            yield session\n    except Exception as e:\n        logger.error(f\"[get_db_session] {e}\")\n        await session.rollback()\n        raise\n    finally:\n        if session:\n            await session.close()\n```\n\n그리고 각 API의 엔드포인트에서 get_db_session을 주입받아 사용한다. \n\n이 엔진은 기본적으로 Read Committed 격리수준만 지원한다. \n\n## 모험\n\n###  sessionmaker 레벨에서 트랜잭션 격리 수준 설정?\n\n이렇게 내용이 조금만 깊어져도 챗지피티는 믿을 놈이 못된다. 챗지피티에서는 sessionmaker 레벨에서 트랜잭션 격리 수준 설정이 가능하다며, sessionmaker의 매개변수에 `isolation_level=\"SERIALIZABLE”` 를 추가해주면 된다고 주장한다. \n\n하지만 sessionmaker의 생성자에는 `isolation_level` 매개 변수가 없어 해당 설정은 바로 오류를 낸다. 이럴 경우 답은 스스로 해결하는 것 밖에 없다. \n\n### 트랜잭션 별 엔진 별도 생성?\n\n첫 구현은 단순하게 생각해서 엔진을 여러개를 만들었다. 즉, `isolation_level` 옵션을 각각 다르게 준create_async_engine을 여러번 하는 것이다. 이러면 간단하게 해결이 된다. 하지만 이렇게 할 경우에는 문제가 있다. 각각의 엔진이 모두 별도의 풀을 생성하면서 의도치 않게 커넥션이 증가할 위험이 있는 것이다. 안그래도 [SqlAlchemy의 QueuePool](https://sharknia.github.io/SqlAlchemy의-QueuePool) 를 겪었었기 때문에 해당 이슈는 꼭 피하고 싶었다. \n\n## 개선(해결)\n\n코드를 짜다보면 “이건 있어야 하는데?”라고 느낄때가 있다. 그런것들은 대부분, 너무 미완성인 라이브러리나 프레임워크가 아니라면 반드시 나온다. 구현 가능한데 필요성을 느끼는 것은 반드시 누군가 만들어둔 것이다. \n\n명확하게 다른 엔진이면서 풀을 공유하는 방법이 존재한다. `execution_options()` 를 사용하면 된다. \n\n### execution_options()\n\n`AsyncEngine`에서 `execution_options()` 메소드를 사용하면 반환되는 엔진도 `AsyncEngine` 타입이 된다. 이 메소드는 Engine, Connection, Session 객체에서도 사용할 수 있으며, 특정 실행 옵션을 동적으로 설정하거나 변경하기 위해 사용된다. \n\n이 메소드를 사용하면 동일한 연결 풀을 공유하면서도 다른 실행 옵션을 가진 엔진을 생성할 수 있다. \n\n개선된 코드는 다음과 같다. \n\n_db_connection 설정 후, _db_connection_serializable를 `execution_options()` 를 사용해 정의한다. \n\n```python\n_db_connection_serializable = _db_connection.execution_options(\n        isolation_level=\"SERIALIZABLE\",\n    )\n```\n\n해당 값을 사용한 의존성 주입용 메소드를 선언한다. \n\n```python\nasync def get_db_connection_serializable() -> AsyncEngine:\n    assert _db_connection_serializable is not None\n    return _db_connection_serializable\n\nasync def get_serializable_db_session(\n    db_conn: AsyncEngine = Depends(get_db_connection_serializable),\n) -> AsyncIterable[AsyncSession]:\n    session = None\n    try:\n        async with sessionmaker(\n            db_conn,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )() as session:\n            yield session\n    except Exception as e:\n        logger.error(f\"[get_serializable_db_session] {e}\")\n        if session:\n            await session.rollback()\n        raise\n    finally:\n        if session:\n            await session.close()\n```\n\n이제 엔드포인트에서 기존 get_db_session 대신 이 메소드를 사용하면 다른 옵션의 격리 레벨을 사용할 수 있다!\n\n"},{"excerpt":"트랜잭션 격리 수준(Transaction Isolation Level)이란? 트랜잭션 격리 수준은 데이터베이스 시스템에서 동시에 여러 트랜잭션이 실행될 때, 트랜잭션 간에 데이터를 어떻게 고립시킬지를 결정하는 설정이다. 이 설정은 트랜잭션이 다른 트랜잭션의 작업에 영향을 받지 않도록 보장하는 동시에 여러 트랜잭션이 데이터베이스의 동일한 데이터에 접근할 때…","fields":{"slug":"/트랜잭션-격리-수준Transaction-Isolation-Level/"},"frontmatter":{"date":"January 29, 2024","title":"트랜잭션 격리 수준(Transaction Isolation Level)","tags":["DataBase","Postgresql"]},"rawMarkdownBody":"## 트랜잭션 격리 수준(Transaction Isolation Level)이란?\n\n트랜잭션 격리 수준은 데이터베이스 시스템에서 동시에 여러 트랜잭션이 실행될 때, 트랜잭션 간에 데이터를 어떻게 고립시킬지를 결정하는 설정이다. 이 설정은 트랜잭션이 다른 트랜잭션의 작업에 영향을 받지 않도록 보장하는 동시에 여러 트랜잭션이 데이터베이스의 동일한 데이터에 접근할 때 발생하는 문제들을 관리하는데 중요한 역할을 한다. \n\n격리 수준이 높을 수록 트랜잭션 간의 고립 정도가 증가하지만, 그만큼 데이터베이스의 처리량(throughput)과 동시성(concurrency)이 감소할 수 있다. 반대로 격리 수준이 낮으면 처리량과 동시성은 증가하지만 데이터 무결성 문제가 발생할 위험이 높아진다. 따라서 적절한 격리 수준을 선택해야 한다. \n\n데이터베이스의 동일한 데이터에 접근할 때 발생할 수 있는 문제들은 다음과 같다. \n\n### Dirty Read\n\n한 트랜잭션이 아직 커밋되지 않은 데이터를 읽는 경우\n\n### Non-repeatable Read\n\n한 트랜잭션이 동일한 쿼리를 여러 번 실행할 떄마다 다른 결과를 얻는 경우. 이는 다른 트랜잭션이 데이터를 변경하거나 삭제했기 때문이다. \n\n### Phantom Read\n\n한 트랜잭션이 동일한 쿼리를 여러 번 실행할 때, 새로 삽입된 행이 결과에 나타나는 현상이다. \n\n### Serialization Anomaly\n\n트랜잭션들이 서로 다른 순서로 실행될 때 발생할 수 있는 일관성 없는 결과를 말한다. \n\n## 격리 수준의 종류\n\n각 데이터베이스 시스템은 아래의 격리 수준을 제공하며, 이는 ANSI SQL 표준에 의해 정의된다. \n\n### Read Uncommitted\n\n- 가장 낮은 격리 수준이다. \n\n- 다른 트랜잭션에서 아직 커밋되지 않은 변경 사항을 조회할 수 있다. (Dirty Read)\n\n- Postgreql에서는 구현되지 있지 않아 Read Committed로 자동 처리된다. \n\n#### 장점\n\n높은 동시성과 성능을 보여준다. \n\n#### 단점\n\n데이터 무결성 문제가 발생할 수 있다. \n\n#### 고려사항\n\n- 데이터의 정확성보다 동시성이 중요한 경우에 고려될 수 있으나, 일반적으로 권장되지 않는다. \n\n### Read Committed\n\n- 각 쿼리가 실행될 때 커밋된 데이터만 읽는다. \n\n- 다른 트랜잭션에서 커밋된 변경 사항을 쿼리 시점에 볼 수 있다. (Non-repeatable Read이 발생할 수 있다. )\n\n#### 장점\n\n적당한 수준의 동시성과 성능을 보여준다. \n\n#### 단점\n\n같은 트랜잭션 내에서 같은 데이터를 두 번 읽을 때 다른 결과가 나올 수 있다. \n\n#### 고려사항\n\n- 일반적인 응용 프로그램에서 광범위하게 사용된다. \n\n- 대부분의 경우에 적합한 격리 수준이다. \n\n### Repeatable Read\n\n- 트랜잭션이 시작될 때의 데이터 상태를 유지한다. \n\n- 트랜잭션 중에 다른 트랜잭션이 커밋한 변경 사항이 보이지 않음(Phantom Read이 발생할 수 있다.)\n\n#### 장점\n\n하나의 트랜잭션 내에서 일관된 조회 결과를 보장한다. \n\n#### 단점\n\n트랜잭션이 길어질수록 다른 트랜잭션과의 충돌 가능성이 증가한다. \n\n#### 고려사항\n\n- 일관된 데이터 읽기가 필요할 때 사용한다. \n\n- 데이터 무결정이 중요한 경우 적합하지만, 긴 트랜잭션은 피해야 한다. \n\n### Serializable\n\n- 데이터베이스 트랜잭션에서 가장 높은 격리 수준이다. \n\n- 트랜잭션이 서로 독립적으로 작동하며, 동시에 여러 트랜잭션이 수행되는 경우에도 하나의 트랜잭션이 완료된 것 처럼 작동한다. \n\n- 다른 트랜잭션이 동시에 같은 데이터를 수정하지 못하도록 하여 데이터의 일관성을 보장한다. \n\n- 하나의 트랜잭션 동안 새로 삽입되거나 삭제된 행이 다른 트랜잭션에 의해 보이지 않도록 해서 Phantom Read를 방지한다. \n\n- 각 트랜잭션이 독립적인 환경에서 실행되며 다른 트랜잭션의 중간 결과를 볼 수 없다. \n\n#### 장점\n\n높은 수준의 데이터 무결성을 제공한다. 데이터베이스 내의 데이터는 모든 트랜잭션에 대해 완전히 일관된 상태를 유지한다. \n\n따라서 동시성 문제도 최소화된다. 동시에 여러 트랜잭션이 실행되는 환경에서 발생할 수 있는 대부분의 동시성 문제를 예방한다. \n\n#### 단점\n\n트랜잭션의 격리 수준이 높을 수록 데이터베이스의 처리량이 감소할 수 있다. 락 경합(lock contention)이 증가하고, 이는 성능저하로 이어질 가능성이 높다. \n\n서로 다른 트랜잭션이 같은 자원을 기다리는 상황이 발생할 수 있으며, 이는 데드락으로 이어질 수 있다. \n\n#### 고려사항\n\n- 데이터 무결성이 중요한 경우에 적합하지만 성능상의 영향을 고려해야 한다. \n\n- 데드락을 방지하기 위해 트랜잭션 설계 시 주의가 필요하다. \n\n- 읽기만 수행하는 트랜잭션의 경우 더 낮은 격리 수준을 고려하는 것이 성능상 이점이 있을 수 있다. \n\n\n\n"},{"excerpt":"열정 추가로 더 진행해버렸다. 정말로 여기까지만 하려고 한다. 오랜만에 탄력 받으니 계속 하게 되어버렸다.  구현 내용 콜아웃, 디바이더, 인용문, 코드, 번호 매기기 , 글머리 기호 목록 컨버터를 추가했다.  콜아웃, 디바이더 html로 구현했다. 인용문은 hr 태그로 처리했으며, 콜아웃은 div 태그를 사용했다. 다만 콜아웃은 스타일 처리가 필요하다.…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅5/"},"frontmatter":{"date":"January 28, 2024","title":"NotionAPI를 활용한 자동 포스팅(5)","tags":["Blogging","Notion-API","Typescript","Hobby"]},"rawMarkdownBody":"## 열정\n\n추가로 더 진행해버렸다. 정말로 여기까지만 하려고 한다. 오랜만에 탄력 받으니 계속 하게 되어버렸다. \n\n## 구현 내용\n\n콜아웃, 디바이더, 인용문, 코드, 번호 매기기 , 글머리 기호 목록 컨버터를 추가했다. \n\n### 콜아웃, 디바이더\n\nhtml로 구현했다. 인용문은 hr 태그로 처리했으며, 콜아웃은 div 태그를 사용했다. 다만 콜아웃은 스타일 처리가 필요하다. 지금은 대충 태그만 만들어놨다. 따라서 이건 미완성이나 다름없다. \n\n```typescript\nprivate convertCallout(calloutBlock: any): string {\n        const textContent = calloutBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n        const icon = calloutBlock.icon ? calloutBlock.icon.emoji : '';\n        const color = calloutBlock.color\n            ? calloutBlock.color\n            : 'gray_background';\n\n        return `\n    <div class=\"callout ${color}\">\n        ${icon} <span>${textContent}</span>\n    </div>\\n`;\n    }\n\nprivate convertDivider(): string {\n        return `<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\\n`;\n    }\n```\n\n### 인용문\n\n인용문은 있을 줄 몰랐는데, 마크다운에서 지원을 해서 쉽게 구현했다. \n\n무엇은 지원하고 무엇은 지원안하고, 기준을 명확히 모르겠긴하다. \n\n```typescript\nprivate convertQuote(quoteBlock: any): string {\n        const quoteText = quoteBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n\n        return `> ${quoteText}\\n\\n`;\n    }\n```\n\n### 번호 매기기, 글머리 기호 \n\n둘은 구현이 상당히 비슷했다. 마치 세트와 같다. 이걸 구현할 때에, 들여쓰기를 하려면 계층 개념이 필요해서 전체적인 코드 수정이 한 번 이뤄졌다. \n\n```typescript\nprivate formatListItemContent(listItemBlock: any): string {\n        return listItemBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n    }\n\n    private convertNumberedList(listItemBlock: any): string {\n        const listItemContent = this.formatListItemContent(listItemBlock);\n        const indent = ' '.repeat(this.indentLevel * 2);\n        return `${indent}1. ${listItemContent}\\n`;\n    }\n\n    private convertBulletedList(listItemBlock: any): string {\n        const listItemContent = this.formatListItemContent(listItemBlock);\n        const indent = ' '.repeat(this.indentLevel * 2);\n        return `${indent}- ${listItemContent}\\n`;\n    }\n```\n\n띄어쓰기 네 번으로 계층을 구분한다. \n\n### 코드 \n\n코드도 쉬웠다. 마크다운에서 지원하는 것은 기본적으로 구현이 쉽다. \n\n```typescript\nprivate convertCode(codeBlock: any): string {\n        const codeText = codeBlock.rich_text\n            .map((textElement: any) => textElement.plain_text)\n            .join('');\n\n        const language = codeBlock.language || '';\n\n        return `\\`\\`\\`${language}\\n${codeText}\\n\\`\\`\\`\\n\\n`;\n    }\n```\n\n### to_do\n\nto_do도 간단하게 구현했다. 마크다운에서는 체크박스인데, 노션에서는 할 일 목록이다.. 그래서 체크박스를 체크하면 스타일이 추가되는데.. 이것까진 따로 구현하지 않았다. \n\n```typescript\nprivate convertToDo(toDoBlock: any): string {\n        const quoteText = toDoBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n        let pre = '- [ ]';\n        if (toDoBlock.checked == true) {\n            pre = '- [x]';\n        }\n        return `${pre} ${quoteText}\\n\\n`;\n    }\n```\n\n## 정리 \n\n### 작업 완료 블록\n\n일단 1차적으로 작업이 완료되었으며, 완료된 지원하는 블록 타입은 다음과 같다. \n\n- paragraph\n\n- heading_1, heading_2, heading_3\n\n- bookmark\n\n- link_to_page\n\n- image\n\n- callout\n\n- divider\n\n- quote\n\n- code\n\n- numbered_list_item\n\n- bulleted_list_item\n\n- to_do\n\n### 작업 예정 블록\n\n#### table, table-row\n\n꼭 필요해보이지만, 구현을 위해서는 현재 구현된 블록의 재귀 호출 구조를 바꿔야 한다. 상대적으로 까다로워 오늘 작업하진 않겠다. \n\n### 작업 미정 블록\n\n#### column (n개의 열로 구성된 블록 생성)\n\n가로로 여러 블록을 놓는 기능이다. 굳이 필요한가? 싶기도 하고 또 구현이 꽤나 까다로워 보인다. 다만 table, table-row를 작업할 때에 호출 구조를 잘 짜놓는다면 또 별 노력 없이 잘 될 것 같기도 한데, 마크다운에 원래 가로를 나누는 기능이 있는지는 또 모르겠다. 그래서 미정이다. \n\n#### 미디어 관련 블록\n\n미디어 관련 기능도 지금보니 노션에 있다. 이건.. 할만하지 않을까? 고려해보겠다. 아직 자세히 살펴보지 못했다. \n\n### 미구현 확정 블록\n\n#### 토글\n\n토글은 미구현 확정이다. 토글은 마크다운에서 지원하지 않고, 만약 하려면 부트스트랩의 collapse 같은 기능을 직접 구현해야 할 것 같다. 이건 쉽지 않다. \n\n#### 임베드 관련 블록\n\n기각이다. 블로그 글에 임베드는 쓰지 말자. \n\n#### 고급 블록\n\n지금보니 토글도 고급에 들어있다. 마크다운 기본 기능이 아닌 것들은 기본적으로 구현에 한계가 있다. 다시 보니 column도 고급에 있다. \n\n나중에 추후 시간이 되면 만들만한 것들은 한 번 고려해보겠다. \n\n#### 데이터베이스 관련 블록\n\n웹사이트에 넣을 수 있는 기능이 (아마도) 아니다. 기각이다. \n\n## 다음 작업 예정은? \n\n이제 배포 자동화를 목표로 해야 한다. 자동화까지 해둬야, 이 프로젝트를 만든 목적 달성이 아닐까? 얼른 자동화도 하고 싶다..\n\n자동화의 자세한 로직은 다음번에 설계 하도록 하겠다.. \n\n"},{"excerpt":"타입스크립트의 Union Type 타입스크립트에는 유니언 타입이라는게 있다. 쉽게 말해 값이 여러 타입을 or 로 가질 수 있는 것이다. 막나간다.  두 개의 이상의 타입을  기호를 사용해 결합하면 유니언 타입이 된다. 이를 통해 변수가 함수 매개변수가 여러 타입 중 하나의 타입을 가질 수 있음을 나타낼 수 있다.  장점 유연성 다양한 타입을 하나의 변수…","fields":{"slug":"/Union-Type/"},"frontmatter":{"date":"January 28, 2024","title":"Union Type","tags":["Typescript","Python"]},"rawMarkdownBody":"## 타입스크립트의 Union Type\n\n타입스크립트에는 유니언 타입이라는게 있다. 쉽게 말해 값이 여러 타입을 or 로 가질 수 있는 것이다. ~~*막나간다.*~~ \n\n두 개의 이상의 타입을 `|` 기호를 사용해 결합하면 유니언 타입이 된다. 이를 통해 변수가 함수 매개변수가 여러 타입 중 하나의 타입을 가질 수 있음을 나타낼 수 있다. \n\n```typescript\ntype StringOrNumber = string | number;\n\nlet value: StringOrNumber;\n\nvalue = 'Hello'; // 유효함\nvalue = 123;     // 유효함\n```\n\n### 장점\n\n#### 유연성\n\n다양한 타입을 하나의 변수에 할당할 수 있어 다양한 시나리오에 대응할 수 있다. \n\n#### 타입 안정성 보장\n\n유니언 타입을 사용하면 타입스크립트 컴파일러가 타입 안정성을 체크해준다. 즉, 할당된 값이 유니언 타입에 명시된 타입 중 하나와 일치하지 않으면 오류를 발생시킨다. \n\n#### 코드 간결성\n\n복잡한 조건에 대한 타입을 간결하게 표현할 수 있다. \n\n### 유니언 타입 사용 시 고려할 사항\n\n#### 타입 가드\n\n유니언 타입은 여러 타입을 허용하기 때문에 실행 시점에 정확한 타입을 확인하기 위해 타입 가드를 사용해야 한다. \n\n```typescript\nfunction process(value: StringOrNumber) {\n    if (typeof value === 'string') {\n        // value는 여기서 string 타입입니다.\n    } else {\n        // value는 여기서 number 타입입니다.\n    }\n}\n```\n\n#### 공통 필드 사용\n\n유니언 타입의 모든 구성원이 공통으로 가진 필드나 메소드에만 접근할 수 있다. 공통되지 않은 필드에 접근하려면 타입 가드를 사용해야 한다. \n\n#### 복잡한 유니언 타입\n\n유니언 타입이 복잡해질 수록 그 타입을 사용하는 코드는 더 복잡해질 수 있다. 따라서 타입을 너무 복잡하게 만들지 않도록 주의해야 한다. \n\n## 파이썬의 Union Type\n\n여기까지 알아보다가 파이썬에서도 왠지 비슷한게 있을 것 같다는 생각이 들어서 찾아봤고, 역시 있음을 알게 됐다. \n\n파이썬은 타입스크립트와 다르게 동적 타이핑 언어이지만 3.5 이상에서는 타입 힌팅을 사용하여 비슷한 기능을 구현할 수 있고, 3.10 이상에서는 `Union` 대신 `|` 연산자를 이용하여 유니언 타입을 정의할 수 있다. \n\n### 파이썬 3.9 이전\n\ntyping 모듈의 Union 을 사용하여 나타낼 수 있다. \n\n```python\nfrom typing import Union\n\ndef process(value: Union[str, int]):\n    print(value)\n```\n\n### 파이썬 3.10 이상\n\n```python\ndef process(value: str | int):\n    print(value)\n```\n\n### 타입스크립트의 Union Type과 다른 점\n\n파이썬은 동적 타이핑 언어이므로 이 제약은 파이썬의 실행에 영향을 미치지 않는다. 즉 런타임에서 타입 안전을 강제하지 않는다. \n\n단순히 개발자의 가독성 향상, IDE의 자동완성 지원, mypy등 정적 타입 체킹 도구를 위한 것이다. \n\n저렇게 해두어도 런타임에서 다른 타입의 값이 할당될 수 있음을 주의해야 한다. \n\n## 사담\n\n노션 API를 활용한 블로그 쉽게 하는걸 만들다가 타입이 복잡하게 선언되어있는걸 보고 혹시나 해서 찾아봤더니 내 예상이 맞았다. \n\nC로 개발을 시작했어서 그런지 처음에 자바스크립트를 접하고 아주 재미있고 자유도가 높다고 생각했는데, 갈 수록 나 자신에게 쇠사슬이 묶여있는게 오히려 편하다고 느낀다. \n\n프로젝트 크기가 커질수록 더 그렇게 느끼는 것 같다. \n\n물론 이런 기능을 잘 활용하면 더 편할때도 많긴 하다. 코드가 짧아지는 것도 맞고..\n\n결국 내 자신이 잘하면 아무일도 안생길거라는 생각도 든다. \n\n\n\n"},{"excerpt":"개인적으로 나스를 하나 운영하고 있다.  이것저것 설정해서 쓰고 있는데, 어느 날 갑자기 토렌트 다운로드가 용량이 없다고 작동하지 않았다.  용량은 다음의 명령어를 통해 확인할 수 있다.  외장하든 왕 큰 놈을 달아서 사용하고 있는데, 오드로이드다보니 메인 OS가 깔리는 드라이브는 15gb짜리 작은 용량이다.. 그 드라이브가 100퍼센트 꽉 차 있었다. …","fields":{"slug":"/우분투-용량-관리/"},"frontmatter":{"date":"January 28, 2024","title":"우분투 용량 관리","tags":["Ubuntu"]},"rawMarkdownBody":"개인적으로 나스를 하나 운영하고 있다. \n\n이것저것 설정해서 쓰고 있는데, 어느 날 갑자기 토렌트 다운로드가 용량이 없다고 작동하지 않았다. \n\n용량은 다음의 명령어를 통해 확인할 수 있다. \n\n```bash\nfurychick@odroid:/HDD2/myHomePage$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           200M   29M  171M  15% /run\n/dev/mmcblk1p2   15G   15G     0 100% /\ntmpfs           996M     0  996M   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/mmcblk1p1  128M   14M  114M  11% /media/boot\n/dev/sdb1       2.7T  656G  2.0T  26% /HDD2\n/dev/sda1        11T  8.3T  2.1T  80% /HDD\ntmpfs           200M     0  200M   0% /run/user/1000\n```\n\n외장하든 왕 큰 놈을 달아서 사용하고 있는데, 오드로이드다보니 메인 OS가 깔리는 드라이브는 15gb짜리 작은 용량이다.. 그 드라이브가 100퍼센트 꽉 차 있었다. \n\n다음의 명령어를 사용해 용량이 큰 디렉토리를 검색했다. 외장하드 경로는 제외하고 검색했다. \n\n```bash\nfurychick@odroid:/$ sudo du -h / --exclude=/HDD2 --exclude=/HDD | sort -hr | head -n 10\ndu: cannot read directory '/proc/sys/fs/binfmt_misc': No such device\ndu: cannot access '/proc/19035/task/19035/fd/3': No such file or directory\ndu: cannot access '/proc/19035/task/19035/fdinfo/3': No such file or directory\ndu: cannot access '/proc/19035/fd/4': No such file or directory\ndu: cannot access '/proc/19035/fdinfo/4': No such file or directory\n15G     /\n11G     /home/furychick\n11G     /home\n8.7G    /home/furychick/.forever\n2.2G    /var\n1.7G    /usr\n1.6G    /var/log\n1.5G    /var/log/journal/dc87f36fc06c441a85ff7269ba4d50fb\n1.5G    /var/log/journal\n1.3G    /usr/lib\n```\n\n아~ 개인 홈페이지를 돌리는 로그가.. 몇 년 째 사이트를 그냥 켜둔채로 방치하다 보니 9기가에 달하게 크게 자라 내 서버를 억누르고 있었다. \n\n당장 로그를 삭제해주었다. \n\n해피엔딩~\n\n\n\n"},{"excerpt":"지난 이야기 NotionAPI를 활용한 자동 포스팅(3) 간만에 복귀를 했다. 별건 아니고.. 그냥 복습을 했다.  오늘의 작업 block.ts 나머지 작업 지난번에 block.ts를 미완성 된 상태로 두었다. 타입 검사에서 걸린 상태로 일단 두었고, Union Type이라는 것이 있다는 것을 알게 되었다. 해당 내용에 대해서는 따로 정리 해두었다.  U…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅4/"},"frontmatter":{"date":"January 27, 2024","title":"NotionAPI를 활용한 자동 포스팅(4)","tags":["Blogging","Notion-API","Typescript","Hobby"]},"rawMarkdownBody":"## 지난 이야기\n\n[NotionAPI를 활용한 자동 포스팅(3)](https://sharknia.github.io/NotionAPI를-활용한-자동-포스팅3)\n\n간만에 복귀를 했다. 별건 아니고.. 그냥 복습을 했다. \n\n## 오늘의 작업\n\n### block.ts 나머지 작업\n\n지난번에 block.ts를 미완성 된 상태로 두었다. 타입 검사에서 걸린 상태로 일단 두었고,\n\n```typescript\nexport type GetBlockResponse = PartialBlockObjectResponse | BlockObjectResponse;\n```\n\nUnion Type이라는 것이 있다는 것을 알게 되었다. 해당 내용에 대해서는 따로 정리 해두었다. \n\n[Union Type](https://sharknia.github.io/Union-Type)  \n\n### 로그 강화\n\n로그도 작업 순서에 맞게 찍히도록 강화했다. json 형태의 데이터여서 [object Object] 와 같이 찍히던 것도 제대로 내용물이 출력되도록 수정했다. 다음과 같은 코드로 json 형태의 데이터도 이쁘게 출력할 수 있다. \n\n```typescript\nconsole.log(\n            `convertParagraph - paragraph : ${JSON.stringify(\n                paragraph,\n                null,\n                2,\n            )}`,\n        );\n```\n\n### ConvertParagraph 완성\n\nparagraph는 노션 블록 타입 주으이 하나로, 텍스트의 기본 단위이다. 텍스트를 입력할 때 기본적으로 생성되는 블록 유형이다. \n\n이 블록 안에 들어있는 내용을 마크다운으로 바꿔주는 메소드를 완성했다. \n\n원본 노션의 내용은 다음과 같다. 일부러 여러가지 케이스를 집어넣었다. \n\n![](image1.png)\n이 paragraph블록의 데이터 형태는 다음과 같다. \n\n#### <u>노</u><u>~~션이~~</u>  **좋습*****니다***..\n\n```json\n{\n  \"rich_text\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"노\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": true,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"노\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"션이\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": true,\n        \"underline\": true,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"션이\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \" \",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \" \",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"좋습\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": true,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"좋습\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"니다\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": true,\n        \"italic\": true,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"니다\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"..\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"..\",\n      \"href\": null\n    }\n  ],\n  \"color\": \"default\"\n}\n```\n\n#### `정말로`.. <span style=\"color: pink;\">좋아합니다</span>.. \n\n```json\n{\n  \"rich_text\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"정말로\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": true,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"정말로\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \".. \",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \".. \",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"좋아합니다\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"pink\"\n      },\n      \"plain_text\": \"좋아합니다\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \".. \",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \".. \",\n      \"href\": null\n    }\n  ],\n  \"color\": \"default\"\n}\n```\n\n이 중에서 밑줄과 색상은 기본적으로 마크다운에서 지원하지 않는 기능이므로 html 태그를 사용했다. 코드는 다음과 같다. \n\n```typescript\nprivate convertParagraph(paragraph: any): string {\n        let markdown = '';\n\n        for (const textElement of paragraph.rich_text) {\n            let textContent = textElement.plain_text;\n\n            // 텍스트 스타일링 처리\n            if (textElement.annotations.bold) {\n                textContent = `**${textContent}**`;\n            }\n            if (textElement.annotations.italic) {\n                textContent = `*${textContent}*`;\n            }\n            if (textElement.annotations.strikethrough) {\n                textContent = `~~${textContent}~~`;\n            }\n            if (textElement.annotations.code) {\n                textContent = `\\`${textContent}\\``;\n            }\n            if (textElement.annotations.underline) {\n                // 마크다운은 기본적으로 밑줄을 지원하지 않으므로, HTML 태그 사용\n                textContent = `<u>${textContent}</u>`;\n            }\n            // 색상 처리 (HTML 스타일을 사용)\n            if (textElement.annotations.color !== 'default') {\n                textContent = `<span style=\"color: ${textElement.annotations.color};\">${textContent}</span>`;\n            }\n            if (textElement.href) {\n                textContent = `[${textContent}](${textElement.href})`;\n            }\n            markdown += textContent;\n        }\n        console.log(\n            `convertParagraph - paragraph : ${JSON.stringify(\n                paragraph,\n                null,\n                2,\n            )}`,\n        );\n        markdown = markdown + '\\n\\n';\n        console.log(`convertParagraph - markdown : ${markdown}`);\n        return markdown; // 문단 끝에 줄바꿈 추가\n    }\n```\n\n각 문장은 각각 다음과 같이 변환된다. \n\n```bash\nconvertParagraph - markdown : <u>노</u><u>~~션이~~</u> **좋습*****니다***..\nconvertParagraph - markdown : `정말로`.. <span style=\"color: pink;\">좋아합니다</span>..\n```\n\n이를 vscode의 마크다운 편집 기능을 통해서 확인하면 다음과 같이 정상적으로 출력되는 것을 확인할 수 있다. \n\n![](image2.png)\n### 마크다운 파일 저장 기능 추가 \n\npage.ts에 block들로부터 받아온 마크다운 내용과 properties에 저장된 내용들을 합해 마크다운 파일로 저장하는 메소드를 완성했다. \n\n```typescript\npublic async printMarkDown() {\n        //contentMarkdown과 properties의 내용을 마크다운 파일로 저장한다.\n        try {\n            // 파일 이름 설정 (페이지 제목으로)\n            const filename = `${this.pageTitle}.md`;\n\n            // 마크다운 메타데이터 생성\n            const markdownMetadata = this.formatMarkdownMetadata();\n\n            // 마크다운 메타데이터와 contentMarkdown을 결합\n            const fullMarkdown = `${markdownMetadata}${this.contentMarkdown}`;\n\n            // 디렉토리 생성 (이미 존재하는 경우 오류를 무시함)\n            await fs.mkdir(this.pageUrl ?? '', { recursive: true });\n            const filePath = join(this.pageUrl ?? '', 'index.md');\n            // 결합된 내용을 파일에 쓰기 (이미 존재하는 경우 덮어쓰기)\n            await fs.writeFile(filePath, fullMarkdown);\n            console.log(`[page.ts] Markdown 파일 저장됨: ${filePath}`);\n        } catch (error) {\n            console.error(`[page.ts] 파일 저장 중 오류 발생: ${error}`);\n        }\n    }\n\n    private formatMarkdownMetadata(): string {\n        // properties를 마크다운 메타데이터로 변환\n        const metadata = [\n            '---',\n            `title: \"${this.properties?.title ?? ''}\"`,\n            `description: \"${this.properties?.description ?? ''}\"`,\n            `date: ${this.properties?.date ?? ''}`,\n            `update: ${this.properties?.update ?? ''}`,\n            // tags가 배열인 경우에만 join 메소드를 호출\n            `tags:\\n  - ${\n                Array.isArray(this.properties?.tags)\n                    ? this.properties.tags.join('\\n  - ')\n                    : ''\n            }`,\n            `series: \"${this.properties?.series ?? ''}\"`,\n            '---',\n            '',\n        ].join('\\n');\n\n        return metadata;\n    }\n```\n\ngithub.io의 블로그 형식에 맞춰서 properties를 바꿔주고, 블로그 형식에 맞는 위치에 일단 마크다운 파일을 저장하게끔 해주었다. 아마, 추후 배포 방식에 따라 해당 위치는 바뀔 수 있을 것이다. 아직 정확하게 어떤 방식으로 블로그를 배포할 것인지까지는 고려하지 않았다. \n\n위 코드를 포함해서 실행하면, \n\n![](image3.png)\n이렇게 저장이 되고, \n\n![](image4.png)\n이렇게 깔끔하게 저장이 된다. 이제 앞으로는 각 블록 타입들에 대한 변환을 추가하면 된다!\n\n### BlockObjectResponse 분석\n\n실질적으로 컨텐츠들은 BlockObjectResponse 타입들로 이루어져있다. BlockObjectResponse는 다음의 Union Type이다. \n\n```typescript\nexport type BlockObjectResponse = ParagraphBlockObjectResponse | Heading1BlockObjectResponse | Heading2BlockObjectResponse | Heading3BlockObjectResponse | BulletedListItemBlockObjectResponse | NumberedListItemBlockObjectResponse | QuoteBlockObjectResponse | ToDoBlockObjectResponse | ToggleBlockObjectResponse | TemplateBlockObjectResponse | SyncedBlockBlockObjectResponse | ChildPageBlockObjectResponse | ChildDatabaseBlockObjectResponse | EquationBlockObjectResponse | CodeBlockObjectResponse | CalloutBlockObjectResponse | DividerBlockObjectResponse | BreadcrumbBlockObjectResponse | TableOfContentsBlockObjectResponse | ColumnListBlockObjectResponse | ColumnBlockObjectResponse | LinkToPageBlockObjectResponse | TableBlockObjectResponse | TableRowBlockObjectResponse | EmbedBlockObjectResponse | BookmarkBlockObjectResponse | ImageBlockObjectResponse | VideoBlockObjectResponse | PdfBlockObjectResponse | FileBlockObjectResponse | AudioBlockObjectResponse | LinkPreviewBlockObjectResponse | UnsupportedBlockObjectResponse;\n```\n\n~~너무 많다~~\n\n일단, 많이 쓸 것 같은 블록들을 예제 파일로 만들고 해당 노션 파일들을 불러와보고 어떤 타입들을 사용하는지 살펴보기로 했다. 나머지는 모르겠다 아직은 그냥 미지원이다. \n\n![](image5.png)\n이것들을 넘는것은 내가 아직은 노션에 쓸 것 같지가 않다. 따라서 위의 블록들을 중점적으로 먼저 변환하기로 하자. \n\n다음의 녀석들이 그 녀석들이다. \n\n#### heading_ 시리즈\n\n제목1, 제목2, 제목3 들이다. heading_1부터 heading_3까지를 노션에서는 사용할 수 있는데, 각각 h2, h3, h4로 변환하면 될 것 같다. 이건 쉬울 것 같은 예감이 든다. \n\n#### bookmark\n\n북마크는 간단할 것 같지만, 간단하지 않은 점이 있다. 노션에는 페이지 링크 기능이 있는데, 이것도 가능하면 (페이지 링크도 어차피 내 블로그 글 일테니까, 아니 사용자가 그렇게 사용해야 한다. ) 블로그 글의 링크로 변환하고 싶다. 이게 가능할까? 이건 쉽지 않을 수도 있을 것 같은 생각이 지금은 든다. 생성된 페이지 링크와 페이지 아이디가 다르다면 근본적으로 불가능한 일이다. \n\n#### code\n\n코드 블록이다. 이건 쉬울 것 같다. \n\n#### table, table_row\n\n마크다운은 표 그리기가 까다롭다. 그래도 어떻게든 할 수 있지 않을까? \n\n#### bulleted_list_item\n\n글머리 기호이다. 이건.. 이건 쉽지 않을까? \n\n#### numbered_list_item\n\n번호 이것도 쉽지 않을까?? \n\n#### toggle\n\n토글도 어떻게든 되지 않을까? 다른것보단 복잡하겠지만 특별한 건 없을 것 같다. \n\n#### quote, divider, callout\n\n이 놈들도 특별한 어려움은 예상되지 않는다. 그냥 스타일이나 이쁘게 주면 될 것 같다. \n\n결국, 표나 북마크를 제외하면 나머지는 그냥 단순 노가다로 예상이 된다. \n\n### heading 시리즈 컨버터 생성\n\n이건 간단했다. 다만, 컨버팅 관련 내용이 길어질 것 같아 해당 역할을 하는 클래스 MarkdownConverter을 만들어서 이걸 이용하기로 했다. 이 클래스는 블록의 내용을 받아 markdown 문자열로 변환해서 리턴하는 역할을 한다. \n\nparagraph와 heading 시리즈에 겹치는 내용이 많다. 사실상 pre 태그만 다르다. 따라서 중복되는 내용을 formatTextElement 메소드로 분리했다. 아마 다른 타입에 대해서도 사용할 수 있을 것 같은 예감이 든다. \n\n완성된 코드는 다음과 같다. \n\n#### 코드 \n\n```typescript\nimport { BlockObjectResponse } from '@notionhq/client/build/src/api-endpoints';\n\nexport class MarkdownConverter {\n    private block: BlockObjectResponse;\n\n    private constructor(block: BlockObjectResponse) {\n        this.block = block;\n    }\n\n    public static async create(block: BlockObjectResponse): Promise<string> {\n        const converter: MarkdownConverter = new MarkdownConverter(block);\n        const result = await converter.makeMarkDown();\n        return result;\n    }\n\n    private async makeMarkDown(): Promise<string> {\n        let block = this.block;\n        let markdown: string = '';\n\n        console.log(\n            `[markdownConverter.ts] makeMarkDown : ${\n                block.type\n            } : ${JSON.stringify(block, null, 2)}`,\n        );\n\n        switch (block.type) {\n            // 텍스트의 기본 단위,텍스트를 입력할 때 기본적으로 생성되는 블록 유형\n            case 'paragraph':\n                markdown += this.convertParagraph(block.paragraph);\n                break;\n            case 'heading_1':\n                markdown += this.convertHeading(block.heading_1, 1);\n                break;\n            case 'heading_2':\n                markdown += this.convertHeading(block.heading_2, 2);\n                break;\n            case 'heading_3':\n                markdown += this.convertHeading(block.heading_3, 3);\n            // 다른 블록 유형에 대한 처리를 여기에 추가...\n            default:\n                console.warn(\n                    `[markdownConverter.ts] makeMarkDown : Unsupported block type - ${block.type}`,\n                );\n        }\n        return markdown;\n    }\n\n    private convertParagraph(paragraph: any): string {\n        let markdown = '';\n        for (const textElement of paragraph.rich_text) {\n            markdown += this.formatTextElement(textElement);\n        }\n        return markdown + '\\n\\n';\n    }\n\n    private convertHeading(heading: any, level: number): string {\n        let markdown = '';\n        const prefix = '#'.repeat(level + 1) + ' ';\n        for (const textElement of heading.rich_text) {\n            markdown += this.formatTextElement(textElement);\n        }\n        return prefix + markdown + '\\n\\n';\n    }\n\n    private formatTextElement(textElement: any): string {\n        let textContent = textElement.plain_text;\n\n        // 텍스트 스타일링 처리\n        if (textElement.annotations.bold) {\n            textContent = `**${textContent}**`;\n        }\n        if (textElement.annotations.italic) {\n            textContent = `*${textContent}*`;\n        }\n        if (textElement.annotations.strikethrough) {\n            textContent = `~~${textContent}~~`;\n        }\n        if (textElement.annotations.code) {\n            textContent = `\\`${textContent}\\``;\n        }\n        if (textElement.annotations.underline) {\n            textContent = `<u>${textContent}</u>`;\n        }\n        if (textElement.annotations.color !== 'default') {\n            textContent = `<span style=\"color: ${textElement.annotations.color};\">${textContent}</span>`;\n        }\n        if (textElement.href) {\n            textContent = `[${textContent}](${textElement.href})`;\n        }\n\n        return textContent;\n    }\n}\n```\n\n### convertLinkToPage 생성\n\n아까 북마크에 대해서 고려할 때, 페이지 링크가 어렵지 않을까? 고민했었는데 해당 문제는 해결이 가능했다. link_to_page 타입에서 페이지 아이디를 제공하고 있었고, 페이지 아이디를 API를 통해서 호출을 하면 해당 데이터를 가져오는 것이 가능했다. 이 경우에는 마크다운 파일은 필요없고 URL만 필요하므로, Page 클래스에 간단한 정보만 가져오는 메소드를 만들고 해당 메소드를 활용해 북마크를 생성해주었다. 또, 설정 파일에 블로그의 주소를 설정하도록 했다. \n\n```typescript\nexport class Page {\n    private pageId: string;\n    private notion: Client;\n\n    public properties?: Record<string, PropertyValue>;\n    public pageTitle?: string;\n    public pageUrl?: string;\n    public contentMarkdown?: string;\n\n    private constructor(pageId: string, notion: Client) {\n        this.pageId = pageId;\n        this.notion = notion;\n    }\n\n    private async init(page: Page) {\n        const properties = await page.getProperties();\n        page.properties = await page.extractDataFromProperties(properties);\n        page.pageUrl = `${\n            page.pageTitle\n                ?.trim()\n                .replace(/[^가-힣\\w\\-_~]/g, '') // 한글, 영어, 숫자, '-', '_', '.', '~'를 제외한 모든 문자 제거\n                .replace(/\\s+/g, '-') ?? // 공백을 하이픈으로 치환\n            ''\n        }`;\n    }\n\n\t\t.... 생략 ....\n\n    public static async getSimpleData(pageId: string) {\n        const notionApi: NotionAPI = await NotionAPI.create();\n        const page: Page = new Page(pageId, notionApi.client);\n        await page.init(page);\n        return {\n            pageTitle: page.pageTitle ?? '',\n            pageUrl: page.pageUrl ?? '',\n        };\n    }\n\n\t\t.... 생략 ....\n}\n```\n\n기존의 생성자 create 메소드와 중복되는 부분을 init 메소드로 분리하고, getSimpleData에서는 title, url만 리턴하도록 수정해주었다. \n\n```typescript\nprivate async convertLinkToPage(linkToPage: any): Promise<string> {\n        try {\n            const envConfig = EnvConfig.create(); // EnvConfig 인스턴스 생성\n            const blogUrl = envConfig.blogUrl; // blogUrl 가져오기\n            const pageId = linkToPage.page_id;\n            const pageData = await Page.getSimpleData(pageId);\n\n            const pageTitle = pageData.pageTitle;\n            const pageUrl = pageData.pageUrl;\n\n            return `[${pageTitle}](${blogUrl}/${pageUrl})\\n\\n`;\n        } catch (error) {\n            console.error('Error converting link_to_page:', error);\n            return '';\n        }\n    }\n```\n\n그리고 이렇게 해당 값과 설정값을 이용해 북마크를 연결할 주소를 만들었다. \n\n### Page 멘션 기능 대응\n\nPage Mention의 경우에는 특이하게 paragraph에 링크 관련 정보가 담겨서 온다. 해당 경우에 대응하기 위해 convertParagraph를 다음과 같이 수정해주었다. \n\n```typescript\nprivate async convertParagraph(paragraph: any): Promise<string> {\n        let markdown = '';\n        for (const textElement of paragraph.rich_text) {\n            if (\n                textElement.type === 'mention' &&\n                textElement.mention.type === 'page'\n            ) {\n                // mention 타입이고, page를 참조하는 경우\n                markdown += await this.convertMentionToPageLink(\n                    textElement.mention.page.id,\n                );\n            } else {\n                // 기타 텍스트 요소\n                markdown += this.formatTextElement(textElement);\n            }\n        }\n        return markdown + '\\n\\n';\n    }\n\n    private async convertMentionToPageLink(pageId: string): Promise<string> {\n        try {\n            const envConfig = EnvConfig.create(); // EnvConfig 인스턴스 생성\n            const blogUrl = envConfig.blogUrl; // blogUrl 가져오기\n            const pageData = await Page.getSimpleData(pageId);\n\n            const pageTitle = pageData.pageTitle;\n            const pageUrl = pageData.pageUrl;\n\n            return `[${pageTitle}](${blogUrl}/${pageUrl})`;\n        } catch (error) {\n            console.error('Error converting mention to page link:', error);\n            return '';\n        }\n    }\n```\n\n이로써 까다로울 것 같았던 두 가지에 대한 대응이 끝났고, 나머지는 노가다만 남은 것 같다!\n\n### imageConverter 구현\n\n이미지도 구현이 쉬웠다. 마크다운과 같은 폴더에 이미지를 다운로드 받고(amazone 저장소 주소가 API에서 제공된다. ) 캡션 형식으로 넣어주면 된다. \n\n```typescript\nprivate async convertImage(imageBlock: any): Promise<string> {\n        try {\n            const imageUrl = imageBlock.file.url;\n            const imageCaption =\n                imageBlock.caption.length > 0\n                    ? this.formatRichText(imageBlock.caption)\n                    : '';\n\n            // 이미지 이름을 순서대로 할당 (image1, image2, ...)\n            const imageName = `image${++MarkdownConverter.imageCounter}.png`;\n            const imageDownDir = `/${this.pageUrl}/${imageName}`;\n            const imagePath = join('contents/post', imageDownDir);\n\n            // 이미지 다운로드 및 로컬에 저장\n            const response = await axios.get(imageUrl, {\n                responseType: 'arraybuffer',\n            });\n            await fs.writeFile(imagePath, response.data);\n\n            // 마크다운 이미지 문자열 생성\n            let markdownImage = `![${imageCaption}](${imageName})\\n`;\n            if (imageCaption) {\n                markdownImage += `<p style=\"text-align:center;\"><small>${imageCaption}</small></p>\\n`;\n            }\n\n            return markdownImage;\n        } catch (error) {\n            console.error('Error converting image:', error);\n            return '';\n        }\n    }\n```\n\n단, 이 클래스는 상당히 분리되어있는 클래스여서 따로 저장될 곳의 디렉토리 명을 알 수 있는 방법이 없었다.. 어쩔 수 없이 Page클래스부터 계속해서 디렉토리 명을 던져줬다.. 깔끔하지 않다.. \n\n또, 저장될 디렉토리 (contents/post) 도 설정의 영역에 넣어야 할 것 같다. \n\n## 오늘의 마무리 \n\n오늘은 이 정도면 될 것 같다. 이미 핵심 부분은 모두 진행이 됐고, 앞으로는\n\n### 남은 타입별 구현\n\n콜아웃, devider, quote는 html, css의 영역으로 생각된다. 이건 추후 따로 정적 파일에 html, css를 추가하거나 해야 할 것 같다. 자세한 방법은 나중에 생각해보려고 한다. 오늘 다섯시간? 정도를 쉬지 않고 했더니 집중력이 많이 떨어진 게 느껴진다. \n\n토글도 의외로 까다로울지도? 뭐 초반엔 내가 그냥 토글을 노션에 안쓰면 되는거 아닐까? \n\n까다로운 부분은 많이 구현된 것 같다. 빠르면 내일, 열심히 한다면 이번주 안에 직접 사용을 해볼 수 있을 것 같다.\n\n\n\n"},{"excerpt":"지난시간 https://sharknia.github.io/Notion-Api-2/ 문제점 아무것도.. 기억이 나지 않는다.. 지난날의 나는 무엇이었나? 5개월만의 복귀가 이렇게 어렵다. 이래서 사람은 꾸준해야 한다.  잡설 최근 업무에 약~간의 여유가 생기면서 IDE를 파이참에서 vs code로 갈아탔다. 파이썬만 할 때에는 파이참이 유리한 것이 사실이지…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅3/"},"frontmatter":{"date":"January 26, 2024","title":"NotionAPI를 활용한 자동 포스팅(3)","tags":["Blogging","Notion-API","Hobby"]},"rawMarkdownBody":"## 지난시간\n\n[https://sharknia.github.io/Notion-Api-2/](https://sharknia.github.io/Notion-Api-2/)\n\n## 문제점\n\n아무것도.. 기억이 나지 않는다.. 지난날의 나는 무엇이었나? 5개월만의 복귀가 이렇게 어렵다. 이래서 사람은 꾸준해야 한다. \n\n## 잡설\n\n최근 업무에 약~간의 여유가 생기면서 IDE를 파이참에서 vs code로 갈아탔다. 파이썬만 할 때에는 파이참이 유리한 것이 사실이지만(심지어 속도도 파이참이 더 빠르다고 느꼈다. ) 추후 여러 언어를 다루게 될 경우 vs code가 유리한 점이 있다고 생각되어 갈아탔고, 환경 세팅을 다시 했다. \n\nformatter나 기타 여러가지 등등.. 그리고 타입스크립트 코드를 오랜만에 보면서 여기에도 formatter 설정을 추가해주었다. \n\n## 설계 되새김질\n\n코드를 다시 읽는데에만 시간을 꽤 투자했다. 예전의 나는 코드를 꽤 잘 짠 것 같다. \n\n블로그 글을 복습하지 않고 코드를 읽고 설계를 다시 했는데 오늘의 나는 예전의 나와 의견이 똑같다. ~~(진작 복습할걸)~~\n\n### 그래서 어떻게 할 것이냐? \n\n노션은 블록 기반의 구조로 이루어져있다. 그래서 나는 노션의 메인 데이터베이스를 블로그 홈이라고 가정하고, 그 안에 있는 블록들의 리스트는 페이지라고 정의하여, 페이지의 양식이나 속성은 고정해두려고 한다. 그리고 비로소 페이지안의 컨텐츠, 블록들을 마크다운으로 변환하려고 한다. \n\n그래서, Page.ts에서 페이지 들을 관리하고, Block.ts에서 블록들을 관리하려고 한다. 블록들은 각자 하위 블록을 다시 가질 수 있는 재귀적 형태를 가진다. \n\n콘텐츠들은 문단, 이미지, 리스트 등 여러 타입을 가지므로 경우에 따라서 block 클래스를 상속한 하위 클래스가 생길수도 있겠다. \n\n각자 블록들은 자신의 내용들을 마크다운으로 변환해서 상위 블록에게 전달하는 메소드를 갖는다. 가장 하위 블록부터 전달된 변경된 마크다운 내용들은 상위로 타고 올라가 결국 페이지 클래스에 전달된다. \n\n페이지 클래스는 이 마크다운들을 모아 파일로 저장한다. \n\n계획은 완벽해 보인다. \n\n하지만, 타입이 아주 많고 이걸 마크다운으로 변환하는 작업은 노가다 그 자체일것이다.. \n\n그래서 일단은 제한된 타입들에 대해서만 변환을 지원하려고 한다. \n\n## 그래서 오늘 작업은? \n\n일단 코드를 읽었고, 분석했고, 설계를 굳이 다시 하고 예전의 나와 의견이 같다는 점을 뒤늦게 확인했으며, \n\nblock.ts의 초안을 작성했다. \n\n```typescript\nimport { Client } from '@notionhq/client';\nimport { BlockObjectResponse } from '@notionhq/client/build/src/api-endpoints';\nimport { GetBlockResponse } from '@notionhq/client/build/src/api-endpoints';\n\nexport class Block {\n    private notion: Client;\n    private blockId: string;\n    private blockData?: GetBlockResponse;\n\n    constructor(notion: Client, blockId: string) {\n        this.notion = notion;\n        this.blockId = blockId;\n    }\n\n    public async getMarkdown(): Promise<string> {\n        this.blockData = await this.fetchBlockData();\n        return await this.processBlock(this.blockData);\n    }\n\n    private async fetchBlockData(): Promise<GetBlockResponse> {\n        return await this.notion.blocks.retrieve({ block_id: this.blockId });\n    }\n\n    private async processBlock(block: BlockObjectResponse): Promise<string> {\n        let markdown = '';\n\n        switch (block.type) {\n            case 'paragraph':\n                markdown += this.convertParagraph(block.paragraph);\n                break;\n            case 'heading_1':\n                markdown += `# ${block.heading_1.rich_text\n                    .map((t) => t.plain_text)\n                    .join('')}\\n\\n`;\n                break;\n            // 다른 블록 유형에 대한 처리를 여기에 추가...\n            default:\n                console.warn(`Unsupported block type: ${block.type}`);\n        }\n\n        // 하위 블록 처리 (재귀적)\n        if (block.has_children) {\n            markdown += await this.processChildBlocks(block.id);\n        }\n\n        return markdown;\n    }\n\n    private async processChildBlocks(blockId: string): Promise<string> {\n        const children = await this.notion.blocks.children.list({\n            block_id: blockId,\n        });\n        let markdown = '';\n        for (const child of children.results) {\n            markdown += await this.processBlock(child as BlockObjectResponse);\n        }\n        return markdown;\n    }\n\n    private convertParagraph(paragraph: any): string {\n        console.log('paragraph:' + paragraph);\n        return paragraph.rich_text;\n        // return paragraph.text.map((t) => t.plain_text).join('') + '\\n\\n';\n    }\n\n    // 기타 블록 유형에 대한 변환 함수를 여기에 추가...\n}\n```\n\n완전히 지극히 초안 그 자체이다. 기초적인 형태만 잡았다. 아직 리턴값에 대한 이해가 충분하지 않아, 해당 부분에 대한 조정이 필요하다. (심지어 오류가 나는 상태이다)\n\n블록의 타입이 모두 정의가 되어있으므로, 해당 타입에 따른 각 클래스를 따로 생성할 필요도 느낀다. 모두 여기에 집중된다면 코드가 너무 길어질 것 같다. \n\n## 결론\n\n결국 오늘은 한 일이 별로 없다.. 복귀에 의의를 두자. \n\n"},{"excerpt":"문제 기존에는 프론트와 주고 받을 데이터 모델을 정의하는데에 dataclass를 사용하고 있었다. 그러다가 FastAPI로 넘어오면서 Pydantic model을 도입했다.  dataclass로는 해결할 수 없는 문제가 있었다. 프론트엔드 개발에서는 주로 카멜케이스를 사용하기 때문에, 파이썬에서는 주로 스네이크 케이스를 사용하는데 (네이밍 규칙(namin…","fields":{"slug":"/Pydantic-Model의-응용/"},"frontmatter":{"date":"January 26, 2024","title":"Pydantic Model의 응용","tags":["Work","Python"]},"rawMarkdownBody":"## 문제\n\n기존에는 프론트와 주고 받을 데이터 모델을 정의하는데에 [dataclass](https://sharknia.github.io/dataclass)를 사용하고 있었다. 그러다가 FastAPI로 넘어오면서 [Pydantic model](https://sharknia.github.io/Pydantic-모델)을 도입했다. \n\ndataclass로는 해결할 수 없는 문제가 있었다. 프론트엔드 개발에서는 주로 카멜케이스를 사용하기 때문에, 파이썬에서는 주로 스네이크 케이스를 사용하는데 ([네이밍 규칙(naming conventions)](https://sharknia.github.io/네이밍-규칙naming-conventions)), 프론트로 나갈 변수 명을 카멜케이스로 통일 하기 위해 어쩔 수 없이 백엔드의 코드 컨벤션을 해치면서 Response로 나갈 모델의 속성만 Camel 케이스로 선언하고 있었다. \n\n하지만 난 이런게 너무 싫다. \n\n## 해결\n\nPydantic Model을 이를 해결하기 위한 옵션이 있다. \n\n### alias_generator\n\n필드에 대한 별칭을 자동으로 생성하는 함수를 지정한다. 주로 모델 필드 이름을 snake 케이스에서 camel케이스로 매핑할 때 사용된다. \n\n### populator_by_name\n\n이 옵션을 True로 설정하면 Pydantic은 별칭 대신 모델 필드의 원래 이름을 사용하여 인스턴스를 생성하고 할당한다. 이는 필드의 별칭과 원래 이름을 혼용하여 사용할 수 있게 한다. 기본적으로는 False로 되어있어 만약 이 설정을 수정하지 않는다면 속성을 snake case로 정의했어도 인스턴스 생성시 변수를 camelCase로 넣어줘야 하는 불상사가 일어난다. \n\n이 설정을 True로 해두면 모델 인스턴스가 직렬화, 역직렬화 되는 과정에서만 별칭을 이용하게 된다. \n\n### 두 옵션을 엮어서..\n\n이 두 옵션을 엮어서 Request로 받을 경우, Response로 내려갈 경우에만 별칭을 사용하고, 별칭은 camelCase에 따라 생성되도록 설정을 할 수 있다. \n\n그리고 이 설정값을 넣은 BastDtoModel 클래스를 생성해서 앞으로 우리가 데이터 통신에 사용할 PydanticModel은 모두 이 클래스를 상속받아 구현하기로 했다. \n\n<details>\n<summary>base_dto_model.py</summary>\n\n```python\nfrom pydantic import BaseModel\n\n\ndef to_camel(string: str) -> str:\n    components = string.split('_')\n    return components[0] + ''.join(x.title() for x in components[1:])\n\n\nclass BaseDtoModel(BaseModel):\n    class Config:\n        alias_generator = to_camel\n        populate_by_name = True\n```\n\n\n</details>\n\nalias_generator에는 to_camel 메소드를 정의해주었다. to_camel 메소드는 snake_case로 작성된 문자열을 camelCase로 변환해주는 함수이다. \n\n\n\n"},{"excerpt":"개발중인 서비스에서 Postgresql을 적용한지 이제 한달 조금 더 지났다.  아직 DB 마이그레이션 작업은 거의 되지 않았으므로, 실제로 Postgresql DB를 이용하는 서비스는 그다지 많지 않았다.  새로 업데이트 하는 기능들에 대해서는 적극적으로 Postgresql을 이용하기로 했고, 이번에 새로 퀴즈 기능을 개발하면서 이 기능은 전부 RDB …","fields":{"slug":"/SqlAlchemy의-QueuePool/"},"frontmatter":{"date":"January 18, 2024","title":"SqlAlchemy의 QueuePool","tags":["SqlAlchemy","DataBase","Python","Work"]},"rawMarkdownBody":"개발중인 서비스에서 Postgresql을 적용한지 이제 한달 조금 더 지났다. \n\n아직 DB 마이그레이션 작업은 거의 되지 않았으므로, 실제로 Postgresql DB를 이용하는 서비스는 그다지 많지 않았다. \n\n새로 업데이트 하는 기능들에 대해서는 적극적으로 Postgresql을 이용하기로 했고, 이번에 새로 퀴즈 기능을 개발하면서 이 기능은 전부 RDB 기반으로 만들어졌다. \n\n퀴즈 기능의 개발은 유저의 접속률을 늘리기 위한 것으로, 모든 테스트를 마친 후 실서버에 배포가 되고 클라이언트까지 패치가 된 후 당당하게 퀴즈 풀러 오시라고 사용자들에게 푸시를 날렸다. \n\n그리고 펑! 서버가 터졌다. \n\n## 현상\n\n실제 서비스가 엄청나게 느리거나 거의 작동하지 않았다. \n\nPostgresql DB와 연결된 API들이 작동하지 않았다. \n\n서버에서는 `QueuePool limit of size 5 overflow 10 reached, connection timed out, timeout 30.00` 오류가 대량으로 발생했다. \n\n서버 모니터링 결과 푸쉬 직후 대량의 트래픽이 집중적으로 발생했다. \n\nFlarelane을 통해 확인한 결과, 2000명 정도가 동시 접속을 시도했다. \n\nSupabase의 DB 커넥션이 순간적으로 크게 늘어났다. (풀러를 사용중인데도 불구하고)\n\n\n\n시간이 지나면서 (약 10분이 안되는 짧은 시간) 트래픽이 줄어들고 자연스럽게 오류가 줄고, 서비스가 정상화되었다. \n\n## 원인 파악 및 해결\n\nSqlalchemy의 QueuePool을 현재 지금 pool size 5, max overflow 10으로 맞춰놓았었는데, 트래픽이 증가하면서 QueuePool의 한도가 다 차고, 대다수 요청이 최대 대기 시간 30초를 꽉 채워 대기하다가 오류가 발생했다. \n\n조사 결과, 대다수 많은 실제 서비스에서 해당 설정값은 대체적으로 너무 적어 값을 늘려서 사용한다고 한다. 정확한 값을 찾기 위해서는 여러번의 테스트가 필요하며, 점진적으로 늘려가면서 적당한 값을 찾아야 한다고 한다. \n\nsupabase 기반의 postgresql을 사용하고 있는데, 일단 postgresql의 connection 여유는 충분한 상황이었으므로 일단 설정값을 pool size 10, max overflow 20으로 각각 두배로 늘려주었다. \n\n이벤트나, 푸쉬가 있을 경우 추가적인 모니터링이 필요할 것이다. \n\n## Sqlalchemy의 QueuePool\n\n데이터베이스 Connection은 리소스가 많이 필요하다. QueuePool은 이런 Connection을 효율적으로 관리하여 성능을 향상시키기 위해 만들어졌다. Connection을 빠르게 재사용함으로써, 애플리케이션의 반응 시간을 단축 시키고 부하 상황에서도 안정적으로 동작하도록 한다. \n\nQuepool은 내부적으로 Connection 객체들을 큐로 관리한다. 동시성을 고려하여 설계되어 여러 스레드 또는 프로세스에서 안전하게 사용될 수 있다. \n\n### QueuePool의 생명주기 \n\n#### 생성\n\nQueuePool은 Sqlalchemy 엔진이 생성될 때 함께 생성된다. 이 때 데이터베이스와의 Connection 설정이 정의된다. \n\n#### Connection 관리 \n\n애플리케이션에서 데이터베이스 Connection이 필요할 때, QueuePool이 Connection을 제공한다. 사용 가능한 Connection이 없으면 새 Connection을 생성하거나 대기열에서 Connection을 기다린다. \n\n#### Connection 반환\n\n작업이 완료되면 Connection은 Pool에 반환되어 재사용된다. \n\n#### 종료\n\n애플리케이션 종료와 함께 Pool에 있는 모든 Connection이 안전하게 종료된다. \n\n### 작동\n\nQueuePool은 기본적으로 미리 설정된 `pool_size` 내에서 Connection을 관리한다. 사용 가능한 Connection이 있다면 그 연결을 제공하고, 없으면 새로은 Connection을 생성한다. \n\n`pool_size`는 동시에 활성화 할 수 있는 최대 연결수이다. 만약 이 한계를 초과한다면, `max_overflow` 에 설정된 값에 따라 추가 Connection을 생성한다. `max_overflow` 는 `pool_size` 를 초과하여 생성할 수 있는 추가 연결의 최대 수이다. \n\n`max_overflow` 까지 초과한다면 추가 Connection 연결 요청은 대기열에 들어가며 사용 가능한 Connection 이 생길때까지 대기한다. 이 때, 연결 대기 시간이 `timeout` 설정을 초과하면 연결 요청은 실패한다. \n\n#### pool_size\n\n이 설정은 가능한 최대 연결의 최대 수를 정의 하는 것이므로, 5로 설정했다고 해서 실제 연결이 5개가 유지되는 것은 아니다. 사용되지 않는 Connection은 pool에 반환되며, 유휴 상태로 남아있다가 `recycle` 설정 시간을 넘어서면 끊어진다. \n\n#### max_overflow\n\npool_size를 초과하는 최대 개수를 정의한다. pool_size 가 5이고, max_overflowrk 10이라면 동시에 최대 5+10 = 15개의 연결을 허용하는 것이다. max_overflow에 해당되어 생성된 Connection이라 할지라도 사용 후에 풀에 반환된다(바로 끊어지는 것이 아니다). 다만, 해당 Connection은 pool_size의 기본 한계를 넘어선 것이므로 풀에서 더 높은 우선 순위로 종료될 수 있다. \n\n### Sqlalchemy의 풀링 전략\n\nSqlalchemy의 기본 풀링 전략은 Queuepool이지만, 이외에도 다른 전략들이 있다. \n\n### StaticPool\n\n연결의 고정된 집합을 유지한다. 모든 Connection 요청은 이 고정된 집합에서 제공된다. 단일 사용자 또는 단일 프로세스 어플리케이션에 적합하다. \n\n### NullPool\n\nConnection Pooling을 사용하지 않는다. 매 요청마다 새로운 DB Connection이 열리고 작업 종료 후 Connection이 닫힌다. \n\n매우 드문 요청이 있는 어플리케이션에 적합하다. \n\n### Singleton ThreadPool\n\n각 스레드에 대해 하나의 Connection을 유지한다. 스레드마다 고유한 Connection을 사용한다. \n\n멀티 스레드 환경에서 각 스레드가 자체 Connection을 가질 필요가 있을 때 유용하다. \n\n\n\n"},{"excerpt":"서론 FatAPI에는 페이징을 위한 공식 라이브러리가 존재한다. 하지만 예제대로 진행해도 코드는 오류를 내뿜었다. 왜냐하면, FastAPI의 페이지네이션 라이브러리는 SqlAlchemy 2.0의 비동기 엔진을 지원하지 않기 때문이다.  그래서 직접 구현했다.  (이 라이브러리를 쓰면 Async pagination 지원합니다. 여러분은 이거 쓰세요.) 목적…","fields":{"slug":"/FastAPI의-Pagenation/"},"frontmatter":{"date":"January 17, 2024","title":"FastAPI의 Pagenation","tags":["Work","FastAPI","SqlAlchemy","Python"]},"rawMarkdownBody":"## 서론\n\nFatAPI에는 페이징을 위한 [공식 라이브러리](https://uriyyo-fastapi-pagination.netlify.app/)가 존재한다. 하지만 예제대로 진행해도 코드는 오류를 내뿜었다. 왜냐하면, FastAPI의 페이지네이션 라이브러리는 SqlAlchemy 2.0의 비동기 엔진을 지원하지 않기 때문이다. \n\n그래서 직접 구현했다. \n\n~~(이~~ [~~라이브러리~~](https://pypi.org/project/fastapi-sqla/)~~를 쓰면 Async pagination 지원합니다. 여러분은 이거 쓰세요.)~~\n\n## 목적\n\n### 응답값 고정\n\n모든 페이징 쿼리에 대해 동일한 응답값을 제공하기 위해 Response에 사용할 Pydantic Model도 함께 정의한다. \n\n### 가능한 한 간단한 사용 방법\n\nSqlalchemy의 Select 객체를 받아 바로 처리할 수 있도록 한다. 즉, 개발자는 쿼리문만 생성하고 사이즈/페이지만 정해주면 바로 고정된 응답값을 받을 수 있다. \n\n### 현재 서버에 필요한 사양에 맞춘 기능\n\n복잡한 쿼리나 여러 층의 정렬을 현재는 필요로 하지 않는다. 이에 발맞춰 기본적인 relationship을 활용한 조인, 단일 컬럼 sort만 지원하여 사용법을 간략화 한다. \n\n## 구현\n\n### PaginationReturn Class 구현\n\n페이지네이션의 결과의 데이터 모델을 미리 정의해 둔 클래스이다. \n\n응답값은 다음의 내용들로 고정된다.\n\n- itemList : 페이지에 포함된 Row의 리스트\n\n- totalCount : 전체 항목의 개수\n\n- totalPage : 전체 페이지의 개수 \n\n- nowPage : 현재 페이지 번호\n\n- nowSize : 현재 설정한 사이즈 \n\n- prevPage : 이전 페이지 번호\n\n- nextPage : 다음 페이지 번호\n\n<details>\n<summary>전체 코드</summary>\n\n```yaml\nclass PaginationReturn(BaseModel, Generic[T]):\n    \"\"\"\n    PaginationReturn 클래스는 페이지네이션 결과를 나타내는 모델입니다.\n\n    Attributes:\n        itemList (List[T]): 페이지에 포함된 항목의 리스트입니다.\n        totalCount (int): 전체 항목의 개수입니다.\n        totalPages (int): 전체 페이지의 개수입니다.\n        nowPage (int): 현재 페이지 번호입니다.\n        nowSize (int): 현재 페이지에 포함된 항목의 개수입니다.\n        prevPage (Optional[int], optional): 이전 페이지 번호입니다. 기본값은 None입니다.\n        nextPage (Optional[int], optional): 다음 페이지 번호입니다. 기본값은 None입니다.\n    \"\"\"\n    itemList: List[T]\n    totalCount: int\n    totalPages: int\n    nowPage: int\n    nowSize: int\n    prevPage: Optional[int] = None\n    nextPage: Optional[int] = None\n```\n\n\n</details>\n\n### Pagination Class 구현\n\npagination은 static 메소드로 만들어서 필요한 값을 넣으면 바로 PaginationReturn을 받을 수 있게 만들었다. static method로 만든것은 FastAPI의 Pagination 라이브러리를 본딴것이다. 이 방법이 가장 사용하기 편해보이기도 했다. \n\n#### `get_paginated_list`\n\n처음 구현한 메소드이다. 모델을 파라미터로 받아 페이징을 한다. 따로 select문을 만들 필요 없이 모델과 where절만 만들어서 넣어줘도 정의된 응답값 데이터 모델에 리스트를 담아 반환해준다. \n\n<details>\n<summary>전체 코드</summary>\n\n```python\n@classmethod\n    async def get_paginated_list(\n        cls,\n        db: AsyncSession,\n        model: T,\n        filters: Optional[List[BinaryExpression]] = None,\n        order_column: Optional[str] = None,\n        order_direction: str = \"desc\",\n        size: int = 10,\n        page: int = 1,\n    ) -> PaginationReturn:\n        # page 와 size 기본 유효성 검증\n        if page < 1 or size < 1:\n            raise PagingException(\"Page and size parameters must be greater than 0\")\n\n        # 기본 쿼리 생성\n        query = select(model)\n\n        # fileter 처리\n        if filters and len(filters) > 0:\n            query = query.filter(*filters)\n\n        # 정렬 처리\n        if order_column:\n            if order_direction.lower() == \"asc\":\n                query = query.order_by(asc(getattr(model, order_column)))\n            else:\n                query = query.order_by(desc(getattr(model, order_column)))\n\n        total_count = 0\n        try:\n            # 페이징\n            offset_value = (page - 1) * size\n            query = query.offset(offset_value).limit(size)\n            result = await db.execute(query)\n            items = result.scalars().all()\n            logger.info(f\"[Pagination Query] {query}\")\n            # total_count 계산\n            total_query = select(func.count()).select_from(model)\n            if filters:\n                total_query = total_query.filter(*filters)\n            total_result = await db.execute(total_query)\n            total_count = total_result.scalar_one()\n        except SQLAlchemyError as e:\n            raise PagingException(f\"An error occurred while fetching data from the database : {e}\")\n        except Exception as e:\n            raise PagingException(f\"An unexpected error occurred : {e}\")\n\n        total_pages, prev_page, next_page = calculate_pagination(total_count, size, page)\n\n        # 응답값 생성\n        items_dict = [serialize_sqlalchemy_obj(item) for item in items]\n\n        res = PaginationReturn(\n            itemList=items_dict,\n            totalCount=total_count,\n            totalPages=total_pages,\n            nowPage=page,\n            nowSize=size,\n            prevPage=prev_page,\n            nextPage=next_page,\n        )\n        return res\n```\n\n\n</details>\n\nBaseModel 객체는 직렬화 기능이 없으므로, Response로 응답을 내려보낼때에 귀찮아지는 문제가 있다. 모델의 키값과 밸류를 사용해 dictionary로 바꾸면 해당 문제를 해결할 수 있는데, 만약 리스트에 들어있는 값들에 대해 커스텀이 필요하다면 dictionary를 그대로 다루는 것은 아무래도 모델을 커스텀 하는 것보다 불편한 문제가 발생한다. \n\n이 문제를 해결하기 위해 모델 객체를 기반으로 Pydantic Model 객체를 동적으로 생성하여 Pydantic model의 리스트를 결과값에 담아 리턴하기로 했다. 이렇게 하면 응답값을 바로 Response로 내려보내도 직렬화가 가능해 문제가 발생하지 않으며, 혹시 아이템 내용을 커스텀 해야 할 때에도 모델을 다룰 때와 같은 방법으로 사용할 수 있으므로 훨씬 간편하다. 개선된 코드는 아래와 같다. \n\n<details>\n<summary>개선된 코드</summary>\n\n```python\n@classmethod\n    async def get_paginated_list(\n        cls,\n        db: AsyncSession,\n        model: T,\n        filters: Optional[List[BinaryExpression]] = None,\n        order_column: Optional[str] = None,\n        order_direction: str = \"desc\",\n        size: int = 10,\n        page: int = 1,\n    ) -> PaginationReturn:\n        \"\"\"\n        페이징을 해서 아이템을 뽑아내서 결과를 반환한다.\n        Args:\n            db: AsyncSession\n            model: 데이터를 가져올 모델\n            filters: [InitQuizList.is_deleted == False, InitQuizList.is_active == True] 의 꼴 where절\n            order_column: 정렬할 컬럼\n            order_direction: 'asc' or 'desc'\n            size: 페이지에 나타낼 아이템의 개수 (기본값 10)\n            page: page 번호 (기본값 1)\n        Returns:\n            PaginationReturn\n        \"\"\"\n\n        # page 와 size 기본 유효성 검증\n        if page < 1 or size < 1:\n            raise PagingException(\"Page and size parameters must be greater than 0\")\n\n        # 기본 쿼리 생성\n        query = select(model)\n\n        # fileter 처리\n        if filters and len(filters) > 0:\n            query = query.filter(*filters)\n\n        # 정렬 처리\n        if order_column:\n            if order_direction.lower() == \"asc\":\n                query = query.order_by(asc(getattr(model, order_column)))\n            else:\n                query = query.order_by(desc(getattr(model, order_column)))\n\n        total_count = 0\n        try:\n            # 페이징\n            offset_value = (page - 1) * size\n            query = query.offset(offset_value).limit(size)\n            result = await db.execute(query)\n            items = result.scalars().all()\n            logger.info(f\"[Pagination Query] {query}\")\n            # total_count 계산\n            total_query = select(func.count()).select_from(model)\n            if filters:\n                total_query = total_query.filter(*filters)\n            total_result = await db.execute(total_query)\n            total_count = total_result.scalar_one()\n        except SQLAlchemyError as e:\n            raise PagingException(f\"An error occurred while fetching data from the database : {e}\")\n        except Exception as e:\n            raise PagingException(f\"An unexpected error occurred : {e}\")\n\n        total_pages, prev_page, next_page = calculate_pagination(total_count, size, page)\n\n        # 응답값 생성\n        # 동적 Pydantic 모델 생성\n        PydanticModel = sqlalchemy_to_pydantic(model)\n        # 페이징 처리된 쿼리 결과를 Pydantic 모델 리스트로 변환\n        pydantic_items = [PydanticModel.model_validate(item.__dict__) for item in items]\n\n        res = PaginationReturn(\n            itemList=pydantic_items,\n            totalCount=total_count,\n            totalPages=total_pages,\n            nowPage=page,\n            nowSize=size,\n            prevPage=prev_page,\n            nextPage=next_page,\n        )\n        return res\n\ndef sqlalchemy_to_pydantic(db_model: Type[DeclarativeMeta]) -> Type[BaseModel]:\n    \"\"\"\n    SQLAlchemy 모델을 동적으로 생성된 동일한 스키마의 Pydantic 모델로 변환합니다.\n    :param db_model: SQLAlchemy 모델 클래스\n    :return: 생성된 Pydantic 모델 클래스\n    \"\"\"\n    fields = {}\n    for column in inspect(db_model).c:\n        python_type = column.type.python_type\n        default = None if column.default is None else column.default.arg\n        if column.nullable:\n            python_type = Optional[python_type]\n        fields[column.name] = (python_type, default)\n\n    pydantic_model = create_model(db_model.__name__ + \"Pydantic\", **fields)\n    return pydantic_model\n```\n\n모델의 컬럼값으로 동적으로 pydantic model을 생성하는 sqlalchemy_to_pydantic 메소드를 생성하고 해당 메소드를 활용해 응답값에 담도록 해주었다. \n\n\n</details>\n\n#### `get_paginated_list_by_query`\n\n처음에 신나서 만들었는데, 기존  `get_paginated_list` 메소드는 치명적인 문제가 있다. 쿼리가 복잡한 경우를 전혀 생각하지 않았다. 말 그대로 단일 테이블에서만 값을 가져올 수 있는 것이다. 그래서 이 점을 개선해야했다. 그래서 개발자가 조금 더 귀찮아지지만 쿼리문은 이 메소드를 이용할 작업자가 직접 구현을 하고, 그 쿼리문을 넣으면 리스트를 담아서 돌려주는 형식으로 다시 만들기로 했다. \n\n기존 메소드에서는 모델만을 조회하는 것이므로 모델을 기반으로 Pydantic model을 생성하면 문제가 없었는데, 이번에는 모델에서 컬럼값을 추출해서 사용할 수 없으므로 키 값을 동적으로 정의하는 코드를 따로 작성해주었다. \n\n쿼리된 내용에서 컬럼값을 조회해서 사용하였으며, 쿼리된 내용에는 시스템에서 사용하는 속성값도 존재하는데 해당 속성값들은 `_`로 시작하므로 해당 내용은 제거하고 동적으로 Pydantic model에 추가해주었다. relationship도 대응할 수 있게 별도의 예외처리를 추가해주었다. relationship 속성은 Select 객체의 컬럼에는 포함이 되지 않는데 조회된 결과물의 속성에는 들어있으므로 해당 값이 relationship 속성이라고 가정하고 따로 예외처리를 해주었다. \n\n<details>\n<summary>전체 코드</summary>\n\n```python\n@classmethod\n    async def get_paginated_list_by_query(\n        cls,\n        db: AsyncSession,\n        query: Select[Any],\n        size: int = 10,\n        page: int = 1,\n    ):\n        \"\"\"\n        Query에 대한 paging 생성\n        Args:\n            db: AsyncSession\n            query: sqlalchemy의 Select의 리턴값\n            size: 한 번에 보여줄 아이템의 개수(기본값 10)\n            page: 페이지(기본값 1)\n        Returns:\n            PaginationReturn\n        \"\"\"\n        # 매개변수 검증\n        if page < 1 or size < 1:\n            raise PagingException(\"Page and size parameters must be greater than 0\")\n\n        # 페이징 적용\n        offset_value = (page - 1) * size\n        paginated_query = query.offset(offset_value).limit(size)\n\n        # 쿼리 실행\n        try:\n            result = await db.execute(paginated_query)\n        except SQLAlchemyError as e:\n            raise PagingException(f\"An error occurred while fetching data from the database : {e}\")\n        except Exception as e:\n            raise PagingException(f\"An unexpected error occurred : {e}\")\n        items = result.scalars().all()\n\n        pydantic_items = []\n        total_count = 0\n        if items:\n            # 컬럼과 데이터 타입 파악\n            columns = query.columns\n            fields = {col.name: (Optional[col.type.python_type], None) for col in columns}\n\n            # relationship 처리\n            relationship_keys = set()\n            sample_item = items[0]\n            item_dict = sample_item.__dict__\n            for key in item_dict.keys():\n                if key not in fields and not key.startswith(\"_\"):\n                    # 관계형 속성의 키를 저장\n                    relationship_keys.add(key)\n\n            # 관계형 속성에 대한 필드 정의 추가\n            for key in relationship_keys:\n                fields[key] = (Optional[Any], None)\n            # 관계형 속성을 포함한 동적 Pydantic 모델 생성\n            DynamicPydanticModel = create_model(\"DynamicPydanticModel\", **fields)\n\n            for item in items:\n                item_dict = {k: v for k, v in item.__dict__.items() if not k.startswith(\"_\")}\n                # 관계형 속성을 딕셔너리로 변환\n                for key in relationship_keys:\n                    relation = getattr(item, key, None)\n                    if relation:\n                        # SQLAlchemy 내부 상태 정보를 제외하고 변환\n                        item_dict[key] = {\n                            k: v for k, v in relation.__dict__.items() if not k.startswith(\"_\")\n                        }\n                    else:\n                        item_dict[key] = None\n\n                pydantic_items.append(DynamicPydanticModel.model_validate(item_dict))\n\n            # 총 개수 계산\n            total_count_query = select(func.count()).select_from(query.subquery())\n            total_count_result = await db.execute(total_count_query)\n            total_count = total_count_result.scalar_one()\n\n        # 결과 반환\n        total_pages, prev_page, next_page = calculate_pagination(total_count, size, page)\n\n        return PaginationReturn(\n            itemList=pydantic_items,\n            totalCount=total_count,\n            totalPages=total_pages,\n            nowPage=page,\n            nowSize=size,\n            prevPage=prev_page,\n            nextPage=next_page,\n        )\n```\n\n\n</details>\n\n## 결론\n\n아직 기능이 많이 부족하지만, 현재 회사에서 사용을 하면서 필요한 기능은 대부분 구현을 했다고 생각이 든다. 그래서 일단 개발은 여기서 멈추었다. \n\n개발에 아주 오랜 시간이 걸리지는 않았다. 집중도와 실제 사용 시간을 따지면 이틀 좀 안되게 썼다고 생각이 든다. \n\n만드는것은 재미있는 과정이었고, 필요한 것이 잘 만들어져 뿌듯함도 있었지만 역시 있는 라이브러리를 쓰는게 시간 최적화에서는 더 좋았던 것 같다. \n\n\n\n"},{"excerpt":"결론 supabase 기준, Sqlalchemy의 비동기 엔진에서 Postgresql pooler (supavisor)에 오류 없이 연결하기 위해서는  을 다음과 같이 구성해주면 된다.  statementcachesize, preparedstatementcache_size를 둘 다 모두 0으로 줘야 한다. 그리고 왜 발생하는지 모르겠는 간헐적인 오류(캐싱…","fields":{"slug":"/Sqlalchemy-비동기-엔진에서의-Postgresql-Pooler/"},"frontmatter":{"date":"January 09, 2024","title":"Sqlalchemy 비동기 엔진에서의 Postgresql Pooler","tags":["DataBase","SqlAlchemy","Python","Work"]},"rawMarkdownBody":"## 결론\n\nsupabase 기준, Sqlalchemy의 비동기 엔진에서 Postgresql pooler (supavisor)에 오류 없이 연결하기 위해서는 `create_async_engine` 을 다음과 같이 구성해주면 된다. \n\n```python\nfrom uuid import uuid4\nfrom asyncpg import Connection\n\n...(생략)...\n\nclass CConnection(Connection):\n    def _get_unique_id(self, prefix: str) -> str:\n        \"\"\"\n        캐싱을 만들 시에 캐싱 아이디가 중복으로 생성되어서 오류가 발생하는 경우가 있다.\n        statement_cache_size를 0으로 해도 일단 캐싱을 만들고 오류를 낸다.\n        따라서 캐싱을 사용하지 않을 것이므로, 아예 중복되지 않게 UUID로 생성해버린다.\n        \"\"\"\n        return f'__asyncpg_{prefix}_{uuid4()}__'\n\n...(생략)...\n\nDATABASE_URL = f\"postgresql+asyncpg://postgres.[Project ID]:[Password]@aws-0-[Region].pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        connect_args={\n            \"statement_cache_size\": 0,\n            'connection_class': CConnection,\n        },\n    )\n```\n\nstatement_cache_size, prepared_statement_cache_size를 둘 다 모두 0으로 줘야 한다.\n\n그리고 왜 발생하는지 모르겠는 간헐적인 오류(캐싱 ID를 중복되게 생성하려고 시도함)를 해결하기 위해 Connection 클래스를 상속받아 _get_unique_id 메소드를 오버라이딩 해주고, 해당 클래스를 connection_class로 사용하도록 했다. 이렇게 하면 id 중복 생성 오류를 방지할 수 있다. \n\n\n\n일반적인 글이라면 결론이 서두에 나온다면 너무나 시시하겠지만 기술 블로그이니 괜찮다. \n\n## 문제 해결에 관한 글\n\n### 발단\n\nsupabase에서 메일이 왔다. \n\n![](image1.png)\n여태 본 기억이 없었는데, Final reminder라도 되어있는걸 보니 이미 여러번 고지했던 모양이다. \n\n읽어보니 Pgbouncer 지원이 종료된다는 내용이다. 지원이 종료되는건 알고 있었는데, 아예 Connection string을 죽여버리는 모양이다. 잔인하다는 생각이 들었지만 어차피 우리는 Pooler는 진작에 포기했으므로 상관이 없었다. \n\n문제는 다음이었는데, DB Direct access에 대한 IPv4 주소 지원도 끝난다니 이건 좀 문제의 소지가 있었다. \n\n마침 어제 지난번 DB 세팅에 대한 회고를 끝내서 다행이었다. \n\n### 전개\n\n선택지는 두가지이다. Supavisor 도입, 네트워크가 IPv6를 지원하도록 설정하고 Direct 연결 방법을 계속해서 사용. \n\n개인적으로 \n\n- 네트워크 설정에 상대적으로 약함\n\n- WebRTC를 하던 때의 기억 때문에 IPv6에 안좋은 감정이 있음 \n\n- 현재 백엔드의 네트워크 설정을 직접 진행하지 않았었음 \n\n- 혼자 출근한 백엔드는 나 혼자임\n\n- Pooler와 씨름을 한 기억이 최근이고, 회고는 더더욱 최근임\n\n의 이유로 Supavisor 도입에 대해 다른 백엔드가 출근하기 전 잠깐 살펴보기로 했다.\n\n그런데 왠걸? 직접 설치부터 해야 하나라고 막연히 생각을 했었는데(supabase의 공식 문서에는 아직 supavisor에 대한 내용이 적어도 찾기 쉬운 곳에 있지는 않다. ) 그렇지 않았다. \n\n[https://github.com/orgs/supabase/discussions/17817](https://github.com/orgs/supabase/discussions/17817)\n\n해당 링크를 보아하니 이미 꽤 지원이 진행된 상태였는지, connection string만 바꾸면 간단하게 Pgbouncer가 아니라 Supavisor를 사용하여 연결할 수 있다고 적혀 있었다. \n\n그래서 해당 링크를 참조해 Connection string은 약간 수정을 해야 했지만, 어쨌든 Supavisor 연결에 간단하게 성공해버렸다. \n\n### 위기\n\n곧장 과거 겪었던 것과 같은 문제가 발생했다. Sqlalchemy 입장에서는 Pgbouncer와 Supavisor를 따로 구분하지 못하는지 Transaction 또는 Session 풀 모드에서는 캐싱이 지원되지 않는다는 에러 문구가 발생했다. \n\n여기서 잠깐, 포기하고 얌전히 IPv6 설정을 보러 갈까 했는데, 아직 다른 백엔드가 아무도 출근하지 않았다. \n\n일단 Pool mode를 바꿔야 다음 단계의 설정을 진행할 수 있었는데, Pgbouncer의 Pool mode는 supabase 콘솔에서 세팅을 지원하지만 supavisor는 어떻게 바꿔야 할지 의문이었다. \n\n해당 의문은 말도 못하게 간단하게 풀렸다. 콘솔에 들어가보니 어느새 쥐도 새도 모르게 Pgbouncer 관련 섹션이 supavisor 관련 섹션으로 바뀌어있었다. \n\n![](image2.png)\n위 이슈에서 설명된 supavisor 관련 connection string은 `user` 로 시작하는 부분이 옳지 않아서 수정이 필요했는데, 콘솔의 pgbouncer 관련 연결 connection string 에는 제대로 적혀있었다. 아무래도 메일에 적어보낸 이슈이지만 따로 수정을 하지는 않은 모양이다.. \n\n아무튼간 pool mode를 변경 후 테스트를 할 수 있었다. \n\n### 절정1\n\nPool mode를 바꿨다고 해서 바로 연결이 가능하지는 않다. 서버 인스턴스를 실행하면 짜잔! 성공적인 DB 연결 대신 다음과 같은 문구를 볼 수 있다. \n\n```plain text\nsqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.InvalidSQLStatementNameError'>: prepared statement \"__asyncpg_stmt_b__\" does not exist\nHINT:  \nNOTE: pgbouncer with pool_mode set to \"transaction\" or\n\"statement\" does not support prepared statements properly.\nYou have two options:\n* if you are using pgbouncer for connection pooling to a\n  single server, switch to the connection pool functionality\n  provided by asyncpg, it is a much better option for this\n  purpose;\n* if you have no option of avoiding the use of pgbouncer,\n  then you can set statement_cache_size to 0 when creating\n  the asyncpg connection object.\n```\n\nSqlalchemy를 사용중이기 때문에 첫번째 방법은 자연스럽게 더 고려할것도 없이 기각이며, statement_cache_size를 0으로 수정하는 것이 현재 내 인프라에서 선택할 수 있는 방법이었다.\n\n[https://docs.sqlalchemy.org/en/20/dialects/postgresql.html](https://docs.sqlalchemy.org/en/20/dialects/postgresql.html)\n\n지난번 기나긴 고생 이후 공식 문서를 항상 참조하는 습관을 들이게 됐다. 아주 긍정적인 방향이라고 생각한다. \n\n공식 문서를 참조하면,\n\n```plain text\nengine = create_async_engine(\"postgresql+asyncpg://user:pass@hostname/dbname?prepared_statement_cache_size=0\")\n```\n\n의 방식으로 statement_cache_size를 0으로 설정할 수 있는 것을 알 수 있다. \n\n이 설정을 해준 후, 다시 서버 인스턴스를 실행하면 짜잔! DB 연결이 아주 잘 된다. 쿼리도 잘 날리고, 결과도 잘 받아온다.\n\n이 와중에 Cache 생성 시에 같은 인덱스로 중복 생성되어서 오류가 발생하기도 했다. 그래서 SqlAlchemy의 Connection 클래스를 상속받아 캐시의 아이디를 생성하는 메소드를 수정해주었다. \n\n```python\nfrom asyncpg import Connection\nfrom uuid import uuid4\n\nclass CConnection(Connection):\n    def _get_unique_id(self, prefix: str) -> str:\n        \"\"\"\n        캐싱을 만들 시에 캐싱 아이디가 중복으로 생성되어서 오류가 발생하는 경우가 있다.\n        statement_cache_size를 0으로 해도 일단 캐싱을 만들고 오류를 낸다.\n        따라서 캐싱을 사용하지 않을 것이므로, 아예 중복되지 않게 UUID로 생성해버린다.\n        \"\"\"\n        return f'__asyncpg_{prefix}_{uuid4()}__'\n```\n\n이후 DB 커넥션을 만들 때에 해당 클래스를 기본 Connection 클래스 대신 사용하도록 설정해주면 된다. \n\n```python\nDATABASE_URL = f\"postgresql+asyncpg://postgres.[Project ID]:[Password]@aws-0-[Region].pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        connect_args={\n            'connection_class': CConnection,\n        },\n    )\n```\n\n여기까지 작업을 해줘서 오류를 해결하고 같은 쿼리를 연속으로 날리면? \n\n짜잔! 다음과 같은 오류를 볼 수 있다. \n\n```plain text\nsqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.InvalidSQLStatementNameError'>: prepared statement \"__asyncpg_stmt_9462b5fe-88dc-46f5-98fb-57981ea56de0__\" does not exist\nHINT:  \nNOTE: pgbouncer with pool_mode set to \"transaction\" or\n\"statement\" does not support prepared statements properly.\nYou have two options:\n* if you are using pgbouncer for connection pooling to a\n  single server, switch to the connection pool functionality\n  provided by asyncpg, it is a much better option for this\n  purpose;\n* if you have no option of avoiding the use of pgbouncer,\n  then you can set statement_cache_size to 0 when creating\n  the asyncpg connection object.\n```\n\n그렇다, 오류는 반복된다. \n\n### 절정2\n\n이미 여기까지 시간을 꽤 쓴 상태였기 때문에 조금만 더 무언가를 해보기로 했다. \n\n캐시 사용을 아무리 꺼도, 도대체가 캐시를 계속 만들고 거기서 오류를 내는 이 상황. \n\n다행히 비슷한 오류를 겪는 사람들을 깃허브 이슈에서 찾을 수 있었다. \n\n이미 나의 신뢰를 많이 잃어버린 챗지피티에게 이 문제를 상담하면 `statement_cache_size` 옵션을 0으로 설정해서 connect_args에 넣으라고 한다. PgBouncer로 붙이려고 애써봤었을 때에 해당 옵션을 넣어도 아무런 변화가 없이 그대로 오류가 발생했었고, 공식문서에는 존재하지 않는 옵션이어서 이번에는 무시하고 넘어갔었다. \n\n근데 충격적이게도 한가지 옵션을 썼을 땐 되지 않고 두 옵션을 동시에 사용하면 된다고 한다.. 이게 무슨 일일까? 나는 왜 두 옵션을 동시에 쓸 생각을 못했을까?(이 생각을 어떻게 하나요?)\n\n최종적으로 DB Connection String을 다음과 같이 설정해주었다. \n\n```python\nfrom uuid import uuid4\nfrom asyncpg import Connection\n\n...(생략)...\n\nclass CConnection(Connection):\n    def _get_unique_id(self, prefix: str) -> str:\n        \"\"\"\n        캐싱을 만들 시에 캐싱 아이디가 중복으로 생성되어서 오류가 발생하는 경우가 있다.\n        statement_cache_size를 0으로 해도 일단 캐싱을 만들고 오류를 낸다.\n        따라서 캐싱을 사용하지 않을 것이므로, 아예 중복되지 않게 UUID로 생성해버린다.\n        \"\"\"\n        return f'__asyncpg_{prefix}_{uuid4()}__'\n\n...(생략)...\n\nDATABASE_URL = f\"postgresql+asyncpg://postgres.[Project ID]:[Password]@aws-0-[Region].pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        connect_args={\n            \"statement_cache_size\": 0,\n            'connection_class': CConnection,\n        },\n    )\n```\n\n### 결말 & 에필로그\n\n지난번 기나긴 고생과 삽질을 통해서 챗지피티에 대한 신뢰를 덜고 공식문서를 신뢰해야 한다는 사실을 깨달았다. \n\n그리고 이번 짧은(반나절 정도) 삽질을 통해서 공식문서도 100% 정답은 아니라는 사실을 깨달았다. (나는 무엇을 믿어야 할까?)\n\n생각해보면, Database url 에다가 옵션을 넣는다고 해서 내부적으로 뭔가 변경되는 것일리가 없다. 결국 엔진을 만들 때에 옵션값을 넣어줘야 내부적으로 뭔가 알고 다른 작동을 하지 않을까? 이 생각을 막바지에 와서 떠올렸다. \n\n항상 시야를 넓게 하고, 의심하면서 만들어야 한다는 것을 다시 한 번 깨달았다. 남들이 시키는대로 해서는 되는것이 없는 것이다. \n\n별개로, 내가 잘못을 한 것인지 뭔지 Sqlalchemy도 FastAPI도 Postgresql도 모두 유명한 것들인데 어째서 이렇게까지 불완전하게 작동하는지 의문이 든다. 역사가 길진 않지만 그래도 수년의 시간은 지난 것들인데, 가장 기본적인 부분이 이렇게 불안정할지는 예상을 못했다. FastAPI의 메모리 누수 문제도 뒤늦게 알게 되었는데, 메모리 누수 문제에 이어서 비동기 엔진에서의 DB Pooler 연결에 이렇게 문제가 있을 줄 알았다면 FastAPI와 SqlAlchemy를 절대 선택하지 않았을 것 같다. \n\n고찰, 의심, 경험.. 등등이 중요하다고 다시 한 번 느꼈다. \n\n"},{"excerpt":"현재 회사의 서비스에서 RDB를 사용하지 않고 NoSQL(DynamoDB)만 사용해서 어플리케이션 서비스를 하고 있었다.  필연적으로 RDB를 도입하게 되었고, supabase를 사용해보고 싶어서 DB는 깊은 고찰 없이 아무도 사용해본 적이 없었던 Postgresql을 고르게 되었다. 이것이 고생의 시작이었다.  그 고생을 기록해둔다.  1. 커넥션 직접…","fields":{"slug":"/FastAPI에서-Postgresql의-커넥션-관리/"},"frontmatter":{"date":"December 15, 2023","title":"FastAPI에서 Postgresql의 커넥션 관리","tags":["Python","FastAPI","DataBase","Work","Postgresql"]},"rawMarkdownBody":"현재 회사의 서비스에서 RDB를 사용하지 않고 NoSQL(DynamoDB)만 사용해서 어플리케이션 서비스를 하고 있었다. \n\n필연적으로 RDB를 도입하게 되었고, supabase를 사용해보고 싶어서 DB는 깊은 고찰 없이 아무도 사용해본 적이 없었던 Postgresql을 고르게 되었다.\n\n이것이 고생의 시작이었다. \n\n그 고생을 기록해둔다. \n\n### 1. 커넥션 직접 연결\n\nMySQL, Oracle, MsSQL등을 사용해봤지만 Postgresql은 처음이었다. 아무 생각없이 커넥션을 바로 연결했고 곧장 문제가 발생했다. \n\nsupabase에서 사용하는 요금제에서 커넥션을 65개까지 만들 수 있게끔 되어있었는데, 수시로 이 수치를 넘어버렸다. 작업 시작 당시 테스트 서버에서는 인스턴스를 5개를 사용하고 있었고, 기본값으로 인스턴스 하나 당 연결 5개를 사용하고 있었서 단순히 산술적으로는 65개를 넘어서는 안되는데 계속해서 65개를 넘겨서 DB 서버가 지속적으로 죽고 있었다. \n\n### 2. 커넥션 폭증, 첫번째 추측\n\n첫번째 추측으로는 세션의 생명주기를 관리하는 코드가 정상적으로 작동하는지 여부를 의심했다. 비동기에서의 세션 관리를 위해 의존성 주입 방식을 통한 세션 관리 방법을 선택하고 있었는데, 해당 코드를 다음과 같이 강화해주었다. \n\n#### 수정 전 코드\n\n```python\nasync def get_db_session(db_conn: Engine = Depends(get_db_connection)) -> AsyncIterable[Session]:\n    async with sessionmaker(db_conn, class_=AsyncSession, expire_on_commit=False)() as session:\n        yield session\n```\n\n#### 수정 후 코드\n\n```python\nasync def get_db_session(db_conn: Engine = Depends(get_db_connection)) -> AsyncIterable[Session]:\n    try:\n        async with sessionmaker(db_conn, class_=AsyncSession, expire_on_commit=False)() as session:\n            yield session\n    except Exception as e:\n        logger.error(f\"[get_db_session] {e}\")\n        await session.rollback()\n        raise\n    finally:\n        await session.close()\n```\n\n이론적으로는 위 코드만 사용해도 정상적으로 세션의 생명주기가 관리가 되어야 하는데, 해외에도 나와 같은 이슈를 겪는 사람이 있는 것으로 보였다. 아래처럼 수정 후에 눈에 띄게 커넥션 숫자가 줄었지만 여전히 DB 서버는 개복치 그 자체였다. 조금 나아지긴 했지만 숨만 쉬면 죽어나가는 상황이 반복되었고 여전히 실사용이 불가능한 심각한 상태의 환자였다. \n\n### 3. 커넥션 폭증, 두번째 추측\n\n두번째 추측으로는 dev는 테스트를 위해서 인스턴스 재생성이 꽤 잦았는데, 이 과정에서 Connection이 계속해서 다시 만들어지면서 연결이 폭증하는 것으로 보였다. \n\nPostgresql 설정에서 IDLE 상태로 들어간지 오래된 커넥션은 강제로 죽이는 옵션도 찾았는데, supabase에서는 해당 옵션값 설정은 지원하지 않았다. 아무래도 일반적인 대응 방법은 아닌것으로 보였다. \n인스턴스가 재생성 될때마다 연결이 안끊긴다면? 애초에 연결을 많이 안만들면 되는게 아닌가? 라는 결론에 다다랐다. 따라서 자연스럽게 Pooler를 사용하기로 했다. Supabase에서도 공식적으로 Pooler를 (당연히) 지원하므로 이는 완벽한 해결책으로 생각됐다. \n\n\n        Pooler란?\n\n데이터베이스 서버와 클라이언트 애플리케이션 간의 연결을 관리하는 소프트웨어이다. 이는 데이터베이스 서버에 대한 연결 요청을 효율적으로 처리하고 성능을 최적화하는 데 도움을 준다. \n\n#### 연결 재사용\n\n풀러는 데이터베이스 서버에 대한 연결을 재사용 할 수 있게 함으로써 매번 새로운 연결을 생성하는 오버헤드를 감소시킨다. \n\n#### 부하 관리\n\n동시에 많은 클라이언트 요청을 처리할 수 있도록 도와주며 데이터베이스 서버에 가해지는 부하를 분산시킨다. \n\n#### 성능 최적화\n\n연결 설정과 해제에 소요되는 시간을 줄임으로써 전반적인 애플리케이션 성능을 향상시킨다. \n\n#### 자원 관리\n\n데이터베이스 연결에 사용되는 리소스를 효율적으로 관리한다. \n\n#### 스케일링\n\n애플리케이션 확장성을 지원하며 사용자 요청 증가 시에도 안정적인 서비스 제공을 돕는다. \n\n풀러의 사용은 특히 많은 수의 동시 사용자와 트랜잭션이 있는 대규모 시스템에서 매우 중요하다. 데이터베이스에 대한 연결을 효과적으로 관리함으로써 시스템의 안정성, 성능, 그리고 확장성을 향상시킬 수 있다. \n\n### 4. Pooler 도입\n\nsupabase의 pooler는 PgBouncer를 제공하며, 사용법이 매우 간단했다. 기존에 사용하던 5432번 포트 대신 6543 포트로 바꿔서 연결해주기만 하면 됐다. \n\n또 다시 곧장 문제가 발생했다. Supabase의 PgBouncer  Poolmode는 기본적으로 Transation 모드였는데, 해당 모드에서는 SqlAlchemy의 비동기 엔진의 캐싱 기능을 지원하지 않았다. \n\n\n        Pgbouncer의 Pool Mode\n\n#### Session\n\n가장 기본적인 풀링 모드로 클라이언트가 연결을 끊을 때까지 PgBouncer는 해당 클라이언트 연결을 데이터베이스 서버에 계속 연결 상태로 유지한다. \n\n#### Transaction\n\n이 모드에서 PgBouncer는 각 SQL 트랜잭션이 완료될 때마다 연결을 풀로 반환한다. 이는 트랜잭션간에 서버 세션 상태를 유지하지 않아므로 세션 수준의 상태 설정이 트랜잭션간에 유지되지 않는다. Supabase의 Postgresql의 Pool mode는 기본적으로 Transacion 모드로 설정되어있다. \n\n#### Statement\n\n가장 제한적인 풀링 모드로 각 SQL 문(statement)이 완료될 때마다 연결을 풀로 반환한다. Supabase의 Postgresql은 해당 모드를 지원하지 않는다. \n\n### 5. Pooler 사용시 인스턴스 생성 에러 발생\n\n이번엔 문제가 바로 발생하지는 않았다. 당일은 지나갔으나 다음날이 되어서야 발견했다. 인스턴스 오류가 발생해서 항사 연결이 끊기는 것도 아니고 간헐적으로 연결이 끊기고 있었다. \n\n#### 당시 발생한 인스턴스 생성 에러\n\n```plain text\nLog ID\n5c0963cf-d16a-4cf3-b3fe-33c99147f879\nLog Timestamp (UTC)\n2023-12-11T02:36:52.732Z\nLog Event Message\npassword authentication failed for user \"postgres\"\nLog Metadata\n[\n  {\n    \"file\": null,\n    \"host\": \"db-fskkusemekciragirqyd\",\n    \"metadata\": [],\n    \"parsed\": [\n      {\n        \"application_name\": null,\n        \"backend_type\": \"client backend\",\n        \"command_tag\": \"authentication\",\n        \"connection_from\": \"61.1.172.3:56746\",\n        \"context\": null,\n        \"database_name\": \"postgres\",\n        \"detail\": \"Connection matched pg_hba.conf line 91: \\\"host  all  all  0.0.0.0/0     scram-sha-256\\\"\",\n        \"error_severity\": \"FATAL\",\n        \"hint\": null,\n        \"internal_query\": null,\n        \"internal_query_pos\": null,\n        \"leader_pid\": null,\n        \"location\": null,\n        \"process_id\": 89111,\n        \"query\": null,\n        \"query_id\": 0,\n        \"query_pos\": null,\n        \"session_id\": \"657675c4.15c17\",\n        \"session_line_num\": 2,\n        \"session_start_time\": \"2023-12-11 02:36:52 UTC\",\n        \"sql_state_code\": \"28P01\",\n        \"timestamp\": \"2023-12-11 02:36:52.732 UTC\",\n        \"transaction_id\": 0,\n        \"user_name\": \"postgres\",\n        \"virtual_transaction_id\": \"47/2627\"\n      }\n    ],\n    \"parsed_from\": null,\n    \"project\": null,\n    \"source_type\": null\n  }\n]\n```\n\n같은 설정에 같은 서버인데 뜬금없이 간헐적으로 패스워드 에러가 발생하니 죽을맛이었다. \n\n그러면 안되지만, 오류의 빈도수라도 적다면 그냥 무시하고 넘어갈텐데 무시하기엔 높은 빈도로 에러가 발생했다. PgBouncer의 지원이 2024년 1월로 끝난다고 하니 애초에 비동기와의 호환성 자체가 좋지 않아보였다. 이 시점에서 Pooler 사용을 포기하고 다른 방법을 찾기로 했다. \n\nPgBouncer 지원이 끊기고 대안으로 Superbase에서는 [https://github.com/supabase/supavisor](https://github.com/supabase/supavisor)를 제시하고 있었는데, 아직 그다지 완성도가 높지 않아보였으며(본인들의 공식 문서에도 없으므로) 시간이 넉넉하지 않아 도입이 망설여졌다. 그래서 다시 커넥션 직접 연결을 선택하고, 커넥션이 폭증하는 문제 자체를 해결하기로 했다. \n\n### 6. 커넥션 직접 연결로 회귀\n\n여담으로, 일을 하면서 챗지피티의 도움을 참 많이 받는다. 그런데 이번 업무를 처리하면서 Deep한 업무일수록 챗지피티가 4.0이라도 헛소리를 많이 한다는 사실을 알게됐다. 어떻게 알았냐고? 나도 알고 싶지 않았다. \n\n아예 처음으로 돌아와서 커넥션 폭증에 대해서 다시 한 번 알아보기로 했다. 이쯤에서 이미 실서버는 오픈을 해야 했기 때문에, 커넥션 직접 연결로 바꾸고 추가 결제를 해서 커넥션 총 개수를 200여개로 늘려버렸다. 실서버에서는 인스턴스가 바뀌는 일이 자주 있는 일이 아니었으므로, 모니터링 결과 IDLE 상태로 접어든 죽은 커넥션도 12시간 안에 종료가 되는 것을 경험적으로 확인했으므로 실서버에서는 200개 정도로 커넥션 연결 가능 개수를 늘려두면 어쨌건 서비스가 가능했다. \n\n이렇게 실서버를 열어두고 문제를 해결하기 위해 시간을 녹이기 시작했다. \n\n커넥션 자체의 관리는 이론적으로는 아주 간단했다. 앱을 시작할 때에 커넥션을 열며, 앱을 종료할 때 커넥션을 닫는다. 이를 위해서 FastAPI의 `on_event(”startup”)`과 `on_event(”shutdown”)` 을 사용했다. 챗지피티도 이 코드를 알려줬고, 어느 블로그를 봐도 모두 이 코드를 사용하고 있었다. \n\n[https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated](https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated)\n\n근데 충격적이게도 이 방법은 이미 지원이 중단된 방법이었다.  이 방법 대신 lifespan 방법을 선택해야 했는데, 이 방법에 대해서 챗지피티는 알고 있음에도 불구하고 여전히 지원 중단된 방법을 추천하는 것을 보고  이 시점에서 챗지피티에 대한 신뢰를 상당 부분 잃어버리고 사용 빈도가 확 줄게 되었다. 나중에 알게 되지만 이 코드의 문제는 아니었지만! 아무튼 신뢰가 엄청나게 없어져버렸다. \n\n해당 코드를 lifespan을 활용한 코드로도 변경하고도 여전히 문제는 반복됐다. 앱이 시작될 때는 코드가 정상적으로 작동했지만, 컨테이너 안에서 앱이 종료될 때에는 종료 코드가 정상적으로 작동하지 않았다. \n\n이를 위해서 sigterm을 활용해 직접 종료 코드를 잡아서 실행하는 방법도 고려했지만 이 방법 역시 작동하지 않았다. \n\n하지만 아주 삽질은 아니어서, 로컬에서 실행한 경우에는 정상적으로 종료 코드가 작동하고 커넥션이 끊어지는 것을 확인해버렸다. \n\n### 7. 문제 해결\n\n힞;민 모드를 바꿨음에도 캐싱은 여전히 작동하지 않아서 캐싱을 강제로 꺼야 했고 잘 꺼지지도 않았는지 이후에도 Sqlalchemy의 캐싱 과정에서 키값이 중복되게 생성되는 문제가 있어 해당 부분을 오버라이드 해서 키 값이 중복되지 않게 만들게 해주는 등 문제가 있었지만 일단은 연결이 끊기거나 커넥션이 폭증하지 않아 해결이 된 것으로 보였다. \n\n사실 이 전에 종료 신호가 정상적으로 어플리케이션에 도달하지 못하는 것이 아닌가? 에 대해서 인지를 하고 있었다. 도커 컨테이너를 강제 종료 하면 기본적으로 docker는 1번 프로세스에만 종료 명령을 정상적으로 전달하고 나머지 프로세스에게는 종료 명령을 정상 전달하지 않는다. 도커 작업은 다른 개발자가 진행했는데, 컨테이너 안에서 실행되는 작업이라곤 Uvicorn 하나이므로 당연히 1번 프로세스일것이다 라는 말을 들어서 이 부분을 점검해볼 생각을 못하고 있었다. \n\n근데, 로컬에서 실행했을때에는 정상적으로 커넥션이 끊어지는데 도커 컨테이너를 통해서 실행한 경우에는 정상 종료가 되지 않는다? 이 문제를 확인하고 나서야 이 부분을 점검해볼 생각이 들었고, 점검 결과 충격적이게도 Uvicorn이 1번 프로세스로 실행되지 않고 있었다. \n\nDumb-init이라는 Docker 컨테이너의 모든 프로세스에게 종료 신호를 보내주는 소프트웨어도 잠깐 고려했지만, 간단하게 Docker exec 부분을 수정해 Uvicorn이 1번 프로세스로 컨테이너 안에서 실행되게끔 하자 거짓말처럼 모든 문제가 해결되었다. \n\n### 8. 회고\n\n전 회사에서도 이미 만들어진 커넥션을 쓰거나, 그냥 가이드대로 연결하면 대충 됐기 때문에 DB와 서버의 연결에 관해서 깊게 고찰해본 적이 없었다. 이번 일을 겪으면서 일단 DB와 서버의 연결에 대해.. 대충 알고 있던 부분들에 대해서 깊은 학습이 되었다. \n\n문제해결 방법을 가장 마지막에 발견했지만, 사실 꼼꼼히 살폈다면 초장에 해결할 수 있던 간단한 문제였다. 돌고 돌아 삥 돌아 해결을 했지만 최종적인 코드 수정은 굉장히 적었다. 공부 했으니 한잔해~ 하고 넘어가기에는 무보수 야근이 너무 잦았기 때문에 (덕분인지 시원하게 A형 독감도 걸렸다. 아무튼 야근 탓이다. ) 억울한 부분이 크다. \n\n챗지피티에 대한 신뢰가 컸는데, 이번 일을 진행하면서는 챗지피티 때문에 뱅뱅 돌아가게 된 부분이 많았다. 확실히 겉핥기 부분만 챗지피티에게 기대야 하며(또는 반복적인 일) 딥 한 문제나 조금이라도 인터넷에 자료가 적은 문제들에 대해서는 공식문서를 신뢰해야 한다는 사실을 다시 한 번 뼈저리게 느꼈다. 이후로 나는 예전처럼 챗지피티를 많이 쓰지는 않는다.. 쓰더라도 조심히 사용한다. \n\n한편으로, 커넥션 직접 연결 방법을 선택해 문제를 해결했지만 이게 완벽히 올바른 방법이라고는 생각이 들지 않는다. 아직은 서비스의 크기가 완벽하게 크지는 않기 때문에 이 방법으로도 가능하지만, Pooler를 사용하는 것이 올바른 해결 방법이 아닐까? 지금도 단순히 문제를 피해 도망친 것이라고밖에 생각이 들지 않는다. 특히 Pooler로 인해 발생한 문제들은 풀리지 않은 미스테리가 너무 많이 남았다. \n\n추후 기회가 있다면 supavisor를 기반으로 다시 만들어보고 싶은 생각이 있다. 아니, 반드시 그래야 한다고 생각이 든다.\n\n\n\n"},{"excerpt":"회사에서 DynamoDB를 사용할 때에, PynamoDB ORM을 사용하여 연결을 하고 있다.  Postgresql을 도입했을 때에 커넥션 문제가 지속적으로 발생해 (연결이.. 끊어지지 않는다!) 이를 처리하던 도중 PynamoDB는 어떻게 커넥션을 관리하는지 궁금해서 찾아보게 되었다.  PynamoDB란? PynamoDB는 DynamoDB를 위한 Pyt…","fields":{"slug":"/PynamoDB와-boto3-PynamoDB의-커넥션/"},"frontmatter":{"date":"December 08, 2023","title":"PynamoDB와 boto3, PynamoDB의 커넥션","tags":["DataBase","AWS","Python","Work"]},"rawMarkdownBody":"회사에서 DynamoDB를 사용할 때에, PynamoDB ORM을 사용하여 연결을 하고 있다. \n\nPostgresql을 도입했을 때에 커넥션 문제가 지속적으로 발생해 (연결이.. 끊어지지 않는다!) 이를 처리하던 도중 PynamoDB는 어떻게 커넥션을 관리하는지 궁금해서 찾아보게 되었다. \n\n### PynamoDB란? \n\nPynamoDB는 DynamoDB를 위한 Python 라이브러리이다. \n\n#### Pythonic Interface\n\n파이썬 개발자들이 익숙한 객체 지향 접근 방식을 제공한다. \n\n#### 간편한 데이터 모델링\n\nPynamoDB를 사용하면 DynamoDB 테이블과 항목을 Python 클래스로 모델링 할 수 있다. \n\n#### CRUD Operation\n\nPynamoDB는 Create, Read, Update, Delete 작업을 간편하게 수행할 수 있는 메소드를 제공한다. \n\n### PynamoDB의 커넥션 관리\n\nPynamoDB에서 커넥션은 boto3를 사용해서 관리하며, Low Level의 API를 사용하면 커넥션을 직접 연결할 수 있지만 기본적으로는 자동으로 관리를 한다. 설정 몇가지만 해주면(또는 하지 않아도) 필요에 따라서 알아서 커넥션을 맺고 끊는다. \n\n[https://pynamodb.readthedocs.io/en/stable/settings.html](https://pynamodb.readthedocs.io/en/stable/settings.html)\n\n### boto3란? \n\nAWS를 위한 AWS 공식 Python SDK이다. 이 라이브러리를 통해 Python 어플리케이션에서 AWS를 쉽게 사용 및 관리 할 수 있다. \n\n#### 광범위한 AWS 서비스 지원\n\nS3, EC2, DynamoDB, IAM 등 AWS에서 제공하는 대부분의 서비스를 지원한다. \n\n#### 높은 수준의 객체 지향 API와 low level API 제공\n\n`Resource API`는 높은 수준의 객체 지향 인터페이스를 제공하여 사용자가 AWS 리소스를 직관적으로 다룰 수 있게 해준다. \n\n`Client API` 는 낮은 수준, 서비스 중심의 인터페이스를 제공하여 더 세밀한 제어가 필요할 때 사용할 수 있다. \n\n#### 자격 증명 관리와 보안\n\nAWS의 자격 증명을 효과적으로 관리할 수 잇는 기능을 제공한다.    \n\n"},{"excerpt":"소개 애플 단축어를 활용해 간단하게 슬랙 상태를 원클릭으로 변경할 수 있습니다. 슬랙의 API를 이용합니다.  저는 개인적으로 raycast의 뽀모도로 익스텐션과 함께 다음과 같이 사용하고 있습니다. \n집중을 시작할 때에  집중 모드 킴 음량 조절 노래 재생 슬랙 방해금지 상태 변경 \n집중을 끝낼 때에 집중 모드 끔 슬랙 방해금지 상태 해제 집중 모드에서…","fields":{"slug":"/단축어로-SLACK-프로필-변경하기/"},"frontmatter":{"date":"November 17, 2023","title":"단축어로 SLACK 프로필 변경하기","tags":["ETC","Work"]},"rawMarkdownBody":"## 소개\n\n애플 단축어를 활용해 간단하게 슬랙 상태를 원클릭으로 변경할 수 있습니다. 슬랙의 API를 이용합니다. \n\n저는 개인적으로 raycast의 뽀모도로 익스텐션과 함께 다음과 같이 사용하고 있습니다.\n\n![](image1.png)\n집중을 시작할 때에 \n\n- 집중 모드 킴\n\n- 음량 조절\n\n- 노래 재생\n\n- 슬랙 방해금지 상태 변경\n\n![](image2.png)\n집중을 끝낼 때에\n\n- 집중 모드 끔\n\n- 슬랙 방해금지 상태 해제\n\n\n\n집중 모드에서는 다른 알림을 모두 꺼두고 슬랙 알림만 켜서 업무 흐름은 놓치지 않게 해두었습니다. \n\n![](image3.png)\n메뉴바에서 원클릭으로 설정할 수 있어 편리합니다. \n\n## 설정법\n\n### 슬랙의 API App 생성\n\n먼저 슬랙의 API App을 만들어야 합니다. \n\n[https://api.slack.com/](https://api.slack.com/)\n\n해당 페이지로 이동 후, 우측 상단의 Your apps으로 이동합니다. \n\n`Create New App` 을 클릭합니다. \n\n`From an app manifest` 를 선택 후, workspace를 고르고 Next 합니다. \n\n`YAML`을 고르고, 다음의 내용을 입력합니다. \n\n```yaml\ndisplay_information:\n  name: Apple 상태변경 단축어\noauth_config:\n  scopes:\n    user:\n      - users.profile:write\nsettings:\n  org_deploy_enabled: false\n  socket_mode_enabled: false\n  token_rotation_enabled: false\n```\n\n권한을 더 원한다면 더 많이 입력해주셔도 됩니다. \n\n`Create`를 눌러 앱을 생성합니다. \n\n\n\n그 후 바로 그 화면에서 `Install to Workspace`를 클릭해 앱을 인스톨합니다. \n\n\n\n그리고, 좌측 메뉴에서 `OAuth & Permissions` 메뉴로 이동합니다. \n\n**OAuth Token** 값을 복사해줍니다. 슬랙 페이지에서 할 일은 여기까지입니다. \n\n### 단축어 설정\n\n단축어 앱을 열고 새로운 단축어를 추가합니다. \n\n`URL 콘텐츠 가져오기` 를 검색해서 다음과 같이 설정합니다. \n\n![](image4.png)\n입력해야 하는 값은 다음과 같습니다. \n\n- 헤더\n\n    키 : Authorization\n\n    값 : Bearer xoxp-xxxxx-xxxx…\n\n    \n\n    키 : Content-Type\n\n    값 : application/json; charset=UTF-8\n\n- 바디\n\n    키 : profile\n\n    값 : \n\n    ```json\n    {\n    \t\t\"status_text\": \"방해금지\",\n    \t\t\"status_emoji\": \":no_entry_sign:\"\n    }\n    ```\n\n    원하는 이모지와 텍스트를 넣어주시면 됩니다. \n\n    \n\n이렇게 하면 단축어가 완성됩니다. \n\n## 참고\n\n[https://api.slack.com/apps?deleted=1](https://api.slack.com/apps?deleted=1)\n\n"},{"excerpt":"aiohttp는 파이썬의 비동기 HTTP 네트워킹 라이브러리이다. 이 라이브러리는 asyncio를 사용하여 비동기 I/O를 수행하고 클라이언트와 서버 양쪽 모두에 대한 HTTP 지원을 제공한다.  즉 aiohttp를 사용하면 비동기적으로 HTTP 요청을 보내고 응답을 받을 수 있다.  주요 특징 비동기/동시성 지원 async, await를 사용하여 동시에…","fields":{"slug":"/aiohttp/"},"frontmatter":{"date":"November 08, 2023","title":"aiohttp","tags":["Python"]},"rawMarkdownBody":"aiohttp는 파이썬의 비동기 HTTP 네트워킹 라이브러리이다. 이 라이브러리는 asyncio를 사용하여 비동기 I/O를 수행하고 클라이언트와 서버 양쪽 모두에 대한 HTTP 지원을 제공한다. \n\n즉 aiohttp를 사용하면 비동기적으로 HTTP 요청을 보내고 응답을 받을 수 있다. \n\n## 주요 특징\n\n#### 비동기/동시성 지원\n\nasync, await를 사용하여 동시에 여러 HTTP 요청을 비동기적으로 처리할 수 있다. \n\n#### 서버와 클라이언트 사이드 지원\n\nHTTP 클라이언트 기능과 HTTP 서버 기능을 모두 제공한다. \n\n#### 웹소켓 지원\n\n웹소켓 연결 및 통신을 지원한다. \n\n#### 신호 및 슬롯 매커니즘\n\n요청 처리 과정에서 다양한 이벤트에 대응할 수 있게 해준다. \n\n## 설치\n\n파이썬 설치에 기본적으로 포함되지 않은 외부 라이브러리이다. \n\n```bash\npip install aiohttp\n```\n\npip를 이용해 간단하게 설치할 수 있다. \n\n## 예제\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(session, url):\n    async with session.get(url) as response:\n        return await response.text()\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        html = await fetch(session, 'http://python.org')\n        print(html)\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n```\n\npython 공식 홈페이지에서 HTML을 비동기적으로 가져오고 출력하는 간단한 예제이다. \n\n## 세션 관리\n\n**ClientSession 객체를 사용하여 HTTP 클라이언트 세션을 관리할 수 있다.** \n\n#### 연결 재사용\n\n여러 요청을 수행할 때, 세션은 자동으로 재연결을 사용한다.\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.get('http://httpbin.org/get') as resp:\n            print(await resp.text())\n        async with session.get('http://httpbin.org/get') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 쿠키 관리\n\n세션은 자동으로 쿠키를 관리한다. 첫 번째 요청에서 서버로부터 받은 쿠키는 자동으로 저장되고 다음 요청에 사용된다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        # 첫 번째 요청\n        async with session.get('http://httpbin.org/cookies/set?cookie_name=cookie_value') as resp:\n            print(await resp.text())\n        # 두 번째 요청 - 첫 번째 요청에서 설정된 쿠키가 포함됩니다.\n        async with session.get('http://httpbin.org/cookies') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 헤더의 기본값 설정\n\n세션은 생성할 때 기본 헤더를 설정할 수 있으며 이후의 모든 요청에 이 헤더가 포함된다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    headers = {'Authorization': 'Bearer your_token'}\n    async with aiohttp.ClientSession(headers=headers) as session:\n        async with session.get('http://httpbin.org/headers') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 타임아웃 관리\n\n세션 또는 개별 요청에 대한 타임아웃을 설정할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\nfrom aiohttp import ClientTimeout\n\nasync def main():\n    timeout = ClientTimeout(total=5)  # 전체 요청에 대한 타임아웃을 5초로 설정\n    async with aiohttp.ClientSession(timeout=timeout) as session:\n        async with session.get('http://httpbin.org/delay/10') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 커스텀 설정\n\nSSL 검증 비활성화, 프록시 설정 등의 커스텀 설정을 할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.get('https://self-signed.badssl.com/', ssl=False) as resp:  # SSL 검증 비활성화\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 자원 해제\n\n`async with` 문을 사용하면 세션 사용이 끝날 때 자동으로 자원을 해제한다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:  # 세션 생성 및 자동 자원 해제\n        async with session.get(url) as response:\n            return await response.text()\n\nasync def main():\n    html = await fetch('http://python.org')\n    print(html)\n\nasyncio.run(main())\n```\n\n## 에러 핸들링\n\n#### HTTP 응답 에러\n\n서버가 4XX 클라이언트 에러 또는 5XX 서버 에러를 반환하는 경우, `ClientResponseError` 가 발생할 수 있다. 이는 `raise_for_status()` 메서드를 사용하여 처리할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url) as response:\n                response.raise_for_status()\n                return await response.text()\n        except aiohttp.ClientResponseError as e:\n            print(f\"HTTP Response Error: {e.status} {e.message}\")\n        except aiohttp.ClientError as e:\n            print(f\"HTTP Client Error: {str(e)}\")\n        except Exception as e:\n            print(f\"Unexpected Error: {str(e)}\")\n\nasync def main():\n    await fetch('http://httpbin.org/status/400')  # 이 URL은 400 Bad Request를 반환합니다.\n\nasyncio.run(main())\n```\n\n#### 연결 에러\n\n네트워크 문제 또는 DNS 문제로 인해 연결을 설정할 수 없는 경우 `ClientConnectError` 예외가 발생할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url) as response:\n                return await response.text()\n        except aiohttp.ClientConnectorError as e:\n            print(f\"Connection Error: {str(e)}\")\n\nasync def main():\n    await fetch('http://nonexistent.url')  # 존재하지 않는 URL\n\nasyncio.run(main())\n```\n\n#### 타임아웃 에러\n\n지정된 타임아웃 내에 서버로부터 응답을 받지 못하는 경우 `asyncio.TimeoutError` 예외가 발생한다.\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url, timeout=1) as response:  # 1초 타임아웃 설정\n                return await response.text()\n        except asyncio.TimeoutError:\n            print(\"Timeout Error: The request timed out\")\n\nasync def main():\n    await fetch('http://httpbin.org/delay/10')  # 응답 지연 URL\n\nasyncio.run(main())\n```\n\n#### 일반 HTTP 클라이언트 에러\n\naiohttp가 발생시킬 수 있는 다른 클라이언트 에러를 처리한다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url) as response:\n                return await response.text()\n        except aiohttp.ClientError as e:\n            print(f\"Client Error: {str(e)}\")\n\nasync def main():\n    await fetch('http://httpbin.org/status/500')  # 서버 에러 URL\n\nasyncio.run(main())\n```\n\n## 스트리밍 응답\n\n스트리밍 응답을 처리하는 것은 큰 데이터를 다룰 때 특히 유용하다. 스트리밍을 사용하여 응답의 전체 내용을 메모리에 한 번에 로드하지 않고 데이터를 조각으로 나누어 처리할 수 있다. 이 방법은 메모리 사용량을 최적화하고 대용량 응답을 효율적으로 처리할 수 있도록 한다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def stream_response(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            # 스트리밍 응답 처리\n            async for data in response.content.iter_any():\n                # 데이터 조각을 처리 (예: 파일에 쓰기, 출력 등)\n                print(data)\n\nasync def main():\n    await stream_response('http://httpbin.org/stream/20')\n\nasyncio.run(main())\n```\n\n이 예제에서는 `response.content.iter_any()` 를 사용하여 응답 스트림에서 데이터를 순차적으로 읽는다. \n\n스트리밍 응답을 사용할 때는 데이터의 양이 많거나 응답을 받는 동안 다른 처리를 동시에 해야 하는 경우에 매우 효과적이다. 예를 들어 다운로드한 데이터를 파일에 쓰면서 동시에 다음 데이터 청크를 받거나 데이터를 받아서 실시간으로 사용자에게 전송하는 경우 등에 유용하다. \n\n스트리밍을 사용할 때에는 네트워크 상황이나 서버의 응답 특성에 따라 데이터를 받는 속도가 일정하지 않을 수 있음을 인지하고 있어야 한다. 데이터를 처리하는 로직이 블로킹되지 않도록 주의해야 하며, 가능하면 각 청크를 처리하는데 시간이 너무 오래 걸리지 않도록 설계해야 한다.\n\n## 테스트\n\naiohttp의 비동기적인 특성 때문에 전통적인 동기 테스트 방식을 그대로 사용할 수 없다. 대신 aiohttp는 비동기 테스트를 지원하기 위해 pytest와 함께 사용할 수 있는 `aitohttp.test_utils` 모듈을 제공한다. 이를 통해 웹서버와 클라이언트 코드를 테스트 할 수 있다. \n\n#### 클라이언트 코드 테스트\n\n클라이언트 코드를 테스트 하기 위해서는 일반적으로 pytest와 pytest-aiohttp 플러그인을 사용한다. 이를 통해 aiohttp 클라이언트 세션을 비동기적으로 테스트 할 수 있다. \n\n```python\nimport aiohttp\nimport pytest\nfrom aiohttp import web\n\nasync def test_example_client(aiohttp_client):\n    async def hello(request):\n        return web.Response(text='Hello, world')\n    \n    app = web.Application()\n    app.router.add_get('/', hello)\n    \n    client = await aiohttp_client(app)\n    resp = await client.get('/')\n    assert resp.status == 200\n    text = await resp.text()\n    assert text == \"Hello, world\"\n```\n\naiohttp_client는 pytest-aiohttp 플러그인에서 제공하는 fixture로, 테스트용 애플리케이션 서버와 상호작용하는 데 사용할 수 있는 aiohttp.ClientSession 객체를 생성한다. \n\n\n        fixture란?\n테스트를 실행하기 전에 필요한 준비 작업과 설정을 의미한다. 일반적으로 fixture는 테스트 환경을 설정하고 테스트가 실행되는 동안 필요한 리소스나 상태를 생성하며, 테스트가 완료된 후에 정리 작업을 수행한다. \n\n#### 서버 코드 테스트\n\naiohttp 웹서버를 테스트 할 때에는 aiohttp.test_utils.TestClient를 사용하여 요청을 보내고 응답을 검사한다. \n\n```python\nfrom aiohttp import web\nfrom aiohttp.test_utils import AioHTTPTestCase, unittest_run_loop\nfrom myapp import create_app  # 가정: myapp 모듈에서 앱을 생성하는 함수\n\nclass MyAppTestCase(AioHTTPTestCase):\n\n    async def get_application(self):\n        \"\"\"\n        AioHTTPTestCase에서 애플리케이션 인스턴스를 생성하기 위해 오버라이드\n        \"\"\"\n        return create_app()\n\n    @unittest_run_loop\n    async def test_example(self):\n        resp = await self.client.request(\"GET\", \"/\")\n        assert resp.status == 200\n        text = await resp.text()\n        assert 'Hello, world' in text\n```\n\nAioHTTPTestCase는 aiohttp.test_utils에서 제공하는 기본 테스트 케이스 클래스이며, unittest_run_loop 데코레이터는 테스트 코루틴을 이벤트 루프에서 실행할 수 있게 해준다.\n\naiohttp를 테스트할 때 주의해야 할 점은 테스트 환경에서도 비동기 코드를 올바르게 실행하기 위해 적절한 테스트 실행기를 설정해야 한다는 것이다. pytest는 비동기 테스트를 위한 좋은 선택이며, pytest-asyncio 플러그인을 사용하면 pytest에서 async def 테스트 함수를 직접 사용할 수 있다.\n\n또한, 실제 HTTP 호출을 모킹하기 위해 aiohttp의 pytest_plugin이 제공하는 aioresponses와 같은 도구를 사용할 수도 있다. 이를 통해 실제 외부 서비스와의 통신 없이 HTTP 요청과 응답을 시뮬레이션할 수 있어, 테스트의 견고성을 높이고 실행 시간을 단축시킬 수 있다.\n\n\n\n"},{"excerpt":"Dependency Injection 시스템과 데코레이터 시스템은 유사한 목적을 가질 수 있다. 하지만 구현과 사용법에서 몇 가지 차이점이 있다.  공통점 명시적인 인증 체크 인증 로직을 명시적으로 라우트에 적용하여 추가 기능이 필요한 지점을 쉽게 식별할 수 있다.  중앙 집중식 관리 로직을 한 곳에서 관리함으로써 코드의 중복을 줄이고 일관성을 유지할 수…","fields":{"slug":"/FastAPI에서-데코레이터와-Dependency/"},"frontmatter":{"date":"November 07, 2023","title":"FastAPI에서 데코레이터와 Dependency","tags":["FastAPI","Python"]},"rawMarkdownBody":"Dependency Injection 시스템과 데코레이터 시스템은 유사한 목적을 가질 수 있다. 하지만 구현과 사용법에서 몇 가지 차이점이 있다. \n\n## 공통점\n\n#### 명시적인 인증 체크\n\n인증 로직을 명시적으로 라우트에 적용하여 추가 기능이 필요한 지점을 쉽게 식별할 수 있다. \n\n#### 중앙 집중식 관리\n\n로직을 한 곳에서 관리함으로써 코드의 중복을 줄이고 일관성을 유지할 수 있다. \n\n## 각 기능 별 장점\n\n### Dependency Injection (의존성 주입)\n\n#### 재사용성\n\nDependency는 여러 라우트에서 재사용 될 수 있으며 필요에 따라 다양한 파라미터와 함께 주입될 수 있다. \n\n#### 자동문서화\n\nFastAPI의 스웨거 UI에서 Dependency가 자동으로 문서화된다. \n\n#### 형식 안정성\n\nFastAPI의 형식 추론을 사용하여 파라미터의 유효성 검사와 변환을 자동으로 수행할 수 있다. \n\n### Decorator (데코레이터)\n\n#### 간결성\n\n데코레이터는 종종 코드를 더 간결하게 만들어주며, 의도를 명확히 표현하는 경우가 많다. \n\n#### 융통성\n\n데코레이터는 함수를 감싸는 방식으로 작동하므로 더 복잡한 로직을 내부에 포함할 수 있다. \n\n## 사용 시 고려 사항\n\n#### 비즈니스 로직의 분리 \n\n데코레이터는 추가를 원하는 로직과 비즈니스 로직을 물리적으로 분리하는데 유용할 수 있다. \n\n#### 코드의 명확성\n\nDependency는 의존성이 명시적이므로 코드를 읽고 이해하기 쉬워진다. \n\n#### 비동기 지원\n\n비동기 코드를 사용할 때에는 데코레이터 내부에서 async 함수를 올바르게 처리할 수 있는지 확인해야 한다. \n\n"},{"excerpt":"asyncio란? 소개 파이썬에서 비동기 프로그래밍을 위한 표준 라이브러리이다. 이 라이브러리는  을 사용하여 동시성 코드를 작성하는 데 필요한 구조를 제공한다. 단일 스레드 내에서도 여러 I/O 바운드 작업과 고수준의 구조화된 네트워크 코드를 동시에 실행할 수 있으며, 이는 효율성과 속도에서 큰 이점을 제공한다.  주요 컴포넌트 Event Loop 프로…","fields":{"slug":"/FastAPI와-asyncio/"},"frontmatter":{"date":"November 07, 2023","title":"FastAPI와 asyncio","tags":["Python","FastAPI"]},"rawMarkdownBody":"## asyncio란?\n\n### 소개\n\n파이썬에서 비동기 프로그래밍을 위한 표준 라이브러리이다. 이 라이브러리는 `coroutine` 을 사용하여 동시성 코드를 작성하는 데 필요한 구조를 제공한다. 단일 스레드 내에서도 여러 I/O 바운드 작업과 고수준의 구조화된 네트워크 코드를 동시에 실행할 수 있으며, 이는 효율성과 속도에서 큰 이점을 제공한다. \n\n### 주요 컴포넌트\n\n#### Event Loop\n\n프로그램의 진입점으로써, 비동기적으로 실행될 다양한 작업들을 관리한다. 작업이 실행 준비가 되면 이벤트 루프는 해당 작업을 실행하고 완료되기를 기다리는 다른 작업으로 제어를 전환한다. \n\n#### Coroutine\n\n코루틴은 `async def` 로 정의되는 비동기 함수이다. 코루틴은 `await`  키워드를 사용하여 실행을 일시 중지하고 이벤트 루프에 제어를 반환할 수 있어 다른 코루틴이 실행될 수 있게 한다. \n\n#### Future\n\n아직 완료되지 않은 작업을 의미한다. 코루틴이 완료될 때 결과를 저장하거나 예외를 전달할 수 있는데 이를 통해 비동기 작업의 최종 상태를 알 수 있다. \n\n#### Task\n\n`Future` 를 상속한다. 이는 이벤트 루프에서 코루틴의 실행을 가능하게 한다. Task는 코루틴을 스케줄링하고 실행 결과를 추적한다. \n\n### 사용 예시\n\n```python\nimport asyncio\n\nasync def main():\n    await asyncio.sleep(1)\n    print('Hello, World!')\n\n# 이벤트 루프 실행\nasyncio.run(main())\n```\n\n이 예제에서 `asyncio.run()` 함수는 `main()` 코루틴을 이벤트 루프에서 실행한다. `main()` 내부에서 `asyncio.sleep(1)` 은 비동기적으로 1초간 대기하도록 한다. 이 대기 시간동안 이벤트 루프는 다른 코루틴을 실행할 수 있다. \n\n### 동기화 기능\n\nasyncio는 비동기 프로그래밍 환경에서 동기화를 위한 여러 도구를 제공한다. asyncio.Lock은 공유 리소스에 대한 동시 액세스를 방지하고 asyncio.Event, asyncio.Condition, asyncio.Semaphore와 같은 다른 동기화 기본 요소도 사용할 수 있다. \n\n### 네트워킹 지원\n\nTCP, UDP, SSL, TLS 등을 비롯한 다양한 네트워크 프로토콜에 대한 지원을 제공한다. asyncio 스트림을 사용하여 비동기적으로 네트워크 I/O 작업을 할 수 있으며 이를 통해 서버와 클라이언트 양쪽 모두에서 비동기 네트워크 어플리케이션을 만들 수 있다. \n\n### 성능과 제한 사항\n\n입출력 바운드 작업과 고수준 구조화된 네트워크 코드를 실행하는데 효율적이다. 입출력 작업이 블로킹 되는 것을 피하면서 동시에 여러 입출력 작업을 관리할 수 있기 때문이다. \n\n예를 들어 웹서버는 많은 수의 동시 http 요청을 처리해야 하며 각 요청은 네트워크 입출력에 의존적이다. asyncio는 각 네트워크 연산이 완료되기를 기다리는 동안 다른 연산으로 전환하여 리소스를 효율적으로 사용할 수 있다. \n\n그러나 계산을 많이 요구 하는 작업(CPU 바운드 작업)에는 적압하지 않다. 이 작업은 병렬 처리가 필요한데, 이는 프로세스나 스레드를 여러개 사용해야 함을 의미한다. asyncio는 싱글 스레드, 싱글 프로세스 디자인이므로 병렬 CPU 작업을 위해서는 별도의 모듈이나 멀티 스레딩을 사용해야 한다. \n\n### 생명 주기\n\n#### 이벤트 루프 생성\n\nasyncio 프로그램은 이벤트 루프라는 중앙 실행기를 사용하여 실행된다. 이벤트 루프는 프로그램의 진입점에서 생성된다. 특히 아래의 함수를 사용하면 적절한 이벤트 루프를 설정하고 프로그램을 실행할 수 있다. \n\n```python\nasyncio.run(main())\n```\n\n#### 코루틴 실행\n\n`async def` 를 통해 코루틴을 생성하고 `await` 를 사용하여 실행을 스케줄한다. 코루틴은 이벤트 루프에 의해 실행될 태스크로 변환된다. \n\n```python\nasync def main():\n    # 코루틴 실행\n    result = await some_async_function()\n```\n\n#### 태스크 스케쥴링\n\n이벤트 루프는 `await` 표현식을 만날 때마다 실행을 일시 중지하고 다른 태스크로 제어를 전환한다. 이를 통해 CPU가 I/O 작업으로 인해 차단되는 것을 방지하고 다른 태스크의 실행을 계속할 수 있다. \n\n#### 비동기 I/O 처리\n\n`asyncio`는 네트워크 I/O, 파일 I/O와 같은 비동기 작업을 효율적으로 관리한다. 이벤트 루프는 모든 I/O 이벤트에 대해 비블로킹 소켓과 파일 디스크립터를 사용한다. \n\n#### 이벤트와 콜백 처리\n\n이벤트 루프는 등록된 이벤트가 발생했을 때 적절한 콜백 함수를 실행한다. 콜백은 완료된 I/O 작업의 결과를 다루거나 타이머 이벤트 같은 다른 유형의 이벤트를 처리하기 위해 사용된다. \n\n#### 종료 처리\n\n모든 태스크가 완료되거나 특정 조건을 만족하면 이벤트 루프는 종료될 수 있다. 이벤트 루프를 종료하기 전에 `close()` 함수를 호출하여 자원을 정리할 수 있다. \n\n```python\nloop = asyncio.get_event_loop()\ntry:\n    loop.run_until_complete(main())\nfinally:\n    loop.close()\n```\n\n`asyncio.run()` 을 사용하는 경우에는 이벤트 루프의 생성과 종료가 자동으로 관리된다. \n\n#### 예외 처리\n\n`asyncio` 는 예외가 발생할 경우 적절한 예외 처리를 위한 매커니즘을 제공한다. 코루틴 내부에서 발생한 예외는 해당 코루틴을 호출한 곳으로 전파되고 이벤트 루프는 처리되지 않은 예외를 포착하여 프로그래머에게 알린다. \n\n## FastAPI와 Async 함수\n\nFastAP의 가장 큰 특징 중 하나는 asyncio 라이브러리를 기반으로 한 비동기 프로그래밍 지원이다. 이를 통해 개발자는 비동기 파이썬 코드를 작성할 수 있으며 이는 특히 I/O 바운드 작업(네트워크 요청 처리, 디스크에서의 데이터 읽기 /쓰기 등)을 비동기적으로 처리하는데 유리하다. \n\n### 비동기 함수의 장점\n\n#### 성능 향상\n\nI/O 작업이 완료되기를 기다리는 동안 다른 코드를 실행할 수 있으므로 I/O 바운드 시스템에서 동시성을 크게 향상시킬 수 있다. \n\n#### 스케일성\n\n적은 수의 워커로 많은 수의 요청을 처리할 수 있으므로 더 효율적으로 리소스를 사용하고 스케일링 할 수 있다. \n\n#### 비용 효율성\n\n리소스 사용의 효율성은 특히 클라우드 환경에서 환경 절감으로 이어질 수 있다. \n\n#### 응답성 향상\n\n서버가 요청에 더 빨리 응답할 수 있기 때문에 사용자 경험이 향상된다. \n\n### 언제 비동기 함수를 쓰는 것이 더 유리할까? \n\n#### 네트워크 요청 처리\n\n웹 API 호출, 원격 데이터베이스 쿼리 등 네트워크 I/O 관련 작업을 처리할 때\n\n#### 고비용 I/O 작업\n\n디스크 작업이나 네트워크를 통한 파일 전송과 같은 I/O 집중적인 작업을 비동기적으로 실행할 때 \n\n#### 동시성이 필요한 작업\n\n다수의 클라이언트나 서비스로부터 오는 동시 요청을 효과적으로 처리해야 할 때\n\n#### WebSocket을 사용할 때 \n\n실시간 통신을 위해 WebSocket 연결을 관리하는 경우 \n\n### 비동기 함수 사용 시 주의할 점\n\n#### 데드락\n\n비동기 프로그래밍은 데드락에 빠질 위험이 있으므로 올바른 `await` 사용과 태스크 관리가 필요하다. \n\n\n        데드락이란?\n두 개 이상의 프로세스나 스레드가 서로의 작업 완료를 무한히 기다리게 되는 상태\n\n#### 에러 핸들링\n\n비동기 코드에서는 예외 처리가 더 복잡해질 수 있으므로 주의가 필요하다. \n\n#### 디버깅의 복잡성\n\n비동기 코드는 디버깅이 더 어려울 수 있다. \n\n#### 블로킹 코드와의 혼용\n\n비동기 코드 내에서 블로킹 동기 코드를 호출하면 성능이 저하될 수 있으므로 주의해야 한다. \n\n### `async`\n\n이 키워드는 두 가지 주요 상황에서 사용된다. \n\n#### 코루틴 함수를 정의할 때\n\n```python\nasync def fetch_data():\n    # 비동기 작업을 수행하는 코드\n```\n\n`async def` 는 이 함수가 코루틴 함수임을 나타낸다. 코루틴 함수는 호출될 때 즉시 실행되지 않고 대신에 코루틴 객체를 반환한다. 이 객체는 나중에 이벤트 루프에 의해 실행된다. \n\n#### async with와 async for를 사용할 때\n\n비동기 컨텍스트 매니저와 비동기 이터레이터를 사용할 때 필요하다. \n\n### `await`\n\n이 키워드는 코루틴의 실행을 일시 중지하고 코루틴의 결과가 준비될 때까지 기다린다. 코루틴, 태스크, 미래 객체 등이 뒤에 올 수 있다. \n\n```python\nasync def fetch_data():\n    data = await some_async_function()\n    return data\n```\n\n여기서 await는 some_async_function이 완료될 때까지 fetch_data 코루틴의 실행을 중지시킨다. 완료되면 그 결과값을 data에 할당하고 실행을 계속한다. \n\n이러한 비동기 프로그래밍 방식의 장점은 I/O 작업이 완료되기를 기다리는 동안 프로그램이 다른 작업을 계속할 수 있어 프로그램의 전반적인 실행 효율을 개선한다는 것이다.\n\n"},{"excerpt":"개요 FastAPI의 데코레이터는 파이썬 데코레이터 패턴을 활용하여 FastAPI 프레임워크에서 제공하는 여러 기능을 함수나 클래스에 적용하는 구문이다. 이 데코레이터들은 FastAPI에서 매우 중요한 역할을 한다.  데코레이터는  기호를 사용하여 함수나 클래스의 위에 선언된다. 데코레이터는 그 아래에 정의된 함수에 추가적인 기능을 부여하거나 특정 작업을…","fields":{"slug":"/FastAPI의-데코레이터/"},"frontmatter":{"date":"November 03, 2023","title":"FastAPI의 데코레이터","tags":["FastAPI","Python"]},"rawMarkdownBody":"## 개요\n\nFastAPI의 데코레이터는 파이썬 데코레이터 패턴을 활용하여 FastAPI 프레임워크에서 제공하는 여러 기능을 함수나 클래스에 적용하는 구문이다. 이 데코레이터들은 FastAPI에서 매우 중요한 역할을 한다. \n\n데코레이터는 `@` 기호를 사용하여 함수나 클래스의 위에 선언된다. 데코레이터는 그 아래에 정의된 함수에 추가적인 기능을 부여하거나 특정 작업을 수행하도록 지시한다. \n\n데코레이터를 사용하여 개발자는 복잡한 로직을 함수에 직접 쓰지 않고 프레임워크가 제공하는 데코레이터를 사용하여 빠르고 쉽게 웹 애플리케이션을 구현할 수 있다. \n\n## 주요 데코레이터\n\n### `@app.middleware(\"http\")`\n\nHTTP 요청-응답 사이클에 관여하는 미들웨어를 등록하는데 사용된다.  이 데코레이터 아래에 정의된 함수는 애플리케이션으로 들어오는 모든 http 요청에 대해 처리되고 그리고 해당 요청에 대한 응답을 반환하기 전에 호출된다. \n\n\n        미들웨어란?\n요청과 응답을 처리하는 과정 사이에 위치하여 들어오는 요청을 가로채 그 요청에 대해 특정 작업을 수행하거나 응답을 조작하는 구성 요소이다. \n\n#### 예제\n\n```python\n@app.middleware(\"http\")\nasync def custom_middleware(request: Request, call_next):\n    # 요청 전에 실행할 코드\n    response = await call_next(request)\n    # 응답 전에 실행할 코드\n    return response\n```\n\n크게 두 부분으로 나눠진다. \n\n- 요청 전에 실행할 코드 : `call_next` 함수에 요청을 전달하기 전에 실행할 코드를 작성한다. \n\n- 응답 전에 실행할 코드 : `await call_next(request)` 는 다음 미들웨어나 실제 요청을 처리하는 엔드포인트를 호출한다. 이후 응답이 반환되면 그 응답에 추가적인 처리를 하고 싶을 때 사용할 수 있다. \n\n#### 미들웨어 체인\n\n만약 `@app.middleware(\"http\")` 가 여러개 정의되어 있다면 FastAPI는 그것을 선언된 순서대로 실행한다. 각 미들웨어는 이전 미들웨어에서 `await call_next(request)` 를 호출한 후의 응답을 받아 처리한다. 이를 미들웨어 체인이라고 하며, 요청이 엔드포인트에 도달하기 전에 여러 미들웨어를 통과한다. \n\n따라서 미들웨어는 가벼운 로직을 수행하는 것이 좋으며, 무거운 작업은 미들웨어에서 피해야 한다. 또한 각 미들웨어는 만드시 `await call_next(request)` 를 호출하여 체인을 계속 진행할 수 있도록 해야 한다. \n\n### `@app.get`**,** `@app.post`**,** `@app.put`**,** `@app.delete`**,** `@app.options`**,** `@app.head`\n\nHTTP 메소드에 맞게 라우트를 설정하는 데코레이터이다. \n\n각각의 데코레이터는 해당 함수가 지정된 HTTP 메서드의 요청을 처리하는 엔드포인트임을 알려준다. 엔드포인트 함수 내부에서는 파라미터 검증, 비즈니스 로직, 데이터 반환 등의 작업을 수행할 수 있다. \n\n```python\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    return {\"item_id\": item_id}\n```\n\n### `@app.api_route`\n\n모든 http 메소드를 하나의 함수로 라우트 할 수 있게 해주는 데코레이터이다. \n\n하나의 엔드포인트에 여러 http 메소드를 지정할 수 있도록 한다. 예를 들어 같은 경로에 대해 Get과 Post 요청을 모두 처리하고 싶은 경우에 사용할 수 있다. 이 데코레이터를 사용하면 각 메서드에 대한 처리 로직을 한 함수에서 정의할 수 있다. \n\n```python\n@app.api_route(\"/items/{item_id}\", methods=[\"GET\", \"POST\"])\nasync def handle_item(item_id: int):\n    # 여기에 GET과 POST를 처리하는 로직을 구현\n    pass\n```\n\n### `@app.websocket`\n\n웹소켓 연결을 처리하는 엔드포인트를 선언하는 데코레이터이다. 클라이언트가 해당 경로로 웹소켓 연결을 시도하면 FastAPI는 해당 함수를 실행해 웹소켓 핸드셰이크를 처리하고 연결을 유지한다. \n\n### `@app.on_event(\"startup\" | \"shutdown\")`\n\n애플리케이션의 시작 시 또는 종료 시 실행할 함수를 등록하는 데코레이터이다. \n\n### `@app.exception_handler(Exc)`\n\n특정 예외를 처리하는 핸들러를 등록하는 데코레이터이다. 특정 예외 유형이 발생했을 때 실행될 커스텀 핸들러를 등록하는 데 사용된다. 표준 예외 로직을 오버라이드 하거나 특정 예외 유형에 대해 특별한 처리를 구현할 수 있다. \n\n예를 들어 ValueError가 발생했을 때, 표준 HTTP 500 Error 대신 더 구체적인 오류 메세지와 HTTP 400 코드를 반환할 수 있다. \n\n#### 예제\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom starlette.requests import Request\n\napp = FastAPI()\n\n@app.exception_handler(ValueError)\nasync def value_error_exception_handler(request: Request, exc: ValueError):\n    return JSONResponse(\n        status_code=400,\n        content={\"message\": str(exc)},\n    )\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    if item_id < 0:\n        raise ValueError(\"Item ID must be positive\")\n    return {\"item_id\": item_id}\n```\n\n이를 이용해 다음과 같은 이점을 얻을 수 있다. \n\n1. 유저 친화적인 오류 메시지 제공\n\n1. 로그 이록\n\n1. 오류 리포팅\n\n1. 커스텀 http 상태 코드 반환 : 기본적으로 변경된 메시지 대신 사용자에게 안내를 줄 수 있다. \n\n이러한 예외 핸들러는 API 의 로버스트성을 늘려준다. \n\n\n        로버스트성이란?\n소프트웨어가 예기치않은 입력이나 사용 상황에서도 안정적으로 작동하는 성질을 의미한다. \n\n### `@app.dependency`\n\n함수가 종속성으로 작동하게 하며 해당 함수가 다른 경로 작업에서 호출될 때마다 실행되게 한다. 이를 이용해 공통 기능을 중앙에서 관리하고 경로 작업에서 필요한 데이터를 제공하거나 사전 처리를 수행할 수 있다. \n\n#### 예제\n\n모든 경로에서 공통으로 사용되는 데이터베이스 세션을 생성하는 경우를 가정하자. 아래는 해당 데코레이터를 사용하여 데이터베이스 세션을 경로에 주입하는 예제이다. \n\n```python\nfrom fastapi import FastAPI, Depends\n\napp = FastAPI()\n\nclass DBSession:\n    # DBSession 클래스는 데이터베이스 세션을 관리합니다.\n    def __init__(self):\n        self.session = \"DB Connection\"\n\n    def close(self):\n        self.session = \"DB Connection Closed\"\n\n# 종속성으로 사용될 함수\n@app.dependency\nasync def get_db_session():\n    db_session = DBSession()\n    try:\n        yield db_session.session\n    finally:\n        db_session.close()\n\n# 경로 작업에서 종속성 사용\n@app.get(\"/items/\")\nasync def read_items(db: str = Depends(get_db_session)):\n    return {\"db_session\": db}\n```\n\nget_db_session 함수가 @app.dependency 데코레이터로 마크되어있다. 이 함수는 호출될 때마다 새로운 DBSession 인스턴스를 생성하고 요청 처리가 완료된 후 세션을 정리한다. \n\n이 함수는 경로 작업 함수 read_items에 Depends를 사용하여 주입된다. 경로 작업에서는 반환된 데이터베이스 세션을 db라는 변수로 받아 사용할 수 있다. \n\nDepends를 사용함으로써 FastAPI는 get_db_session 함수를 실행하고 그 반환값을 read_items 경로 작업의 매개변수로 전달한다. 이 패턴은 서비스 계층, 데이터 접근 계층 등에서 특히 유용하며 코드 중복을 줄이고 테스트 용이성을 높여준다. \n\nFastAPI에서 해당 데코레이터를 사용해 정의된 함수는 생성기(generator) 패턴을 사용한다. yield 키워드를 사용해 이 함수는 값을 반환하기 전과 후에 코드를 실행할 수 있다. \n\n1. yield를 만날 때까지 함수를 실행한다. 이 때, DBSession 인스턴스가 생성되고, 세션이 초기화된다. \n\n1. yield에서 함수는 호출한 측에 db_session.session 값을 넘겨주고 실행을 일시 중지(pause) 한다. \n\n1. 이제 read_items 경로 함수의 본문을 실행한다. 이 때, db 매개변수로 전달된 값을 사용한다. \n\n1. read_items 함수가 완료되고 응답이 반환되면 yield문 이후의 코드가 실행된다. 이 코드에서는 finally 블록이다. \n\n즉 finally 블록은 http 요청 처리가 완전히 끝나고 응답이 클라이언트에게 전송된 후에 실행된다. 이는 DBSession 객체의 리소스를 안전하게 정리할 수 있게 해준다. yield를 사용하는 이 패턴은 파이썬의 컨텍스트 매니저와 유사한 방식으로 자원의 정리를 보장한다. \n\n### `@Query`**,** `@Path`**,** `@Header`**,** `@Cookie`**,** `@Body`**,** `@Form`\n\n엔드포인트의 각 파라미터를 특정 데이터 위치(쿼리 파라미터, 경로 파라미터, 헤더, 쿠키, 요청 본문, 폼 데이터)에 연결한다. \n\n### `@Response`**,** `@JSONResponse`**,** `@HTMLResponse`**,** `@FileResponse`\n\n특정 응답 클래스를 사용하여 응답을 반환한다. 예를 들어 `@JSONResponse`는 JSON 형식의 응답을 반환할 때 사용된다. \n\n\n\n"},{"excerpt":"소개 파이썬의 중요한 특징 중 하나로 이터레이터 프로토콜을 사용하여 데이터의 시퀀스를 느긋하게(lazily) 생성하는데 사용된다. 즉, 생성기는 시퀀스의 전체 항목을 메모리에 한 번에 로드하지 않고 반복(iteration) 할 때마다 하나씩 항목을 생성한다.  이를 통해 메모리 사용을 줄이고 대용량 또는 무한한 시퀀스를 다룰 수 있다.  이터레이터 프로토…","fields":{"slug":"/생성기generate-패턴/"},"frontmatter":{"date":"November 03, 2023","title":"생성기(generate) 패턴","tags":["Python"]},"rawMarkdownBody":"## 소개\n\n파이썬의 중요한 특징 중 하나로 이터레이터 프로토콜을 사용하여 데이터의 시퀀스를 느긋하게(lazily) 생성하는데 사용된다. 즉, 생성기는 시퀀스의 전체 항목을 메모리에 한 번에 로드하지 않고 반복(iteration) 할 때마다 하나씩 항목을 생성한다. \n\n이를 통해 메모리 사용을 줄이고 대용량 또는 무한한 시퀀스를 다룰 수 있다. \n\n## 이터레이터 프로토콜\n\n파이썬에서 반복 가능한 객체(컬렉션)를 순회하기 위한 규약이다. 이 프로토콜은 반복(iteration) 동작을 정의하며 일련의 요소들에 대해 순차적으로 접근할 수 있게 해준다. 파이썬의 모든 반복 가능한 객체는 이 이터레이터 프로토콜을 구현해야 하며 이는 주로 두 가지 매직 매소드(magic method)로 구성된다. \n\n#### `__iter__()`\n\n이터레이터 객체를 반환해야 하며, 보통 self를 반환하여 객체 자체가 이터레이터인 경우를 처리한다. 이 메소드는 for 루프와 같은 반복 연산이 시작될 때 호출된다. \n\n### `__next__()`\n\n컬렉션의 다음 요소를 반환해야 한다. 만약 더 이상 요소가 없으면 `StopIteration` 예외를 발생시켜 반복을 종료해야 한다. \n\n## 생성기 만드는 방법\n\n### 생성기 함수 (Generator function)\n\n일반 함수와 비슷하지만 `return` 대신 `yield` 문을 사용하여 값을 하나씩 반환한다. `yield` 를 사용하면 함수는 해당 상태에서 실행을 일시 정지하고 다음 값이 요청될 때까지 대기 상태가 된다. \n\n### 생성기 표현식 (Generator expression)\n\n리스트 컴프리헨션과 유사하지만 괄호를 사용하여 정의한다. 이 방식은 단순한 경우에 대한 생성기를 간결하게 작성할 수 있게 해준다. \n\n## 예제\n\n### 생성기 함수 예제\n\n```python\ndef countdown(n):\n    print(\"Starting to count from\", n)\n    while n > 0:\n        yield n\n        n -= 1\n    print(\"Done!\")\n\n# 생성기 객체를 생성\ncd = countdown(3)\n\n# 생성기를 사용\nfor count in cd:\n    print(count)\n```\n\n이 코드를 실행하면 3, 2, 1이 출력되고 각 숫자 사이에 다른 코드를 실행할 수 있는 기회가 주어진다. 각 yield 문마다 함수의 상태는 일시 중단되고 다음 반복에서 계속된다. \n\n### 생성기 표현식 예제\n\n```python\nsquares = (x*x for x in range(10))\n\n# 생성기를 사용\nfor square in squares:\n    print(square)\n```\n\n이 코드는 0부터 9까지의 숫자에 대한 제곱을 출력한다. 반복(iteration)할 때마다 다음 제곱값이 생성되고 모든 값이 한 번에 메모리에 저장되지 않는다. \n\n## yield\n\nyield는 함수가 값을 반환(return)하면서도 그 상태를 기억하고 일시 중지(pause)되는 것을 가능하게 한다. 이는 함수의 실행을 중단시키지만 함수의 로컬 변수와 실행 상태가 유지되어 나중에 이 함수가 다시 호출될 때 이전 상태에서부터 실행을 재개할 수 있다. \n\n일반적인 return은 함수의 실행을 완전히 종료하고 해당 함수의 모든 상태(로컬 변수, 실행 컨텍스트)를 버리지만 yeild는 함수가 여전히 살아있다는 점에서 return과 다르다. yeild를 통해 반환된 함수(즉 생성기)는 `__next__()` 메소드를 호출함으로써 (또는 `next()` 함수를 사용하여) 다시 실행될 수 있으며, yield문 바로 다음부터 실행이 이어진다. \n\n## 이터러블(iterable) 객체\n\n### 소개\n\n멤버를 한 번에 하나씩 반환할 수 있는 객체로, 파이썬에선 주로 순회 가능한 모든 객체를 의미한다. 이터러블 객체는 for 루프와 같은 반복문을 사용해 순회할 수 있으며 이는 그 객체가 반복 가능한(iterable) 인터페이스를 구현하고 있기 때문이다. \n\n이터러블 객체가 되기 위해서는 객체 내에 `__iterm__()` 메소드를 구현해야 한다. 이 메소드는 이터레이터를 반환하고 이터레이터는 `__next__()` 메소드를 구현하여 순차적으로 값을 반환할 수 있어야 한다. `topIteration` 예외는 이터레이터가 더 이상 반환할 값이 없을 때 발생시켜 반복이 끝났음을 알린다.  \n\n### 파이썬의 이터러블 객체\n\n리스트, 튜플, 문자열, 딕셔너리, 집합 등이 있다. 또한 파일(file) 객체도 이러터블하며 파일의 각 줄을 순회할 수 있다. \n\n### 예제\n\n```python\nmy_list = [1, 2, 3, 4]  # 리스트는 이터러블 객체입니다.\nfor item in my_list:\n    print(item)  # 1, 2, 3, 4가 순차적으로 출력됩니다.\n```\n\n직접 이터러블 객체를 만들고 싶다면 `__iter__()` 메소드를 포함하는 클래스를 정의할 수 있다. \n\n```python\nclass Counter:\n    def __init__(self, low, high):\n        self.current = low\n        self.high = high\n\n    def __iter__(self):\n        return self  # self.__next__()를 호출할 이터레이터로 자신을 반환합니다.\n\n    def __next__(self):\n        if self.current < self.high:\n            num = self.current\n            self.current += 1\n            return num\n        else:\n            raise StopIteration\n\n# Counter 인스턴스는 이터러블 객체입니다.\ncounter = Counter(1, 4)\nfor num in counter:\n    print(num)  # 1, 2, 3이 출력됩니다.\n```\n\n위는 사용자 정의 이터러블 객체이다. \n\n위의 `Counter` 클래스는 `__iter__()`와 `__next__()` 메소드를 정의하여 이터러블 인터페이스를 구현한다. `Counter` 인스턴스를 순회할 때마다 `__next__()` 메소드가 호출되어 숫자를 하나씩 반환하고, 더 이상 반환할 숫자가 없을 때 `StopIteration` 예외가 발생한다.\n\n\n\n"},{"excerpt":"특징 및 장점 타입 힌트를 활용한 API 선언 Python 3.6 이상의 타입 힌트를 활용하여 API 매개변수 및 응답 모델을 선언한다. 이를 통해 데이터 검증, 직렬화, 문서화를 자동화한다. 이러한 방식은 깔끔한 코드 작성과 함께 명확한 API 명세를 제공한다.  속도 Starlette(ASGI 기반) 및 Pydanic의 결합으로 다른 파이썬 프레임워크…","fields":{"slug":"/FastAPI/"},"frontmatter":{"date":"November 02, 2023","title":"FastAPI","tags":["FastAPI","Python"]},"rawMarkdownBody":"## 특징 및 장점\n\n### 타입 힌트를 활용한 API 선언\n\nPython 3.6 이상의 타입 힌트를 활용하여 API 매개변수 및 응답 모델을 선언한다. 이를 통해 데이터 검증, 직렬화, 문서화를 자동화한다. 이러한 방식은 깔끔한 코드 작성과 함께 명확한 API 명세를 제공한다. \n\n### 속도\n\nStarlette(ASGI 기반) 및 Pydanic의 결합으로 다른 파이썬 프레임워크에 비해 월등히 높은 성능을 자랑한다. \n\n### 자동화된 문서\n\n타입 힌트와 함께 추가된 FateAPI 데코레이터를 사용하여 자동으로 실시간 API 문서(Swagger UI 및 ReCod)를 생성한다. \n\n### 데이터 검증\n\nPydantic을 이용하여 입력 데이터의 유효성을 자동으로 검사하고 오류에 대한 명확한 정보를 반환한다. 유효하지 않은 데이터에 대한 명확하고 읽기 쉬운 오류 메시지를 자동으로 제공한다. \n\n### 비동기 지원\n\n`async/await` 문법을 사용하여 DB 작업, 파일 작업 및 네트워크 작업 등의 비동기 코드를 쉽게 통합할 수 있다. \n\n### 보안 기능\n\nOAuth2, 비밀번호 해싱, JWT 토큰 생성 및 검증, CORS 등 웹 API 보안에 필요한 기능들이 아웃 오브 더 박스에서 제공된다. \n\n### 확장성\n\n사용자의 요구에 따라 쉽게 확장할 수 있으며 다양한 서드파티 라이브러리와 통합이 용이하다. \n\n### 의존성 주입 시스템\n\n요청 처리 함수의 매개변수로 의존성을 선언하면 이를 자동으로 처리하고 제공한다. 이를 통해 공통적인 기능(데이터베이스 세션, 토큰 검증 등)을 재사용할 수 있다. \n\n## 다른 웹 프레임워크와의 비교\n\n- Flask : 가볍고 간단하며 확장 가능하다. 하지만 기본적으로 동기식이며 ASGI를 사용한 비동기 처리를 지원하지 않는다. 데이터 검증이나 자동 문서화 같은 기능을 위해서는 추가 확장 또는 라이브러리가 필요하다. \n\n- Django : 많은 기능(관리자 인터페이스, ORM, 인증..)등이 내장되어 있다. 큰 커뮤니티와 문서화가 잘 되어있다. 그렇지만 무거울 수 있으며 프로젝트의 구조나 방식이 명확하게 정해져있어 높은 수준의 커스터마이제이션을 원하는 개발자는 제약이 될 수 있다. 기본적으로 동기식이다.\n\n- Tornado : 비동기 네트워크 라이브러로 시작하여 웹 프레임워크로 발전했다. WenSocket과 같은 비동기 작업에 적합하다. 다만 최신 ASGI 프레임워크보다는 성능면에서 뒤쳐질 수 있다. 자동 문서화나 타입 힌트 기반의 데이터 검증과 같은 기능이 없다. \n\n## 단점\n\n### 상대적으로 작은 커뮤니티의 크기 \n\nFlask나 Django처럼 오랜 기간 사용된 프레임워크에 비해 커뮤니티가 상대적으로 작아 확장 기능, 플러그인, 해결책을 찾는데 어려움을 겪을 수 있다. \n\n### 기술 부채\n\n비교적 최근에 개발되어 예전의 기존 시스템과 통합하거나 레거시 코드와 병합할 때 어려움을 겪을 수 있다. \n\n### 학습 곡선\n\n타입 힌트, Pydantix, ASGI 등 다양한 기능들이 초기 사용자에게는 약간의 학습 곡선을 만들 수 있다. 특히 비동기 프로그래밍에 익숙하지 않다면 어려움을 겪을 수 있다. \n\n### 완전한 ORM의 부재\n\n데이터베이스 통합을 위해 SQLAlchemy나 Tortoise-ORM과 같은 외부 라이브러리를 사용한다. 프레임워크 자체의 통합된 ORM을 제공하지 않는다. \n\n### 제한된 동기 지원\n\n주로 비동기 환경을 위해 설계되어 동기 코드를 사용하는 라이브러리 또는 시스템과 통합할 때 추가적인 노력이 필요할 수 있다. \n\n## WSGI와 ASGI\n\nWSGI와 ASGI는 모두 웹서버와 파이썬 웹 어플리케이션 사이의 표준 인터페이스를 정의하는 규격이다. \n\n### WSGI (Web Server Gateway Interface)\n\n동기적인 웹 어플리케이션을 위한 표준 인터페이스를 제공하며, 전통적인 웹 어플리케이션 개발에 널리 사용된다. \n\n- 목적 : 웹서버와 파이썬 웹 어플리케이션 혹은 프레임워크가 서로 통신할 수 있도록 하는 간단한 호출 규약을 정의한다. \n\n- 동작 방식 : 동기적이며, 한 번에 하나의 요청만 처리할 수 있다. 이로 인해 여러 동시 요청을 처리하기 위해서는 다중 프로세스나 다중 스레드 방식을 사용해야 한다. \n\n- 대표적인 서버 및 미들웨어 : uWSGI, gunicorn, mod_wsgi\n\n- 사용되는 프레임워크 : Flask, Django 등 대부분의 전통적인 파이썬 웹 프레임워크\n\n### ASGI (Asynchronous Server Gateway Interface)\n\n비동기 웹 어플리케이션과 다양한 웹 프로토콜을 위한 확장된 인터페이스를 제공한다. 최근의 웹 트렌드에 맞게 설계되었다. \n\n- 목적 : WSGI의 확장 버전으로 웹소켓과 같은 비동기 프로토콜의 지원과 더 빠른 성능을 목표로 한다. \n\n- 동작 방식 : 비동기이며, 이를 통해 한 번에 여러 요청을 처리할 수 있다. 주로 `async/awit` 구문을 활용한 비동기 프로그래밍을 가능하게 한다. \n\n- 대표적인 서버 : Daphne, Uvicorn, Hypercorn 등\n\n- 사용되는 프레임 워크 : FastAPI, Starlette 등\n\n## FastAPI의 작동 플로우\n\n### 1. API 선언\n\nAPI 엔드포인트를 선언하기 위해 파이썬 함수를 사용한다. 타입 힌트와 함께 Pydantic 모델을 사용하여 요청 및 응답 객체를 선언한다. \n\n이렇게 선언된 엔드포인트는 HTTP 메서드에 맞게 데코레이터로 데코레이트 된다. \n\n### 2. 데이터 검증 및 직렬화\n\n요청이 들어오면 Pydantic 모델을 사용하여 들어온 데이터를 검증한다. \n\n유효하지 않은 요청 데이터가 감지되면 자동으로 오류 응답을 생성하여 클라이언트에 반환한다. \n\n### 3. 의존성 해결\n\n각 엔드포인트에 선언된 의존성(예: 데이터베이스 세션, 인증 토큰 확인)을 자동으로 해결한다. \n\n이를 통해 반복적인 작업 없이 재사용 가능한 컴포넌트들을 생성할 수 있다. \n\n### 4. 요청 처리\n\n데이터 검증 및 의존성 해결이 완료되면 해당 엔드포인트 함수를 호출하고 처리한다. \n\n이 때, 비동기 함수(`async def`)를 사용하면 비동기 방식으로 요청을 처리할 수 있다. \n\n### 5. 응답 생성\n\n엔드포인트 함수의 반환값은 자동으로 JSON으로 직렬화된다. \n\n반환값이 Pydantic 모델 인스턴스라면 해당 모델을 사용하여 데이터 직렬화 및 검증이 이루어진다. \n\n### 6. 자동 문서화\n\n타입 힌트와 함께 선언된 엔드포인트를 기반으로 자동으로 API 문서를 생성한다. \n\n이 때 Swagger UI 및 ReDoc와 같은 도구를 사용하여 실시간 API 문서를 제공한다. \n\n### 7. 응답 반환\n\n최종적으로 처리된 응답이 클라이언트에 반환된다. 이 때 HTTP 상태 코드, 헤더, 쿠키 등도 함께 설정할 수 있다. \n\n## Uvicorn\n\nUvicorn은 ASGI(Asynchronous Server Gateway Interface)를 기반으로 한 빠르고 경량화된 웹 서버이다. Uvicorn은 주로 파이썬으로 작성된 비동기 웹 애플리케이션과 프레임워크의 서빙에 사용된다. \n\n### 특징 및 장점\n\n#### 비동기\n\nUvicorn은 비동기 I/O를 지원하며 이를 통해 동시에 여러 요청을 효율적으로 처리할 수 있다. 파이썬 3.6 이상에서 `async/await` 문법을 활용한다. \n\n#### 성능\n\nuvloop와 httptools를 사용하여 성능을 최적화한다. uvloop는 파이썬의 asyncio 모듈을 대체하는 빠른 이벤트 루프 구현이며 httptools는 http 파싱을 위한 라이브러리이다. \n\n#### ASGI 호환성\n\nASGI 3.0 사양을 구현하여 다양한 ASGI 호환 웹 애플리케이션 및 프레임워크와 함께 작동할 수 있다. \n\n#### 단순성\n\n코드 기반이 단순하며 웹 서버의 기본 기능에 집중한다. 보다 복잡한 배포나 튜닝이 필요한 경우 Uvicorn을 Gunicorn과 같은 다른 웹 서버 래퍼와 함께 사용하는 것이 일반적이다. \n\n### 단점\n\n#### 기능 제한\n\n핵심적인 웹 서버 기능에 중점을 둬 복잡한 배포 또는 다양한 설정, 최적화 옵션과 같은 기능이 제한적이다. \n\n#### 비동기 환경의 복잡성\n\n비동기 애플리케이션을 위한 웹 서버로 설계되어 비동기 코드와 동기 코드의 혼합 사용, 특히 데이터베이스와의 연결과 같은 경우 문제가 발생할 수 있다. 모든 코드를 비동기적으로 작성하거나 동기 코드가 올바르게 스레드 또는 프로세스에서 실행되도록 관리되어야 한다.\n\n## FastAPI 시작하기\n\n### 1. 설치\n\n```python\npip install fastapi[all] uvicorn\n```\n\n### 2. 기본 애플리케이션 생성\n\n```python\n# main.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n```\n\n### 3. 애플리케이션 실행\n\n```bash\nuvicorn main:app --reload\n```\n\n`—reload` : 개발 중 코드 변경을 자동으로 반영하기 위해 사용한다. \n\n### 4. 자동 문서화\n\nFastAPI는 자동으로 API 문서를 생성해준다. 애플리케이션을 실행 후 브라우저에서 다음 URL을 방문하여 문서를 확인할 수 있다. \n\n- Swagger UI: `http://127.0.0.1:8000/docs`\n\n- ReDoc: `http://127.0.0.1:8000/redoc`\n\n\n\n"},{"excerpt":"은 파이썬의 타입 힌트 시스템을 기반으로 데이터 검증 및 설정 관리를 제공하는 라이브러리이다. 특히 FastAPI에서는 Pydantic을 주로 요청 및 응답 객체의 데이터 검증, 직렬화 및 역직렬화에 사용한다.  주요 특징 타입 힌트 기반 Pydantic 모델은 파이썬의 타입 힌트를 활용하여 선언된다. 이를 통해 코드 내에서 명확하게 데이터의 구조와 타입…","fields":{"slug":"/Pydantic-모델/"},"frontmatter":{"date":"November 02, 2023","title":"Pydantic 모델","tags":["Python"]},"rawMarkdownBody":"`Pydantic` 은 파이썬의 타입 힌트 시스템을 기반으로 데이터 검증 및 설정 관리를 제공하는 라이브러리이다. 특히 FastAPI에서는 Pydantic을 주로 요청 및 응답 객체의 데이터 검증, 직렬화 및 역직렬화에 사용한다. \n\n## 주요 특징\n\n### 타입 힌트 기반\n\nPydantic 모델은 파이썬의 타입 힌트를 활용하여 선언된다. 이를 통해 코드 내에서 명확하게 데이터의 구조와 타입을 지정할 수 있다. \n\n### 자동 데이터 검증\n\n모델에 데이터를 할당할 때에 자동으로 데이터 검증이 수행된다. 유효하지 않은 데이터는 오류를 발생시킨다. \n\n### 데이터 변환\n\n입력 데이터를 적절한 타입으로 자동 변환한다. 예를 들어 문자열로 들어온 숫자 데이터를 정수로 변환할 수 있다. \n\n### 기본값 및 유효성 검사\n\n모델 필드에 기본값을 제공하거나 유효성 검사 규칙을 추가할 수 있다. \n\n### JSON 직렬화 및 역직렬화\n\n자동으로 JSON 형태로 직렬화되며 JSON 데이터를 역직렬화 하여 모델 객체로 변환할 수 있다. \n\n## 예시\n\n```python\nfrom pydantic import BaseModel, ValidationError\n\nclass User(BaseModel):\n    id: int\n    name: str\n    age: int\n    email: str\n\n# 사용 예\nuser_data = {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"email\": \"johndoe@example.com\"\n}\n\nuser = User(**user_data)\nprint(user.json())  # 모델 객체를 JSON 형태로 출력\n\ntry:\n    invalid_data = {\n        \"id\": \"invalid\",\n        \"name\": \"John\",\n        \"age\": \"invalid\",\n        \"email\": \"johndoe\"\n    }\n    User(**invalid_data)\nexcept ValidationError as e:\n    print(e.errors())  # 데이터 검증 오류 출력\n```\n\n\n\n"},{"excerpt":"데이터베이스의 동시성 제어란? 여러 트랜잭션 또는 연산이 동시에 수행될 때 데이터의 일관성을 유지하고 충돌을 방지하기 위한 매커니즘을 말한다. 잘못된 동시성 제어는 데이터의 무결성을 해치고 시스템의 성능을 저하시키며 예측할 수 없는 결과를 초래할 수 있다.  동시성 제어의 주요 개념 락 (Locking) 데이터베이스 항목에 대한 동시 액세스를 제어하기 위…","fields":{"slug":"/DynamoDB의-동시성-제어Concurrency-Control/"},"frontmatter":{"date":"November 01, 2023","title":"DynamoDB의 동시성 제어(Concurrency Control)","tags":["AWS","DataBase"]},"rawMarkdownBody":"## 데이터베이스의 동시성 제어란?\n\n여러 트랜잭션 또는 연산이 동시에 수행될 때 데이터의 일관성을 유지하고 충돌을 방지하기 위한 매커니즘을 말한다. 잘못된 동시성 제어는 데이터의 무결성을 해치고 시스템의 성능을 저하시키며 예측할 수 없는 결과를 초래할 수 있다. \n\n### 동시성 제어의 주요 개념\n\n#### 락 (Locking)\n\n데이터베이스 항목에 대한 동시 액세스를 제어하기 위해 사용된다. 트랜잭션은 데이터에 액세스 하기 전에 락을 획득해야 하며, 작업이 완료되면 락을 해제해야 한다. \n\n일반적으로 비관적 잠금과 낙관적 잠금의 두 형태로 구분된다. \n\n#### 트랜잭션 (Transaction)\n\n일련의 데이터베이스 작업을 하나의 논리적 단위로 묶는 것이다. 트랜잭션은 모두 성공하거나 모두 실패해야 하며, 중단 상태에서 중단되어서는 안된다. \n\n#### 일관성 (Consistency)\n\n데이터베이스의 모든 트랜잭션이 데이터를 올바른 상태에서 유지하도록 보장하는 속성이다. \n\n- 강력한 일관성 (Strong Consistency)\n\n    데이터베이스의 모든 노드가 항상 동일한 데이터를 보게 됨을 보장한다. \n\n    데이터베이스에 쓰기 작업이 수행된 후 모든 후속 읽기 작업이 그 쓰기 작업의 결과를 반영해야 한다. \n\n    실시간 시스템, 금융 시스템 또는 기타 높은 무결성이 필요한 시스템에서 중요하다. \n\n    일반적으로 높은 네트워크 지연과 함께 오며, 시스템의 전반적인 성능과 확장성에 영향을 미칠 수 있다. \n\n- 최종 일관성 (Eventual Consistency)\n\n    시간이 지나면서 데이터베이스의 모든 노드가 동일한 데이터를 볼 수 있게 됨을 보장한다. \n\n    쓰기 작업이 수행된 후 일정 시간이 지나면 모든 읽기 작업이 쓰기 작업의 결과를 반영한다. \n\n    일반적으로 높은 확장성과 낮은 지연 시간을 제공하며 분산 시스템에서 높은 성능을 달성하는데 유리하다. \n\n    일시적인 데이터 불일치를 허용하므로 애플리케이션의 로직이 이러한 불일치를 처리할 수 있도록 설계되어야 한다. \n\n#### 분리성 (Isolation)\n\n각 트랜잭션이 독립적으로 실행되도록 보장하는 속성이다. 트랜잭션들 사이에 어떤 중간 상태도 다른 트랜잭션에게 보여져셔는 안된다. \n\n#### 지속성 (Durability)\n\n트랜잭션이 성공적으로 완료된 후에는 시스템의 어떤 실패에도 영향 받지 않고 데이터베이스 변경이 유지되도록 보장하는 속성이다. \n\n## DynamoDB의 동시성 제어 매커니즘\n\nDynamoDB는 동시성 제어와 데이터 일관성을 관리하기 위한 여러 매커니즘을 제공한다. \n\n### 조건식 사용 (Conditional Writes)\n\n항목을 작성하거나 업데이트 할 때 조건식을 사용하여 해당 항목이 변경되지 않았음을 확인할 수 있다. 이것은 항목이 기존에 있던 상태에 따라 연산을 수행하도록 할 때 유용하다. 예를 들어 항목이 특정값에 해당하지 않으면 업데이트 하지 않도록 요청할 수 있다. \n\n### 컨설턴트 (Consistent Reads)\n\n기본적으로 최종적으로 일관성 있는 읽기를 제공한다. 이것은 약간의 지연을 허용하면서 빠른 응답 시간을 제공한다. 하지만 바로 이전의 쓰기 작업 이후에 항목의 최신 상태를 확인하려면 Strongly Consistent Read 옵션을 사용할 수 있다. 이 옵션은 데이터의 최신 상태를 보장한다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n하지만 Dynamo DB에서 동시성은 주로 다음의 두 가지 방법을 사용하여 수행된다. \n\n## 낙관적 잠금 (Optimistic Locking)\n\n낙관적 잠금은 클라이언트가 업데이트(또는 삭제) 하려는 항목이 DynamoDB의 항목과 동일한지 확인하는 전략이다. 이 전략을 사용하면 데이터베이스 쓰기는 다른 사용자의 쓰기에 의해 덮여지는 일을 방지할 수 있다. \n\n이 전략은 DynamoDB의 테이블에 `버전 번호` 라는 속성을 사용하여 구현된다. 클라이언트가 데이터 항목을 수정할 때 클라이언트 측의 버전 번호가 테이블 항목의 버전 번호와 동일해야 한다. 버전 번호가 동일하면 다른 사용자가 레코드를 변경하지 않았음을 의미하며 쓰기를 수행할 수 있다. \n\n그러나 버전 번호가 다르면 다른 사용자가 이미 레코드를 업데이트 했을 가능성이 있으며, 이 경우 DynamoDB는 `ConditionalCheckFailedException` 예외를 발생시켜 쓰기를 거부한다. \n\n낙관적 잠금을 사용하기 위해서는 코드에서 `ConditionalCheckFailedException` 을 포함하여 `put`, `delete`, `update` 작업에 대한 조건부 쓰기를 구현해야 한다. \n\n## 비관적 잠금 (Pessimistic Locking)\n\n비관적 잠금은 동시 업데이트를 방지하기 위해 사용하는 또 다른 전략이다. 낙관적 잠금이 버전 번호를 클라이언트에 전송하는 반면, 비관적 잠금은 버전 번호를 유지하지 않고 동시 업데이트를 방지하려고 한다. \n\n대부분의 데이터베이스 서버는 트랜잭션 시작 시 잠금을 획득하고 트랜잭션이 완료된 후에만 잠금을 해제하지만, DynamoDB는 이를 약간 다르게 처리한다. DynamoDB는 데이터를 잠그지 않고, 대신 트랜잭션을 사용하여 검토 중인 데이터에 대한 다른 스레드의 변경을 식별하며, 변경이 감지되면 DynamoDB는 트랜잭션을 취소하고 오류를 발생시킨다. \n\n비관적 잠금을 구현하기 위해 `taransacWrite` 라는 메소드를 제공한다. 이를 사용하면 처리 중인 항목을 잠그지 않지만 항목을 모니터링하고, 다른 스레드가 데이터틀 수정하면 전체 트랜잭션이 데이터 변경으로 인해 실패하고 데이터를 롤백한다. \n\n## RDB 와의 차이점\n\n### 데이터 모델\n\n- RDB\n\n    관계형 데이터 모델을 사용하여 테이블, 행, 열의 데이터로 형태를 데이터 구조하화며, 테이블 간의 관계를 정의하여 데이터의 무결성을 유지한다. \n\n- DynamoDB\n\n    NoSQL 데이터베이스로 키-값과 문서 데이터 모델을 사용한다. 데이터 무결성 보장을 위해 관계형 데이터베이스와 다른 매커니즘을 사용한다. \n\n### 동시성 제어\n\n- RDB\n\n    트랜잭션을 사용하여 동시성을 제어한다. 트랜잭션을 시작할 때 데이터에 잠금을 걸고, 트랜잭션이 완료될 때까지 잠음을 유지하여 다른 트랜잭션에 의한 동시 수정을 방지한다. \n\n- DynamoDB\n\n    낙관적 잠금과 조건부 업데이트를 사용하여 동시성을 제어하거나 `taransacWrite` API를 사용하여 트랜잭션을 지원한다. \n\n### 일관성\n\n- RDB\n\n    보다 강력한 일관성(Strong Consistency)을 지원한다. 트랜잭션의 모든 변경이 즉시 모든 다른 트랜잭션에 표시된다. \n\n- DynamoDB\n\n    기본적으로 최종 일관성(Eventual Consistency)을 제공한다. 필요한 경우 강력한 일관성(Strong Consistency)을 요청할 수 있다. \n\n### 분리성\n\n- RDB\n\n    트랜잭션의 분리성을 보장하여 각 트랜잭션이 독립적으로 실행되도록 한다. \n\n- DynamoDB\n\n    `TransactWrite` API를 통해 여러 항목에 대한 트랜잭션을 지원하여 분리성을 제공한다. \n\n### 지속성\n\n두 DB 모두 트랜잭션이 성공적으로 완료된 후에는 시스템의 어떤 실패에도 영향을 받지 않고 데이터베이셔 변경이 유지되도록 지속성을 보장한다. \n\n## 참고 문헌\n\n[https://dynobase.dev/dynamodb-locking/](https://dynobase.dev/dynamodb-locking/)\n\n"},{"excerpt":"개요 Poetry는 파이썬 프로젝트의 의존성 관리와 패키징을 도와주는 도구이다. Poetry의 주요 목표는 프로젝트의 설정과 관리를 단순화하고, 이를 위한 통합된 솔루션을 제공하는 것이다.  파이썬 개발자에게 일관된 개발 환경을 제공하며, 의존성 해결, 버전 관리, 패키징 배포와 같은 일반적인 작업을 처리하는 통합된 도구를 제공하여 파이썬 프로젝트의 생명…","fields":{"slug":"/Poetry/"},"frontmatter":{"date":"October 31, 2023","title":"Poetry","tags":["Python"]},"rawMarkdownBody":"## 개요\n\nPoetry는 파이썬 프로젝트의 의존성 관리와 패키징을 도와주는 도구이다. Poetry의 주요 목표는 프로젝트의 설정과 관리를 단순화하고, 이를 위한 통합된 솔루션을 제공하는 것이다. \n\n파이썬 개발자에게 일관된 개발 환경을 제공하며, 의존성 해결, 버전 관리, 패키징 배포와 같은 일반적인 작업을 처리하는 통합된 도구를 제공하여 파이썬 프로젝트의 생명 주기를 관리하는데 필요한 모든 기능을 제공한다. \n\n기존의 requirements.txt, venv, setup.py, wheel, twine 등을 모두 통합해 하나의 인터페이스와 프로세스로 간소화하여 제공한다. \n\n## 기능과 이점\n\n### 의존성 관리\n\n`pyproject.toml` 파일을 사용하여 프로젝트의 의존성을 선언적으로 정의하고 관리한다. \n\n의존성이 충돌하지 않고 서로 호환되는지 자동으로 확인하여 프로젝트의 의존성 그래프를 안정적으로 유지한다. \n\n### 패키징\n\n프로젝트를 패키지화하고 배포할 수 있는 도구를 제공한다. \n\n`build` 및 `publish` 명령을 사용하여 프로젝트를 빌드하고 PyPi와 같은 패키지 저장소에 배포할 수 있다. \n\n### 가상 환경 관리\n\n프로젝트에 대한 가상 환경을 자동으로 생성하고 관리하여 프로젝트의 의존성이 시스템 전역의 Python 환경으로부터 격리되도록 한다. \n\n`poetry shell` 과 `poetry run` 명령어를 사용하여 가상 환경 내에서 명령을 실행할 수 있다. \n\n### 버전 관리\n\n프로젝트의 버전을 쉽게 관리할 수 있도록 돕는다. \n\n`version` 명령어를 사용하여 프로젝트의 버전을 업데이트 할 수 있다. \n\n### 프로젝트 생성과 설정\n\n`new` 명령을 사용하여 프로젝트를 생성하고 기본 구조와 설정 파일을 자동으로 생성한다. \n\n### 플러그인 시스템\n\n사용자 정의 플러그인을 지원하여 기능을 확장하고 사용자 정의 작업을 수행할 수 있다. \n\n### 다양한 설정 옵션\n\n다양한 설정 옵션을 제공하여 프로젝트의 특별한 요구 사항에 따라 도구의 동작을 사용자 정의할 수 있다.  \n\n## requirements.txt 와의 비교\n\nrequirements.txt는 파이썬 프로젝트에서 아요되는 의존성과 그 버전을 나열하는 텍스트 파일이다. 프로젝트에 필요한 모든 패키지와 버전을 명시하며, 이를 통해 다른 개발자가 다른 환경에서 동일한 의존성을 쉽게 유지할 수 있다. \n\n- requirements.txt는 단순히 프로젝트의 의존성과 해당 버전을 나열하는 데 사용되며, 이를 넘어서는 확장성을 제공하지 않는다.\n\n- 의존성 간의 충돌을 자동으로 해결하지 않는다. \n\n- 가상 환경을 자동으로 생성하거나 관리하지 않는다. \n\n- 프로젝트 패키징, 배포 기능을 제공하지 않는다. \n\n- 프로젝트의 구조나 설정에 대한 정보를 제공하지 않는다. \n\n## venv와의 비교\n\nvenv는 파이썬 가상환경을 생성하고 관리하는 기본 도구이다. venv의 주요 목적은 프로젝트의 의존성을 시스템 전역의 파이썬 환경으로부터 격리하는 것이다. \n\n- venv는 의존성을 관리하지 않는다. 가상 환경 내에서 pip를 사용하여 수동으로 패키지를 설치하고 관리해야 한다. \n\n- 프로젝트 패키징, 배포 기능을 제공하지 않는다. \n\n- 프로젝트의 구조나 설정에 대한 정보를 제공하지 않는다. \n\n- poetry는 install 명령을 실행할 때에 자동으로 가상환경을 생성하지만, venv는 별도의 명령어를 사용해 가상환경을 생성해야한다. \n\n- 기본적인 명령줄 인터페이스만 제공한다. \n\n## 단점\n\n### 학습곡선\n\n그 자체로 너무 많은 기능이 들어있기 때문에 기존 도구에들에 익숙한 개발자들에게는 적응 시간이 필요할 수 있다. \n\n### 호환성 문제\n\n일부 레거시 프로젝트나 특정 시스템에서는 호환성 문제가 발생할 수 있다. \n\n### 성능\n\n큰 프로젝트에서 종속성의 해결이 다소 느릴 수 있다. \n\n### 통합 문제\n\n아직 모든 플랫폼이나 서비스에 완벽하게 통합되지 않았을 수 있다. CI/CD 도구와의 통합, 또는 특정 클라우드 서비스와의 통합에서 약간의 설정 작업이 필요할 수 있다. \n\n## 주요 사용법 및 명령어\n\n### 설치\n\n```bash\ncurl -sSL https://install.python-poetry.org | python3 -\n```\n\n### 새 프로젝트 시작\n\n```bash\npoetry new project-name\n```\n\n### 기존 프로젝트에서 시작\n\n이미 존재하는 프로젝트에서도 poetry를 사용할 수 있다. 프로젝트의 루트 디렉토리에서 다음 명령어를 실행한다. \n\n```bash\npoetry init\n```\n\n### 종속성 추가\n\n```bash\npoetry add package-name\n```\n\n한편 개발중에만 필요한 패키지를 추가하려면 다음을 실행한다. \n\n```bash\npoetry add package-name --dev\n```\n\n### 종속성 제거\n\n```bash\npoetry remove package-name\n```\n\n### 종속성 설치\n\n`pyproject.toml`에 정의된 종속성을 설치하려면 다음 명령어를 실행한다. \n\n```bash\npoetry install\n```\n\n### 스크립트 실행\n\n```bash\npoetry run python your_script.py\n```\n\n### 빌드 및 배포\n\n프로젝트를 패키징 하려면 다음의 명령어를 실행한다. 이렇게 하면 `dist`  디렉토리에 패키지가 생성된다. \n\n```bash\npoetry build\n```\n\n### 가상환경\n\npoetry는 자동으로 가상환경을 생성한다. 가상 환경에 들어가기 위해서는 다음의 명령어를 실행한다. \n\n```bash\npoetry shell\n```\n\n### 기능 탐색\n\n```bash\npoetry --help\n```\n\n\n\n"},{"excerpt":"소개 Blue-Green 배포 전략은 지속적인 통합 및 지속적인 배포 환경에서 자주 사용되는 소프트웨어 배포 패턴 중 하나이다. 이 전략의 주요 목표는 시스템의 중단 없이 안전하게 애플리케이션을 배포하고 업데이트 하는 것이다.  핵심 아이디어 두 개의 독립적인 환경 Blue와 Green이라는 두 개의 별도의 환경(또는 색상)이 있다. 일반적으로 한 환경(…","fields":{"slug":"/Blue-Green-Deploy-전략/"},"frontmatter":{"date":"October 30, 2023","title":"Blue Green Deploy 전략","tags":["ETC","BackEnd"]},"rawMarkdownBody":"## 소개\n\nBlue-Green 배포 전략은 지속적인 통합 및 지속적인 배포 환경에서 자주 사용되는 소프트웨어 배포 패턴 중 하나이다. 이 전략의 주요 목표는 시스템의 중단 없이 안전하게 애플리케이션을 배포하고 업데이트 하는 것이다. \n\n## 핵심 아이디어 \n\n### 두 개의 독립적인 환경\n\nBlue와 Green이라는 두 개의 별도의 환경(또는 색상)이 있다. 일반적으로 한 환경(Blue)은 실제 사용자에게 서비스를 제공하는 라이브 환경이며, 다른 환경(Green)은 새 버전의 애플리케이션을 배포하기 위한 준비 환경이다. \n\n### 스위치 오버\n\n새 버전의 애플리케이션을 Green 환경에 배포한 후, 모든 테스트와 검증 절차를 거쳐 문제가 없다고 판단되면 트래픽을 Blue 환경에서 Green 환경으로 빠르게 전환(또는 스위치 오버) 한다. 이렇게 하면 실제 사용자는 서비스 중단 없이 새 버전의 어플리케이션을 사용할 수 있다. \n\n### 롤백 용이\n\n만약 Green 환경의 새 버전에 문제가 발생하면, 트래픽을 다시 Blue 환경으로 즉시 전환하여 이전 버전의 애플리케이션을 사용하게 할 수 있다. 이는 문제가 발생했을 때 빠르게 롤백하는데 큰 도움이 된다. \n\n## 개요\n\nBlue-Green 배포 전략에서는 실제로 “배포” 하는 작업을 여러번 반복하지 않는다. 대신, 두 개의 독립적인 환경을 유지하고 트래픽을 이 두 환경 사이에서 스위칭한다. 이를 구체적으로 설명하면 다음과 같다. \n\n1. 초기 상태\n\n    Blue 환경이 라이브 상태이고 실제 사용자 트래픽을 처리하고 있다고 가정한다. \n\n1. 새 버전 배포\n\n    Green 환경에 새로운 버전의 애플리케이션을 배포한다. 이 시점에서 아직 Blue가 여전히 실제 트래픽을 처리하고 있다. Green은 배포 및 테스트 준비 단계에 있다. \n\n1. 스위칭\n\n    Green 환경에서의 테스트와 검증이 완료되면 트래픽을 Blue에서 Green으로 스위칭한다. 이제 Green이 라이브 상태가 되어 사용자 트래픽을 처리한다. \n\n1. 다음 배포\n\n    다음 번 배포 시, Blue 환경을 업데이트하고 검증한 후 트래픽을 다시 Blue로 스위칭한다. \n\n## 장점\n\n### 빠른 롤백\n\n문제가 발생하면 즉시 이전 버전으로 롤백할 수 있다. \n\n### 중단시간 없음\n\n사용자에게 서비스 중단 없이 애플리케이션을 배포할 수 있다. \n\n### 통합 테스트\n\nGreen 환경에서 새 버전의 애플리케이션을 배포하기 전에 광범위한 테스트와 검증을 수행할 수 있다. \n\n## 단점\n\n### 자원 중복\n\n두 개의 독립적인 환경을 유지해야 하므로 추가적인 인프라와 자원이 필요하다. \n\n### 데이터베이스 동기화\n\n데이터베이스 스키마나 데이터에 변동이 있는 경우 Blue와 Green 환경 사이의 동기화 문제가 발생할 수 있다. \n\n## Blue-Green은 테스트 서버일까? \n\n배포 과정에서 한 환경이 새 버전의 검증을 위한 임시 테스트 역할을 하게 되는 것은 맞다. 이렇게 설명하면 테스트 서버와 실서버 역할을 그린과 블루가 번갈아가면서 한다고 착각하기 쉽지만 이보다는 둘 다 실서버의 역할을 할 수 있는 독립적인 프로덕션 환경임을 알아야 한다. 몇 가지 중요한 차이점이 있다. \n\n### 프로덕션 준비\n\nBlue와 Green 둘 다 프로덕션 트래픽을 처리할 준비가 되어있다. 그러나 일반적인 테스트 서버는 프로덕션 수준의 리소스나 성능을 가지고 있지 않을 수 있다. \n\n### 스케일\n\nBlue와 Green은 실제 사용자의 트래픽을 처리할 수 있을 만큼 큰 규모로 구축되어야 한다. 반면 테썹은 보통 작은 규모로 운영된다. \n\n### 데이터 \n\nBlue와 Green은 실제 데이터를 다룬다. \n\n## Blue-Green 배포에서 DB 관리\n\nBlue-Green 배포 전략에서 데이터베이스의 관리는 중요한 고려사항 중 하나이다. 데이터베이스는 상태를 가지고 있으므로 애플리케이션 코드와는 달리 간단히 스위칭하는게 어렵다. \n\n### 단일 데이터베이스 사용\n\n같은 DB를 사용하는 것은 가능하다. 하지만 이렇게 할 때 고려해야 할 몇가지 중요한 문제가 있다. \n\n#### 스키마 변경\n\n데이터베이스 스키마에 변경이 필요한 새 버전의 애플리케이션을 배포할 때 문제가 발생할 수 있다. 새 버전에서 필요한 스키마 변경이 이전 버전과 호환되지 않으면 Green 환경에서의 배포 동안 Blue 환경의 애플리케이션에 문제가 발생할 수 있다. \n\n#### 데이터 무결성\n\n두 환경이 동일한 데이터베이스를 공유할 경우, 새 버전의 애플리케이션에서 예상치 못한 데이터 변경이 발생하면 이전 버전의 실행에도 영향을 줄 수 있다. 따라서, 롤백에도 영향을 줄 수 있다. \n\n\n\n이를 극복하기 위한 여러 전략이 있다. 예를 들면,\n\n#### 스키마 변경 전략\n\n데이터 베이스 스키마 변경을 두 단계로 나누어 수행한다. 먼저 호환성이 있는 스키마 변경을 실시한 후, 새 버전의 애플리케이션을 배포한다. 그 다음 이전 버전과 호환되지 않는 스키마 변경을 실시한다. \n\n#### 피쳐 토글\n\n데이터베이스에 영향을 주는 새로운 기능이나 변경 사항을 피쳐 토글로 구현하여, 실제로 데이터베이스에 적용되기 전까지는 활성화하지 않을 수 있다. \n\n#### 결론\n\n만약 이 방법을 선택할 경우, 새로운 버전의 애플리케이션은 데이터베이스 스키마에 대한 변경이 없거나 하위 호환성을 유지해야 한다. \n\n### 데이터베이스 마이그레이션\n\n- 배포 전에 필요한 데이터베이스 마이그레이션을 신중하게 계획하고 실행한다. \n\n- 마이그레이션은 트래픽을 Green 환경으로 전환하기 전에 실행되어야 한다. \n\n- 롤백 시나리오에 대한 계획도 있어야 한다. \n\n### 데이터 동기화\n\n- 동기화를 위해 DynamoDB Streams와 Lambda를 사용하여 두 테이블 간의 데이터를 동기화 할 수 있다. \n\n## ALB-ECS 환경에서의 Blue-Green 배포 전략 시나리오\n\n### 1. 초기 환경 설정\n\n- ECS 클러스터에서 두 개의 서비스(Blue와 Green)을 생성한다. 초기에는 Blue만 활성 상태(현재 트래픽을 받고 있는 상태)이다. \n\n- ALB 설정: ECS 서비스에 트래픽을 라우팅하기 위해 사용된다. \n\n- Route 53에 도메인 이름을 설정하고 ALB의 DNS 이름을 지정한다. \n\n### 2. CodePipeline 설정\n\n- Source: 예를 들면 깃허브 또는 CodeCommit의 소스 변경을 감지하여 파이프라인을 트리거한다. \n\n- Build: CodeBuild를 사용하여 Docker 이미지를 빌드하고 ECR(Elastic Container Registry)에 푸시한다. \n\n- Deploy: CodeDeploy를 사용하여 ECS에 새로운 이미지를 배포한다. \n\n### 3. Blue-Green 배포\n\n- 새로운 코드 변경이 파이프라인을 트리거하면 CodeBuild가 새 Docker 이미지를 빌드하고 ECR에 푸시한다. \n\n- CodeDeploy는 Green ECS 서비스를 대상으로 새 이미지를 사용하여 작업 정의를 업데이트한다. \n\n- Green 서비스의 태스크가 성공적으로 시작되면 Blue-Green 배포 설정에 따라 CodeDeploy는 자동으로 ALB의 대상 그룹을 업데이트하여 Green 서비스로 트래픽을 전환한다. \n\n### 4. 검증 및 롤백\n\n- Green 환경에서 서비스의 동작을 검증한다. \n\n- CodeDeploy는 선택적으로 사용자 지정 후크를 사용하여 배포 후 검증 스크립트를 실행할 수 있다. \n\n- 문제가 발생한 경우 원래의 Blue 서비스로 빠르게 전환하여 롤백한다. CodeDeploy는 자동 롤백을 지원하여 이전 상태로 되돌릴 수 있다. \n\n### 5. 다음 배포 준비\n\n- 다음 배포는 Blue가 새 환경이고 Green이 이전 환경이 된다. 이렇게 두 환경을 번갈아 사용하면서 배포를 진행한다. \n\n## 기타 배포 전략\n\n1. 카나리아 배포\n\n1. 롤링 배포\n\n1. 기능 토글\n\n1. A/B 테스팅\n\n1. 섀도우 배포\n\n\n\n"},{"excerpt":"Amazon CloudFront 아마존에서 제공하는 CDN(Content Delivery Network) 서비스이다.  CDN이란? 사용자에게 웹 콘텐츠를 더 빠르게 제공하기 위해 전 세계 여러 위치에 콘텐츠를 분산 및 저장하는 네트워크이다. 주요 기능과 특징 글로벌 엣지 네트워크 전 세계에 200개가 넘는 엣지 로케이션과 리전을 갖추고 있다.  동적 및…","fields":{"slug":"/CloudFront/"},"frontmatter":{"date":"October 30, 2023","title":"CloudFront","tags":["AWS"]},"rawMarkdownBody":"## Amazon CloudFront\n\n아마존에서 제공하는 CDN(Content Delivery Network) 서비스이다. \n\n#### CDN이란?\n\n사용자에게 웹 콘텐츠를 더 빠르게 제공하기 위해 전 세계 여러 위치에 콘텐츠를 분산 및 저장하는 네트워크이다.\n\n## 주요 기능과 특징\n\n#### 글로벌 엣지 네트워크\n\n전 세계에 200개가 넘는 엣지 로케이션과 리전을 갖추고 있다. \n\n#### 동적 및 정적 콘텐츠 전송\n\n정적 콘텐츠와 동적 콘텐츠를 모두 효과적으로 전송한다. \n\n#### 오리진 서버\n\n배포를 생성할 때 오리진 서버를 지정해야 한다. 이 오리진은 S3 버킷, EC2 인스턴스, ELB, 외부 서버 등이 될 수 있다. 오리진에서 컨텐츠를 가져와 사용자에게 제공한다. \n\n#### 성능향상\n\nCDN은 사용자에게 가까운 서버에서 콘텐츠를 제공하여 웹 사이트의 로딩시간을 단축시킨다. \n\n#### 가용성 및 장애 복구\n\n여러 서버에 콘텐츠를 복사하므로, 한 서버에 문제가 발생하더라도 다른 서버에서 콘텐츠를 제공할 수 있다. \n\n#### 부하 분산\n\n웹 트래픽을 여러 서버에 분산시켜 오리진 서버의 부하를 줄인다. \n\n#### 비용 절감\n\n캐시된 콘텐츠를 제공함으로써 오리진 서버의 대역폭 사용량과 비용을 줄일 수 있다. \n\n#### 보안 향상\n\n웹사이트에 대한 DDoS 공격과 같은 일부 보안 위협으로부터 보호할 수 있다. \n\n## 동작 플로우\n\n1. **배포 생성**\n\n    사용자는 AWS Management Console, SDK, CLI 등을 사용하여 CloudFront 배포를 생성한다. 이 때, 오리진 서버(S3 버킷, EC2 인스턴스, HTTP 서버 등)와 다양한 배포 설정(예: 캐싱 동작, SSL 인증서, 액세스 제한 등)을 지정한다. \n\n1. **DNS 등록**\n\n    CloudFront는 배포에 대해 고유한 도메인 이름을 제공한다. 사용자는 Route 53 같은 DNS 서비스를 사용해 이 도메인 이름을 웹 사이트나 애플리케이션에 연결할 수 있다. \n\n1. **사용자 요청**\n\n    사용자가 콘텐츠에 액세스 하기 위해 웹 브라우저나 애플리케이션으로 요청을 보내면 DNS 시스템은 해당 요청을 CloutFront 배포의 도메인 이름으로 Resolve 한다. \n\n1. **엣지 로케이션 라우팅**\n\n    요청은 사용자게에 가장 가까운 CloudFront 엣지 로케이션으로 라우팅된다. \n\n1. **캐시 확인**\n\n    해당 엣지 로케이션의 캐시에서 요청된 콘텐츠를 확인한다. \n\n    - 캐시에 콘텐츠가 있는 경우 (Cache Hit) : 캐시된 콘텐츠를 사용자에게 직접 반환한다. \n\n    - 캐시에 콘텐츠가 없는 경우 (Cache Miss) : CloudFront는 설정된 오리진 서버로 요청을 전달한다. \n\n1. **오리진에서 콘텐츠 가져오기**\n\n    오리진 서버에서 요청된 콘텐츠를 가져온다. \n\n1. **콘텐츠 반환 및 캐싱**\n\n    가져온 콘텐츠를 사용자에게 반환하면서 동시에 해당 콘텐츠를 엣지 로케이션의 캐시에 저장한다. 이후 동일한 콘텐츠에 대한 요청이 들어오면 캐시에서 빠르게 제공할 수 있다. \n\n1. **캐시 만료**\n\n    캐싱 동작은 TTL(Time to Live) 설정에 따라 관리된다. TTL이 만료되면 콘텐츠는 자동으로 캐시에서 제거된다. 다음 요청이 들어오면 다시 오리진에서 콘텐츠를 가져온다. \n\n## 동적 콘텐츠의 캐싱\n\n동적 콘텐츠는 사용자별로 또는 시간에 따라 달라지는 콘텐츠를 의미한다. 예를 들면 사용자 프로필 페이지, 실시간 주식 시세, 뉴스 피드 등이 있을 수 있다. CDN 서비스들은 동적 콘텐츠를 캐싱하기 위해 특별한 전략을 사용한다. \n\n동적 콘텐츠 캐싱의 핵심 아이디어는 “모든 동적 콘텐츠가 항상 동적인 것은 아니다.”라는 점이다. 즉, 일부 동적 콘텐츠는 짧은 시간 동안에는 변하지 않을 수도 있다. \n\n캐시의 동작을 잘못 설정하면 데이터 베이스의 변경사항이 반영된 새로운 응답이 캐시에 저장되지 않을 수 있으므로, 그 결과로 사용자는 오래된 또는 기대하지 않은 응답을 받을 수 있다. 이런 문제를 피하기 위해 캐싱은 항상 주의해야 한다. \n\n캐싱 전략을 변경하거나 새로운 캐싱 매커니즘을 도입할 때에는 항상 테스트 환경에서 먼저 검증하는 것이 좋다. \n\n동적 콘텐츠를 캐싱하는 방법은 다음과 같다. \n\n### 조정 가능한 TTL\n\n동적 콘텐츠에 대한 캐싱 정책의 TTL을 짧게 설정할 수 있다. 이렇게 하면 콘텐츠가 잠시동안만 캐싱되어 그 이후에는 다시 오리진에서 최신 데이터를 가져온다. \n\n### 캐시 키 매개변수\n\n쿼리 문자열, 헤더, 쿠키 등의 요청 요소를 기반으로 캐시 버전을 구분할 수 있다. 이를 사용해 동일한 URL에 대한 다양한 버전의 콘텐츠를 캐싱할 수 있다. \n\n### 컨텐츠 기반의 캐싱 정책\n\n오리진 서버는 `Cache-Control` 또는 `Expires` 헤더를 사용하여 콘텐츠의 캐싱 동작을 제어할 수 있다. \n\n#### Cache-Control 헤더\n\n반환된 응답에 Cache-Control 헤더가 포함되어 있는 경우 이 헤더의 지시어를 따라 캐싱한다. 예를 들어 `Cache-Control: max-age=3600` 은 해당 응답을 1시간 동안 캐싱하라는 의미이다. \n\n또는 `Cache-Control: no-store` 나 `Cache-Control: private` 헤더를 포함하고 있으면 해당 응답은 캐싱되지 않는다. \n\n#### Expires 헤더\n\n반환된 응답이 Expires 헤더를 포함하고 있으면 해당 시간까지 응답을 캐싱한다. \n\n### Lambda@Edge\n\nLambda@Egde를 이용하여 요청 및 응답을 가로채고 수정하여 캐싱 동작을 더욱 세밀하게 제어할 수 있다. \n\n### Bypass Cache\n\n특정 조건 하에서 캐싱을 우회하고 항상 오리진 서버로 요청을 전달할 수 있다. 이렇게 하면 특정 요청에 대해서는 항상 최신의 동적 콘텐츠를 제공할 수 있다. \n\n### 데이터 변경 시 캐시 무효화\n\n특정 콘텐츠의 캐시를 수동으로 무효화 하는 기능을 제공한다. 데이터베이스에 중요한 변경이 발생한 경우, 이 기능을 사용하여 해당 콘텐츠의 캐시를 즉시 제거할 수 있다. \n\n\n\n"},{"excerpt":"개요 AWS ECS는 Docker 컨테이너를 쉽게 실행, 중지 및 관리할 수 있도록 해주는 컨테이너 오케스트레이션 서비스이다. 컨테이너 배포, 작업 정의, 서비스 정의, 클러스터 관리 등을 지원한다.  오케스트레이션 서비스란? 컨테이너의 배포, 관리, 스케일링, 네트워킹 및 가용성을 자동화하는 프로세스 및 도구의 집합이다.  즉, 컨테이너화된 애플리케이션…","fields":{"slug":"/AWS-ECSElastic-Container-Service/"},"frontmatter":{"date":"October 25, 2023","title":"AWS ECS(Elastic Container Service)","tags":["AWS"]},"rawMarkdownBody":"## 개요\n\nAWS ECS는 Docker 컨테이너를 쉽게 실행, 중지 및 관리할 수 있도록 해주는 컨테이너 오케스트레이션 서비스이다. 컨테이너 배포, 작업 정의, 서비스 정의, 클러스터 관리 등을 지원한다. \n\n### 오케스트레이션 서비스란?\n\n- 컨테이너의 배포, 관리, 스케일링, 네트워킹 및 가용성을 자동화하는 프로세스 및 도구의 집합이다. \n- 즉, 컨테이너화된 애플리케이션의 라이프사이클을 관리하는 것을 의미한다. \n\n## 주요 특징\n\n### 서비스 \n\n여러 개의 작업 인스턴스를 관리하며, 로드 밸런싱과 함께 사용하여 트래픽 분산을 지원한다. \n\n### 클러스터\n\n컨테이너를 실행하는데 사용되는 논리적인 그룹이다. ECS에서 실행되는 리소스의 집합을 의미한다. 클러스터는 EC2 인스턴스나 Fargate 인스턴스를 포함할 수 있다. \n\n클러스터를 사용하여 워크로드와 관련된 리소스를 논리적으로 그룹화 할 수 있다. 예를 들면 **“production” 클러스터와 “development” 클러스터를 별도로 운영할 수 있다.** \n\n기본적으로 단순한 논리적 이름을 지닌 메타 데이터 구조체로, 클러스터 안의 실제 컴퓨팅 리소스와 독립적이다. \n\n### 네임스페이스\n\n리소스를 논리적으로 분리하고 관리하기 위한 도구이다. 동일한 도메인 이름 아래에서 운영되는 서비스들의 논리적 그룹의 형성에 사용된다. \n\n### 테스크\n\n단일 컨테이너 또는 여러 컨테이너의 그룹이다. \n\n`작업 정의` : 컨테이너의 속성을 정의하는 JSON 템플릿이다. CPU, 메모리 할당, 네트워크 모드, 사용할 도커 이미지 등을 포함한다. \n\n테스크는 작업 정의에 정의된 내용에 따라 실행된다. 예를 들면 웹 서버와 관련된 데이터베이스를 함께 실행하는 테스크를 생성할 수 있다. \n\n## 순서와 구조\n\n1. 컨테이너\n\n    독립적으로 실행 가능한 소프트웨어의 최소 단위. AWS ECS에서는 이 컨테이너를 EC2 인스턴스나 Fargate에서 실행할 수 있다.  \n\n1. 테스크\n\n    ECS의 실행 단위, 하나 이상의 컨테이너로 구성됨\n\n1. 클러스터\n\n    테스크와 서비스를 실행하고 관리하는 논리적 단위\n\n1. 네임스페이스\n\n    서비스와 관련된 논리적 그룹을 형성\n\n1. ECS\n\n각 단계는 이전 단계의 구성 요소를 더 큰 단위로 그룹화하거나 관리하는 역할을 한다. \n\n## 웹 애플리케이션의 개발 후 ECS 배포\n\n1. 웹 애플리케이션 개발\n\n    웹 애플리케이션을 개발한다. \n\n    로컬에서 테스트하여 작동을 확인한다. \n\n1. Docker 이미지 생성\n\n    `Dockerfile` 을 작성하여 웹애플리케이션을 컨테이너화 한다. \n\n    로컬에서 Docker를 사용하여 이미지를 빌드하고 테스트한다. \n\n1. Docker 이미지를 ECR(Elastioc Container Registry)에 푸시\n\n    AWS ECR에 저장소를 생성한다. \n\n    로컬에서 빌드한 Docker 이미지를 ECR 저장소에 PUSH 한다. \n\n1. ECS 클러스터 및 작업 정의 설정\n\n    ECS 클러스터를 생성한다. (Fargate 또는 EC2 인스턴스)\n\n    작업 정의(Task Definition)을 생성한다. 작업 정의에는 ECR에수 푸시한 Docker 이미지, 필요한 CPU와 메모리, 환경 변수, 포트 매핑 등을 지정한다. \n\n1. ECS 서비스 생성\n\n    ECS 클러스터 내에서 작업정의를 기반으로 서비스를 생성한다. 서비스를 사용하면 컨테이너의 실행 상태를 유지하거나 원하는 개수의 작업 인스턴스를 실행할 수 있다. \n\n1. 도메인 및 DNS 설정\n\n    AWS Route 53을 사용하여 도메인을 설정하고 ALB(Application Load Balancer) 의 주소를 해당 도메인에 연결한다. \n\n1. 보안 그룹 및 네트워크 설정\n\n1. 모니터링 및 로깅\n\n## 기존 서비스에서 ECS 기반 서비스로의 전환\n\n1. ECS, Fargate 설정\n\n1. ALB 설정\n\n    Application Load Balancer를 생성한다. \n\n    리스너와 타겟 그룹을 설정하여 ECS 서비스의 작업에 트래픽을 라우팅 하도록 한다. \n\n1. Route 53 업데이트\n\n    현재 서비스에 연결된 Route53 레코드를 수정하여 ALB의 DNS 이름을 가리키도록 변경한다. \n\n1. 모니터링 및 튜닝\n\n### 주의점\n\n기본적으로는 CES, Fargate, ALB를 설정한 후 마지막 단계로 Route 53의 레코드 설정만 변경하면 트래픽은 새로운 ECS-Fargate 기반의 서비스로 라우팅된다. \n\n그러나 다음과 같은 주의 사항이 있다. \n\n1. 테스트 \n\n    ALB와 ECS가 올바르게 작동하는지 사전에 충분히 테스트 해야 한다. \n\n1. DNS 전파 지연\n\n    DNS 업데이트는 즉시 전파되지 않을 수 있다. 일부 사용자는 변경 전의 엔드포인트로 트래픽이 라우팅 될 수 있으므로 업데이트 후에도 기존 서비스를 적어도 몇 시간 동안 실행 상태로 유지하는 것이 좋다. \n\n1. 롤백 계획\n\n    문제가 발생할 경우를 대비하여 롤백 계획을 준비해야 한다. 예를 들어 새로운 서비스에 문제가 발생한 경우 Route 53 설정을 원래대로 돌릴 수 있어야 한다. \n\n"},{"excerpt":"어느 순간 morethan-log에 노션 글 업로드가 잘 되지 않았다.  글을 그 동안 마구 마구 써버리긴 했다.. 그래서 찔리는 점이 있었는데, 최근 이직 후 일을 하면서 조금 덜 쓰면서, 그리고 잠깐 옵시디언으로 한 눈을 팔면서 고칠 생각을 안하고 있었다.  그러면서 일도 쪼끔 익숙해지고 옵시디언도 아 별로다 라고 결론을 내고 다시 여기에 글을 썼는데…","fields":{"slug":"/vercel-배포-자동화/"},"frontmatter":{"date":"October 23, 2023","title":"vercel 배포 자동화","tags":["Hobby","Blogging"]},"rawMarkdownBody":"어느 순간 morethan-log에 노션 글 업로드가 잘 되지 않았다. \n\n\n\n글을 그 동안 마구 마구 써버리긴 했다.. 그래서 찔리는 점이 있었는데, 최근 이직 후 일을 하면서 조금 덜 쓰면서, 그리고 잠깐 옵시디언으로 한 눈을 팔면서 고칠 생각을 안하고 있었다. \n\n그러면서 일도 쪼끔 익숙해지고 옵시디언도 아 별로다 라고 결론을 내고 다시 여기에 글을 썼는데, 업로드가 되지 않는걸 보니 결국은 고쳐야 할 때가 왔음을 알게 됐다. \n\n\n\nmorethan-log는 일정 시간마다 특정 api를 호출해서 노션에서 글을 긁어오는 함수를 실행한다. \n\n그리고 vercel에서는 그 api가 실행한 함수의 실행 로그를 볼 수 있다… \n\n\n\n로그를 봤더니 예상과 같았다. \n\n무료 플랜에서는 함수 실행 시간을 10초까지만 허용하는데, 10초를 당연한듯이 넘고 있었다. \n\n\n\nnotion 비공식 api를 호출하는건데.. \n\ndeploy를 직접 했을땐 제대로 긁어오는 것으로 보아 api가 막힌 것 같진 않고 글이 많아지면서 발생한 것 같다.. \n\n길게 고민하지 않고 Vercel의 Deploy hooks와 Pipedream의 기능의 무료 기능들만 사용해서 매일 새벽 두시에 자동 배포를 하기로 했다. \n\n### Vercel Deploy Hooks\n\nVercel의 Deploy Hooks 기능을 이용하면 특정 URL에 HTTP 요청이 이뤄질 때마다 사이트를 다시 빌드하고 배포할 수 있다. \n\n내 프로젝트를 고르고, Setting를 고른 다음 좌측의 Git을 고른다. \n\nDeploy Hooks로 이동해서 `Create Hook`을 해 url을 생성한다. 이제 해당 URL로 접속하면 deploy 되는것을 볼 수 있다. \n\n### Pipedream 설정\n\nPipedream은 서로 다른 애플리케이션, 데이터 및 API를 연결하고 자동화 하는데 사용할 수 있는 서버리스 통합 플랫폼이다. 단적으로, 이번엔 내가 원하는 시간에 Hook을 위한 URL을 호출하도록 스케쥴링 할 수 있다. \n\nPipedream 대시보드에서 `New`를 클릭하고 `Workflow`를 선택하여 새 워크플로우를 생성한다. \n\n트리거로 `Schedule` 을 선택하고 원하는 스케쥴을 설정한다. \n\n`Send Http Request` 를 선택하고, Vercel Deploy Hooks URL을 입력한다. \n\n테스트 후, 저장 및 배포한다. \n\n\n\n이로써 매일 사이트를 다시 배포하고 빌드함으로써 블로깅에 아무 문제가 없게 되었다. \n\n해피~\n\n"},{"excerpt":"기본 개념 DDD는 복잡한 애플리케이션과 시스템의 개발에 있어서 비즈니스 요구사항을 중심으로 설계하고 구현하는 접근법입니다. 이 방식은 비즈니스 도메인의 복잡성을 이해하고, 그 복잡성을 모델링하여 소프트웨어에 반영하는 것에 중점을 둡니다. 핵심 요소 Ubiquitous Language (모든 곳에서 사용되는 언어): 개발팀과 비즈니스 팀 간에 공통적으로 …","fields":{"slug":"/FastAPI와-DDD/"},"frontmatter":{"date":"October 20, 2023","title":"FastAPI와 DDD","tags":["FastAPI","DDD"]},"rawMarkdownBody":"## 기본 개념\n\nDDD는 복잡한 애플리케이션과 시스템의 개발에 있어서 비즈니스 요구사항을 중심으로 설계하고 구현하는 접근법입니다. 이 방식은 비즈니스 도메인의 복잡성을 이해하고, 그 복잡성을 모델링하여 소프트웨어에 반영하는 것에 중점을 둡니다.\n\n## 핵심 요소\n\n1. **Ubiquitous Language (모든 곳에서 사용되는 언어)**: 개발팀과 비즈니스 팀 간에 공통적으로 사용되는 언어. 이를 통해 불필요한 혼동을 줄이고, 의사소통의 효율을 높입니다.\n\n1. **Bounded Context (경계가 정의된 맥락)**: 특정한 맥락 내에서만 유효한 모델을 정의. 다른 맥락에서는 같은 용어도 다른 의미를 가질 수 있습니다.\n\n1. **Entities, Value Objects, Aggregates, Repositories**: 도메인 모델을 구성하는 주요 요소들입니다.\n\n1. **Domain Events**: 도메인 내에서 중요한 비즈니스 이벤트를 나타냅니다.\n\n## FastAPI에서 DDD의 기본 구조\n\n```bash\n├── api                               # 첫 번째 API 모듈\n│   ├── domain                        # 도메인 로직과 엔티티 정의\n│   │   ├── entities                  # 엔티티 정의 (비즈니스 모델)\n│   │   └── value_objects             # 값 객체 정의 (데이터 검증 및 전송 용도)\n│   ├── presentation                  # 애플리케이션의 사용자 인터페이스 레이어\n│   │   ├── dto                       # 데이터 전송 객체 정의 (Pydantic 모델)\n│   │   └── rest                      # REST 엔드포인트 정의\n│   │       └── controllers           # REST 컨트롤러 (FastAPI 경로 작업 처리)\n│   ├── repository                    # 영속성 관리를 위한 인터페이스 정의\n│   │   └── impl                      # 리포지토리 구현체 (영속성 레이어)\n│   │       └── sql                   # SQL 기반 리포지토리 구현체\n│   │           ├── models.py         # 데이터베이스 모델 (ORM 모델)\n│   │           ├── crud.py           # CRUD 연산 로직\n│   │           └── database.py       # 데이터베이스 세션 및 연결 관리\n│   └── service                       # 도메인 서비스 정의, 도메인 로직 구현\n│       └── impl                      # 서비스 구현체 (비즈니스 로직의 구현)\n├── api2                              # 두 번째 API 모듈\n│   └── ...                           # 같은 구조를 가져간다\n├── core                              # 공통 핵심 기능이나 공유 컴포넌트\n└── router                            # 애플리케이션의 라우팅 로직, URL 라우트 정의\n    └── routes                        # 개별 라우트 정의 파일\n```\n\nDDD의 원칙에 따라 애플리케이션을 조직화하면 각 도메인이 뚜렷하게 분리되고 관리하기 쉬워진다. 위에 제시된 디렉토리 구조는 DDD를 적용한 예시이다. \n\n도메인 레이어, 애플리케이션 레이어, 인프라 레이어로 분리되어 코드의 응집도를 높이고 결합도를 낮출 수 있다. 또한 각 레이어가 제공하는 인터페이스와 구현체를 분리함으로써 유지보수와 확장성에 유리한 구조를 만들 수 있다. \n\n### 도메인 레이어\n\n#### domain 디렉토리\n\nDDD에서 가장 중요한 부분으로 복잡한 비즈니스 로직과 애플리케이션 핵심 기능을 표현하는 모델을 담고 있다. \n\n- entities\n\n    - 비즈니스 도메인의 기본적인 데이터 단위로 고유한 식별자를 가지고 있다. \n- 비즈니스 규칙을 캡슐화하고 FastAPI 애플리케이션에서 데이터베이스 레코드와 1:1로 매핑되는 경우가 많다. \n- FastAPI는 이러한 엔티티에 직접적으로 연관되지능 않지만 ORM 도구를 사용하여 엔티티 클래스를 데이터 모델로 변환하고 데이터베이스와 상호작용을 할 수 있다. \n\n- value\\_objects\n\n    - 식별자를 가지지 않는 불변 객체로 엔티티의 속성을 표현한다. \n- FastAPI에서는 값 객체를 표현하기 위해 Pydantic의 기능을 활용할 수 있으며 요청 검증이나 응답 시리얼라이징에 유용하게 사용된다. \n\n### 애플리케이션 레이어(presentation, service)\n\n#### presentaion 디렉토리\n\n이 디렉토리는 사용자 인터페이스와 외부에 노출되는 API 엔드포인트의 정의를 담당한다. \n\n- DTO\n\n    - DTO는 클라이언트와 서버간에 데이터를 전달할 때 사용하는 객체이다.\n- FastAPI에서는 Pydantic 모델을 이용해 DTO를 정의한다. 이 모델들은 본문, 쿼리 파라미터, 경로 파라미터, 그리고 응답 모델을 정의하는데 사용된다.\n- DTO는 엔드포인트가 받아들이고 반환하는 데이터의 구조와 유효성 검증 규칙을 명시한다.\n\n- REST\n\n    - Restful 원칙을 따르는 API 엔드포인트를 정의한다. \n- FastAPI에서는 각 API 경로에 해당하는 함수를 작성하고 이를 HTTP 메서드에 매핑한다. \n- 각 함수는 Pydantic DTO를 매개변수로 사용하여 요청 데이터의 유효성을 검사하고 이를 도메인 모델로 변환하거나 서비스 레이어로 전달한다. \n\n- Controllers\n\n    - API 엔드포인트에 대한 요청을 처리하는 로직을 담당한다. \n- FastAPI에서는 주로 경로 작업 함수로 표현되며 경로 데코레이터를 사용하여 라우트를 설정한다. \n- 컨트롤러는 요청을 받아 DTO로 파싱하고 필요한 비즈니스 로직을 수행한 후 적절한 HTTP 응답을 반환한다.\n\n이 구성 요소들은 FastAP의 강점인 타입 힌트, 자동 문서화, 요청 검증 등을 활용하여 효율적인 API 인터페이스 레이어를 구성하는데 중요한 역할을 한다. \n\n#### service 디렉토리\n\n애플리케이션의 비즈니스 로직을 구현하는 부분을 담당한다. 컨트롤러와 데이터 액세스 계층 사이에서 중간 역할을 한다. 주요 목적은 비즈니스 규칙과 애플리케이션 로직을 캡슐화하여 재사용성을 높이고 애플리케이션의 다룬 부분과의 결합도를 낮추는 것이다. \n\n- Service Implementations (impl)\n\n    실제 로직을 구현한다. 이 구현체는 데이터베이스 호출, 외부 서비스 호출, 비즈니스 규칙 처리 등을 담당한다. \n\n### 인프라 레이어(repository)\n\n도메인 레이어와 애플리케이션 레이어의 기술적 세부 사항을 추상화하는 역할을 한다. 주로 데이터베이스, 메시징 시스템, 파일 시스템와 같은 외부 자원과의 통신을 담당한다. 애플리케이션의 나머지 부분이 기술적인 세부사항에 직접적으로 의존하지 않도록 추상화를 제공한다. \n\n"},{"excerpt":"단위 테스트는 소프트웨어 개발의 기본 구성 단위를 테스트하는 것을 말한다. 이 경우 기본 구성 단위는 보통 함수나 메소드, 작은 클래스 등이 될 수 있다. 단위 테스트의 주된 목적은 개별 구성 단위가 예상대로 동작하는지 확인하고, 코드의 특정 부분에 대한 논리적 오류를 식별하는 것이다.  특징과 목적 독립성 : 각 단위 테스트는 독립적으로 실행되어야 하며…","fields":{"slug":"/Unit-Test단위-테스트/"},"frontmatter":{"date":"October 17, 2023","title":"Unit Test(단위 테스트)","tags":["TDD"]},"rawMarkdownBody":"단위 테스트는 소프트웨어 개발의 기본 구성 단위를 테스트하는 것을 말한다. 이 경우 기본 구성 단위는 보통 함수나 메소드, 작은 클래스 등이 될 수 있다. 단위 테스트의 주된 목적은 개별 구성 단위가 예상대로 동작하는지 확인하고, 코드의 특정 부분에 대한 논리적 오류를 식별하는 것이다. \n\n### 특징과 목적\n\n1. 독립성 : 각 단위 테스트는 독립적으로 실행되어야 하며, 다른 테스트의 결과에 영향을 받지 않아야 한다. \n\n1. 작고 빠름 : 단위 테스트는 빠르게 실행되어야 하며, 코드의 작은 부분만을 테스트해야 한다. \n\n1. 예측 가능함 : 동일한 입력에 대해 항상 동일한 출력을 반환해야 한다. \n\n1. 자동화 가능 : 단위 테스트는 자동으로 실행되어야 하며 수동 테스트 없이도 결과를 제공해야 한다. \n\n1. 명확함 : 테스트의 결과는 명확하게 통과 또는 실패로 나타나야 하며 이유를 쉽게 파악할 수 있어야 한다. \n\n\n\n"},{"excerpt":"메소드는 Python의  모듈에 있는  데코레이터를 사용하여 클래스를 정의할 때 사용된다.   메소드는 객체가 초기화된 직후에 자동으로 호출된다. 즉,  메소드가 호출된 후에  메소드가 호출된다. 클래스의 인스턴스가 생성될 때  메소드가 호출되어 인스턴스 변수들을 초기화하고, 바로 이어서  메소드가 호출되어 추가적인 초기화 작업을 수행할 수 있다. 이러한…","fields":{"slug":"/__post_init__/"},"frontmatter":{"date":"October 17, 2023","title":"__post_init__","tags":["Python"]},"rawMarkdownBody":"`__post_init__` 메소드는 Python의 `dataclasses` 모듈에 있는 `dataclass` 데코레이터를 사용하여 클래스를 정의할 때 사용된다. \n\n`__post_init__` 메소드는 객체가 초기화된 직후에 자동으로 호출된다. 즉, `__init__` 메소드가 호출된 후에 `__post_init__` 메소드가 호출된다.\n\n클래스의 인스턴스가 생성될 때 `__init__` 메소드가 호출되어 인스턴스 변수들을 초기화하고, 바로 이어서 `__post_init__` 메소드가 호출되어 추가적인 초기화 작업을 수행할 수 있다. 이러한 방식으로, 객체의 초기화 과정 중에 추가적인 로직을 실행할 수 있다.\n\n```python\ndto = ContentListQueryDto(**payload)\n```\n\n`ContentListQueryDto` 클래스의 인스턴스를 생성하면서 `__init__` 메소드가 자동으로 호출되고, 이어서 `__post_init__` 메소드도 자동으로 호출된다. `__post_init__` 메소드는 `__init__` 메소드가 완료된 직후에 실행되므로, `__init__` 메소드에서 설정된 필드 값을 기반으로 추가적인 초기화 작업을 수행할 수 있다.\n\n이 경우에, `dto = ContentListQueryDto(**payload)` 코드를 실행하면 `__init__` 메소드가 호출되어 `payload` 딕셔너리의 키-값 쌍을 이용하여 인스턴스 변수들을 초기화하고, 이어서 `__post_init__` 메소드가 호출되어 해당 메소드에 정의된 작업들을 수행한다. \n\n\n\n"},{"excerpt":"dataclass는 [파이썬]3.7부터 표준 라이브러리의 일부로 도입된 데코레이터이다.\n주로 클래스를 정의할 때 반복적으로 필요한 특수 메서드들(, ,  등)을 자동으로 생성해주는 역할을 한다. 이를 통해 데이터를 저장하고 처리하는 클래스를 보다 간결하게 정의할 수 있다. 주요 특징 자동 생성된 생성자 자동 생성된 표현 메서드 자동 생성된 비교 메서드 불…","fields":{"slug":"/dataclass/"},"frontmatter":{"date":"October 17, 2023","title":"dataclass","tags":["Python"]},"rawMarkdownBody":"dataclass는 [[파이썬]]3.7부터 표준 라이브러리의 일부로 도입된 데코레이터이다.\n주로 클래스를 정의할 때 반복적으로 필요한 특수 메서드들(`__init__`, `__repr__`, `__eq__` 등)을 자동으로 생성해주는 역할을 한다. 이를 통해 데이터를 저장하고 처리하는 클래스를 보다 간결하게 정의할 수 있다.\n\n## 주요 특징\n\n1. 자동 생성된 생성자\n\n1. 자동 생성된 표현 메서드\n\n1. 자동 생성된 비교 메서드\n\n1. 불변성 옵션\n\n## 예제\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Point:\n    x: float\n    y: float\n\n\n```\n\n이 코드는 아래의 코드를 간결하게 표현한 것이다.\n\n```python\nclass Point:\n    def __init__(self, x: float, y: float):\n        self.x = x\n        self.y = y\n\n    def __repr__(self):\n        return f\"Point(x={self.x}, y={self.y})\"\n\n    def __eq__(self, other):\n        if not isinstance(other, Point):\n            return NotImplemented\n        return self.x == other.x and self.y == other.y\n\n\n```\n\n## 결론\n\ndataclass는 데이터 구조나 간단한 객체를 정의할 때 유용하게 사용된다. 그러나 복잡한 로직이나 특별한 초기화가 필요한 경우에는 일반 클래스를 사용하는 것이 더 나을 수 있다.\n\n\n\n"},{"excerpt":"개요 브랜치 관리 전략은 소프트웨어 개발에서 코드 베이스의 다양한 변경 사항을 조직하고 관리하기 위한 방법이나 원칙이다. 특히 버전관리 시스템을 사용하는 프로젝트에서 브랜치를 효과적으로 활용하면 여러 개발자나 팀이 동시에 다양한 기능, 버그 수정, 실험 등을 독립적으로 처리할 수 있다.  목적 코드의 안정성 유지 여러 개발자나 팀 간의 협업 촉진 배포와 …","fields":{"slug":"/브랜치-관리-전략/"},"frontmatter":{"date":"October 16, 2023","title":"브랜치 관리 전략","tags":["GitHub"]},"rawMarkdownBody":"### 개요\n\n브랜치 관리 전략은 소프트웨어 개발에서 코드 베이스의 다양한 변경 사항을 조직하고 관리하기 위한 방법이나 원칙이다. 특히 버전관리 시스템을 사용하는 프로젝트에서 브랜치를 효과적으로 활용하면 여러 개발자나 팀이 동시에 다양한 기능, 버그 수정, 실험 등을 독립적으로 처리할 수 있다. \n\n### 목적\n\n1. 코드의 안정성 유지\n\n1. 여러 개발자나 팀 간의 협업 촉진\n\n1. 배포와 통합을 위한 과정 단순화\n\n### 대표적인 전략\n\n#### Feature branching\n\n각 기능마다 별도의 브랜치를 만들어 개발한다. 기능이 완성되면 메인 브랜치에 병합한다. \n\n- 장점\n\n    각 기능이 독립적으로 개발되므로 다양한 기능과 버그 수정을 동시에 진행할 수 있다. \n\n    충돌이 발생할 경우 해당 기능의 브랜치에서만 처리하면 되므로 메인 브랜치는 상대적으로 안정적이다. \n\n- 단점\n\n    병합 시점에 복잡한 병합 충돌이 발생할 수 있다. \n\n    오랜 시간 분리된 브랜치에서 작업할 경우 메인 브랜치와의 동기화가 어려워질 수 있다. \n\n- 적합한 상황\n\n    개발자들이 독립적으로, 혹은 소규모 팀으로 작업을 진행하는 경우\n\n#### Gitflow\n\nmaster, develop, feture/, hotfix/ 등 여러 브랜치를 사용하여 작업의 단계와 종류를 구분한다. \n\n- 장점\n\n    명확한 브랜치 구조를 가지고 있어 프로젝트의 다양한 단계와 상태를 잘 구분한다. \n\n    핫픽스와 기능 개발을 동시에 진행할 수 있다. \n\n- 단점\n\n    브랜치 관리가 복잡하다. \n\n    많은 규칙과 워크 플로우를 따라야 하므로 초보자에게는 진입 장벽이 될 수 있다. \n\n- 적합한 상황\n\n    큰 규모의 프로젝트나, 여러 단계의 배포 과정을 가진 프로젝트에서 사용\n\n#### Github flow\n\n간단하게 master와 feature 브랜치만을 사용한다. 기능이 준비되면 master에 PR을 통해 병합 요청한다. \n\n- 장점\n\n    브랜치 관리가 간단하다. \n\n    지속적인 통합 및 배포 환경에 적합하다. \n\n- 단점\n\n    복잡한 프로젝트에서는 세부적인 브랜치 구분이 부족할 수 있다. \n\n- 적합한 상황\n\n    중소 규모의 프로젝트나 간단한 배포 과정을 가진 프로젝트에서 사용\n\n#### Trunk Based Deveolpment(TBD)\n\n모든 개발자가 하나의 trunk 혹은 main 브랜치에서 직접 작업한다. 기능 토글등을 활용하여 미완성 기능을 숨긴다. \n\n- 장점\n\n    브랜치 관리가 매우 간단하다. \n\n    지속적인 통합 및 배포 환경에 최적화되어 있다. \n\n- 단점\n\n    안정적인 CI/CD 파이프라인 및 테스트 환경이 필요하다. \n\n    기능 토글 등 추가적인 관리 도두가 필요할 수 있다. \n\n- 적합한 상황\n\n    빠른 개발 주기와 지속적인 배포를 원하는 프로젝트에서 사용\n\n    \n\n"},{"excerpt":"이 오류는 Python 클래스 또는 함수에서 기본값이 설정된 인자 뒤에 기본값이 설정되지 않은 인자가 위치할 때 발생한다. 예를 들어, 나는 다음과 같았다.   필드에 기본값 이 설정되어 있고, 이 필드 뒤에 기본값이 설정되지 않은 와  필드가 위치하고 있다. 해결 방법 이 문제가 발생한 이유는 기본값이 설정된 매개변수 뒤에 기본값이 설정되지 않은 매개변…","fields":{"slug":"/TypeError-non-default-argument-content-follows-default-argument/"},"frontmatter":{"date":"October 10, 2023","title":"TypeError: non-default argument 'content' follows default argument","tags":["Python"]},"rawMarkdownBody":"`TypeError: non-default argument 'content' follows default argument`\n\n\n\n이 오류는 Python 클래스 또는 함수에서 기본값이 설정된 인자 뒤에 기본값이 설정되지 않은 인자가 위치할 때 발생한다. 예를 들어, 나는 다음과 같았다. \n\n```python\n@dataclass\nclass CsDetailQueryReturnDto:\n    ...\n    occurrence_datetime: Optional[str] = None\n    ...\n    content: str\n    cs_type: CsType\n    ...\n```\n\n`occurrence_datetime` 필드에 기본값 `None`이 설정되어 있고, 이 필드 뒤에 기본값이 설정되지 않은 `content`와 `cs_type` 필드가 위치하고 있다.\n\n### 해결 방법\n\n이 문제가 발생한 이유는 기본값이 설정된 매개변수 뒤에 기본값이 설정되지 않은 매개변수가 올 수 없다는 규칙 때문이다. \n\n함수를 호출할 때, Python은 매개변수를 위치나 키워드로 판별할 수 있어야 한다. 만약 기본값이 없는 매개변수가 기본값이 있는 매개변수 뒤에 온다면, Python은 어떤 매개변수에 값을 할당해야 하는지 혼동할 수 있다.\n\n\n\n"},{"excerpt":"1. 정의 AWS에서 제공하는 완전 관리형 NoSQL 데이터베이스 서비스이다. 높은 트래픽의 웹 스케일 애플리케이션에 적합한 특성을 가지며, 밀리초 단위의 지연 시간으로 대량의 데이터를 처리할 수 있다.\n확장 가능하고 높은 성능의 NoSQL 데이터베이스로 다양한 웹 기반 애플리케이션과 서비스에서 데이터 관리를 위한 핵심 선택지이다. AWS의 다른 서비스와…","fields":{"slug":"/Dynamo-DB/"},"frontmatter":{"date":"September 20, 2023","title":"Dynamo DB","tags":["DataBase","AWS"]},"rawMarkdownBody":"## 1. 정의\n\nAWS에서 제공하는 완전 관리형 NoSQL 데이터베이스 서비스이다. 높은 트래픽의 웹 스케일 애플리케이션에 적합한 특성을 가지며, 밀리초 단위의 지연 시간으로 대량의 데이터를 처리할 수 있다.\n확장 가능하고 높은 성능의 NoSQL 데이터베이스로 다양한 웹 기반 애플리케이션과 서비스에서 데이터 관리를 위한 핵심 선택지이다. AWS의 다른 서비스와 통합이 잘 되어 있어 복잡한 애플리케이션 구축에 적합하다.\n\n## 2. 주요 특징\n\n### 성능\n\n밀리초 미만의 응답 시간을 제공한다.\n\n### 확장성\n\n사용자의 요구에 따라 자동으로 확장 및 축소 된다.\n\n### 내구성 및 가용성\n\n자동으로 여러 AWS 리전 및 가용 영역에 데이터를 복제하여 데이터 손실과 서비스 중단 없이 운영된다.\n\n### 완전관리형\n\n서버 인프라스트럭쳐 또는 관리 작업 없이 실행된다.\n\n### 이벤트 소싱\n\nDynamoDB Streams를 통해 테이블의 데이터 변경을 캡쳐할 수 있다.\n\n### 글로벌 테이블\n\n여러 AWS 리전에서 동일한 테이블에 액세스 할 수 있다.\n\n### 3. 데이터 모델\n\n- 테이블 : 데이터의 컬렉션. RDBMS와 유사하지만 스키마가 고정되어있지 않다.\n\n- 항목 : 테이블 내의 개별 데이터 항목. RDBMS의 Row와 유사하다.\n\n- 속성 : 항목의 데이터 요소. RDBMS의 칼럼과 유사하다.\n\n### 4. 주요 기능\n\n- 초당 읽기/쓰기 용량 조절 : 요구에 따라 자동으로 조정된다.\n\n- 보조 인덱스 : 다양한 쿼리 옵션을 제공한다.\n\n- 트랜잭션 : 여러 항목 및 테이블에 걸친 ACID 트랜잭션을 지원한다.\n\n- 보안 : IAM을 통해 데이터 액세스 및 암호화를 제어한다.\n\n### 5. 사용 사례\n\n- 웹 애플리케이션 : 세션 데이터, 사용자 프로필, 소셜 그래프 등의 저장\n\n- 모바일 애플리케이션 : 빠른 액세스와 확장성이 필요한 데이터를 저장\n\n- IoT 애플리케이션 : 대량의 장치 데이터를 실시간으로 수집 및 분석\n\n- 게임 : 게임 상태, 플레이어 프로필, 리더보드 등을 저장\n\n\n\n"},{"excerpt":"1. 등장 배경 RDBMS는 정해진 스키마와 트랜잭션의 ACID 속성을 기반으로 설계되었다. 하지만 웹 2.0 시대가 도래하면서 대용량 데이터와 급격한 트래픽 증가, 다양한 데이터 구조의 요구가 생기면서 RDBMS의 한계가 드러나 NoSQL이 등장하게 되었다. 2. RDBMS와의 주요 차이점 스키마 RDBMS는 고정된 스키마를 가지고 있으나, NoSQL은…","fields":{"slug":"/NoSQL/"},"frontmatter":{"date":"September 20, 2023","title":"NoSQL","tags":["DataBase"]},"rawMarkdownBody":"## 1. 등장 배경\n\nRDBMS는 정해진 스키마와 트랜잭션의 ACID 속성을 기반으로 설계되었다. 하지만 웹 2.0 시대가 도래하면서 대용량 데이터와 급격한 트래픽 증가, 다양한 데이터 구조의 요구가 생기면서 RDBMS의 한계가 드러나 NoSQL이 등장하게 되었다.\n\n## 2. RDBMS와의 주요 차이점\n\n### 스키마\n\nRDBMS는 고정된 스키마를 가지고 있으나, NoSQL은 동적 스키마를 가진다.\n\n### 확장성\n\nRDBMS는 수직 확장을 위해 설계 되었지만 NoSQL은 수평 확장이 가능하다.\n\n### 데이터 모델\n\nRDBMS는 테이블 기반의 데이터 모델을 사용하지만 NoSQL은 키-값, 문서, 컬럼, 그래프 등 다양한 데이터 모델을 사용한다.\n\n### 트랜잭션\n\nRDMBS는 ACID 트랜잭션을 지원하지만 NoSQL은 일반적으로 이를 지원하지 않거나 부분적으로 지원한다.\n\n## 3. NoSQL의 일관성과 무결성\n\n### 일관성(Consistency)\n\nRDBMS에서는 ACID 트랜잭션 속성을 지원하여 강력한 일관성을 보장한다.\n반면, 많은 NoSQL 데이터베이스는 BASE 모델을 채용한다. 이는 즉각적인 일관성보다는 가용성과 복원력을 우선시하며 일시적인 불일관성을 허용하지만 결국 일관된 상태로 수렴한다는 의미이다.\n일부 NoSQL 시스템은 일관성 수준을 조정할 수 있도록 설정할 수 있다.\n\n### 무결성(Intergirty)\n\nRDBMS는 외래 키 제약 조건, 유니크 제약 조건 등을 통해 데이터 무결성을 보장한다.\nNoSQL 데이터베이스는 스키마가 유연하므로 같은 강도의 무결성 제약 조건을 갖지 않을 수 있다. 그러나 일부 NoSQL 시스템은 도큐먼트의 유효성 검사나 스키마 유효성 검사를 지원하여 무결성을 일정 수준 보장하기도 한다.\n\n### 정리\n\nNoSQL 데이터베이스는 일관성과 무결성을 \"없애는\" 것이 아니라 다양한 요구 사항과 환경에 맞게 그 특성과 규칙을 조정하고 최적화 한 것이다. 따라서 특정 응용 프로그램의 요구 사항에 따라 적절한 데이터베이스와 설정을 선택하는 것이 중요하다.\n\n## 4. 주요 특징 및 장점\n\n### 유연성\n\n다양한 데이터 및 동적 스키마를 지원함으로써 다양한 데이터 구조에 유연하게 대응할 수 있다.\n\n### 확장성\n\n수평 확장을 통해 대량의 데이터와 트래픽을 처리할 수 있다. NoSQL은 데이터를 여러 노드에 복제하거나 분할하는 매커니즘을 내장하고 있다. 이는 고가용성과 분산 처리를 위한 핵심 요소로 수평 확장을 자연스럽게 지원한다.\n\n### 빠른 성능\n\n간단한 쿼리와 특정 작업에 최적화된 데이터베이스 설계로 빠른 성능을 제공한다.\n\n### 다양한 데이터 처리\n\n구조화, 반구조화, 비구조화 데이터를 모두 처리할 수 있다.\n\n## 5. 단점\n\n### 표준 부재\n\n다양한 NoSQL 데이터베이스 중 특정 표준이 없어 학습 곡선이 높을 수 있다.\n\n### 일관성\n\nACID 트랜잭션의 일부를 희생하므로 데이터 일관성 문제가 발생할 수 있다.\n\n## 6. NoSQL이 필요한 경우\n\n### 빠른 개발\n\n동적 스키마로 개발과 변화에 빠르게 대응할 수 있다.\n\n### 대량의 데이터 처리\n\n수평 확장을 통해 대량의 데이터를 처리하는데 적합하다.\n\n### 다양한 데이터 구조\n\n구조화 된 데이터 뿐 아니라 로그, 소셜 미디어 포스트 등 다양한 데이터를 저장하고 처리할 때 유용하다.\n\n"},{"excerpt":"1. 정의 REST는 웹서비스를 설계하기 위한 아키텍쳐 스타일이다. 웹의 기본 프로토콜인 HTTP를 최대한 활용하기 위한 일련의 제약사항 및 원칙을 포함한다. 2. 기본 원칙 Stateless 각 요청은 모든 정보를 가지고 있어야 한다. 즉, 서버는 클라이언트의 상태 정보를 저장하면 안된다. 클라이언트-서버 구조 클라이언트와 서버가 별도로 존재하고 각각의…","fields":{"slug":"/REST/"},"frontmatter":{"date":"September 20, 2023","title":"REST","tags":["BackEnd"]},"rawMarkdownBody":"## 1. 정의\n\nREST는 웹서비스를 설계하기 위한 아키텍쳐 스타일이다. 웹의 기본 프로토콜인 HTTP를 최대한 활용하기 위한 일련의 제약사항 및 원칙을 포함한다.\n\n## 2. 기본 원칙\n\n### Stateless\n\n각 요청은 모든 정보를 가지고 있어야 한다. 즉, 서버는 클라이언트의 상태 정보를 저장하면 안된다.\n\n### 클라이언트-서버 구조\n\n클라이언트와 서버가 별도로 존재하고 각각의 역할이 분리되어 있어야 한다.\n\n### 캐시 가능\n\n클라이언트에서 캐시를 사용할 수 있어야 하며, 응답이 캐시 가능한지 아닌지를 명시해야 한다.\n\n### 계층화된 시스템 (Layered System)\n\n클라이언트는 중간 계층(서버)을 알 필요 없이, 끝단 서버에만 접속하면 된다.\n\n### 코드 온 디맨드 (optional)\n\n서버는 코드를 클라이언트에게 보낼 수 있으며, 클라이언트는 그 코드를 실행할 수 있다.\n\n### 인터페이스 일관성 (Uniform Interface)\n\n아키텍쳐의 규칙을 정의하며, 이를 통해 시스템의 분리와 통합이 용이해진다.\n\n- 자원의 식별 : 각 리소스는 URI로 식별되어야 한다.\n\n- 메시지를 통한 자원 조작 : 리소스는 HTTP Method를 통해 조작된다.\n\n- 자기 설명적 메시지(Self-descriptive message): 각 메시지는 자신을 어떻게 처리해야 하는지에 대한 충분한 정보를 포함해야 한다.\n\n- HATEOAS(Hypermedia as the Engine of Application State): 응답에 다음 가능한 액션에 대한 정보도 포함되어야 한다.\n\n## 3. HTTP 메서드\n\n### GET\n\n리소스를 조회하는데 사용된다.\n\n### POST\n\n리소스를 생성한다.\n\n### PUT\n\n리소스를 수정한다.\n\n### DELETE\n\n리소스를 삭제한다.\n\n### PATCH\n\n리소스의 일부를 수정한다.\n\n## 4. 상태코드 (Status Code)\n\n### 2xx (Successful)\n\n요청이 성공적으로 수행됨\n\n### 3xx (Redirection)\n\n요청을 완료하기 위해 추가 동작이 필요함\n\n### 4xx (Client Error)\n\n클라이언트의 잘못된 요청으로 인한 오류\n\n### 5xx (Server Error)\n\n서버 오류\n\n## 5. URI 설계\n\n- 리소스를 명시하며, 명사를 사용해야 한다. (ex) `/users` , `/products`\n\n- 리소스의 계층 관계를 나타날 때는 `/`를 사용한다. (ex) `/users/123/orders`\n\n## 6. 미디어 타입\n\n- JSON, XML등 다양한 미디어 타입을 사용하여 데이터를 표현한다.\n\n- `Accept` 헤더를 통해 클라이언트가 원하는 미디어 타입을 서버에게 알릴 수 있다.\n\n\n\n"},{"excerpt":"AWS API Gateway는 개발자가 간편하게 RESTful API와 WebSocket API를 생성, 배포, 유지 관리 할 수 있도록 지원하는 AWS의 완전 관리형 서비스이다. 클라이언트와 백엔드 서비스 간의 호출을 중개한다. 1. 주요 기능 API 생성 및 배포 빠르게 API를 생성하고 배포할 수 있다. 호출 중개 클라이언트와 백엔드 서비스 간의 호…","fields":{"slug":"/AWS-API-Gateway/"},"frontmatter":{"date":"September 19, 2023","title":"AWS API Gateway","tags":["BackEnd","AWS"]},"rawMarkdownBody":"AWS API Gateway는 개발자가 간편하게 RESTful API와 WebSocket API를 생성, 배포, 유지 관리 할 수 있도록 지원하는 AWS의 완전 관리형 서비스이다. 클라이언트와 백엔드 서비스 간의 호출을 중개한다.\n\n## 1. 주요 기능\n\n### API 생성 및 배포\n\n빠르게 API를 생성하고 배포할 수 있다.\n\n### 호출 중개\n\n클라이언트와 백엔드 서비스 간의 호출을 중개한다.\n\n### 요청 및 응답 변환\n\n데이터 매핑 템플릿을 사용하여 호출 중 요청 및 응답 데이터를 변환할 수 있다.\n\n### 액세스 제어\n\nAWS Identity and Access Management (IAM) 또는 AWS Lambda 권한 기반 접근 제어를 사용하여 API 호출을 보호할 수 있다.\n\n### API 키 및 요금 제한\n\nAPI키를 생성하고 배포하고 사용량을 추적하여 API 호출에 요금을 부과할 수 있다.\n\n### API 캐싱\n\nAPI 호출의 성능을 향상시키기 위해 캐시를 사용할 수 있다.\n\n### API 버전 관리\n\nAPI의 다양한 버전 및 스테이지를 관리할 수 있다.\n\n## 2. 통합\n\n### AWS 서비스 통합\n\nAWS Lambda, Dynamo DB, Amazone S3등의 AWS 서비스와 손쉽게 통합할 수 있다.\n\n### 모니터 및 로깅\n\nAmazone CloudWatch와 통합하여 API 호출 및 오류 데이터를 모니터링 및 로깅할 수 있다.\n\n## 3. 보안\n\n### SSL/TLS 인증서\n\n커스텀 도메인 이름에 대한 SSL/TLS 인증서를 제공하고 관리한다.\n\n### 사용자 인증 및 인가\n\nAmazone Cognito와 통합하여 사용자 인증 및 인가를 수행할 수 있다. 또한, OAuth토큰을 사용하여 API 엑세스를 제어할 수 있다.\n\n## 4. 비용\n\n사용한 만큼의 비용만 지불한다. 요청 수 및 데이터 전송량에 따라 비용이 발생한다.\n\n## 5. 사용 사례\n\n### Serverless\n\nAWS Lambda와 결합하여 서버리스 애플레키션의 백엔드를 구축할 수 있다.\n\n### 모바일 및 웹 애플리케이션\n\n다양한 클라이언트에서 사용될 RESTful API를 제공한다.\n\n### 웹사이트의 서드파티 애플리케이션 통합\n\n다른 웹 서비스나 데이터 소스와의 통합을 위한 API를 제공한다.\n\n\n\n"},{"excerpt":"먼저 ColdStart를 이해하기 위해서는 Serverless의 동작방식을 이해할 필요가 있다. 간단히 알아보자.\nServerless 컴퓨팅에서 사용자의 요청이 오면 해당 요청을 처리하는 함수(Function)가 실행된다. 이 함수는 이벤트에 응답하여 실행되는데, 이 함수는 항상 실행 상태를 유지하고 있지 않다. 대신에 요청이 들어오면 새롭게 시작되거나 …","fields":{"slug":"/ColdStart/"},"frontmatter":{"date":"September 19, 2023","title":"ColdStart","tags":["AWS","BackEnd"]},"rawMarkdownBody":"## 먼저\n\nColdStart를 이해하기 위해서는 Serverless의 동작방식을 이해할 필요가 있다. 간단히 알아보자.\nServerless 컴퓨팅에서 사용자의 요청이 오면 해당 요청을 처리하는 함수(Function)가 실행된다. 이 함수는 이벤트에 응답하여 실행되는데, 이 함수는 항상 실행 상태를 유지하고 있지 않다. 대신에 요청이 들어오면 새롭게 시작되거나 재사용된다.\n\n이제 콜드 스타트의 개념에 대해서 알아볼 수 있다.\n\n## 콜드 스타트와 웜 스타트\n\n### 콜드 스타트\n\n함수가 처음 호출될 때, 아직 사용 가능한 인스턴스가 없거나 지나치게 오래된 인스턴스에 대한 요청이 들어왔을 때 함수 인스턴스가 새롭게 생성된다. 이런 상태에서 함수가 실행될 때를 '콜드 스타트' 라고 한다.\n콜드 스타트는 초기화에 필요한 시간과 리소스를 요구하기 때문에 함수의 실행 시작까지의 대기 시간이 길어질 수 있다. 이로 인한 지연은 사용자 경험에 영향을 줄 수 있다.\n\n### 웜 스타트\n\n이미 실행된 함수 인스턴스가 재사용될 때 발생한다. 초기화 시간이 필요 없기 때문에 대기 시간이 짧다.\n\n## 지연 최소화를 위한 전략\n\n### 최적의 함수 크기 할당\n\n메모리와 CPU를 적절히 할당하여 함수의 시작 시간을 줄일 수 있다.\n\n### 초기화 코드 최적화\n\n함수 내에서 필요한 라이브러리나 자원을 로드하는 초기화 코드를 최적화 하여 실행 시간을 줄인다.\n\n### 외부 연결 최소화\n\n외부 시스템이나 데이터베이스에 대한 연결을 미리 확립하거나 재사용하는 방식을 고려한다.\n\n### 함수 미리 실행\n\n일정 주리고 함수를 호출하여 웜 상태를 유지한다.\n\n서버리스 제공 업체에 따라 콜드 스타트의 영향도와 대응 전략이 다를 수 있다.\n\n"},{"excerpt":"서버가 \"없는\" 것이 아니다.\n서버리스는 개발자 또는 운영 팀이 서버의 운영, 관리, 확장 등에 대한 걱정 없이 어플리케이션 코드에만 집중할 수 있게 해주는 컴퓨팅 모델을 의미한다. 특징 자동확장 사용량에 따라 자동으로 리소스를 확장하거나 축소한다. 따라서 트래픽이 급증하더라도 시스템이 알아서 처리한다. 이벤트 주도 대부분의 서버리스 어플리케이션은 이벤트…","fields":{"slug":"/Serverless/"},"frontmatter":{"date":"September 19, 2023","title":"Serverless","tags":["BackEnd"]},"rawMarkdownBody":"서버가 \"없는\" 것이 아니다.\n서버리스는 개발자 또는 운영 팀이 서버의 운영, 관리, 확장 등에 대한 걱정 없이 어플리케이션 코드에만 집중할 수 있게 해주는 컴퓨팅 모델을 의미한다.\n\n## 특징\n\n### 자동확장\n\n사용량에 따라 자동으로 리소스를 확장하거나 축소한다. 따라서 트래픽이 급증하더라도 시스템이 알아서 처리한다.\n\n### 이벤트 주도\n\n대부분의 서버리스 어플리케이션은 이벤트에 응답하여 동작한다.\n\n### 비용 효율\n\n실제로 사용된 컴퓨팅 리소스만큼만 비용을 지불한다. 즉, 코드가 실행되지 않을 때에는 비용이 발생하지 않는다.\n\n### 서버 관리 불필요\n\n인프라의 설정, 관리, 유지보수와 같은 작업들이 서버리스 플랫폼 제공업체에 의해 자동으로 처리된다.\n\n- 대표적인 서버리스 서비스로는 AWS Lambda, Google Cloud Function, Azure Function 등이 있다.\n\n## 단점\n\n### ColdStart 문제\n\n### 실행 시간 제한\n\n대부분의 서버리스 플랫폼은 함수의 최대 실행 시간에 제한을 두고 있다.\n\n### 상태 관리\n\n서버리스 함수는 상태가 없기 때문에 외부 저장소나 캐싱 시스템을 통해 상태를 관리해야 한다.\n\n### 종속성 및 복잡성\n\n여러 서버리스 컴포넌트와 서비스의 상호작용은 의존성과 복잡성을 증가시킬 수 있다.\n\n### 보안 및 권한 관리\n\n특히 여러 서비스와 컴포넌트가 연동되는 환경에서 권한 설정과 보안 관리가 복잡해질 수 있다.\n\n### 로컬 개발 환경 설정 어려움\n\n"},{"excerpt":"지난 시간 지난시간에는 DataBase를 불러오는 비동기 로직 때문에 Factory Method를 사용해서 노션의 DataBase를 불러오는 코드를 작성했다.  기능만 점검 후, 전체적인 설계를 다시 하고 프로젝트의 디렉토리 구조도 다시 짰다.  새로운 설계 일단, 크게 네 가지의 클래스를 만들기로 했다.  DataBase 클래스 생성시에 날짜를 입력 받…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅2/"},"frontmatter":{"date":"September 04, 2023","title":"NotionAPI를 활용한 자동 포스팅(2)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"## 지난 시간\n\n지난시간에는 DataBase를 불러오는 비동기 로직 때문에 Factory Method를 사용해서 노션의 DataBase를 불러오는 코드를 작성했다. \n\n기능만 점검 후, 전체적인 설계를 다시 하고 프로젝트의 디렉토리 구조도 다시 짰다. \n\n## 새로운 설계\n\n일단, 크게 네 가지의 클래스를 만들기로 했다. \n\n### DataBase 클래스\n\n- 생성시에 날짜를 입력 받아 파라미터마다 다른 조건으로 쿼리 하여 노션 API에서 결과값을 받아오는 필터링 기능을 가진다. \n\n### Page 클래스\n\n- Page 타입의 Block이다. \n\n- 해당 페이지를 마크다운으로 저장(Print)하는 메소드를 가지고 있다. \n\n- Property들을 가지고 있다. (페이지의 속성)\n\n### Content 클래스\n\n- Page에 귀속된다. \n\n- 종류가 여러가지이다. h1, h2, h3.. 등등 직접적으로 포스팅의 내용이 될 블록이다.\n\n- 가장 상위의 Content는 부모가 Page이다. \n\n- 자기 자신을 마크다운으로 변환하여 부모에게 리턴하는 method를 가지고 있다. 만약 child가 있다면, 해당 child도 함께 변환하여 부모에게 리턴한다. \n\n- 일단 기본적으로 다음의 속성을 가진다. \n\n    - **id** : idx\n\n    - **parent** : 부모 ID\n\n    - **hasChidren** : 자식 요소가 있는지 여부 (true, false)\n\n    - **type** : `heading_1` 등등 블록의 종류. 종류마다 마크다운 변환 전략이 달라야 하므로 중요하다. \n\n    - **is_toggleable** : **** 토글 여부\n\n### Posting 클래스\n\n- 생성자에 날짜를 넣어서 생성하면 해당하는 DataBase를 조회하고, Page → Content 를 돌면서 마크다운으로 페이지를 변환하여 저장하는 클래스들의 메소드를 모두 여기서 실행한다. 메인함수 같은 개념이다. \n\n위 내용은 언제든지 달라질 수 있다. 실제로 매일매일 하루하루 숨 쉬듯이 달라지고 있다. \n\n## 디렉토리 구조\n\nsrc/models 디렉토리를 새로 만들고 여기에 모든 클래스, 비즈니스 로직을 넣기로 했다. \n\n## database.ts\n\n일단 이번에는 database.ts를 1차적으로 완성했다. \n\n```typescript\nimport { Client } from \"@notionhq/client\";\nimport { QueryDatabaseResponse } from \"@notionhq/client/build/src/api-endpoints\";\nimport * as dotenv from \"dotenv\";\n\n\nexport class DataBase {\n    private notion: Client;\n    private databaseId: string;\n    public database: QueryDatabaseResponse;\n\n    public pageIds: { pageId: string }[] = [];\n\n    private constructor(notion: Client, databaseId: string) {\n        this.notion = notion;\n        this.databaseId = databaseId;\n        this.database = {} as QueryDatabaseResponse;\n    }\n\n    public static async create(filterUdate?: string): Promise<DataBase> {\n        dotenv.config({ path: `${__dirname}/../../.env` });\n        const notionkey: string = process.env.NOTION_KEY || \"\";\n        const databaseid: string = process.env.NOTION_DATABASE_ID || \"\";\n\n        if (!notionkey || !databaseid) {\n            throw new Error(\"NOTION_KEY or NOTION_DATABASE_ID is missing in the environment variables.\");\n        }\n\n        const notion = new Client({ auth: notionkey });\n        const instance = new DataBase(notion, databaseid);\n        if (filterUdate == \"lastest\") {\n            const today = new Date(); // 현재 날짜와 시간을 가져옵니다.\n            today.setDate(today.getDate() - 1); // 날짜를 하루 전으로 설정합니다.\n\n            // YYYY-MM-DD 형식의 문자열로 날짜를 가져옵니다.\n            filterUdate = `${today.getFullYear()}-${String(today.getMonth() + 1).padStart(2, '0')}-${String(today.getDate()).padStart(2, '0')}`;\n        }\n        instance.database = await instance.queryDatabase(filterUdate);\n        return instance;\n    }\n\n    public async queryDatabase(filterUdate?: string): Promise<QueryDatabaseResponse> {\n        try {\n            const response = await this.notion.databases.query({\n                database_id: this.databaseId,\n                filter: {\n                    and: [\n                        {\n                            property: '상태',\n                            select: {\n                                equals: \"POST\",\n                            },\n                        },\n                        ...(filterUdate ? [{\n                            property: 'update',\n                            date: { on_or_after: filterUdate }\n                        }] : []),\n                    ]\n                },\n                sorts: [\n                    {\n                        property: 'update',\n                        direction: 'descending',\n                    },\n                ],\n            });\n            // pageId 리스트 업데이트\n            this.pageIds = response.results.map(page => ({ pageId: page.id }));\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n}\n```\n\n전에 제작한 클래스를 수정하여 만들었으며, 필터 기능을 추가하였다. \n\nNotion 서버에는 반드시 세계공통시? 로 저장이 되는 문제가 있다. 서울 시간대가 달라서 필터에도 현재 시간을 변환하여 조회하는 로직을 짜야 할 것 같다. \n\n`pageIds` 속성으로 DataBase에 저장된 페이지들의 ID를 받을 수 있다. \n\n## 코드 작성시 고려한 점\n\n### DataBase 클래스가 Page 인스턴스 리스트 관리 책임을 가진다면?\n\n- SRP(Single Responsibility Principle) 원칙에 의거해 하나의 클래스는 하나의 책임만을 져야 한다. DataBase 클래스가 Page 인스턴스 리스트 관리 책임도 가진다면 이 원칙에 어긋날 수 있다. \n\n- 두 클래스가 강한 연결성을 가지게 되어 유연성이 떨어질 수 있다. \n\n- 코드가 더 직관적으로 보일 수 있다. (추적이 쉽다)\n\n결론적으로, DataBase 클래스와 Page 클래스의 연결을 줄이고(유연성을 늘리고) SRP를 준수하기 위해 현재의 방식을 선택했다. \n\n### `databaseId` , `notionkey` 를 생성자의 파라미터로 전달한다면? \n\n- 해당 값을 생성자의 파라미터로 값을 전달할 경우 코드가 길어지는 단점이 있다. 또한, 다양한 notionkey나 databaseid로 클래스를 생성할 수 있다. \n\n- 하지만, 해당 프로젝트에서 Database는 여러개를 생성하거나 값을 바꿔가면서 클래스를 생성할 일이 없으므로 현재의 방법을 택했다. \n\n- 다만 지금처럼 할 경우 클래스 내부에서 외부 환경 변수에 직접 접근하므로 캡슐화 원칙에 어긋나기는 한다. 하지만 앞서 언급한대로 해당 변수가 바뀔일은 거의 없다고 여겨지므로 지금의 방법을 택했다. \n\n## 추가 수정\n\n### 고려사항\n\n- DataBase는 여러개일 필요가 없으므로, Singleton 패턴을 고려한다.\n\n- `DataBase` 클래스는 Notion과의 통신 뿐만 아니라, 환경 변수의 로딩 및 데이터베이스 ID 및 키의 유효성 검사까지 담당하고 있다. 이러한 기능들을 분리하여 각각의 책임을 명확히 하는 것이 좋다. \n\n- Typings: 현재 코드에서는 TypeScript를 사용하고 있다. 가능한 한 모든 변수, 함수 매개변수 및 반환 타입에 타입 주석을 추가하는 것이 좋다.\n\n## 완성코드\n\n위의 문제를 고려해 Notion과의 통신을 NotionAPI 클래스로 분리했다. Page나 Block 같은 다른 클래스에서도 동일한 Notion Client를 사용해야 하므로, 이는 아주 타당한 선택이었다. 또 하나의 프로젝트에 API는 유일하므로 싱글톤 패턴을 사용했다. 다음은 이를 위해 만들어진 notionapi.ts 코드이다. \n\n### notionapi.ts\n\n```typescript\nimport { Client } from \"@notionhq/client\";\n\nexport class NotionAPI {\n    private static instance: NotionAPI | null = null;\n    public client: Client;\n\n    private constructor(notionKey: string) {\n        this.client = new Client({ auth: notionKey });\n    }\n\n    public static async create(notionKey: string = \"\") {\n        if (!this.instance) {\n            if (!notionKey) {\n                throw new Error(\"NOTION_KE is missing\");\n            }\n            this.instance = new NotionAPI(notionKey);\n        }\n        return this.instance;\n    }\n}\n```\n\n최초 클래스 생성시에만 notionKey가 필요하도록 해두었다. 나중에 이 클래스를 호출할 때에는 번거롭게 환경변수를 조회할 필요가 없다. \n\n### database.ts\n\n```typescript\nimport { Client } from \"@notionhq/client\";\nimport { QueryDatabaseResponse } from \"@notionhq/client/build/src/api-endpoints\";\nimport * as dotenv from \"dotenv\";\nimport { NotionAPI } from \"./notionapi\";\n\nexport class DataBase {\n    private static instance: DataBase | null = null;\n\n    private notion: Client;\n    private databaseId: string;\n    public database: QueryDatabaseResponse;\n\n    public pageIds: { pageId: string }[] = [];\n\n    private constructor(notion: Client, databaseId: string) {\n        this.notion = notion;\n        this.databaseId = databaseId;\n        this.database = {} as QueryDatabaseResponse;\n    }\n\n    public static async create(filterUdate?: string): Promise<DataBase> {\n        if (!this.instance) {\n            dotenv.config({ path: `${__dirname}/../../.env` });\n            const notionkey: string = process.env.NOTION_KEY || \"\";\n            const databaseid: string = process.env.NOTION_DATABASE_ID || \"\";\n\n            if (!notionkey || !databaseid) {\n                throw new Error(\"NOTION_KEY or NOTION_DATABASE_ID is missing in the environment variables.\");\n            }\n\n            const notionApi: NotionAPI = await NotionAPI.create(notionkey);\n\n            this.instance = new DataBase(notionApi.client, databaseid);\n            if (filterUdate === \"lastest\") {\n                const today: Date = new Date();\n                today.setDate(today.getDate() - 1);\n                filterUdate = `${today.getFullYear()}-${String(today.getMonth() + 1).padStart(2, '0')}-${String(today.getDate()).padStart(2, '0')}`;\n            }\n            this.instance.database = await this.instance.queryDatabase(filterUdate);\n        }\n        return this.instance;\n    }\n\n    public async queryDatabase(filterUdate?: string): Promise<QueryDatabaseResponse> {\n        try {\n            const response: QueryDatabaseResponse = await this.notion.databases.query({\n                database_id: this.databaseId,\n                filter: {\n                    and: [\n                        {\n                            property: '상태',\n                            select: {\n                                equals: \"POST\",\n                            },\n                        },\n                        ...(filterUdate ? [{\n                            property: 'update',\n                            date: { on_or_after: filterUdate }\n                        }] : []),\n                    ]\n                },\n                sorts: [\n                    {\n                        property: 'update',\n                        direction: 'descending',\n                    },\n                ],\n            });\n            // pageId 리스트 업데이트\n            this.pageIds = response.results.map(page => ({ pageId: page.id }));\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n}\n```\n\n타입 주석을 모두 추가해주었고, 싱글톤 패턴으로 변경되었다. Notion Client의 생성부가 다른 곳으로 분리되었다. \n\n환경변수 역시 분리하는것을 고려중이지만, \n\n일단은 database.ts를 이 정도로 완성 하려고 한다. \n\ndatabase.ts 완성!\n\n[https://github.com/Sharknia/Notion-to-Markdown/tree/database-class-complete](https://github.com/Sharknia/Notion-to-Markdown/tree/database-class-complete)\n\n"},{"excerpt":"팩토리 패턴은 객체 생성에 관련된 로직을 클래스 내부에 포함시키는 대신 별도의 클래스나 메소드에 위임하는 패턴이다.  개요 팩토리 패턴은 객체를 생성하는 코드와 객체의 사용 코드를 분리하는 역할을 한다.  객체를 생성하는 로직을 별도의 팩토리 클래스나 팩토리 메소드에 위임하여 객체 생성을 캡슐화 한다.  특징 유연성 : 객체 생성 과정에 변화가 생겼을 때…","fields":{"slug":"/Factory-Pattern/"},"frontmatter":{"date":"September 03, 2023","title":"Factory Pattern","tags":["DesignPattern"]},"rawMarkdownBody":"팩토리 패턴은 객체 생성에 관련된 로직을 클래스 내부에 포함시키는 대신 별도의 클래스나 메소드에 위임하는 패턴이다. \n\n### 개요\n\n- 팩토리 패턴은 객체를 생성하는 코드와 객체의 사용 코드를 분리하는 역할을 한다. \n\n- 객체를 생성하는 로직을 별도의 팩토리 클래스나 팩토리 메소드에 위임하여 객체 생성을 캡슐화 한다. \n\n### 특징\n\n- 유연성 : 객체 생성 과정에 변화가 생겼을 때, 클라이언트 코드는 변경되지 않고 팩토리 코드만 변경하면 된다. \n\n- 확장성 : 새로운 타입의 객체를 지원하기 위해 클라이언트 코드를 변경할 필요 없이 팩토리 클래스를 확장하면 된다. \n\n### 장점\n\n- 분리 및 재사용\n\n- 유지보수\n\n- 클라이언트 코드 간소화\n\n### 단점\n\n- 복잡도\n\n- 학습곡선\n\n### 사용 시나리오\n\n- 객체 생성 로직이 복잡하거나 초기화에 많은 설정이 필요한 경우\n\n- 다양한 타입의 객체를 생성해야 하며, 이런 객체들이 공통의 인터페이스나 추상 클래스를 가지고 있는 경우\n\n- 시스템에서 아용되는 객체의 구체적인 타입을 시스템에서 분리하고자 할 때\n\n"},{"excerpt":"갑자기 짚고 넘어가는 프로젝트의 목표 이번 프로젝트의 목표는 다음과 같다.  개발 내적인 목표 타입스크립트를 사용한다.  변경에는 닫혀있고, 확장에는 열린 코드를 작성한다.  이를 위해 구현에만 집중하지 않고 설계에 신경을 써서 진행해본다. 이를 위해 디자인 패턴을 가능한 한 적극적으로 활용해본다.  가능한 한 사용이 쉽도록 만들어본다. 명확한 명명 규칙…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅/"},"frontmatter":{"date":"September 03, 2023","title":"NotionAPI를 활용한 자동 포스팅","tags":["Hobby","Blogging","Notion-API","DesignPattern"]},"rawMarkdownBody":"### 갑자기 짚고 넘어가는 프로젝트의 목표\n\n이번 프로젝트의 목표는 다음과 같다. \n\n#### 개발 내적인 목표\n\n1. 타입스크립트를 사용한다. \n\n1. 변경에는 닫혀있고, 확장에는 열린 코드를 작성한다. \n\n1. 이를 위해 구현에만 집중하지 않고 설계에 신경을 써서 진행해본다.\n\n1. 이를 위해 디자인 패턴을 가능한 한 적극적으로 활용해본다. \n\n1. 가능한 한 사용이 쉽도록 만들어본다.\n\n1. 명확한 명명 규칙을 사용한다.\n\n    1. PascalCase를 클래스 이름에 사용한다.\n\n    1. camelCase를 메소드 및 변수에 사용한다.\n\n#### 개발 외적인 목표\n\n1. 노션으로 공부만 해도 포스팅/풀심기가 모두 되는 꿈의 프로그램을 만든다. \n\n### 그래서 이번에는?\n\n[Notion API(2)](https://sharknia.github.io/Notion-API2)  \n\n지난번에 API 정상 작동을 확인만 한 코드를 타입스크립트로 바꾸고, 하나의 클래스로 만들려고 한다. \n\n#### 최초 작성 코드\n\n```typescript\nimport * as dotenv from \"dotenv\";\nimport { Client } from \"@notionhq/client\";\nimport { QueryDatabaseResponse } from \"@notionhq/client/build/src/api-endpoints\";\n\nclass NotionToMarkdown {\n    private notion: Client;\n    private databaseID: string;\n    private database?: QueryDatabaseResponse;\n\n    constructor() {\n        dotenv.config();\n        \n        if (!process.env.NOTION_KEY) {\n            throw new Error(\"Environment variable NOTION_KEY is not defined.\");\n        }\n\n        if (!process.env.NOTION_PAGE_ID) {\n            throw new Error(\"Environment variable NOTION_PAGE_ID is not defined.\");\n        }\n\n        this.notion = new Client({ auth: process.env.NOTION_KEY });\n        this.databaseID = process.env.NOTION_PAGE_ID;\n        this.initializeDatabase();\n    }\n    \n    private async initializeDatabase(): Promise<void> {\n        this.database = await this.queryDatabase();\n    }\n    \n    public async queryDatabase(): Promise<QueryDatabaseResponse> {\n        try {\n            const response = await this.notion.databases.query({ database_id: this.databaseID });\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n    // ... 다른 메소드들 (retrievePage, retrieveBlock, listBlockChildren) ...\n}\n```\n\n이런 식으로 구성을 했는데 위 코드의 문제는 생성자에서 비동기 작업을 직접 실행하는 것에 있다. \n\n비동기 작업을 생성자에서 실행할 경우 잠재적으로 발생할 수 있는 문제는 다음과 같다.\n\n1. 비동기 로직이 실패할 경우, 객체 생성 자체에 문제가 생길 수 있다. 생성자는 객체 초기화를 수행하는 로직만 포함해야 하며, 부작용(side-effect)이 발생할 수 있는 코드는 포함되어서는 안된다. \n\n1. 비동기 로직이 포함되면 생성자의 수행 시간이 길어질 수 있다. \n\n이 문제는 Factory 패턴을 사용하여 해결할 수 있다. Factory 메소드를 사용하여 비동기 로직을 수행하고 완료되면 객체를 반환한다. 아래와 같이 Factory 패턴을 사용하여 코드를 수정했다.  \n\n#### Factory Pattern을 활용한 코드\n\n```typescript\nclass NotionToMarkdown {\n    private notion: Client;\n    private databaseId: string;\n    public database: QueryDatabaseResponse;\n\n    private constructor(notion: Client, databaseID: string, database: QueryDatabaseResponse) {\n        this.notion = notion;\n        this.databaseId = databaseID;\n        this.database = database;\n    }\n\n    public static async create(): Promise<NotionToMarkdown> {\n        dotenv.config();\n\n        if (!process.env.NOTION_KEY || !process.env.NOTION_DATABASE_ID) {\n            throw new Error(\"Environment variable is not defined.\");\n        }\n\n        const notion = new Client({ auth: process.env.NOTION_KEY });\n        const databaseId = process.env.NOTION_DATABASE_ID;\n        const database = await new NotionToMarkdown(notion, databaseId, {} as QueryDatabaseResponse).queryDatabase();\n\n        return new NotionToMarkdown(notion, databaseId, database);\n    }\n\n    public async queryDatabase(): Promise<QueryDatabaseResponse> {\n        try {\n            const response = await this.notion.databases.query({ database_id: this.databaseId });\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n\n    // ... 다른 메소드들 (retrievePage, retrieveBlock, listBlockChildren) ...\n}\n\n// 사용 예제:\nNotionToMarkdown.create().then(handler => {\n    console.log(handler.database);  // database 속성 출력\n});\n```\n\n위 코드에서 create 메소드가 비동기 팩토리 메소드이다. 이 메소드는 `NotionToMarkdown` 객체를 비동기적으로 생성하고 반환한다. 생성자는 private로 선언되어 직접 호출할 수 없고, 반드시 create 메소드를 통해 객체를 생성해야 한다. \n\n### 진행 코드\n\n[https://github.com/Sharknia/Notion-to-Markdown/blob/42911777c0146c4260a5769fe9a7d5b1f9ac4c32/notionApi.ts](https://github.com/Sharknia/Notion-to-Markdown/blob/42911777c0146c4260a5769fe9a7d5b1f9ac4c32/notionApi.ts)\n\n\n\n"},{"excerpt":"naming conventions는 클래스, 메소드 또는 변수의 이름을 지을 때 사용되는 명명 규칙이다.  camelCase 첫번째 단어는 소문자로 시작하고 그 후의 단어는 대문자로 시작한다. 변수 또는 함수 명으로 주로 사용된다.  예 :    PascalCase 모든 단어가 대문자로 시작한다. 주로 클래스명으로 사용된다.  예 :    snake_ca…","fields":{"slug":"/네이밍-규칙naming-conventions/"},"frontmatter":{"date":"September 03, 2023","title":"네이밍 규칙(naming conventions)","tags":["ETC"]},"rawMarkdownBody":"naming conventions는 클래스, 메소드 또는 변수의 이름을 지을 때 사용되는 명명 규칙이다. \n\n1. **camelCase**\n\n    첫번째 단어는 소문자로 시작하고 그 후의 단어는 대문자로 시작한다. 변수 또는 함수 명으로 주로 사용된다. \n\n    예 : `variableName`  `printMyName`\n\n1. **PascalCase**\n\n    모든 단어가 대문자로 시작한다. 주로 클래스명으로 사용된다. \n\n    예 : `ClassName`  `PrintMyName`\n\n1. **snake_case**\n\n    모든 단어를 소문자로 작성하고 단어 사이를 `_` 로 구분한다. 변수나 함수명 또는 데이터베이스 필드명으로 사용된다. \n\n    예 : `variable_name`  `print_my_name`\n\n1. **kebab-case**\n\n    모든 단어를 소문자로 작성하고 단어 사이를 `-` 로 구분한다. 주로 URL 또는 css 클래스 명에 사용된다. \n\n    예 : `variable-name`  `print-my-name`\n\n1. **UPPER_SNAKE_CASE**\n\n    모든 단어를 대문자로 작성하고 단어 사이를 `_` 로 구분한다. 주로 상수를 나타낼 때 사용된다. \n\n    예 : `CONSTANT_VALUE`\n\n물론 이것들은 강제되는 것은 아니다. 하지만 일관된 스타일을 여러 개발자 사이에 유지하면 협업이나 가독성에 도움이 될 것이다. \n\n"},{"excerpt":"TypeScript TypeScript란? TypeScript는 Microsoft에서 개발한 오픈 소스 프로그래밍 언어이다. JavaScript의 SuperSet(상위 집합)이므로, 기존의 자바스크립트 코드도 타입스크립트에서 동작한다. 타입스크립트의 주요 목적은 큰 규모의 어플리케이션 개발을 돕기 위해 정적 타입, 인터페이스, 클래스, 모듈 등의 기능을 …","fields":{"slug":"/Typescript-시작하기/"},"frontmatter":{"date":"September 02, 2023","title":"Typescript 시작하기","tags":["Typescript"]},"rawMarkdownBody":"## TypeScript\n\n### TypeScript란?\n\nTypeScript는 Microsoft에서 개발한 오픈 소스 프로그래밍 언어이다. JavaScript의 SuperSet(상위 집합)이므로, 기존의 자바스크립트 코드도 타입스크립트에서 동작한다. 타입스크립트의 주요 목적은 큰 규모의 어플리케이션 개발을 돕기 위해 정적 타입, 인터페이스, 클래스, 모듈 등의 기능을 자바스크립트에 추가하는 것이다. \n\n### 주요 특징\n\n1. 정적 타이핑 : 변수, 함수 매개변수, 함수 반환 값에 대한 타입을 지정할 수 있다. 이를 통해 컴파일 시점에 오류를 발견할 수 있으며, IDE와 에디터는 타입 정보를 사용하여 더 나은 코드 제안과 리팩토링을 제안할 수 있다. \n\n1. 객체 지향 프로그래밍 : 클래스, 인터페이스, 상속, 제네릭 등과 같은 객체 지향 프로그래밍 기능을 제공하여 코드의 구조와 디자인을 개선한다. \n\n1. 도구 및 에디터 지원 : 코드 자동완성, 타입검사, 코드 리팩토링과 같은 기능을 지원하는 다양한 도구 및 에디터에 통합된다. \n\n1. 다운레벨 타겟팅 : 최신 자바스크립트 기능을 사용하여 코드를 작성하고 ES3, ES5, ES6등 이전 버전의 자바스크립트로 컴파일 할 수 있다. 이를 통해 최신 기능을 사용하면서도 구버전 브라우저와의 호환성을 유지할 수 있다. \n\n1. 확장성 : 모듈화 된 방식으로 동작하여 외부 라이브러리와의 통합을 쉽게 만들어준다. \n\n## TypeScript의 설치\n\n1. TypeScript 설치\n\n    ```bash\n    npm install -g typescript\n    ```\n\n1. 새 프로젝트 시작\n\n    ```bash\n    mkdir my-ts-project\n    cd my-ts-project\n    npm init -y\n    ```\n\n1. TypeScript 설정 파일 생성\n\n    ```bash\n    tsc --init\n    ```\n\n    `tsconfig.json` 파일이 생성된다. 이 파일에서 TypeScript의 컴파일 옵션을 설정할 수 있다.\n\n1. 필요한 TypeScript 타입 정의 설치\n\n    Node.js와 TypeScript를 함 사용하기 위해 해당 타입 정의를 설치한다. \n\n    ```bash\n    npm install --save @types/node\n    ```\n\n1. 첫 TypeScript 파일 작성\n\n    `index.ts` 파일에 다음과 같은 내용을 작성해보자. \n\n    ```typescript\n    const greet = (name: string): string => {\n        return `Hello, ${name}!`;\n    }\n    \n    console.log(greet(\"TypeScript\"));\n    ```\n\n1. 컴파일 및 실행\n\n    아래 명령어로 컴파일 할 수 있다. \n\n    ```bash\n    tsc index.ts\n    ```\n\n    index.js가 생성되었다. 이제 Node.js로 실행할 수 있다. \n\n    ```bash\n    node index.js\n    ```\n\n1. 자동 컴파일 설정(선택사항)\n\n    `ts-node` 라는 패키지를 설치하여 TypeScript 코드를 바로 실행할 수 있다.\n\n    ```bash\n    npm install -g ts-node\n    ```\n\n    설치 후, 아래 명령어로 TypeScript 코드를 바로 실행할 수 있다.\n\n    ```bash\n    ts-node index.ts\n    ```\n\n## TypeScript 설정 파일(**tsconfig.json)**\n\n`tsconfig.json` 은 타입스크립트 프로젝트의 루트 디렉토리에 위치하는 설정 파일이다. 타입스크립트 컴파일러에 대한 구성 옵션을 정의한다. 이 파일을 통해 타입스크립트 코드가 자바스크립트 코드로 변환되는지, 어떤 파일들이 포함되거나 제외되는지 등을 정의할 수 있다. \n\n주요 옵션들은 다음과 같다. \n\n- **files**: 컴파일에 포함할 파일의 목록이다. 이 옵션을 사용하면 특정 파일만 명시적으로 포함시킬 수 있다.\n\n- **include**: 컴파일에 포함할 파일이나 디렉토리의 패턴 목록다. Glob 패턴을 사용할 수 있다. 예: `[\"src/**/*.ts\"]`\n\n- **exclude**: 컴파일에서 제외할 파일이나 디렉토리의 패턴 목록다. 예: `[\"node_modules\"]`\n\n- **extends**: 다른 `tsconfig.json` 파일을 기반으로 현재 설정을 확장하려면 파일 경로를 지정한다.\n\n- **compilerOptions**: 컴파일러에 대한 다양한 설정을 제공하는 핵심 옵션이다.\n\n- **typeRoots와 types**: 사용자 정의 타입 선언의 위치와 포함될 타입 선언 패키지를 지정한다.\n\ncompilerOptions에 대해 좀 더 자세히 알아보면 다음과 같다. \n\n- **target**: 컴파일된 JavaScript의 버전을 지정한다. 예: ES5, ES6 등\n\n- **module**: 모듈 시스템을 지정한다. 예: CommonJS, ESNext, UMD 등\n\n- **outDir**: 컴파일된 JavaScript 파일이 저장될 디렉토리를 지정한다.\n\n- **rootDir**: 입력 파일의 루트 디렉토리를 지정한다.\n\n- **strict**: 모든 엄격한 타입 검사 옵션을 활성화한다.\n\n- **noImplicitAny**: 암시적인 'any' 타입이 없는 경우 오류를 발생시킨다.\n\n- **esModuleInterop**: CommonJS와 ES 모듈 간의 상호 운용성을 위한 코드를 생성한다.\n\n- **skipLibCheck**: 선언 파일의 타입 검사를 건너뛴다.\n\n"},{"excerpt":"gitignore란?  는 Git에서 프로젝트에 특정 파일 또는 디렉터리를 추적하지 않도록 지시하는 파일이다.  파일에 나열된 패턴과 일치하는 파일/디렉터리는 Git에서 무시된다. 이는 민감한 데이터, 컴파일 된 바이너리, 로그 파일 등 Git 저장소에 저장할 필요가 없거나 저장해서는 안되는 파일들을 제외하는 데에 유용하다.  문법  파일에는 여러 패턴을…","fields":{"slug":"/gitignore/"},"frontmatter":{"date":"September 02, 2023","title":"gitignore","tags":["GitHub"]},"rawMarkdownBody":"## gitignore란?\n\n`.gitignore` 는 Git에서 프로젝트에 특정 파일 또는 디렉터리를 추적하지 않도록 지시하는 파일이다. `.gitignore` 파일에 나열된 패턴과 일치하는 파일/디렉터리는 Git에서 무시된다. 이는 민감한 데이터, 컴파일 된 바이너리, 로그 파일 등 Git 저장소에 저장할 필요가 없거나 저장해서는 안되는 파일들을 제외하는 데에 유용하다. \n\n## 문법\n\n`.gitignore` 파일에는 여러 패턴을 사용할 수 있는데, 이를 알고 있으면 파일과 디렉터리를 무시하는 데 더 유용하게 사용할 수 있다.\n\n1. `` : 어떤 문자든지 0개 이상을 의미한다. 예를 들어, `.log`는 모든 로그 파일을 무시한다.\n\n1. `/` : 디렉터리 구분자로 사용된다. `/logs`는 logs라는 이름의 디렉터리만 무시하며, `logs/`는 logs로 시작하는 모든 디렉터리를 무시한다.\n\n1. `!` : 패턴을 부정한다. 예를 들면, `.log` 아래에 `!important.log`를 추가하면, 모든 로그 파일은 무시되지만 important.log 파일은 예외로 추적된다.\n\n1. `#` : 주석을 작성하는 데 사용된다.\n\n## 전역 설정하기\n\n프로젝트마다 반복적으로 무시 해야하는 파일이나 디렉터리가 있다면, 전역 `.gitignore` 파일을 만들 수 있다.\n\n1. 전역 `.gitignore` 파일 생성: `touch ~/.gitignore_global`\n\n1. Git 설정에 전역 `.gitignore` 파일 추가: `git config --global core.excludesFile ~/.gitignore_global`\n\n## TypeScript 프로젝트에서의 gitignore\n\n일반적으로 아래와 같지만, 프로젝트의 상황과 요구에 따라 추가/제외되는 항목이 당연히 있을 수 있다. \n\n### 포함하지 말아야 할 파일들\n\n1. `/node_modules/`: `npm install`이나 `yarn` 명령으로 설치한 패키지들이 이 디렉터리에 저장된다. 이는 대부분의 경우에 매우 크며, 이를 Git에 저장하는 것은 비효율적이다. 대신, `package.json`에 의존성이 기록되므로 다른 개발자들은 동일한 패키지를 쉽게 설치할 수 있다.\n\n1. `/dist/` **또는** `/build/`: TypeScript를 JavaScript로 컴파일할 때 생성되는 출력 디렉터리이다. 이것은 원본 코드에서 파생된 것이므로 저장소에 포함할 필요가 없다.\n\n1. `.env`: 환경 변수가 포함된 파일이다. 이 파일은 민감한 정보 (API 키, 비밀번호 등)를 포함할 수 있으므로 공개 저장소에 저장되어서는 안된다.\n\n1. `.DS_Store`: macOS에서 파일 시스템이 생성하는 메타데이터 파일이다. 이는 프로젝트와 관련이 없으므로 무시해야 한다.\n\n### 포함되어야 할 파일들\n\n1. `.js`**와** `.js.map`: TypeScript를 JavaScript로 컴파일하면, 일반적으로 `.js`와 `.js.map` 파일이 생성된다. 이러한 파일은 `/dist/`나 `/build/` 디렉터리에 포함될 수 있지만, 그렇지 않은 경우에도 이러한 파일을 무시하는 것이 좋다.\n\n1. `.tsbuildinfo`: TypeScript 3.4 이후로 컴파일러는 프로젝트 빌드 정보를 이 파일에 저장할 수 있다. 이것은 증분 빌드(incremental build)를 빠르게 만들기 위한 것이므로 저장소에 포함시키지 않아도 된다.\n\n1. `/coverage/`: 테스트 코드의 코드 커버리지를 저장하는 디렉터리이다. 보통 이 디렉터리는 코드 커버리지 도구로 생성된다.\n\n1. **로그 파일**: 예를 들면, `.log` 확장자를 가진 파일들이다.\n\n## 정의되었는데, 추적되는 경우\n\n예를 들어 나의 경우에는 *.js를 정의했는데 특정 js 파일이 계속해서 추적당했다. 이는 몇가지 이유가 있을 수 있다. \n\n1. 이전에 추적되었던 파일\n\n1. 위치 문제\n\n1. 다른 `.gitignore` 규칙\n\n나의 경우에는 1번의 경우였다. `git rm --cached 파일명.js` 명령어를 사용해 추적에서 따로 제거해줘야 한다. \n\n\n\n"},{"excerpt":"브랜치란? 브랜치는 코드의 다양한 버전을 동시에 관리하고 작업할 수 있게 해주는 도구이다. 개발자는 브랜치를 통해 작업을 병렬적으로 진행하고 나중에 원하는 시점에 이 작업들을 병합할 수 있다.  브랜치의 핵심 개념 Commit Git에서 변경점을 나타내는 스냅샷이다. 각 커밋은 이전 커밋에 대한 참조를 가진다.  HEAD 현재 작업중인 브랜치의 가장 최신…","fields":{"slug":"/Branch/"},"frontmatter":{"date":"September 01, 2023","title":"Branch","tags":["GitHub"]},"rawMarkdownBody":"### 브랜치란?\n\n브랜치는 코드의 다양한 버전을 동시에 관리하고 작업할 수 있게 해주는 도구이다. 개발자는 브랜치를 통해 작업을 병렬적으로 진행하고 나중에 원하는 시점에 이 작업들을 병합할 수 있다. \n\n### 브랜치의 핵심 개념\n\n#### Commit\n\nGit에서 변경점을 나타내는 스냅샷이다. 각 커밋은 이전 커밋에 대한 참조를 가진다. \n\n#### HEAD\n\n현재 작업중인 브랜치의 가장 최신 커밋이다. \n\n#### Merge\n\n두 브랜치의 변경 내용을 합치는 과정이다. \n\n### 브랜치의 사용목적\n\n브랜치를 사용하여 코드의 특정 시점을 백업하자. 이것은 Git의 핵심 기능 중 하나이다. \n\n브랜치를 사용하는 것은 코드의 특정 상태나 특정 기능을 분리하고, 나중에 필요한 경우 그 상태로 되돌아가거나 다른 브랜치와 병합하는 데 유용하다. \n\n브랜치를 백업 목적으로 사용하는 것의 장점은 다음과 같다. \n\n#### 장점\n\n1. 상태보존 : 특정 코드의 상태를 보존하고 싶을 때 브랜치를 생성하면 그 시점의 코드 상태를 유지할 수 있다. \n\n1. 안전한 테스트 : 새로운 기능이나 변경 사항을 시도하고 싶을 때 별도의 브랜치에서 작업하면 메인 브랜치를 안전하게 보호할 수 있다. \n\n1. 히스토리 관리 : 브랜치를 통해 어떤 변경 사항이 이루어졌는지의 히스토리를 명확하게 관리할 수 있다. \n\n#### 주의사항\n\n많은 수의 브랜치가 생성되면 관리가 복잡해질 수 있다. 정기적으로 더 이상 필요하지 않은 브랜치를 정리하는 습관을 들이는 것이 좋다. \n\n코드의 모든 작은 변경마다 브랜치를 생성한다면(그럴 리 없겠지만), 과도하게 많은 브랜치가 생성될 수 있다. 백업 목적으로 브랜치를 사용할 때에는 특히 실제로 중요한 시점에서만 브랜치를 생성하는 것이 바람직하다. \n\n### 브랜치 이름 짓기 \n\n브랜치 이름을 지을 때 고려할 수 있는 일반적인 가이드라인은 다음과 같다. \n\n1. 명확하게\n\n1. 짧게\n\n1. 소문자 사용 : 대소문자 구문을 위해 소문자로 통일하는 것이 좋다. \n\n1. 하이픈 사용 : 띄어쓰기가 안되므로 단어와 단어를 하이픈으로 구분한다. \n\n1. 카테고리/분류 사용 : 작업의 유형 또는 카테고리를 브랜치 이름의 접두사로 사용한다. 예시는 다음과 같다. \n\n    - `feat/` 새로운 기능 추가\n\n    - `fix/` 버그 수정\n\n    - `refactor/` 코드 리팩토링\n\n    - `docs/` 문서 관련 작업\n\n    예) `feat/user-authentication`\n\n1. 티켓 번호 포함 : 이슈 트래커나 프로젝트 관리도구와 연계된 작업의 경우 해당 IDX를 브랜치 이름에 포함시키는기 유용할 수 있다. \n\n1. 명사보다 동사를 선호 \n\n1. 확장자 사용 주의 : 브랜치 이름에 txt, md 같은 확장자를 포함하지 않도록 주의한다. \n\n"},{"excerpt":"DataTable 개요 DataTable은 .Net 프레임워크와 .Net Core, 그리고 .Net 5 이후 버전에서 제공하는 ADO.Net의 일부이다. 메모리 상에서 데이터의 테이블 형태를 관리하기 위한 .Net의 클래스이다.  DataTable은 클래스이다. 객체를 인스턴스화하고 메서드와 속성에 액세스 할 수 있다.  메모리상에서 데이터의 테이블 형태…","fields":{"slug":"/DataTable-클래스/"},"frontmatter":{"date":"September 01, 2023","title":"DataTable 클래스","tags":["ASP.Net"]},"rawMarkdownBody":"### DataTable 개요\n\nDataTable은 .Net 프레임워크와 .Net Core, 그리고 .Net 5 이후 버전에서 제공하는 ADO.Net의 일부이다. 메모리 상에서 데이터의 테이블 형태를 관리하기 위한 .Net의 클래스이다. \n\n1. DataTable은 클래스이다. 객체를 인스턴스화하고 메서드와 속성에 액세스 할 수 있다. \n\n1. 메모리상에서 데이터의 테이블 형태를 표현한다. 열(DataColumn)과 행(DataRow)로 이루어진다. \n\n1. 데이터베이스 쿼리 결과 저장, 사용자 인터페이스에 데이터 표시, 데이터 처리 및 변환, XML과 같은 다른 형식으로 데이터 직렬화 등 다양한 곳에 사용될 수 있다. \n\n1. DataTable은 DataSet 내에 존재할 수 있다. DataSet은 여러 DataTable 객체를 포함할 수 있으며, 이러한 테이블간에 관계(DataRelation)를 정의할 수 있다. \n\n### 장점과 단점\n\n#### 장점\n\n1. 자유로운 데이터 접근 : 행과 열을 사용하여 데이터에 액세스 할 수 있으므로 데이터를 쉽게 탐색/수정 할 수 있다. \n\n1. 자체적인 변경 추적 : 데이터 변경 내역을 추적할 수 있다. \n\n1. 연결 독립성 : 메모리 상의 데이터 구조이므로 데이터베이스 연결이 필요 없다. \n\n1. 스키마 정보 포함 : 스키마 정보를 포함하므로 데이터의 형식 및 관계를 정의하고 이해하기 쉽다. \n\n1. 필터 및 계산 기능 : 데이터를 필터링 하거나 계산 할 수 있다. \n\n#### 단점\n\n1. 메모리 사용량 : 메모리 기반의 구조이므로 큰 데이터 세트를 로딩할 경우 상당한 양의 메모리를 사용할 수 있다. \n\n1. 성능 : 엔터티나 객체와 같은 스트림라인된 모델에 비해 처리 및 직렬화에 상대적으로 더 많은 리소스를 요구한다. \n\n1. 타이핑 : 데이터를 액세스 할 때 타입 캐스팅이 필요할 수 있다. \n\n1. 학습 곡선 : ORM이나 LINQ 같은 현대적인 접근법에 익숙한 개발자에게는 학습곡선이 필요하다. \n\n1. 모듈성과 확장성 : 객체지향적인 접근법이나 도메인 주도 설계와 같은 설계 패턴에 직접 통합하기 어려울 수 있다. \n\n### DataTable.Compute의 사용 가능한 주요 표현식\n\n#### 집계함수\n\n| Sum | 합계 |\n| --- | --- |\n| Avg | 평균 |\n| Min | 최소값 |\n| Max | 최대값 |\n| Count | 항목의 수 |\n| StDev | 표준 편차 |\n| Var | 분산 |\n\n#### 산술 연산자\n\n`+`, `-`, `*`, `/`, `%`\n\n#### 비교 연산자\n\n`=`, `<>`, `>`, `<`, `>=`, `<=`\n\n#### 논리 연산자\n\n`AND`, `OR`, `NOT`\n\n#### 문자열 함수\n\n| LEN | 문자열의 길이 |\n| --- | --- |\n| SUBSTRING | 부분 문자열 |\n| TRIM | 앞 뒤 공백 제거 |\n| UPPER | 대문자로 변환 |\n| LOWER | 소문자로 변환 |\n\n#### 기타 함수\n\n| ISNULL | NULL일 경우 대체할 값 반환 |\n| --- | --- |\n| CONVERT | 데이터 타입 변환 |\n\n### 기타 다른 주요 메소드\n\n| Select | 특정 조건을 만족하는 행을 DataRow 배열로 반환 |\n| --- | --- |\n| NewRow | 새 DataRow를 생성 |\n| Rows.Add | 행을 추가 |\n| Rows.Remove | 행을 삭제 |\n| Rows.RemoveAt | 지정한 위치의 행을 삭제 |\n| Clear | 모든 행을 삭제 |\n| Copy | DataTable의 사본을 생성 |\n| Clone | 구조만 복사한 새 DataTable을 반환 |\n| GetChanges | 변경된 행들만을 포함하는 새 DataTable을 반환 |\n| AcceptChanges | 변경 내용을 확정 |\n| RejectChanges | 변경 내용을 취소 |\n\n"},{"excerpt":"무엇을 하는가? Notion API를 이용해서 내가 쓴 글 들을 불러와 MD 파일로 만들려고 한다. 그 과정을 실시간으로 기록하려고 한다. 명확하게 가능한가? 는 사실 아직 알아보지 않았다.  왜 하는가? 너무나 너무나 귀찮기 때문이다. 블로그 포스팅을 위해서 마크다운 에디터로 노션을 쓰고 싶기 때문이다. 아니? 노션으로 쓴 것을 그대로 블로그에 커밋해버…","fields":{"slug":"/Notion-API2/"},"frontmatter":{"date":"September 01, 2023","title":"Notion API(2)","tags":["Hobby","Blogging","Notion-API"]},"rawMarkdownBody":"## 무엇을 하는가? \n\nNotion API를 이용해서 내가 쓴 글 들을 불러와 MD 파일로 만들려고 한다. 그 과정을 실시간으로 기록하려고 한다. 명확하게 가능한가? 는 사실 아직 알아보지 않았다. \n\n## 왜 하는가? \n\n너무나 너무나 귀찮기 때문이다.\n\n블로그 포스팅을 위해서 마크다운 에디터로 노션을 쓰고 싶기 때문이다. 아니? 노션으로 쓴 것을 그대로 블로그에 커밋해버리고 싶기 때문이다. 그 과정을 자동화해버리고 싶기 때문이다. \n\n즉, 노션에 글만 써두면 자동으로 마크다운으로 받아져서 깃허브에 커밋이 되고 깃허브 페이지에 배포가 되는 원대한 자동화를 원한다. \n\n## 어떻게 하는가?\n\n### 환경\n\nNode.js 18.17.1 버전을 사용한다.\n\nNode.js 프로젝트 시작\n\n```bash\nnpm init\n```\n\n.env 생성\n\n```bash\nNOTION_KEY=<노션키>\nNOTION_PAGE_ID=<PageId>\n```\n\n.gitignore에 .env 등록\n\n```bash\n.env\n```\n\n.env 안의 내용을 가져올 수 있도록 `dotenv` 패키지 설치\n\n```bash\n$ npm install dotenv\n```\n\nNotion API를 이용할 수 있도록 패키지 설치\n\n```bash\n$ npm install @notionhq/client\n```\n\n### API 확인\n\n#### 코드\n\nNotion API reference에 아주 친절하게 JavaScript 예제 코드까지 전부 붙어있다. Node.js를 선택한 이유이기도 하다. 예제 코드가 자바스크립트로 되어있어 그대로 갖다 붙이기 쉬워 보였기 때문이다. 일단 API의 결과물이 어떻게 날아오는지 확인하기 위해 있는 대로 갖다 붙였다. \n\n```javascript\nrequire(\"dotenv\").config();\nconst { Client } = require(\"@notionhq/client\");\n\nconst pageId = process.env.NOTION_PAGE_ID;\nconst key = process.env.NOTION_KEY;\n\nconst notion = new Client({ auth: key });\n\n(async () => {\n    try {\n        const response = await notion.databases.query({ database_id: pageId });\n        // console.log(response);\n        //페이지들을 돌면서 id로 페이지를 읽어온다. \n        response.results.forEach(page => {\n            (async () => {\n                const pageId = page.id;\n                //page properties\n                const response = await notion.pages.retrieve({ page_id: pageId });\n                console.log(\"----------Page Properties----------\")\n                console.log(JSON.stringify(response, null, 2));\n                //page contents\n                (async () => {\n                    const blockId = pageId;\n                    const response = await notion.blocks.retrieve({\n                        block_id: blockId,\n                    });\n                    console.log(\"----------Page Contents as Block----------\")\n                    console.log(JSON.stringify(response, null, 2))\n                })();\n                (async () => {\n                    const blockId = pageId;\n                    const response = await notion.blocks.children.list({\n                        block_id: blockId,\n                        page_size: 50,\n                    });\n                    console.log(\"----------Page Block List----------\");\n                    console.log(JSON.stringify(response, null, 2));\n                })();\n            })();\n        });\n    } catch (error) {\n        console.error(\"Error querying the database:\", error);\n    }\n})();\n```\n\n#### 노션의 페이지 구성\n\n![](image1.png)\npage properties는 정확하게 마크다운의 Front Matter에 사용하면 될 것 같다. \n\n그리고, 마크다운의 콘텐츠에 page content가 들어가면 되겠다. \n\npage properties를 얻기 위해서는 notion.pages.retrieve method를 이용하면 된다. \n\n#### 코드 실행 결과문 분석\n\n- notion.pages.retrieve\n\n```json\n{\n  \"object\": \"page\",\n  \"id\": \"...id...\",\n  \"created_time\": \"2023-08-23T13:32:00.000Z\",\n  \"last_edited_time\": \"2023-09-01T12:45:00.000Z\",\n  \"created_by\": {\n    \"object\": \"user\",\n    \"id\": \"...id...\"\n  },\n  \"last_edited_by\": {\n    \"object\": \"user\",\n    \"id\": \"...id...\"\n  },\n  \"cover\": null,\n  \"icon\": null,\n  \"parent\": {\n    \"type\": \"database_id\",\n    \"database_id\": \"...id...\"\n  },\n  \"archived\": false,\n  \"properties\": {\n    \"tags\": {\n      \"id\": \"PHNF\",\n      \"type\": \"multi_select\",\n      \"multi_select\": [\n        {\n          \"id\": \"...id...\",\n          \"name\": \"AWS\",\n          \"color\": \"brown\"\n        },\n        {\n          \"id\": \"...id...\",\n          \"name\": \"TAG2\",\n          \"color\": \"pink\"\n        }\n      ]\n    },\n  },\n  \"url\": \"https://www.notion.so/...\",\n  \"public_url\": \"https://name.notion.site/...\"\n}\n```\n\n정확하게 페이지의 속성들을 가져오고 있다. Front Matter에 필요한 내용들을 속성으로 만들어두면 완벽할 것 같다. \n\n- notion.blocks.retrieve\n\nnotion.blocks.retrieve는 page ID를 넣으면 해당 페이지의 정보를 가져다 주는데 왜냐면 노션에서 모든 단위가 블록이기 때문이다. 해당 페이지의 블록으로서의 데이터를 주는 것이기 때문에 지금 내 상황에선 그다지 사용할 필요가 없어 보인다. \n\n- notion.blocks.children.list\n\n```json\n{\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"object\": \"block\",\n      \"id\": \"id\",\n      \"parent\": {\n        \"type\": \"page_id\",\n        \"page_id\": \"id\"\n      },\n      \"created_time\": \"2023-09-01T12:00:00.000Z\",\n      \"last_edited_time\": \"2023-09-01T12:12:00.000Z\",\n      \"created_by\": {\n        \"object\": \"user\",\n        \"id\": \"id\"\n      },\n      \"last_edited_by\": {\n        \"object\": \"user\",\n        \"id\": \"id\"\n      },\n      \"has_children\": false,\n      \"archived\": false,\n      \"type\": \"heading_1\",\n      \"heading_1\": {\n        \"rich_text\": [\n          {\n            \"type\": \"text\",\n            \"text\": {\n              \"content\": \"제목1\",\n              \"link\": null\n            },\n            \"annotations\": {\n              \"bold\": false,\n              \"italic\": false,\n              \"strikethrough\": false,\n              \"underline\": false,\n              \"code\": false,\n              \"color\": \"default\"\n            },\n            \"plain_text\": \"제목1\",\n            \"href\": null\n          }\n        ],\n        \"is_toggleable\": false,\n        \"color\": \"default\"\n      }\n    },\n\t...\n}\n```\n\n우리가 원하는 Page Content들의 내용이 드디어 보인다. 위의 결과에는 나타나지 않았지만, 들여쓰기가 된 경우에는 해당 들여쓰기가 된 block의 상위 요소에게 종속된다. 다시 말 내 다음 블록에 들여쓰기를 한 블록이 있다면 `has_children` 속성이 true가 된다. 그 안을 다시 파고 들어가면 될 것 같다. \n\n## 오늘의 결론\n\n#### 코드 기록\n\n[https://github.com/Sharknia/Notion-to-Markdown/tree/check-api-test-complete](https://github.com/Sharknia/Notion-to-Markdown/tree/check-api-test-complete)\n\n이렇게 해서, API를 통해서 내 노션에 올려진 글들을 가져오는 데에 성공했다. 이제 다음 시간에는 이 JSON들을 가공해서 Markdown으로 변환하려고 한다. \n\n방법은 세 가지 정도를 생각하고 있다. \n\n1. 노션 내부 API 사용\n\n    실제 노션 페이지를 띄울 때 오고 가는 요청값 들을 확인해 이를 사용하여 마크다운 파일을 바로 만들 수 있는 것으로 보인다. 이는 만들기가 가장 쉬워 보이나, 언제 폭파 될지 모른다는 문제점을 갖고 있다.  만약 폭파된다면 귀찮게도 다시 만들어야 한다는 치명적인 문제 때문에 지금 당장은 고려하고 있지 않지만 실패할 가능성도 가장 적어 보이므로, 귀찮아지면 언제든지 이쪽으로 틀도록 하자. \n\n1. 오픈 소스 사용\n\n    Notion 블록을 마크다운 형식으로 바꿔주는 오픈 소스가 있는 것으로 보인다. 아직 자세히 살펴보지는 않았다. \n\n1. 직접 구현\n\n    그냥.. 직접 만든다. 아마 많은 기능을 지원하지는 못하겠지만 노션을 쓰면서 느낀 건데 많은 기능을 딱히 사용하지 않는다. 정말 개인적인 용도라면 그냥 생각보다 쉽게 될 것 같기도 하다.\n\n일단 코드를 막 짜고 있는데, 이번에는 변화에 닫힌, 확장에 열린 코드를 짜보려고 한다. 설계도 신경써서 하고 싶다. \n\n또 여유가 허락된다면 타입스크립트도 써보고 싶다. \n\n## 참고문서\n\n[https://developers.notion.com/reference](https://developers.notion.com/reference)\n\n[https://blog.hwahae.co.kr/all/tech/10960](https://blog.hwahae.co.kr/all/tech/10960)\n\n"},{"excerpt":"npx란? npx는 Node.js와 함께 제공되는 패키지 실행도구이다.  npm에서 제공하는 패키지를 설치하지 않고 즉시 실행하게 해주는 역할을 한다.  npx의 특징 및 기능 전역설치 없이 실행 패키지를 전역으로 설치하지 않고 즉시 명령어를 바로 실행할 수 있다. 에를 들어  gh-pages와 같은 CLI 도구를 전역적으로 설치하지 않아도 npx gh-…","fields":{"slug":"/npx/"},"frontmatter":{"date":"August 31, 2023","title":"npx","tags":["Node.js"]},"rawMarkdownBody":"#### npx란?\n\nnpx는 Node.js와 함께 제공되는 패키지 실행도구이다.  npm에서 제공하는 패키지를 설치하지 않고 즉시 실행하게 해주는 역할을 한다. \n\n#### npx의 특징 및 기능\n\n1. 전역설치 없이 실행\n\n    패키지를 전역으로 설치하지 않고 즉시 명령어를 바로 실행할 수 있다.\n\n    에를 들어  gh-pages와 같은 CLI 도구를 전역적으로 설치하지 않아도 npx gh-pages로 바로 실행할 수 있다.\n\n1. 일회성 실행\n\n    npx를 사용하여 패키지를 실행하면 해당 패키지는 일시적으로 다운로드 되어 실행되고 실행 후에는 시스템에서 제거된다. \n\n1. 명령어 실행\n\n1. Github gist 실행\n\n1. 임시 패키지 실행\n\n    특정 버전의 패키지나 다른 레지스트리의 패키지를 실행하는데 npx를 사용할 수 있다. \n\n#### npx의 장점\n\n1. 디스크 공간 절약\n\n    사용 후에 패키지가 제거되므로, 불필요한 디스크 공간을 사용하지 않는다. \n\n1. 항상 최신 버전 사용\n\n    최신 버전의 패키지를 가져와 실행한다. \n\n1. 전역 충돌 방지\n\n    패키지를 전역 설치하지 않으므로 다른 버전의 동일한 패키지가 설치되어 있을 때 충돌을 피할 수 있다. \n\n#### npx의 단점\n\n매번 다시 받을 필요 없는 자주 사용하는 패키지나 용량이 큰 패키지의 경우 의미 없이 시간이 걸릴 수 있다. \n\n#### npx의 실행순서\n\n예를 들어 `npx gh-pages -d public` 를 실행하면 다음의 과정을 거친다. \n\n1. 시스템에 `gh-pages`가 설치되어 있는지 확인한다. \n\n1. 설치되어 있지 않다면, 일시적으로 `gh-pages`를 다운로드한다. \n\n1. 다운로드 된 `gh-pages` 패키지의 실행 파일을 사용하여 `-d public` 옵션과 함께 실행한다. \n\n1. 실행이 완료된 후에 `gh-pages` 패키지는 시스템에서 제거된다. \n\n\n\n"},{"excerpt":"왜 하는가 나는 귀찮은게 너무 귀찮다.  커밋 한 이후에  를 해야 배포가 되는 것도 너무 귀찮았다.  무엇을 하는가 그래서, 이걸  을 이용해서 자동화를 했다.  에 임의의 yml 파일을 넣어주면 해당 작업을 github action에서 진행한다.  어떻게 했는가 yml 파일 내용은 다음과 같다.  문제는 없었는가 처음에는 npm install을 사용했…","fields":{"slug":"/githubio-자동배포/"},"frontmatter":{"date":"August 30, 2023","title":"github.io 자동배포","tags":["Blogging","Hobby"]},"rawMarkdownBody":"#### 왜 하는가\n\n나는 귀찮은게 너무 귀찮다. \n\n커밋 한 이후에 \n\n```bash\nnpm run deploy-gh\n```\n\n를 해야 배포가 되는 것도 너무 귀찮았다. \n\n#### 무엇을 하는가\n\n그래서, 이걸 `github action` 을 이용해서 자동화를 했다. \n\n`.gitbub/workflows`에 임의의 yml 파일을 넣어주면 해당 작업을 github action에서 진행한다. \n\n#### 어떻게 했는가\n\nyml 파일 내용은 다음과 같다. \n\n```bash\nname: Deploy\n\non: # 어떤 작업이 수행될 때 deploy.yml 작업이 수행된다. (트리거)\n  push: # push 작업이 수행될 때\n    branches: # 특정 브랜치를 대상으로\n      - master\n\npermissions: # github action이 수행되는 환경에서 특정 권한을 준다\n  contents: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 20.3.1\n\n      - name: Install node packages\n        run: yarn\n        \n      - name: Check lint\n        run: yarn check:lint\n        \n      # 아래와 같은 오류가 남\n      # error Command failed with exit code 1.\n      # info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n      # - name: Check prettier\n      #   run: yarn check:prettier\n      \n      - name: Build\n        run: yarn build\n        \n      - name: Set up GitHub token\n        env:\n          GITHUB_TOKEN: ${{ secrets.Token}}\n        run: git config --global user.email \"email입력\" && git config --global user.name \"name입력\"\n\n      - name: Set Git remote URL with token\n        run: git remote set-url origin https://${{ secrets.classicToken }}@github.com/Sharknia/Sharknia.github.io\n\n      - name: Deploy to GitHub Pages\n        run: npx gh-pages -d public\n```\n\n#### 문제는 없었는가\n\n처음에는 npm install을 사용했었는데, 패키지가 없다던지 여러가지 문제가 났다. 해당 문제는 npx 명령을 사용하는 것으로 해결돼\n\nDeploy 까지 자동으로 하기 위해서는 깃허브 설정 관련 명령어가 필요했다. \n\n특히, 반드시 토큰을 발행하고 Github Action의 환경 변수에 추가를 해주어야 한다. \n\n"},{"excerpt":"https://devhudi.github.io/gatsby-starter-hoodie/quick-start-kr/ 디자인이 마음에 들고, 필요로 하는 기능이 모두 들어가 있기 때문에 해당 테마를 선택했다. (시리즈 기능, 목차 기능, 댓글 기능) 5번까지는 무사히 테스트 했는데,  6번 특히 Netlify를 활용한 배포에서 막혔다.  Repository를…","fields":{"slug":"/githubio를-이용한-블로그/"},"frontmatter":{"date":"August 28, 2023","title":"github.io를 이용한 블로그","tags":["Blogging","Hobby"]},"rawMarkdownBody":"[https://devhudi.github.io/gatsby-starter-hoodie/quick-start-kr/](https://devhudi.github.io/gatsby-starter-hoodie/quick-start-kr/)\n\n디자인이 마음에 들고, 필요로 하는 기능이 모두 들어가 있기 때문에 해당 테마를 선택했다. (시리즈 기능, 목차 기능, 댓글 기능)\n\n\n\n5번까지는 무사히 테스트 했는데, \n\n6번 특히 Netlify를 활용한 배포에서 막혔다. \n\nRepository를 생성 후, 해당 저장소를 바로 Netlify에서 빌드하려고 하면 의존성 오류가 났다. \n\nNetlify를 활용하려고 한 이유는 Github Pages를 통해 배포할 경우 마스터 브랜치와 별도의 브랜치가 생성되는것이 번잡스러웠기 때문이다. \n\n\n\n조금 더 알아본 결과,\n\n```bash\n$ npm run build\n```\n\n를 실행하면, \n\n`/public` 에 빌드 결과물이 생성되고 해당 폴더만 배포하면 될 것 같긴하다. 또는 `github action` 을 이용할 수도 있겠다. \n\n\n\n조금 더 편하게 블로그를 하려는 길은 아직 멀고 험하다. \n\n"},{"excerpt":"Notion API를 사용해보려고 한다.  이것으로 무엇을 할 것인가? 는 생각해둔게 있지만, 가능하다고 생각된 시점에서 본격적으로 해보려고 하고, 일단 오늘은 Notion API를 살펴보려고 한다.  간단한 소개 https://developers.notion.com/ Notion의 공개 Rest API를 이용하여 Notion Workspace와 상호 작…","fields":{"slug":"/Notion-API1/"},"frontmatter":{"date":"August 23, 2023","title":"Notion API(1)","tags":["Notion-API","Blogging","Hobby"]},"rawMarkdownBody":"Notion API를 사용해보려고 한다. \n\n이것으로 무엇을 할 것인가? 는 생각해둔게 있지만, 가능하다고 생각된 시점에서 본격적으로 해보려고 하고, 일단 오늘은 Notion API를 살펴보려고 한다. \n\n#### 간단한 소개\n\n[https://developers.notion.com/](https://developers.notion.com/)\n\nNotion의 공개 Rest API를 이용하여 Notion Workspace와 상호 작용 할 수 있다.\n\n페이지, 데이터베이스, 사용자, 페이지 및 인라인 주석, Workspace의 포스팅에 대한 검색 등등.. \n\n점점 지원 영역이 늘어나는 것 같다. change-log가 꽤 활발해보인다. \n\n#### API 생성 및 권한 부여\n\n일단 우선, API를 생성하고 권한 부여 작업을 해야 한다. \n\n[https://www.notion.so/my-integrations](https://www.notion.so/my-integrations)\n\n올라와 있는 한글 블로그들과 현재의 API 권한 부여 방식이 달라지는 바람에  여기서 은근 헤맸다… 역시 공식 가이드를 우선적으로 봐야한다. \n\n위 페이지에서 새 `API 통합 만들기` 를 선택한다. \n\nAPI의 권한은 워크스페이스별로 관리되므로, `연결된 워크스페이스` 를 정확히 선택해준다. \n\n필수 항목을 입력해주고 `제출` 을 눌러주자. \n\n![](image1.png)\n그럼 바로 위와 같은 화면으로 넘어온다. 표시를 눌러서 시크릿 키를 복사해서 보관해주자. \n\n![](image2.png)\n기능도 커스텀 할 수 있는데, 일단 기본적인 기능을 넣어주었다. \n\n배포는 당연히 공개로 해두지 않았다. \n\n이후, 노션으로 돌아와 API와 연결된 워크스페이스를 선택하고 \n\n![](image3.png)\n오른쪽 상단의 … 를 눌러서 연결추가 를 선택한 다음, 아까 만들어준 API의 이름을 검색해 해당 API와 연결을 해주면 된다.\n\n이 부분이 UI가 달라진 부분이어서 공식 가이드를 보기 전까지 한참 헤맸던 부분이었다.\n\n이제, API와 Notion Workspace가 연결되었다!\n\n"},{"excerpt":"정의 PRG 패턴은 웹 어플리케이션에서 폼 제출 후의 중복 제출 문제를 해결하기 위한 웹 디자인 패턴입니다. 사용자가 웹 폼을 제출한 후 (Post), 서버가 해당 요청을 처리하고 사용자를 새로운 위치로 리다이렉트 (Redirect) 시킵니다. 이후 사용자는 리다이렉트된 위치의 정보를 (Get) 요청하여 표시합니다. 특징 중복 폼 제출 방지: 사용자가 새…","fields":{"slug":"/PRG-패턴-PostRedirectGet/"},"frontmatter":{"date":"August 23, 2023","title":"PRG 패턴 (Post/Redirect/Get)","tags":["DesignPattern"]},"rawMarkdownBody":"#### 정의\n\nPRG 패턴은 웹 어플리케이션에서 폼 제출 후의 중복 제출 문제를 해결하기 위한 웹 디자인 패턴입니다. 사용자가 웹 폼을 제출한 후 (Post), 서버가 해당 요청을 처리하고 사용자를 새로운 위치로 리다이렉트 (Redirect) 시킵니다. 이후 사용자는 리다이렉트된 위치의 정보를 (Get) 요청하여 표시합니다.\n\n#### 특징\n\n- 중복 폼 제출 방지: 사용자가 새로고침을 눌렀을 때, 같은 폼 데이터가 다시 제출되는 것을 방지합니다.\n\n- 향상된 사용자 경험: 사용자가 폼 제출 후 브라우저의 뒤로 가기 버튼을 사용할 때, \"폼 데이터 제출 확인\"과 같은 경고 메시지가 나타나지 않습니다.\n\n- 검색 엔진 최적화: 리다이렉트된 URL은 검색 엔진에 의해 색인화될 수 있기 때문에, POST 요청으로 인한 잘못된 URL 색인화를 방지합니다.\n\n#### 장점\n\n- 브라우저 호환성: 대부분의 웹 브라우저에서 지원되므로 호환성 문제가 적습니다.\n\n- 직관적인 URL: 리다이렉트 후의 URL은 대체로 사용자에게 직관적이며, 북마크나 공유하기에 적합합니다.\n\n#### 단점\n\n- 추가적인 서버 응답: 리다이렉트로 인해 추가적인 서버 응답이 필요하므로, 약간의 성능 저하가 발생할 수 있습니다.\n\n- 구현 복잡성: 일부 웹 어플리케이션 프레임워크에서는 PRG 패턴의 구현이 복잡할 수 있습니다.\n\n#### 쇠퇴 이유\n\n- 프레임워크의 발전: 많은 현대 웹 프레임워크와 라이브러리가 이 문제를 내부적으로 처리합니다.\n\n- Single Page Applications (SPA): AJAX와 함께 SPA의 인기가 높아지면서 전통적인 폼 제출 방식이 적게 사용되게 되었습니다.\n\n\n\n"},{"excerpt":"venv란? Python의 표준 라이브러리에 포함된 가상 환경 모듈이다. 가상환경은 프로젝트마다 독립적인 Python 환경을 생성하여 패키지 의존성을 분리하고 관리할 수 있는 기능을 제공한다.  한 시스템 내에서 여러 프로젝트를 개발하거나 실행할 때 각 프로젝트별로 필요한 패키지를 격리된 환경에서 관리할 수 있다.  Python 3.3부터 venv가 표준…","fields":{"slug":"/Python-venv-Windows/"},"frontmatter":{"date":"August 23, 2023","title":"Python venv (Windows)","tags":["Python"]},"rawMarkdownBody":"#### venv란?\n\nPython의 표준 라이브러리에 포함된 가상 환경 모듈이다. 가상환경은 프로젝트마다 독립적인 Python 환경을 생성하여 패키지 의존성을 분리하고 관리할 수 있는 기능을 제공한다. \n\n한 시스템 내에서 여러 프로젝트를 개발하거나 실행할 때 각 프로젝트별로 필요한 패키지를 격리된 환경에서 관리할 수 있다. \n\nPython 3.3부터 venv가 표준 라이브러리에 탑재되었다. \n\n#### venv의 장점\n\n1. 독립성\n\n1. 의존성 분리\n\n1. 가볍고 효율적\n\n1. 파일 분리\n\n#### venv를 이용한 가상환경 생성\n\n1. 가상환경을 생성할 디렉토리를 생성하고 이동한다. \n\n1. 가상환경 생성\n\n    ```bash\n    phthon -m venv venv\n    ```\n\n    마지막 “venv”는 가상환경 이름이다. 이렇게 하면 현재 디렉토리 하에 가상환경의 이름으로 새로운 디렉토리가 생성된다. \n\n1. 가상환경 활성화\n\n    ```bash\n    venv\\Scripts\\activate\n    ```\n\n    첫번째 venv는 가상환경의 이름이다. 가상환경이 활성화되면, 프롬포트에서 가장 앞에 가상 환경의 이름이 나타난다. 이제부터 설치한 패키지는 해당 가상환경에만 적용이 된다. \n\n1. 가상환경 비활성화\n\n    가상환경이 활성화된 상태에서 \n\n    ```bash\n    deactivate\n    ```\n\n"},{"excerpt":"print와 pprint는 모두 Python에서 사용되는 출력 관련 함수이다.  print 값을 터미널에 간단히 출력할 때 사용한다.  pprint pprint는 pretty print의 약자로, 복잡한 데이터 구조를 읽기 쉽게 출력하는 데에 사용된다. 데이터 구조를 계층적으로 출력하고 들여쓰기를 적용하여 가독성을 향상시킨다.  예를 들어 위와 같은 코드…","fields":{"slug":"/print와-pprint/"},"frontmatter":{"date":"August 23, 2023","title":"print와 pprint","tags":["Python"]},"rawMarkdownBody":"print와 pprint는 모두 Python에서 사용되는 출력 관련 함수이다. \n\n### print\n\n값을 터미널에 간단히 출력할 때 사용한다. \n\n```python\nx = 10\ny = \"Hello\"\nprint(x, y)  # 출력: 10 Hello\n```\n\n### pprint\n\npprint는 pretty print의 약자로, 복잡한 데이터 구조를 읽기 쉽게 출력하는 데에 사용된다. 데이터 구조를 계층적으로 출력하고 들여쓰기를 적용하여 가독성을 향상시킨다. \n\n```python\nimport pprint\n\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}\npprint.pprint(data)\n```\n\n예를 들어 위와 같은 코드를 실행하면,\n\n```python\n{'age': 30,\n 'city': 'New York',\n 'name': 'John'}\n```\n\n로 출력이 된다. \n\n딕셔너리, 리스트, 중첩된 데이터 구조 등을 보기 좋게 출력해주어 디버깅이나 데이터 분석시에 유리하다. \n\n"},{"excerpt":"정규화 정규화는 데이터베이스 설계 과정 중에서 중복을 최소화하고 데이터의 일관성과 무결성을 유지하기 위한 기법이다. 정규화는 데이터를 더 작은 논리적 단위로 분할하고 이를 통해 중복 데이터를 제거하여 데이터베이스의 효율성과 유지보수 용이성을 향상 시킨다.  정규화는 대표적으로 업데이트 이상, 삽입 이상, 삭제 이상과 같은 무제를 해결한다.  주요 정규화 …","fields":{"slug":"/정규화와-역정규화/"},"frontmatter":{"date":"August 23, 2023","title":"정규화와 역정규화","tags":["DataBase"]},"rawMarkdownBody":"## 정규화\n\n정규화는 데이터베이스 설계 과정 중에서 중복을 최소화하고 데이터의 일관성과 무결성을 유지하기 위한 기법이다. 정규화는 데이터를 더 작은 논리적 단위로 분할하고 이를 통해 중복 데이터를 제거하여 데이터베이스의 효율성과 유지보수 용이성을 향상 시킨다. \n\n정규화는 대표적으로 업데이트 이상, 삽입 이상, 삭제 이상과 같은 무제를 해결한다. \n\n#### 주요 정규화 단계\n\n1. 제 1정규형(1NF)\n\n1. 제 2정규형(2NF)\n\n1. 제 3정규형(3NF)\n\n1. 보이스-코드 정규형(BCNF)\n\n#### 정규화의 필요성\n\n1. 데이터 중복 최소화\n\n    중복 데이터는 데이터베이스 내에서 일관성 문제를 일으킨다. 중복된 데이터가 변경될 경우 모든 복사본을 일관되게 업데이트 해야 하며, 이를 놓칠경우 불일치가 발생할 수 있다. \n\n    정규화는 데이터를 더 작은 단위로 분할하여 중복을 최소화 하고 이로 인한 불일치 가능성을 낮춘다. \n\n1. 데이터 일관성 유지\n\n    데이터를 여러 테이블에 나누고 관계를 설정함으로써 데이터 간의 의미적 관계를 명확하게 정의할 수 있다. \n\n1. 데이터 이상 방지\n\n    삽입, 업데이트, 삭제 시 발생할 수 있는 이상 현상을 방지한다. \n\n1. 검색 성능 개선\n\n1. 데이터베이스 구조의 명확성\n\n1. 유지보수 용이성\n\n    정규화된 데이터는 데이터베이스 변경이 필요한 경우에도 수정 작업이 해당 테이블에서만 이루어질 가능성이 높다. 이로써 수정이 다른 부분에 미치는 영향을 최소화 하고 유지보수를 용이하게 만든다. \n\n1. 데이터베이스 확장성\n\n    추후 DB 확장 또는 새로운 기능 추가할 때 더 유연하게 대응할 수 있는 기반이 된다. \n\n1. 데이터 무결성 강화\n\n    중복이 최소화 되고 일관성이 유지되면서 데이터의 무결성이 강화된다.\n\n#### 정규화의 단점\n\n- 정규화되면 데이터의 일부가 여러 테이블에 나눠져 저장되어 테이블이 증가하므로 아래와 같은 문제점들이 발생할 수 있다. \n\n1. 조인의 증가\n\n1. 읽기 작업의 복잡성\n\n1. 데이터 일관성 유지의 어려움\n\n1. 갱신 작업의 비효율성\n\n#### 정규화시 주의해야 할 점\n\n1. 과도한 정규화\n\n    너무 과도한 정규화는 복잡한 쿼리, 늘어난 조인으로 성능 저하의 원인이 될 수 있다. \n\n1. 데이터베이스 디자인의 복잡성\n\n    데이터가 분산되어 복잡한 조인과 서브쿼리가 늘어나면서 쿼리 작성, 유지보수가 어려워 질 수 있다. \n\n1. 역정규화의 필요성\n\n    때로 성능 향상이나 특정 요구사항을 위해 데이터 무결성을 해칠 위험을 각오하고도 역정규화가 필요할 수 있다. \n\n## 역정규화\n\n성능 향상 또는 특정 비즈니스 요구에 맞추기 위해 데이터베이스의 구조를 다시 조정하는 과정을 말한다. 정규화된 데이터를 다시 조합하여 중복 데이터를 허용하는 등의 작업이다.\n\n#### 역정규화가 필요한 이유\n\n1. 성능향상\n\n    과도한 정규화는 여러 테이블에 데이터를 나누어 저장하므로 데이터 추출 또는 조인 시 성능 저하의 우려가 있다. 데이터를 조합하여 빠른 검색과 조인을 가능하게 하여 성능을 향상 시킬 수 있다. \n\n1. 복잡한 쿼리 간소화\n\n    마찬가지로 과도한 정규화는 많은 조인으로 인해 쿼리문이 복잡해진다. 역정규화를 통해 쿼리문의 간소화를 노릴 수 있다. \n\n1. 비즈니스 요구 사항 충족\n\n    보고서 생성 및 분석이나 실시간 대시보드 처럼 데이터를 실시간으로 수집하거나 집계해야 하는 경우 데이터베이스 구조를 최적화 하거나 데이터를 임의로 중복저장 하여 성능향상을 노릴 수 있다. \n\n### 결론\n\n정규화와 역정규화는 서로 장점, 단점을 역으로 공유한다. 따라서 데이터의 무결성이나 성능, 일관성 등 여러가지를 신중하게 검토하여 최적의 전략을 선택해야 한다. \n\n[DB 튜닝 경험](https://sharknia.github.io/DB-튜닝-경험)  \n\n"},{"excerpt":"트래픽 튜닝(traffic tuning)은 네트워크 성능을 최적화하기 위한 방법 중 하나이다. 트래픽 튜닝은 대부분 네트워크의 병목 현상을 최소화하고, 서비스 응답 시간을 개선하며, 사용자의 경험을 향상시키는 데 목표를 둔다. 다음은 트래픽 튜닝과 관련된 몇 가지 주요 측면과 전략이다. 로드 밸런싱 (Load Balancing) 로드 밸런싱은 서비스의 안…","fields":{"slug":"/트래픽-튜닝/"},"frontmatter":{"date":"August 23, 2023","title":"트래픽 튜닝","tags":["Network"]},"rawMarkdownBody":"트래픽 튜닝(traffic tuning)은 네트워크 성능을 최적화하기 위한 방법 중 하나이다.\n\n트래픽 튜닝은 대부분 네트워크의 병목 현상을 최소화하고, 서비스 응답 시간을 개선하며, 사용자의 경험을 향상시키는 데 목표를 둔다. 다음은 트래픽 튜닝과 관련된 몇 가지 주요 측면과 전략이다.\n\n1. **로드 밸런싱 (Load Balancing)**\n\n    로드 밸런싱은 서비스의 안정성을 유지하는 핵심 요소이다. 서비스에 과부하가 걸리지 않도록 여러 서버간에 트래픽을 균등하게 분산시켜 주는 역할을 한다. 이는 서비스의 가용성을 향상시키며, 단일 서버의 과부하를 방지한다.\n\n    로드 밸런서는 일반적으로 클라이언트와 서버 중간에 위치하여 요청을 전달하는 역할을 한다. \n\n    - 로드 밸런싱의 특징 및 이점\n\n    1. 고가용성/장애복구/무정지 업그레이드\n\n        로드 밸런서의 도입으로 서버 중 하나에 문제가 발생해도 다른 서버가 작업을 처리하므로 서비스 중단을 최소화 가능하다. \n\n        또한 특정 서버에 장애가 발생했을 때에도 로드 밸런서가 그 사실을 인지하고 자동으로 정상 작동하는 서버로 트래픽을 전환할 수 있다. \n\n        또, 서버의 유지보수나 업그레이드시 서비스 중단 없이 서버를 순차적으로 점검 또는 업그레이드 할 수 있다. \n\n    1. 확장성\n\n        트래픽이 늘어나면 추가 서버를 쉽게 추가하여 트래픽을 처리할 수 있다. \n\n    1. 균등한 분산\n\n        서버 간의 트래픽이나 작업 부하를 균등하게 분산하여 각 서버의 효율적인 작동을 지원한다. \n\n    - 로드 밸런싱의 주요 유형\n\n    1. DNS 로드 밸런싱\n\n        DNS 쿼리를 기반으로 요청을 다양한 서버 IP 주소로 리디렉션한다. \n\n    1. 레이어 4 로드 밸런싱\n\n        전송 계층(TCP/UDP)에서 작동하며, IP 주소와 포트 정보를 기반으로 트래픽을 분산시킨다. \n\n    1. 레이어 7 로드 밸런싱\n\n        응용 프로그램 계층에서 작동하며 HTTP 헤더, 쿠키, URL 등의 내용을 기반으로 트래픽을 분산시킨다. \n\n    - 로드 밸런싱의 주의 또는 단점\n\n    1. 설정의 복잡성\n\n        올바르게 설정하지 않으면 문제 발생 가능\n\n    1. 단일 장애점\n\n        로드 밸런서 자체가 단일 장애점(SPOF, Single Point Of Failure)이 될 수 있다. 이를 예방하기 위해서 고가용성 구성을 가진 여러 로드 밸런서를 사용해야 한다. \n\n    1. 세션 유지 문제\n\n        로드 밸런서는 상태가 없는(stateless) 방식으로 작동하므로 특정 사용자의 세션을 특정 서버에 고정 시키기 위해 추가적인 설정이 필요\n\n    1. 데이터 동기화 문제\n\n        여러 서버에 걸쳐 데이터가 저장된다면 이를 동기화 하는 것이 중요하다. \n\n    1. 기타\n\n        비용, 지연 시간 등이 추가로 발생할 수 있다. 또한 로드 밸런서는 시스템 중앙에 위치하므로 보안 설정 및 업데이트에 주의를 기울여야한다. \n\n1. **데이터베이스 최적화**\n\n    대형 서비스에서는 많은 양의 데이터가 데이터베이스에 저장되기 때문에, 데이터베이스의 성능은 서비스의 성능에 직접적인 영향을 미친다. 쿼리 최적화, 인덱싱, 캐싱 등의 방법으로 데이터베이스 응답 시간을 최적화하는 것이 중요하다.\n\n    [DB 튜닝 경험](https://sharknia.github.io/DB-튜닝-경험)  \n\n1. **캐싱 (Caching)**\n\n    많은 사용자가 동일한 정보나 페이지에 접근할 가능성이 높다. 캐싱을 통해 빈번하게 접근되는 데이터나 페이지를 빠르게 제공할 수 있으며, 네트워크 트래픽과 데이터베이스 부하를 줄일 수 있다.\n\n1. **데이터 압축**\n\n    대규모 서비스에서는 데이터 전송량이 많을 수 있다. 압축을 통해 네트워크 부하를 줄이고 응답 속도를 향상시킬 수 있다.\n\n1. **로깅 및 모니터링**\n\n    실시간으로 서비스의 상태를 모니터링하고, 문제가 발생할 경우 빠르게 대응하기 위해 로깅 및 모니터링 시스템의 구축이 중요하다.\n\n1. **CDN 사용**\n\n    이미지나 정적 파일과 같은 컨텐츠를 전세계적으로 빠르게 제공하기 위해 CDN 사용을 고려할 수 있다.\n\n    \n\n"},{"excerpt":"파일의 확장자를 으로 변경합니다. 예를 들어, 을 으로 변경합니다. ZIP 압축 해제 도구 (예: WinRAR, 7-Zip 등)를 사용하여 변경된  파일을 엽니다.  파일 내에서  폴더나 해당하는 폴더를 찾아  파일을 찾습니다. 해당  파일을 추출합니다. 이렇게 하면 에서 원하는  파일을 얻을 수 있습니다. 이렇게 하면 의존성, 버전 호환성 등의 문제가 …","fields":{"slug":"/Nuget-패키지-dll-추출/"},"frontmatter":{"date":"August 22, 2023","title":"Nuget 패키지 dll 추출","tags":["ASP.Net"]},"rawMarkdownBody":"1. `.nupkg` 파일의 확장자를 `.zip`으로 변경합니다. 예를 들어, `tiktokensharp.1.0.6.nupkg`을 `tiktokensharp.1.0.6.zip`으로 변경합니다.\n\n1. ZIP 압축 해제 도구 (예: WinRAR, 7-Zip 등)를 사용하여 변경된 `.zip` 파일을 엽니다.\n\n1. `.zip` 파일 내에서 `lib` 폴더나 해당하는 폴더를 찾아 `.dll` 파일을 찾습니다.\n\n1. 해당 `.dll` 파일을 추출합니다.\n\n이렇게 하면 `.nupkg`에서 원하는 `.dll` 파일을 얻을 수 있습니다.\n\n\n\n이렇게 하면 의존성, 버전 호환성 등의 문제가 발생할 수 있고 NuGet 패키지 관리 기능도 사용할 수 없기 때문에 권장하지 않는 방법입니다.\n\n"},{"excerpt":"gh-pages (GitHub Pages) 설명: GitHub Pages는 GitHub 저장소를 기반으로 정적 웹사이트를 무료로 호스팅할 수 있는 서비스입니다. 주로 프로젝트 페이지, 개인 포트폴리오, 단순한 블로그 등을 호스팅하는 데 사용됩니다. Jekyll과 같은 정적 사이트 생성 도구와 함께 사용될 수 있습니다. 비슷한 서비스: GitLab Page…","fields":{"slug":"/무료-웹-호스팅-비교/"},"frontmatter":{"date":"August 22, 2023","title":"무료 웹 호스팅 비교","tags":["Hobby"]},"rawMarkdownBody":"1. **gh-pages (GitHub Pages)**\n\n    - **설명**: GitHub Pages는 GitHub 저장소를 기반으로 정적 웹사이트를 무료로 호스팅할 수 있는 서비스입니다. 주로 프로젝트 페이지, 개인 포트폴리오, 단순한 블로그 등을 호스팅하는 데 사용됩니다. Jekyll과 같은 정적 사이트 생성 도구와 함께 사용될 수 있습니다.\n\n    - **비슷한 서비스**: GitLab Pages, Bitbucket Pages.\n\n1. **Heroku**\n\n    - **설명**: Heroku는 클라우드 플랫폼으로 개발자들이 다양한 프로그래밍 언어로 작성된 애플리케이션을 쉽게 배포, 운영, 스케일링 할 수 있게 해줍니다. 기본적으로 무료 티어를 제공하며, 필요에 따라 리소스를 추가적으로 구매할 수 있습니다.\n\n    - **비슷한 서비스**: Google Cloud Run, AWS Elastic Beanstalk, Microsoft Azure App Service.\n\n1. **Vercel**\n\n    - **설명**: Vercel은 주로 프론트엔드와 서버리스 함수를 위한 배포 및 호스팅 플랫폼입니다. Next.js, Gatsby와 같은 프론트엔드 프레임워크에 특화되어 있지만, 다른 프로젝트에도 사용될 수 있습니다. 푸시를 할 때마다 자동으로 배포되는 Continuous Deployment를 지원합니다.\n\n    - **비슷한 서비스**: Netlify, Surge.\n\n### Vercel, Netlify, Surge에 대해 더 알아보자. \n\n1. **Vercel**\n\n    - Next.js의 개발팀에 의해 만들어진 서비스로, 특히 Next.js와의 통합이 매우 간편함.\n\n    - 서버리스 함수의 지원.\n\n    - 자동화된 Continuous Deployment (CD).\n\n    - 무료 플랜과 함께 커스텀 도메인 연결 지원.\n\n    - 최적의 사용 사례: Next.js를 사용한 프로젝트, React 프로젝트, 서버리스 함수가 필요한 프로젝트.\n\n1. **Netlify**\n\n    - 다양한 정적 사이트 생성기와의 통합 (Gatsby, Hugo, Jekyll 등).\n\n    - 서버리스 함수 지원.\n\n    - 자동화된 Continuous Deployment (CD)와 Git 리포지토리 연동.\n\n    - Netlify Identity, Netlify CMS 등의 추가 기능을 제공.\n\n    - 무료 플랜에도 커스텀 도메인 연결 지원.\n\n    - 분할 A/B 테스팅, 인증 및 더 많은 기능 제공.\n\n    - 최적의 사용 사례: 다양한 정적 사이트 생성 도구를 사용하는 프로젝트, 서버리스 함수가 필요한 프로젝트, 웹사이트에 추가 기능이 필요한 경우.\n\n1. **Surge**\n\n    - 매우 간단하고 빠른 정적 웹사이트 배포.\n\n    - CLI를 통한 간단한 배포 경험.\n\n    - 무료 플랜에서 커스텀 도메인 연결 가능.\n\n    - HTTPS 자동 지원.\n\n    - 최적의 사용 사례: 빠르게 정적 사이트를 배포하고자 하는 프로젝트.\n\n### 요약\n\n- Vercel은 특히 Next.js와 함께 사용하기에 최적화된 서비스입니다.\n\n- Netlify는 다양한 정적 사이트 생성 도구와의 호환성과 다양한 추가 기능을 통해 풍부한 개발 경험을 제공합니다.\n\n- Surge는 정적 웹사이트를 빠르고 간단하게 배포하는데 초점을 맞춘 서비스입니다.\n각 프로젝트의 요구 사항에 따라 적합한 플랫폼을 선택할 수 있습니다.\n\n"},{"excerpt":"웹 사이트 프로젝트 특징 웹 사이트 프로젝트는 파일 시스템을 기반으로 하는 프로젝트 유형입니다. 웹 애플리케이션의 파일들이 프로젝트 디렉토리 내에 그대로 위치하며, ASP.NET 컴파일러에 의해 실시간으로 컴파일됩니다. 프로젝트 파일이 없고, 각 파일은 개별적으로 관리됩니다. 이로 인해 각 파일의 수정이 간단하며, 특히 작은 프로젝트나 신속한 개발에 유리…","fields":{"slug":"/웹-사이트-프로젝트-vs-웹-응용-프로그램-프로젝트/"},"frontmatter":{"date":"August 21, 2023","title":"웹 사이트 프로젝트 vs 웹 응용 프로그램 프로젝트","tags":["ASP.Net"]},"rawMarkdownBody":"## 웹 사이트 프로젝트\n\n### 특징\n\n1. 웹 사이트 프로젝트는 파일 시스템을 기반으로 하는 프로젝트 유형입니다. 웹 애플리케이션의 파일들이 프로젝트 디렉토리 내에 그대로 위치하며, [ASP.NET](http://asp.net/) 컴파일러에 의해 실시간으로 컴파일됩니다.\n\n1. 프로젝트 파일이 없고, 각 파일은 개별적으로 관리됩니다. 이로 인해 각 파일의 수정이 간단하며, 특히 작은 프로젝트나 신속한 개발에 유리합니다.\n\n1. 런타임 시 컴파일되므로 변경된 파일만 다시 컴파일되고 재시작하지 않아도 됩니다.\n\n1. 소스 코드 파일이 웹 서버에 그대로 배포되므로, 보안 측면에서 주의가 필요합니다.\n\n1. 개발 및 디버깅이 상대적으로 더 간단합니다.\n\n### 장단점\n\n#### 장점\n\n- 신속한 개발과 테스트가 가능합니다.\n\n- 작은 규모의 프로젝트나 간단한 웹 사이트에 적합합니다.\n\n- 파일 단위로 변경 및 배포가 가능하여 유연성이 높습니다.\n\n#### 단점:\n\n- 대규모 프로젝트에서는 파일의 분산 관리 및 성능 문제가 발생할 수 있습니다.\n\n- 보안 취약성이 있을 수 있으며, 코드를 직접 노출시키기 때문에 보안에 더욱 신경을 써야 합니다.\n\n## 웹 응용 프로그램 프로젝트\n\n### 특징\n\n1. 웹 응용 프로그램 프로젝트는 Visual Studio 솔루션 파일에 의해 관리되며, 컴파일된 어셈블리로 배포됩니다.\n\n1. 컴파일된 어셈블리 파일은 웹 서버에 배치되므로, 소스 코드가 직접 노출되지 않습니다.\n\n1. 코드 비하인드 파일과 레이어 분리가 쉽게 가능하며, 따라서 대규모 및 복잡한 프로젝트에 적합합니다.\n\n1. 런타임 이전에 컴파일되므로, 타입 오류 등을 컴파일 단계에서 미리 확인할 수 있습니다.\n\n### 장단점\n\n#### 장점:\n\n- 대규모 및 복잡한 프로젝트에 적합하며, 레이어 구조와 코드 분리를 지원하여 유지 보수가 용이합니다.\n\n- 컴파일된 어셈블리의 배포로 인해 보안 측면에서 더욱 안전합니다.\n\n#### 단점:\n\n- 컴파일 단계가 추가되므로 개발 및 디버깅이 웹 사이트 프로젝트에 비해 상대적으로 복잡할 수 있습니다.\n\n- 프로젝트 설정이 복잡하거나 변경되어야 할 때 번거로울 수 있습니다.\n\n\n\n"},{"excerpt":"아주 마음에 드는 오픈소스로, 다만 내 취향에 맞게 짜잘짜잘 임의로 몇 가지 부분을 수정했다.  Tags 정렬 등록한 태그들이 블로그 좌측에 나열되는데, 이를 이름 순서대로 나오게 정렬했다.  자동썸네일 기능 테스트 src\\routes\\Detail\\PostDetail\\PostHeader.tsx 에서 썸네일을 가져오고 있고,  src\\pages[slug].…","fields":{"slug":"/MORETHAN-LOG-수정/"},"frontmatter":{"date":"August 20, 2023","title":"MORETHAN-LOG 수정","tags":["Blogging","Hobby"]},"rawMarkdownBody":"아주 마음에 드는 오픈소스로, 다만 내 취향에 맞게 짜잘짜잘 임의로 몇 가지 부분을 수정했다. \n\n1. Tags 정렬\n\n    등록한 태그들이 블로그 좌측에 나열되는데, 이를 이름 순서대로 나오게 정렬했다. \n\n    ```typescript\n    // itemObj를 item name으로 정렬\n      const sortedItemObj = Object.entries(itemObj)\n        .sort((a, b) => a[0].localeCompare(b[0]))\n        .reduce((acc, [key, val]) => {\n          acc[key] = val\n          return acc\n        }, {} as { [itemName: string]: number })\n    \n      return sortedItemObj\n    ```\n\n    \n\n1. 자동썸네일 기능 테스트\n\n    src\\routes\\Detail\\PostDetail\\PostHeader.tsx 에서 썸네일을 가져오고 있고, \n\n    src\\pages\\[slug].tsx 에서 ogImageGenerateURL을 이용한 부분이 있다. \n\n    다만 ogImageGenerateURL을 이용한 부분은 meta에만 적용되고 있고, 자동 썸네일 용 [https://og-image-korean.vercel.app](https://og-image-korean.vercel.app/) 는 만약 사용을 위해서라면 내 이미지를 활용한것으로 수정해야 할 것 같다. \n\n"},{"excerpt":"모 회사 과제 때문에 NestJS를 설치해 볼 일이 생겨 기록해둔다.  Node.js 설치 NestJS는 Node.js를 기반으로 한다. Node.js에서 맞는 버전을 설치한다.  NestJS 프로젝트 생성 Node.js에 npm이 포함되어있다. NestJS 서버를 구성하기 위해서는 @nestjs/cli 를 설치해야 한다.  원하는 디렉토리로 이동해서 프…","fields":{"slug":"/NestJS-설치/"},"frontmatter":{"date":"August 20, 2023","title":"NestJS 설치","tags":["NestJS"]},"rawMarkdownBody":"모 회사 과제 때문에 NestJS를 설치해 볼 일이 생겨 기록해둔다. \n\n#### Node.js 설치\n\nNestJS는 Node.js를 기반으로 한다. [Node.js](https://nodejs.org/ko/download)에서 맞는 버전을 설치한다. \n\n#### NestJS 프로젝트 생성\n\nNode.js에 npm이 포함되어있다. NestJS 서버를 구성하기 위해서는 @nestjs/cli 를 설치해야 한다. \n\n```bash\nnpm i -g @nestjs/cli\n```\n\n\n\n원하는 디렉토리로 이동해서 프로젝트를 초기화한다. \n\n```bash\nnest new <project-Name>\n```\n\n<project-Name> 에는 원하는 프로젝트 이름을 입력한다. 패키지 매니저를 선택할 수 있는데, npm을 선택했다. \n\n#### NestJS 프로젝트 구동\n\nnpm run start 명령어로 구동할 수 있지만, \n\nNestJS에는 내부적으로 내부적으로 [**webpack**](https://webpack.js.org/)과 함께 작동하는 [`@nestjs/cli`](https://docs.nestjs.com/cli/overview)를 제공한다. \n\n이 CLI 도구는 소스 코드의 변경을 감지하고 자동으로 애플리케이션을 재시작하는 기능을 포함하고 있다. \n\n개발 모드로 애플리케이션을 실행하려면 다음 명령어를 사용하면 된다. \n\n```bash\nnpm run start:dev\n```\n\n\n\n이제 localhost:3000 에서 서버가 실행되었음을 확인할 수 있다. \n\n\n\n"},{"excerpt":"NestJS는 MVC 패턴을 지원하지만, 전통적인 Express.js 스타일의 MVC 구조와는 약간 차이가 있다.  NestJS의 기본적인 디렉토리 구조는 다음과 같다.  NestJS에서는 각 기능별로 (예: 사용자, 포스트, 댓글 등) 모듈을 분리하는 것을 권장한다. 따라서 각 기능별로 모듈을 생성하고, 해당 모듈 내에서 MVC 구조를 구성하는 것이 더…","fields":{"slug":"/NestJS의-디렉토리-구조/"},"frontmatter":{"date":"August 20, 2023","title":"NestJS의 디렉토리 구조","tags":["NestJS"]},"rawMarkdownBody":"NestJS는 MVC 패턴을 지원하지만, 전통적인 Express.js 스타일의 MVC 구조와는 약간 차이가 있다. \n\nNestJS의 기본적인 디렉토리 구조는 다음과 같다. \n\n```lua\nsrc/\n|-- app.module.ts         // 앱의 주 모듈\n|-- main.ts               // 앱의 진입점\n|-- controllers/          // 컨트롤러 폴더\n|   |-- app.controller.ts\n|-- services/             // 서비스 폴더\n|   |-- app.service.ts\n```\n\nNestJS에서는 각 기능별로 (예: 사용자, 포스트, 댓글 등) 모듈을 분리하는 것을 권장한다. 따라서 각 기능별로 모듈을 생성하고, 해당 모듈 내에서 MVC 구조를 구성하는 것이 더 일반적이다.\n\n```lua\nsrc/\n|-- app.module.ts\n|-- main.ts\n|-- user/\n|   |-- user.module.ts\n|   |-- user.controller.ts\n|   |-- user.service.ts\n|   |-- user.model.ts (또는 user.entity.ts)\n|-- post/\n|   |-- post.module.ts\n|   |-- post.controller.ts\n|   |-- post.service.ts\n|   |-- post.model.ts (또는 post.entity.ts)\n```\n\n#### MVC 패턴과의 비교\n\n- NestJS의 장점:\n\n    1. 모듈화: NestJS는 기능별로 코드를 모듈로 분리하고 관리하도록 설계되었다. 이로 인해 각 기능이 독립적으로 개발되고 테스트될 수 있으며, 코드 재사용성도 향상된다.\n\n    1. 의존성 주입: NestJS는 내장된 의존성 주입 컨테이너를 제공하므로, 코드의 결합도를 낮추고 유닛 테스트를 쉽게 할 수 있다.\n\n    1. 타입 안전성: TypeScript를 기본 언어로 사용하기 때문에, 타입 검사와 관련된 오류를 컴파일 시점에 발견할 수 있다.\n\n    1. 데코레이터 기반: 데코레이터를 사용하여 메타데이터를 기반으로 하는 다양한 기능을 쉽게 추가할 수 있다.\n\n    1. 플랫폼 독립성: NestJS는 기본적으로 Express.js를 사용하지만, Fastify와 같은 다른 HTTP 서버 라이브러리로 쉽게 전환할 수 있다.\n\n- NestJS의 단점:\n\n    1. 학습 곡선: NestJS에는 다양한 개념과 기능이 있으므로 초기에 학습하기가 다소 어려울 수 있다.\n\n    1. 추가적인 추상화: 모듈화와 의존성 주입과 같은 기능들은 유용하지만, 간단한 애플리케이션에서는 오버헤드로 느껴질 수 있다.\n\n- MVC 패턴의 장점:\n\n    1. 간단함: 전통적인 MVC 패턴은 많은 웹 개발자들에게 잘 알려져 있으므로 학습 곡선이 낮다.\n\n    1. 직관성: 모델, 뷰, 컨트롤러의 구조는 웹 애플리케이션의 데이터 흐름을 이해하기 쉽다.\n\n- MVC 패턴의 단점:\n\n    1. 확장성: 애플리케이션의 복잡도가 증가하면 MVC 구조만으로는 코드의 복잡성을 관리하기 어려울 수 있다.\n\n    1. 타이트 커플링: 모델, 뷰, 컨트롤러 간에 타이트 커플링이 발생할 수 있어, 변경 사항이 여러 컴포넌트에 영향을 줄 수 있다.\n\n"},{"excerpt":"var와 let, 그리고 const는 JavaScript에서 변수를 선언할 때 사용되는 키워드이다. 그러나 둘 사이엔 중요한 차이점들이 있으며, 이를 명확히 이해하고 사용해야 한다.  let과 const는 ES6에서 도입된 키워드로, ES6 이전에 변수를 선언하는 유일한 방법은 var 사용 뿐이었다.  var는  블록범위를 지원하지 않는다.  Hoisti…","fields":{"slug":"/var와-letconst/"},"frontmatter":{"date":"August 19, 2023","title":"var와 let,const","tags":["Javascript"]},"rawMarkdownBody":"var와 let, 그리고 const는 JavaScript에서 변수를 선언할 때 사용되는 키워드이다. 그러나 둘 사이엔 중요한 차이점들이 있으며, 이를 명확히 이해하고 사용해야 한다. \n\n\n\nlet과 const는 ES6에서 도입된 키워드로, ES6 이전에 변수를 선언하는 유일한 방법은 var 사용 뿐이었다. \n\nvar는 \n\n- 블록범위를 지원하지 않는다. \n\n- Hoisting되어 해당 범위의 맨 위로 이동하게 되어 변수 선언 전에 변수를 사용해도 오류가 발생하지 않는다. \n\n- 동일한 범위 내에서 같은 이름의 변수를 여러 번 선언할 수 있다. \n\n이러한 문제는 개발자가 의도하지 않은 오류를 발생시킬 가능성이 높았기 때문에 ES6에서 let과 const이 등장하게 되었다. \n\nlet과 const는 var와 비교해서 아래와 같은 특성을 가진다. \n\n1. Scope\n\n    블록범위를 지원한다. 가장 가까운 중괄호 내에서만 변수가 유효하다. \n\n1. Hoisting\n\n    선언 자체는 호이스팅 되지만, 선언 전에 접근하려면 오류가 발생한다. 이를 일시적 죽은 영역(Temporal Dead Zone, TDZ)이라고 한다. \n\n1. 재선언\n\n    동일한 범위 내에서 변수를 재선언 할 수 없다. \n\n    다만, let은 재할당이 가능하지만 const는 재할당도 할 수 없다. (상수 선언)\n\n    따라서 let은 초기 선언 시 초기 값을 제공하지 않을 수 있지만, const는 초기 선언시에 반드시 초기값을 제공해야 한다. \n\n\n\n언뜻 보면 마구 쓰기에 var 선언이 더 편하다고 생각할 수 있다. 하지만 인간은 늘 실수를 하기 때문에, 특히 코드가 길어지고 복잡해질수록 이런 제한 조건이 더욱 필요해지는 것 같다. \n\n원래 브라우저에서만 쓰이려고 태어난 JavaScript 언어가 브라우저의 역할이 늘어나고, 심지어 단순히 브라우저에서만 쓰이는 걸 넘어서 다양한 플랫폼과 환경에서 더 많은 역할을 하게 되면서 이런 것들이 생기는 것 같다. 이러한 맥락에서 ES6 → TypeScript가 생긴게 아닐까?\n\n\n\n"},{"excerpt":"DB  처음에는 단순히 코드 짜는것에만 집중했었는데, 어느 순간 단순한 에러 고침 또는 간단한 로직 수정을 넘어서 더 큰 의미의 유지보수 또는 고도화를 겪으면서 DB 튜닝이 속도에 큰 영향을 미침을 인지하고, 더 빠른 페이지를 만드는것에 집중했다. 그때 아래와 같은 것들을 시도하였다. 인덱싱 : 자주 사용하는, 자주 불려오는 테이블들, 자주 사용되는 컬럼…","fields":{"slug":"/DB-튜닝-경험/"},"frontmatter":{"date":"August 18, 2023","title":"DB 튜닝 경험","tags":["DataBase"]},"rawMarkdownBody":"DB \n\n처음에는 단순히 코드 짜는것에만 집중했었는데, 어느 순간 단순한 에러 고침 또는 간단한 로직 수정을 넘어서 더 큰 의미의 유지보수 또는 고도화를 겪으면서 DB 튜닝이 속도에 큰 영향을 미침을 인지하고, 더 빠른 페이지를 만드는것에 집중했다.\n\n\n\n그때 아래와 같은 것들을 시도하였다.\n\n\n\n- 인덱싱 : 자주 사용하는, 자주 불려오는 테이블들, 자주 사용되는 컬럼에 대해 인덱싱\n\n    - 인덱싱이란?\n\n        - 데이터를 빠르게 조회하기 위한 자료구조(B-tree 등)\n\n        - 색인과 유사한 개념\n\n        - 특정행을 찾기 위한 경로를 미리 만들어 두는 것과 같아 검색 시간을 크게 줄일 수 있다. \n\n    - 인덱싱 사용할 때 주의할 점\n\n        - 추가적인 공간을 사용하므로 무작정 늘려서는 안된다. 가장 자주 조회되는 \n\n        - 복합 인덱스 생성시에는 가장 많이 쿼리되는 열을 인덱스의 앞부분에 둔다. \n\n        - 인덱스는 쓰기 연산(Insert, Update, Delete)시에 유지 비용이 발생(쉽게 생각하면 색인을 새로 만들어야 하므로)하므로 연산이 빈번하게 일어나는 테이블에 대해 많은 인덱스를 만들면 성능 저하가 일어날 수 있음 \n\n- 쿼리 최적화 \n\n    - 불필요한 컬럼 제거(select *(스타) 지양)\n\n    - 서브쿼리 대신 조인 사용\n\n    - 데이터 타입을 최적화 하기 위해 노력(적절한 크기 지정, 숫자형이나 날짜형 형식을 주의해서 용도에 맞게 사용, 예를 들면 idx로 사용할건데 또는 학번에는 숫자만 사용되는데 varchar형식으로 선언되는 경우도 있었는데 그런 경우를 피하기 위해 노력)\n\n- 정규화와 반정규화\n\n    - 회사의 테이블들이 중복된 데이터들이 저장되어있는 경우가 많았다. 따라서 데이터의 무결성을 해치거나 해치지 않더라도 유지보수에 어려움을 야기하는 경우가 자주 발생 → 이런것들을 최대한 피하고 싶어서 직접 설계한 스키마는 이런것들을 최대한 피하고자 했다. 중복 데이터를 피하기 위한 정규화를 진행.\n\n    - 정규화를 빈틈없이 했더니 테이블이 많이 쪼개지고, 중복된 데이터들이 하나도 없어서 조인을 여러 테이블을 해야 하는 경우가 많았다. 정규화를 한 경우에는 반드시 자체 협업툴에 스키마에 대한 문서를 남겼다(정규화가 덜 된 테이블에 비해 상대적으로 알아보기 어려우므로).  또한, 정규화된 테이블들을 이쁘게 조인해서 뷰로 만들어서 문서로 함께 제공했다. \n\n    - 소수의 데이터만 필요한데 조인을 너무 많이 해야 하는 경우나, 특정 집계 결과가 필요한 경우 또는 학생의 역량별 성적같이 조회가 많은데 잦은 계산이 필요한 경우 에는 반정규화를 하기도 했다. 반정규화를 한 경우에는, 테이블 관리(update, insert, edit등)하는 코드를 가능한 한 한군데로 모으고 자체 협업툴에 문서를 자세히 적어서 다른 개발자가 유지보수 시 최대한 알 수 있도록 했다. \n\n    \n\n\n\n"},{"excerpt":"개발자로서 어엿한 블로그 하나는 있어야 하지 않을까, 생각을 항상 하곤 했다.   하지만 블로그라는게 여간 귀찮은 일이 아니다.  공부한 내용을 노션에 적어두기 시작한지 좀 됐는데, 티스토리나 velog나 여러 방법을 시도한적이 있었지만 노션에 적은 내용을 거기로 옮기는게 얼마나 귀찮은지, 시작은 해도 도무지 꾸준히 이어할 수가 없었다.  그냥 언젠가 노…","fields":{"slug":"/MORETHAN-LOG-설치/"},"frontmatter":{"date":"August 18, 2023","title":"MORETHAN-LOG 설치","tags":["Blogging","Hobby"]},"rawMarkdownBody":"개발자로서 어엿한 블로그 하나는 있어야 하지 않을까, 생각을 항상 하곤 했다.  \n\n하지만 블로그라는게 여간 귀찮은 일이 아니다.  공부한 내용을 노션에 적어두기 시작한지 좀 됐는데, 티스토리나 velog나 여러 방법을 시도한적이 있었지만 노션에 적은 내용을 거기로 옮기는게 얼마나 귀찮은지, 시작은 해도 도무지 꾸준히 이어할 수가 없었다. \n\n그냥 언젠가 노션에 쓰기만 하면 블로그가 되는게 없나 그냥 가끔 검색만 하는 정도였는데, 어느샌가 나온 것이다. 그런 천지개벽할 물건이. \n\n\n\n![](image1.png)\n커밋할 필요도, 포스트 할 필요도 없다. 단지 노션에 글을 적으면 바로 포스팅 된다. \n\n게다가 Readme에 설치방법이 10단계로 스크린샷까지 동봉되어서 너무나 친절하게 나와있다. \n\n[https://github.com/morethanmin/morethan-log#readme](https://github.com/morethanmin/morethan-log#readme)\n\n\n\n블로그로 대성하겠다, 이런 마음가짐은 없기 때문에 심플한 구성과 간단한 기능이 너무 마음에 들었다. 누구나 따라할 수 있을 만큼 쉽게 되어있기 때문에 그대로 그대로 따라하는데,\n\n```javascript\nFailed to compile.\n./src/routes/Detail/PostDetail/CommentBox/Utterances.tsx:28:11\nType error: Type '{ repo: string | undefined; \"issue-term\": string; label: string; }' is not assignable to type '{ [key: string]: string; }'.\n  Property 'repo' is incompatible with index signature.\n    Type 'string | undefined' is not assignable to type 'string'.\n      Type 'undefined' is not assignable to type 'string'.\n  26 |     script.setAttribute(\"issue-term\", issueTerm)\n  27 |     script.setAttribute(\"theme\", theme)\n> 28 |     const config: { [key: string]: string } = CONFIG.utterances.config\n     |           ^\n  29 |     Object.keys(config).forEach((key) => {\n  30 |       script.setAttribute(key, config[key])\n  31 |     })\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nError: Command \"yarn run build\" exited with 1\n```\n\n글쎄 대체 나만 Deploy가 안되는것이다. \n\nISSUE에 나와 같은 오류를 겪은 외국인이 한 명 더 있었는데, 다른 플랫폼으로 갈아탄 모양인지 해결방법은 적혀있지 않았다. \n\n타입스크립트, 리액트는 처음이었기 때문에 정확히 알 수 없었는데, site.config.js 의 utterances-config 의 값을 가져올 때 오류가 나는 것이라고 짐작을 할 수 있었기 때문에 해당 부분을 챗지피티의 도움을 받아 코드를 수정했다. \n\n```typescript\nconst config: { [key: string]: string } = Object.entries(CONFIG.utterances.config)\n  .filter(([_, value]) => value !== undefined)\n  .reduce((acc, [key, value]) => ({ ...acc, [key]: value }), {});\n```\n\n이렇게 하여, 무사히 deploy를 마치고 블로그 포스팅에 성공한것이다. \n\n\n\n문제는 여기서 끝나지 않았는데 노션에 글을 써도 도무지 블로그에 새로운 글이 포스팅되지 않았다. \n\n이것 때문에 잠도 못자고 설쳐가면서 새벽에 새로고침을 해보기도 했는데 자고 일어나도 되어있지 않았다. \n\n\n\n결론부터 이야기하자면, 아무것도 오류는 아니었다. \n\n[https://vercel.com/docs/functions/serverless-functions](https://vercel.com/docs/functions/serverless-functions)\n\nvercel의 Serverless Functions 기능은 모든 plan에서 이용가능하며, 이 기능을 이용해서 Notion API를 활용해서 글 내용을 불러오도록 되어있었다. \n\nsite.config.js를 확인하면 revalidateTime 변수가 있는데 처음에는 이 값이 42시간(!)으로 설정되어있었으므로, 이를 임의로 1시간으로 수정해주었다. \n\n성격이 급해서 1시간 단위로 갱신되게 해두었는데, 지금 와서 생각해보면 굳이 1시간일 필요도 없다는 생각이 든다. \n\n"},{"excerpt":"RDB(관계형 데이터베이스)란? RDB(Relational Database) Oracle, MySQL 둘 다 관계형 데이터베이스이다.  모든 데이터를 2차원의 테이블 형태로 표현한다.  데이터의 독립성이 높고 고수준의 데이터 조작 언어를 사용하여 결합, 제약, 투영등의 관계 조작에 의해 비약적으로 표현 능력을 높일 수 있다.  관계 조작에 의해 자유롭게 …","fields":{"slug":"/RDB관계형-데이터베이스-RDBMS/"},"frontmatter":{"date":"August 18, 2023","title":"RDB(관계형 데이터베이스) + RDBMS","tags":["DataBase"]},"rawMarkdownBody":"1. RDB(관계형 데이터베이스)란?\n\n    - RDB(Relational Database)\n\n    - Oracle, MySQL 둘 다 관계형 데이터베이스이다. \n\n    - 모든 데이터를 2차원의 테이블 형태로 표현한다. \n\n    - 데이터의 독립성이 높고 고수준의 데이터 조작 언어를 사용하여 결합, 제약, 투영등의 관계 조작에 의해 비약적으로 표현 능력을 높일 수 있다. \n\n    - 관계 조작에 의해 자유롭게 구조를 변경할 수 있다. \n\n1. RDBMS란?\n\n    - RDBMS(Relational Database Management System)\n\n        - R\n\n            관계형은 DBMS의 특정한 종류를 의미하고, 여러개의 테이블을 조합해 원하는 데이터를 찾아올 수 있게한다. 보통 한 개의 테이블로 답을 얻을 수 없는 상황에서 이 관계성을 사용해 더 복잡한 요구를 실현할 수 있다. \n\n            관계형을 지원하기 위해 트랜잭션, ACID 등의 개념이 도입되었다. \n\n        - DB\n\n            일종의 저장소(Storage)\n\n            정보를 단순하고 규칙적인 모양새로 구성한 저장소\n\n        - MS\n\n            관리 시스템은은 레코드들을 삽입, 탐색, 수정, 삭제 할 수 있도록 해주는 소프트웨어를 지칭\n\n            데이터를 처리할 수 있는 기능을 의미함\n\n    - 관계형 데이터베이스를 생성하고 수정/관리할 수 있는 소프트웨어\n\n    - 모든 데이터를 2차원 테이블로 표현\n\n    - 테이블은 Row, Column으로 이루어진 기본 데이터 저장 단위\n\n    - 상호 관련성을 가진 테이블의 집합\n\n    - 만들거나 이용하기도 비교적 쉽지만, 확장이 가장 용이함\n\n    - 데이터베이스의 설계도를 ER(Entity Relationship) 모델로 나타낼 수 있다. \n\n    - ER모델에 따라 데이터베이스가 만들어지며, 데이터베이스는 하나 이상의 테이블로 구성된다. \n\n    - ER 모델에서 엔티티를 기반으로 테이블이 만들어진다. \n\n1. JOIN\n\n    - INNER JOIN\n\n        교집합\n\n    - OUTER JOIN\n\n        특정 테이블 기준으로 데이터를 보여줌\n\n        ex) Left Outer Join : 왼쪽 테이블 A의 모든 데이터와 A와 B 테이블의 중복 데이터가 검색됨\n\n"},{"excerpt":"Nest.js란? Node.js 서버측 애플리케이션을 구축하기 위한 프레임워크 TypeScript 기반으로 구축되어 완벽하게 지원 Express를 기본으로 채택하고 그 위에 여러 기능을 미리 구현한 것이 NestJS Nest.js의 특징 확장 가능하며 유지 관리가 쉬운 애플리케이션을 개발할 수 있다 TypeScript, OOP, FP, FRP 요소의 결합…","fields":{"slug":"/NestJS-소개/"},"frontmatter":{"date":"January 25, 2023","title":"NestJS 소개","tags":["NestJS"]},"rawMarkdownBody":"1. Nest.js란?\n\n    Node.js 서버측 애플리케이션을 구축하기 위한 프레임워크\n\n    TypeScript 기반으로 구축되어 완벽하게 지원\n\n    Express를 기본으로 채택하고 그 위에 여러 기능을 미리 구현한 것이 NestJS\n\n1. Nest.js의 특징\n\n    - 확장 가능하며 유지 관리가 쉬운 애플리케이션을 개발할 수 있다\n\n    - TypeScript, OOP, FP, FRP 요소의 결합\n\n    - 타입스크립트 적극 도입\n\n    - 모듈로 감싸는 형태로 테스트 코드의 작성이 용이함\n\n    - 모듈을 사용하여 확장이 용이함\n\n    - 스프링과 유사한 경험을 제공한다\n\n1. Express\n\n    1. Express 특징\n\n        - 전 세계 node.js 프레임워크 1위, 개발 규칙이 강제되어 코드 및 구조의 통일성이 향상됨\n\n        - 레퍼런스가 많다\n\n        - 구조에 대한 자유도가 높다\n\n    1. 차이점\n\n        - NestJS는 타입스크립트를 적극 채용하여 json 파일을 만들고 세팅하는 과정이 복잡하지 않다. \n\n        - NestJS는 아키텍쳐 정의가 프레임워크에서 제공되므로 각 개발자들의 아키텍쳐가 통일되어있다. (장점이자 단점)\n\n        - 라우팅의 차이\n\n            Express는 라우팅 할 때 app.use 처럼 등록해서 사용하지만 NestJS는 모듈별로 나누어서 라우팅한다. \n\n        - 컨트롤러\n\n            NestJS는 클래스별로 컨트롤러에서 메소드에 데코레이터를 사용한다.\n\n        - 서비스\n\n            NestJS는 create 함수 인자로 인터페이스로 정한 값들이 들어오고 return 타입을 공통 coreOutput이라는 인터페이스로 정한 ok, message로 리턴한다. \n\n        \n\n"},{"excerpt":"클로저란? 자신을 포함하고 있는 외부함수보다 내부함수가 더 오래 유지되는 경우, 외부 함수 밖에서 내부함수가 호출되더라도 외부함수의 지역 변수에 접근할 수 있는데 이러한 함수를 클로저라고 부른다.  클로저는 반환된 내부함수가 자신이 선언됐을 때의 환경인 스코프를 기억하여 자신이 선언됐을 때의 환경밖에서 호출되어도 그 환경에 접근할 수 있는 함수를 말한다.…","fields":{"slug":"/JavaScript-클로저/"},"frontmatter":{"date":"January 21, 2023","title":"JavaScript 클로저","tags":["Javascript"]},"rawMarkdownBody":"1. 클로저란?\n\n    자신을 포함하고 있는 외부함수보다 내부함수가 더 오래 유지되는 경우, 외부 함수 밖에서 내부함수가 호출되더라도 외부함수의 지역 변수에 접근할 수 있는데 이러한 함수를 클로저라고 부른다. \n\n    클로저는 반환된 내부함수가 자신이 선언됐을 때의 환경인 스코프를 기억하여 자신이 선언됐을 때의 환경밖에서 호출되어도 그 환경에 접근할 수 있는 함수를 말한다.\n\n    간단히 말하면 클로저는 자신이 생성될 때의 환경을 기억하는 함수 라고 말할 수 있다. \n\n    내부 함수가 클로저이며, 외부함수는 자유변수라고 부른다. 클로저라는 이름은 자유변수에 함수가 닫혀있다는 의미로 의역하면 자유변수에 엮여있는 함수라는 뜻이다. \n\n1. 클로저의 활용\n\n    1. 상태 유지\n\n        클로저가 가장 유용하게 사용되는 상황은 현재 상태를 기억하고 변경된 최신 상태를 유지하는 것이다. \n\n        클로저가 아니고 전역변수를 사용할 수 있지만 전역 변수는 언제든지 누구나 접근할 수 있고 변경할 수 있기 때문에 많은 부작용을 유발하므로 사용하지 않는 것이 좋다. \n\n    1. 전역 변수 사용의 억제\n\n        변수의 값은 누군가에 의해 언제든지 변경될 수 있어 오류 발생의 근본적인 원인이 될 수 있다. 상태 변경이나 가변 데이터를 피하고 불변성을 유지향하는 함수형 프로그래밍에서 사이드 이펙트를 최대한 억제하여 오류를 피하고 프로그램의 안정성을 높이기 위해 클로저를 사용한다. \n\n    1. 정보의 은닉\n\n        자신이 생성됐을 때의 렉시컬 환경인 생성자 함수의 변수에 접근 가능한 클로저의 특징을 사용해 private 키워드를 흉내낼 수 있다. \n\n"},{"excerpt":"타입변환이란? 자바스크립트의 모든 값은 타입이 있다. 값의 타입은 다른 타입으로 개발자에 의해 의도적으로 변환할 수 있다. 또는 자바스크립트 엔진에 의해 암묵적으로 자동 변환될 수 있다. 개발자가 의도적으로 타입을 변경하는 것을 명시적 타입 변환(Explict coercion) 또는 타입 캐스팅(Type casting) 이라고 한다.  자바스크립트 엔진이…","fields":{"slug":"/JavaScript의-타입-변환과-단축-평가/"},"frontmatter":{"date":"January 10, 2023","title":"JavaScript의 타입 변환과 단축 평가","tags":["Javascript"]},"rawMarkdownBody":"1. 타입변환이란?\n\n    자바스크립트의 모든 값은 타입이 있다. 값의 타입은 다른 타입으로 개발자에 의해 의도적으로 변환할 수 있다. 또는 자바스크립트 엔진에 의해 암묵적으로 자동 변환될 수 있다.\n\n    개발자가 의도적으로 타입을 변경하는 것을 명시적 타입 변환(Explict coercion) 또는 타입 캐스팅(Type casting) 이라고 한다. \n\n    자바스크립트 엔진이 타입을 자동 변환 하는 것을 암묵적 타입 변환(Implict coercion) 또는 타입 강제 변환(Type coercion)이라고 한다. \n\n    타입변환이 값을 직접 변경하는 것은 아니다. 변수 값을 변경하려면 재할당을 통해 새로운 메모리 공간을 확보하고 그 곳에 원시 값을 저장한 후 변수명이 재할당된 원시값이 저장된 메모리 공간의 주소를 기억하도록 해야 한다. \n\n    압묵적 타입 변환은 변수 값을 재할당하는 것이 아니라 자바스크립트 엔진이 표현식을 에러없이 평가하기 위해 기존 값을 바탕으로 새로운 타입의 값을 만들어 단 한 번 사용하고 버린다. \n\n1. 암묵적 타입 변환\n\n    자바스크립트 엔진은 표현식을 평가할 때에 컨텍스트를 고려하여 가급적 에러를 발생시키지 않도록 암묵적으로 타입 변환을 실시한다. \n\n    - boolean 타입으로 변환\n\n        자바스크립트 엔진은 다음과 같은 값을 false로 평가한다. 아래의 값을 제외한 값은 모두 true로 판단한다. \n\n        - false\n\n        - undefined\n\n        - null\n\n        - 0, -0\n\n        - NaN\n\n        - ‘’(빈 문자열)\n\n1. 명시적 타입 변환\n\n    암묵적 타입 변환을 이용해서 명시적 타입 변환을 할 수도 있다. \n\n1. 단축평가\n\n    논리곱(&&) 논리합(||) 연산자\n\n    단축평가는 다음과 같은 상황에서 유리하다. \n\n    - 객체가 null 인지 확인하고 프로퍼티를 참조할 때에\n\n    - 함수의 인수를 초기화할 때\n\n        함수를 호출할 때에 인수를 전달하지 않으면 매개변수는 undefined를 갖는다. 이때 단축 평가를 사용하여 매개변수의 기본값을 설정하면 undefined로 인해 발생할 수 있는 에러를 방지할 수 있다. \n\n        \n\n"},{"excerpt":"변수 호이스팅 (Variable Hoisting) 자바스크립트에서는 모든 변수는 호이스팅 된다. 호이스팅이란 var 선언문이나 function 선언문 등 모든 선언문이 해당 Scope의 선두로 옮겨진 것처럼 동작하는 특성을 말한다. 즉 자바스크립트에서는 모든 선언문(var, let, const, function, function*, class)이 선언되기…","fields":{"slug":"/JavaScript의-변수/"},"frontmatter":{"date":"January 07, 2023","title":"JavaScript의 변수","tags":["Javascript"]},"rawMarkdownBody":"- 변수 호이스팅 (Variable Hoisting)\n\n    자바스크립트에서는 모든 변수는 호이스팅 된다. 호이스팅이란 var 선언문이나 function 선언문 등 모든 선언문이 해당 Scope의 선두로 옮겨진 것처럼 동작하는 특성을 말한다. 즉 자바스크립트에서는 모든 선언문(var, let, const, function, function*, class)이 선언되기 이전에도 참조가 가능하다. \n\n- var 키워드로 생성된 변수의 문제점\n\n    ES5에서는 오직 var 키워드로만 변수의 선언이 가능하다. 이는 다음과 같은 특징을 가지며 주의를 기울이지 않으면 심각한 문제를 발생시킨다. \n\n    1. 함수 레벨 스코프\n\n        - 전역변수의 남발\n\n        - for문에서 초기화식에 사용한 변수를 for문 외부 또는 전역에서 참조할 수 있다. \n\n    1. var 키워드 생략 허용\n\n        - 의도하지 않은 변수의 전역화\n\n    1. 중복 선언 허용\n\n        - 의도하지 않은 변수값 변경\n\n    1. 변수 호이스팅\n\n    ES6는 이런 단점을 보완하기 위해 let, const 키워드를 도입하였다. \n\n"},{"excerpt":"변수(Variable) 메모리상의 주소(위치)를 기억하는 저장소. 즉 메모리 주소에 접근하기 위해 사람이 이해할 수 있는 언어로 지정한 식별자 값 데이터 타입 - 프로그래밍 언어에서 사용할 수 있는 값의 종료 변수 - 값이 저장된 메모리를 가리키는 식별자 리터럴 - 소스코드 안에 직접 만들어낸 상수 값 자체. 값의 최소 단위 값은 프로그램에 의해 조작될 …","fields":{"slug":"/JavaScript-기본-문법/"},"frontmatter":{"date":"January 06, 2023","title":"JavaScript 기본 문법","tags":["Javascript"]},"rawMarkdownBody":"1. 변수(Variable)\n\n    메모리상의 주소(위치)를 기억하는 저장소. 즉 메모리 주소에 접근하기 위해 사람이 이해할 수 있는 언어로 지정한 식별자\n\n1. 값\n\n    1. 데이터 타입 - 프로그래밍 언어에서 사용할 수 있는 값의 종료\n\n    1. 변수 - 값이 저장된 메모리를 가리키는 식별자\n\n    1. 리터럴 - 소스코드 안에 직접 만들어낸 상수 값 자체. 값의 최소 단위\n\n    값은 프로그램에 의해 조작될 수 있는 대상을 말한다. \n\n    자바스크립트의 모든 값은 자바스크립트가 제공하는 7가지 데이터 타입 중 하나를 갖는다. \n\n    - 데이터타입\n\n        - 원시타입(Privitive data type)\n\n            - number\n\n            - string\n\n            - boolean\n\n            - null\n\n            - undefined\n\n            - symbol(ES6)\n\n        - 객체 타입(Object data type)\n\n            - object\n\n    자바스크립트는 데이터 타입을 미리 지정하지 않는다. 값의 타입에 의해 동적으로 변수의 타입이 결정되며, 이를 동적 타이핑이라고 한다. \n\n1. 연산자\n\n    피연산자의 타입이 반드시 일치할 필요가 없다. 암묵적 타입 강제 변환을 통해 연산을 수행한다. \n\n1. 키워드\n\n    키워드는 수행할 동작을 규정한다. 예를 들어 var 는 변수를 생성할 것을 지시한다. \n\n1. 주석\n\n    // 로 쓴다. 또는 /* */ 사이에 쓴다. \n\n    주석은 해석기가 무시하며 실행되지 않는다. \n\n1. 문\n\n    브라우저에 의해 단계별로 수행될 명령들의 집합. 문은 리터럴, 연산자, 표현식, 키워드 등으로 구성되며 세미콜론으로 끝나야 한다. \n\n    문은 코드 블록으로 그룹화 할 수 있다. \n\n    자바스크립트는 블록 유효범위 (Block-Level Scope)를 생성하지 않는다. 함수 단위의 유효범위 (Function-Level Scope)만 생성된다.\n\n1. 표현식\n\n    표현식은 하나의 값으로 평가된다.\n\n1. 문과 표현식의 비교\n\n    문은 표현식을 포함한다. 표현식은 그 자체로 하나의 문이 될 수도 있다. \n\n    표현식은 평가되어 값을 만들지만 그 이상의 행위는 할 수 없다. \n\n    표현식을 통해 평가한 값을 통해 실제로 컴퓨터에게 명령을 하여 무언가를 하는 것은 문이다.\n\n1. 함수\n\n    어떤 작업을 수행하기 위해 필요한 문들의 집함. 이름과 매개변수를 가진다. \n\n1. 객체\n\n    자바스크립트는 객체(object) 기반의 스크립트 언어로 자바스크립트를 이루는 거의 모든 것이 객체이다. \n\n    원시 타입을 제외한 나머지 값들은 모두 객체이다. (함수, 배열, 정규표현식 등)\n\n    자바스크립트 객체는 키(이름)와 값으로 구성된 프로퍼티(property)의 집합이다. 프로퍼티의 값으로 자바스크립트에서 사용할 수 있는 모든 값을 사용할 수 있다. 자바스크립트의 함수는 일급 객체이므로 값으로 취급할 수 있다. \n\n    따라서 프로퍼티 값으로 함수를 사용할수도 있으며, 프로퍼티 값이 함수일 경우 일반 함수와 구분하기 위헤 메소드라 부른다. \n\n    객체는 데이터를 의미하는 프로퍼티와 데이터를 참조하고 조작할 수 있는 동작을 의미하는 메소드로 구성된 집합이다. 객체는 데이터(프로퍼티)와 그 데이터에 관련되는 동작(메소드)을 모두 포함할 수 있기 때문에 데이터와 동작을 하나의 단위로 구조화할 수 있어 유용하다. \n\n    자바스크립트의 객체는 객체지향의 상속을 구현하기 위해 “프로토타입”이라고 불리는 객체의 프로퍼티와 메소드를 상속받을 수 있다. \n\n1. 배열\n\n    자바스크립트의 배열은 객체이다. 유용한 내장 메소드를 포함하고 있다. \n\n"},{"excerpt":"자바스크립트의 특징 웹브라우저에서 동작하는 유일한 프로그래밍 언어 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어  명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어 강력한 프로토타입 기반의 객체지향 언어 브라우저 동작 원리 브라우저의 핵심 기능은 사용자가 참조하고자 하는 페이지를 서버에 요청(Request…","fields":{"slug":"/JavaScript의-특징-브라우저-동작-원리/"},"frontmatter":{"date":"January 05, 2023","title":"JavaScript의 특징, 브라우저 동작 원리","tags":["Javascript"]},"rawMarkdownBody":"1. 자바스크립트의 특징\n\n    - 웹브라우저에서 동작하는 유일한 프로그래밍 언어\n\n    - 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어 \n\n    - 명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어\n\n    - 강력한 프로토타입 기반의 객체지향 언어\n\n1. 브라우저 동작 원리\n\n    - 브라우저의 핵심 기능은 사용자가 참조하고자 하는 페이지를 서버에 요청(Request) 하고 서버의 응답(Response)를 받아 브라우저에 표시하는 것. 브라우저는 서버로부터 HTML, CSS, JavaScript, 이미지 파일 등을 응답받는다. HTML, CSS 파일은 렌더링 엔진의 HTML 파서와 CSS 파서에 의해 파싱되어 DOM, CSSOM 트리로 변환되고 렌더 트리로 결합된다. 렌더 트리를 기반으로 브라우저는 웹페이지를 표시한다. \n\n    - 자바스크립트는 렌더링 엔진이 아닌 자바스크립트 엔진이 처리한다. HTML 파서는 script 태그를 만나면 자바스크립트 코드를 실행하기 위해 DOM 생성 프로세스를 중지하고 자바스크립트 엔진으로 제어 권한을 넘긴다. 제어 권한을 넘겨 받은 자바스크립트 엔진은 script 태그 내의 자바스크립트 코드 또는 script 태그의 src 속성에 정의된 자바스크립트 파일을 로드하고 파싱하여 실행한다. 자바스크립트의 실행이 완료되면 다시 HTML 파서가 제어 권한을 받아 DOM 생성을 재개한다. \n\n    - 이처럼 브라우저는 동기적으로 HTML, CSS, JAVASCRIPT를 처리한다. 이것은 script 위치에 따라 블로킹이 발생하여 DOM 구조 생성이 지연될 수 있음을 의미한다. 따라서 script 태그의 위치는 중요한 의미를 갖는다. \n\n    - body 요소의 가장 아래에 자바스크립트를 위치시키는 것은 좋은 아이디어다. \n\n        - HTML 요소들이 지연없이 렌더링 되므로 페이지 로딩 시간이 단축된다. \n\n        - DOM이 완성되지 않은 상태에서 자바스크립트가 DOM을 조작한다면 에러가 발생한다. \n\n"},{"excerpt":"Stun 서버와 Turn 서버를 위한 Coturn Server도 함께 설치 환경구성 Ubuntu 18.04 bionic Python 3.7 설치 명령어 정리 파이썬 및 기본 소프트웨어 설치 Janus 설치 https://ourcodeworld.com/articles/read/1197/how-to-install-janus-gateway-in-ubuntu-s…","fields":{"slug":"/화상상담을-위한-Janus-구성/"},"frontmatter":{"date":"January 03, 2023","title":"화상상담을 위한 Janus 구성","tags":["WebRTC","Work"]},"rawMarkdownBody":"\n        Janus를 우분투에 설치하면서 사용한 명령어 정리\nStun 서버와 Turn 서버를 위한 Coturn Server도 함께 설치\n\n### 환경구성\n\n- Ubuntu 18.04 bionic\n\n- Python 3.7\n\n### 설치 명령어 정리\n\n1. 파이썬 및 기본 소프트웨어 설치\n\n1. Janus 설치\n\n    [https://ourcodeworld.com/articles/read/1197/how-to-install-janus-gateway-in-ubuntu-server-18-04](https://ourcodeworld.com/articles/read/1197/how-to-install-janus-gateway-in-ubuntu-server-18-04)\n\n    - Janus 설치\n\n        ```bash\n        packagelist=( \n        git \n        libmicrohttpd-dev \n        libjansson-dev \n        libssl-dev \n        libsrtp-dev \n        libsofia-sip-ua-dev \n        libglib2.0-dev \n        libopus-dev \n        libogg-dev \n        libcurl4-openssl-dev \n        liblua5.3-dev \n        libconfig-dev \n        pkg-config \n        gengetopt \n        libtool \n        automake \n        gtk-doc-tools \n        cmake \n        ) \n        apt-get install ${packagelist[@]}\n        ```\n\n    - libnice 설치\n\n        *※ 최소 파이썬 3.7을 요구한다.* \n\n        ```bash\n        pip3 install meson==0.61.5 \n        ln -s /usr/local/bin/meson /usr/bin/ \n        wget https://github.com/ninja-build/ninja/releases/download/v1.10.1/ninja-linux.zip \n        unzip ninja-linux.zip \n        cp ninja /usr/bin/ \n        git clone https://gitlab.freedesktop.org/libnice/libnice.git\n        cd libnice \n        meson --prefix=/usr build \n        ninja -C build \n        ninja -C build install\n        ```\n\n    - libstrp 설치\n\n        ```bash\n        wget https://github.com/cisco/libsrtp/archive/v2.2.0.tar.gz \n        tar xfv v2.2.0.tar.gz \n        cd libsrtp-2.2.0 \n        ./configure —prefix=/usr —enable-openssl \n        make shared_library && make install\n        ```\n\n    - usrctp 설치\n\n        ```bash\n        git clone https://github.com/sctplab/usrsctp \n        cd usrsctp \n        ./bootstrap \n        ./configure --prefix=/usr && make && make install\n        ```\n\n    - libwebsockets 설치\n\n        ```bash\n        git clone https://github.com/warmcat/libwebsockets.git \n        cd libwebsockets \n        mkdir build \n        cd build \n        cmake -DLWS_MAX_SMP=1 -LWS_IPV6=ON -DCMAKE_INSTALL_PREFIX:PATH=/usr -DCMAKE_C_FLAGS=\"-fpic\" .. \n        make && make install\n        ```\n\n    - mqtt 설치\n\n        ```bash\n        git clone https://github.com/eclipse/paho.mqtt.c.git \n        cd paho.mqtt.c \n        prefix=/usr make install\n        ```\n\n    - NanoMSG 설치\n\n        ```bash\n        apt-get install libnanomsg-dev -y\n        ```\n\n    - RabbitMQ C AMQP 설치\n\n        ```bash\n        git clone https://github.com/alanxz/rabbitmq-c \n        cd rabbitmq-c \n        git submodule init \n        git submodule update \n        mkdir build && cd build \n        cmake -DCMAKE_INSTALL_PREFIX=/usr ..\n        make && make install\n        ```\n\n    - janus 컴파일링\n\n        ```bash\n        git clone https://github.com/meetecho/janus-gateway.git\n        cd janus-gateway\n        sh autogen.sh\n        ./configure —prefix=/opt/janus\n        make && make install\n        make configs\n        ```\n\n1. Janus 설정\n\n    설정 파일 위치 : /opt/janus/etc/janus\n\n    - janus.jcfg\n\n        ```bash\n        - log_to_file : 주석해제 \n        - admin_secret : 변경 \"PW\" \n        - rtp_port_range : 주석해제 및 변경 \"20000-60000\" \n        - stun_server : 변경 \"Stun Server Domain\" \n        - stun_port : 주석해제 \n        - nat_1_1_mapping : 변경 \"Server IP\" \n        - turn_server : 변경 \"Turn Server Domain\" \n        - turn_port : 주석해제 \n        - turn_type : 주석해제 \n        - turn_user : 주석해제 및 변경 \"Turn ID\" \n        - turn_pwd : 주석해제 및 변경 \" Turn PW\" \n        - ipv6 : 주석해제 및 변경 \"true\"\n        ```\n\n    - janus.plugin.videoroom.jcfg\n\n        ```bash\n        general{ \n        admin_key : 주석해제 및 변경 \"비밀번호\" \n        publishers : 생성 값 \"10\" \n        }\n        ```\n\n    - janus.transport.http.jcfg\n\n        ```bash\n        https : 값 변경 \"true\" \n        secure_port : 주석 해제 \n        admin_https : 주석처리 값 변경 \"true\" \n        cert_pem : 값 변경 \"crt.pem 위치\" \n        cert_key : 값 변경 \"key.pem 위치\" \n        cert_pwd : 값 변경 \"인증서 비밀번호\"\n        ```\n\n1. Coturn 설치\n\n    [http://john-home.iptime.org:8085/xe/index.php?mid=board_sKSz42&document_srl=1546](http://john-home.iptime.org:8085/xe/index.php?mid=board_sKSz42&document_srl=1546)\n\n    \n\n"}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}