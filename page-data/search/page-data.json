{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"서론 지난 시간에 이어 GraphQL에 대해 좀 더 자세히 알아보는 시간을 가지려고 합니다.  GraphQL의 장점과 현실적인 문제 GraphQL은 기존 REST API의 단점을 보완하고, 클라이언트가 원하는 데이터만 요청할 수 있는 유연한 구조를 제공합니다. 하지만 현실적으로 적용할 때는 여러 문제를 고려해야 합니다. GraphQL의 이점 유연한 데이터…","fields":{"slug":"/GraphQL과-Django-2/"},"frontmatter":{"date":"January 24, 2025","title":"GraphQL과 Django 2","tags":["GraphQL","Django","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n[지난 시간](https://sharknia.github.io/GraphQL과-Django)에 이어 GraphQL에 대해 좀 더 자세히 알아보는 시간을 가지려고 합니다. \n\n## GraphQL의 장점과 현실적인 문제\n\nGraphQL은 기존 REST API의 단점을 보완하고, 클라이언트가 원하는 데이터만 요청할 수 있는 유연한 구조를 제공합니다. 하지만 현실적으로 적용할 때는 여러 문제를 고려해야 합니다.\n\n### GraphQL의 이점\n\n#### 유연한 데이터 요청\n\n- REST API는 백엔드에서 정해준 응답 형식을 그대로 받아야 합니다.\n\n- 하지만 GraphQL은 클라이언트가 필요한 데이터만 선택적으로 요청할 수 있어 불필요한 데이터(Over-fetching)를 줄이고, 필요한 데이터를 추가 요청해야 하는 문제(Under-fetching)도 해결할 수 있습니다.\n\n#### 단일 엔드포인트\n\n- REST는 리소스마다 엔드포인트가 다릅니다. (`/users`, `/users/1/posts`, `/users/1/comments` 등)\n\n- GraphQL은 단일 엔드포인트(`/graphql`)에서 다양한 요청을 처리할 수 있어 API 유지보수가 간편합니다.\n\n### 실제 프로젝트에서 GraphQL을 도입할 때 겪는 문제들\n\n#### 쿼리가 복잡해지면 성능 이슈 발생\n\nGraphQL은 유연한 데이터 요청이 가능하지만, 필요한 데이터를 어떻게 가져올지 효율적으로 설계하지 않으면 오히려 성능이 저하될 수 있습니다. 이에 대한 내용은 [이전 블로그 글](https://sharknia.github.io/GraphQL과-Django)에서 다룬바가 있습니다. \n\n#### 캐싱의 어려움\n\n단일 엔드포인트에서 다양한 요청을 처리하고 응답의 꼴이 정해져 있지 않기 때문에 기본적인 HTTP 캐싱이 어렵고, 별도의 캐싱 전략을 고려해야 합니다.\n\n#### 쿼리 비용 제한 필요\n\nGraphQL은 클라이언트가 원하는 데이터를 자유롭게 요청할 수 있기 때문에, 너무 깊거나 복잡한 쿼리 요청이 들어올 경우 서버 부담이 커질 수 있습니다. \n\n```graphql\n{\n  user(id: 1) {\n    name\n    posts {\n      comments {\n        author {\n          name\n          posts {\n            comments {\n              author {\n                ...\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n예를 들어 이처럼 너무 깊은 중첩 쿼리는 서버 성능에 심각한 영향을 미칠 수 있습니다. 이를 위해 GraphQL 쿼리의 Depth 제한 및 요청 복잡도 제한(Query Cost Analysis) 설정이 가능합니다. 예를 들어, 5단계 이상 중첩된 요청은 막거나, 특정 요청당 비용을 계산해 제한하는 방식을 사용할 수 있습니다. \n\n```python\nfrom graphql.validation.rules import QueryDepthLimitRule\n\nclass CustomGraphQLView(GraphQLView):\n    def get_validation_rules(self):\n        return [QueryDepthLimitRule(5)]  # 최대 5단계 중첩 제한\n```\n\n이런 식으로 적용하면 GraphQL의 무분별한 깊은 요청을 막아 서버 부하를 줄일 수 있습니다.\n\n### GraphQL을 무조건 적용하면 안 되는 경우\n\n이와 같이 발생할 수 있는 문제 때문에 항상 모든 서비스에 적합한 것은 아닙니다. 다음과 같은 경우에는 REST API 가 더 나을 수 있습니다. \n\n#### 단순한 API\n\nCRUD API가 단순한 경우, GraphQL의 유연성이 오히려 필요 없을 수 있습니다. 예를 들어 \"게시판 서비스\"처럼 정형화된 API는 REST API가 더 효율적일 수 있습니다. \n\n#### 캐싱이 중요한 경우\n\n위에서 설명한 것처럼 캐싱에 있어서는 REST API가 더 유리합니다. 정적 데이터를 많이 제공하는 등 캐싱이 성능에 영향을 크게 미칠 수 있는 서비스라면 REST API가 훨씬 효율적입니다. \n\n#### **API 소비자가 다양할 경우**\n\nGraphQL은 프론트엔드 개발자가 직접 원하는 데이터를 요청하는 방식이므로, \"내부 서비스\"에서 사용하기에는 좋지만 공개 API(예: 오픈 API)에서는 REST API가 더 적절할 수 있습니다. \n\n## GraphQL의 성능 최적화: DataLoader 패턴을 적용하기\n\n[Django ORM 최적화](https://sharknia.github.io/GraphQL과-Django)만으로도 성능이 좋아질 수 있지만, GraphQL 특성상 동일한 요청을 중복 실행하는 문제가 발생할 수 있습니다. 이를 해결하기 위해 DataLoader 패턴을 적용하면 더욱 효율적으로 데이터 조회가 가능합니다.\n\n### DataLoader란?\n\nGraphQL은 resolver마다 개별적으로 데이터베이스를 조회하는데, 이 과정에서 동일한 데이터에 대한 중복 쿼리가 발생할 수 있습니다. DataLoader는 동일한 요청을 한 번에 처리(Batching)하고, 결과를 캐싱하는 패턴을 말합니다. \n\n### DataLoader 적용 예시 (Django + Graphene)\n\n```python\nfrom promise import Promise\nfrom promise.dataloader import DataLoader\nfrom Django.db.models import Prefetch\nfrom .models import Post\n\nclass PostLoader(DataLoader):\n    def batch_load_fn(self, user_ids):\n        posts = Post.objects.filter(user_id__in=user_ids)\n        post_map = {user_id: [] for user_id in user_ids}\n\n        for post in posts:\n            post_map[post.user_id].append(post)\n\n        return Promise.resolve([post_map[user_id] for user_id in user_ids])\n```\n\n`batch_load_fn()`을 사용해 한 번의 쿼리로 여러 사용자의 `posts` 데이터를 가져오고 있습니다. \n\n```python\nfrom .dataloaders import PostLoader\n\nclass UserType(DjangoObjectType):\n    class Meta:\n        model = User\n\n    posts = graphene.List(PostType)\n\n    def resolve_posts(self, info):\n        loader = PostLoader()\n        return loader.load(self.id)  # DataLoader를 통해 batch 처리\n```\n\nGraphQL Resolver에서 DataLoader를 사용해 각 사용자별 개별 쿼리 실행 없이 한 번의 쿼리로 처리할 수 있습니다. \n\nDataLoader 적용 전에는 \n\n```sql\nSELECT * FROM user;\nSELECT * FROM post WHERE user_id = 1;\nSELECT * FROM post WHERE user_id = 2;\nSELECT * FROM post WHERE user_id = 3;\n...\n```\n\n이와 같이 쿼리가 발생해 사용자가 많을수록 쿼리 수 증가하지만 적용 후에는\n\n```sql\nSELECT * FROM user;\nSELECT * FROM post WHERE user_id IN (1, 2, 3, ...);\n```\n\n쿼리 실행 횟수가 감소해 성능이 최적화됩니다. \n\n### IN 절 개수 제한을 고려한 DataLoader 최적화\n\n다만 이 방법도 만능은 아닙니다. Oracle을 포함한 일부 DBMS에서는 IN 절에 들어갈 수 있는 값의 개수가 제한되어 있습니다. 예를 들어 Oracle은 1000개 제한이 걸려있으며, 사실상 제한이 없는 DBMS라고 하더라도 너무나 긴 IN 절은 성능 저하를 유발할 수 있습니다. 이에 IN 절이 너무 커지지 않도록 방지하는 로직을 추가하는 것이 중요합니다. \n\n```python\nfrom promise import Promise\nfrom promise.dataloader import DataLoader\nfrom Django.db.models import Prefetch\nfrom .models import Post\n\n# DBMS IN 절 제한 (Oracle 기준 1000개)\nBATCH_SIZE = 1000  \n\nclass PostLoader(DataLoader):\n    def batch_load_fn(self, user_ids):\n        results = {}\n\n        # IN 절 개수 제한을 고려한 배치 처리\n        for i in range(0, len(user_ids), BATCH_SIZE):\n            batch_ids = user_ids[i : i + BATCH_SIZE]\n            posts = Post.objects.filter(user_id__in=batch_ids)\n\n            for post in posts:\n                results.setdefault(post.user_id, []).append(post)\n\n        # DataLoader는 user_id 순서대로 반환해야 함\n        return Promise.resolve([results.get(user_id, []) for user_id in user_ids])\n\n```\n\n이렇게 작성하면\n\n```sql\nSELECT * FROM post WHERE user_id IN (1, 2, ..., 1000);\nSELECT * FROM post WHERE user_id IN (1001, 1002, ..., 2000);\n...\n\n```\n\n쿼리가 1000개씩 나눠서 실행됩니다. \n\n## GraphQL API 설계: 유지보수 가능한 스키마 설계 원칙\n\n### Pagination(페이지네이션) 패턴 (Offset vs Cursor)\n\nGraphQL에서 대량의 데이터를 처리할 때는 **Pagination(페이지네이션)**이 필수입니다. 페이지네이션을 구현하는 방법에는 Offset 방식과 Cursor 방식이 있습니다.\n\n#### Offset Pagination (기본적인 방식)\n\n`OFFSET`을 이용해 페이지를 이동하는 방식으로 SQL의 LIMIT + OFFSET을 그대로 사용 가능합니다. \n\n```graphql\nquery {\n  users(limit: 10, offset: 20) {\n    id\n    name\n  }\n}\n```\n\nDjango에서는 아래와 같이 구현 할 수 있습니다. \n\n```python\nclass Query(graphene.ObjectType):\n    users = graphene.List(UserType, limit=graphene.Int(), offset=graphene.Int())\n\n    def resolve_users(self, info, limit=10, offset=0):\n        return User.objects.all()[offset: offset + limit]  # OFFSET 적용\n```\n\n이 방법은 구현이 SQL의 OFFESET/LIMIT을 활용하므로 직관적으로 이해가 쉽고 구현이 간단합니다. 다만 데이터가 많아질수록 성능 저하가 일어날 수 있고, 새로운 데이터가 추가되면 데이터가 밀려서 중복 조회가 발생할 수 있습니다. \n\n#### Cursor Pagination (Relay-style)\n\nRelay는 GraphQL에서 Cursor 기반 Pagination을 공식적으로 정의한 방식입니다. Cursor Pagination은 특정 필드를 기준으로 다음 데이터를 가져오는 방식으로 OFFSET이 아니라 고유한 Cursor값을 사용합니다. 페이징이 빠르고 안정적으로 Offset 방식의 단점을 개선할 수 있지만 구현이 상대적으로 복잡합니다. \n\n```graphql\nquery {\n  users(first: 10, after: \"cursor123\") {\n    edges {\n      node {\n        id\n        name\n      }\n      cursor\n    }\n    pageInfo {\n      hasNextPage\n      endCursor\n    }\n  }\n}\n```\n\nDjango에서는 아래와 같이 구현 가능합니다. \n\n```python\nimport graphene\nfrom graphene_Django.types import DjangoObjectType\nfrom graphene.relay import Node\nfrom graphene_Django.filter import DjangoFilterConnectionField\nfrom .models import User\n\nclass UserNode(DjangoObjectType):\n    class Meta:\n        model = User\n        interfaces = (Node,)  # Relay Node 사용\n\nclass Query(graphene.ObjectType):\n    users = DjangoFilterConnectionField(UserNode)  # Relay-style Pagination 적용\n```\n\n이 방식은 대량 데이터에서도 성능이 뛰어나고 데이터가 추가/삭제되어도 안전하지만, 정렬 기준이 필요한 필드가 있어야 하는 문제점이 있습니다.  정식 GraphQL 표준 방식으로 Cursor 기반이라 성능 최적화가 가능하면서도 `pageInfo.hasNextPage` 등을 제공하여 클라이언트에서 쉽게 처리 가능합니다. \n\n### GraphQL API 변경 관리(버전 관리 vs Deprecated 필드)\n\nGraphQL API는 REST처럼 v1, v2 버전을 사용하지 않는 것이 일반적입니다. 대신 Deprecated 필드를 활용하여 API 변경을 최소화하는 방식을 추천합니다.\n\n```graphql\ntype User {\n  id: ID!\n  name: String!\n  email: String @deprecated(reason: \"Use contactEmail instead\")\n  contactEmail: String\n}\n```\n\n기존의 `email` 필드는 유지하면서, 새로운 `contactEmail` 필드를 추가했습니다. 클라이언트가 점진적으로 변경할 수 있도록 유도할 수 있습니다. \n\nDjango에서는 다음과 같이 구현이 가능합니다. \n\n```python\nclass UserType(DjangoObjectType):\n    class Meta:\n        model = User\n\n    email = graphene.String(deprecation_reason=\"Use contactEmail instead\")\n```\n\n하지만 대규모 변경이 필요한 경우, 특정 API 그룹만 버전 관리를 하는 것이 유리할 수도 있습니다. 상황에 따라 유연한 대처가 필수입니다. \n\n## GraphQL과 캐싱\n\n### HTTP 캐싱이 어려운 이유\n\nGraphQL은 REST API보다 강력한 데이터 요청 기능을 제공하지만, 기본적인 HTTP 캐싱이 어렵다는 단점이 있습니다. 단일 엔드포인트를 사용하므로 REST API처럼 URL 기반의 캐싱이 불가능하기 때문입니다. \n\nREST API의 경우에는 URL이 정해져 있기 때문에, 브라우저나 CDN에서 `Cache-Control`, `ETag` 등을 활용하여 응답을 캐싱할 수 있습니다. \n\n```plain text\nGET /users/1  → 캐싱 가능\nGET /posts/10 → 캐싱 가능\n```\n\nGraphQL에서는 모든 요청이 같은 엔드포인트(`/graphql`)에서 처리되므로, URL 기반의 HTTP 캐싱이 불가능합니다.\n\n```plain text\nPOST /graphql   → 캐싱 불가능\n```\n\n모든 요청이 POST method로 이루어지며, 요청 본문(Body)에 있는 GraphQL Query 내용이 다를 수 있기 때문입니다.\n\n이 문제를 해결하기 위해 다음의 방법들을 활용해 캐싱할 수 있습니다. \n\n### GraphQL의 `Persisted Queries` 활용법\n\n#### Persisted Queries란?\n\nGraphQL 쿼리를 사전에 저장해두고, 해시(hash) 값을 이용해 요청하는 방식입니다. 즉, 쿼리 내용을 생략하고 해시값만 보내므로, 동일한 요청을 쉽게 캐싱할 수 있습니다. \n\n```graphql\nquery {\n  user(id: 1) {\n    name\n    email\n  }\n}\n```\n\n이 쿼리를 아래처럼 쿼리 해시값을 사용한 요청으로 변환합니다.\n\n```json\n{\n  \"id\": \"e3cbbd2f0f5e4b80b4\",\n  \"variables\": { \"id\": 1 }\n}\n```\n\n해시 값이 동일하면 캐싱이 가능해집니다. 클라이언트와 서버에서 동일한 해시값을 가진 요청을 캐싱할 수 있습니다. \n\n#### Django의 Persisted Queries\n\n클라이언트에서 Query 해시값 생성을 생성합니다. 클라이언트는 Apollo Client등을 활용해 해시값을 생성할 수 있습니다. Django에서는 해시값 기반으로 응답을 캐싱합니다. \n\n```python\nfrom django.core.cache import cache\nfrom graphql.execution.executors.sync import SyncExecutor\n\nclass CachedGraphQLView(GraphQLView):\n    def execute_graphql_request(self, request, data, query, *args, **kwargs):\n        cache_key = f\"graphql:{hash(query)}\"\n        response = cache.get(cache_key)\n\n        if response is None:\n            response = super().execute_graphql_request(request, data, query, *args, **kwargs)\n            cache.set(cache_key, response, timeout=3600)  # 1시간 캐싱\n\n        return response\n```\n\n쿼리를 해시값으로 변환하여 동일한 요청을 캐싱 가능합니다. \n\n### Apollo Client의 캐싱 전략\n\nGraphQL에서 클라이언트 캐싱(Apollo Client) 을 활용하면 프론트엔드에서 불필요한 네트워크 요청을 줄일 수 있습니다. Apollo Client의 캐싱을 활용하면, 동일한 요청을 여러 번 보낼 필요 없이 프론트엔드에서 데이터를 재사용할 수 있습니다. \n\n### Redis를 활용한 GraphQL 응답 캐싱\n\nPersisted Queries와 같은 개념으로 결국 같은 쿼리에 대해 동일한 응답을 반복해서 제공할 수 있기 때문에, Redis를 활용하는 방법으로도 캐싱을 해 성능 향상을 꾀할 수 있습니다. \n\n#### Django + Redis 기반 GraphQL 캐싱 구현\n\n```python\nimport redis\nimport hashlib\nfrom django.core.cache import cache\nfrom graphene_django.views import GraphQLView\n\nredis_client = redis.StrictRedis(host=\"localhost\", port=6379, db=0)\n\nclass CachedGraphQLView(GraphQLView):\n    def execute_graphql_request(self, request, data, query, *args, **kwargs):\n        query_hash = hashlib.sha256(query.encode()).hexdigest()\n        cache_key = f\"graphql_cache:{query_hash}\"\n        \n        # Redis에서 캐시 확인\n        cached_response = redis_client.get(cache_key)\n        if cached_response:\n            return cached_response\n\n        # 캐시 없으면 원래 GraphQL 요청 실행\n        response = super().execute_graphql_request(request, data, query, *args, **kwargs)\n        \n        # Redis에 캐싱 (60분 유지)\n        redis_client.setex(cache_key, 3600, response)\n\n        return response\n```\n\n이제 동일한 GraphQL 요청이 오면, Django가 데이터베이스가 아니라 Redis에서 즉시 응답하는 것이 가능합니다. \n\n"},{"excerpt":"Elastic APM Elastic APM (Application Performance Monitoring)은 Elastic Stack(ELK 스택)의 일부로, 애플리케이션의 성능을 모니터링하고 성능 병목 현상과 오류를 추적하는 도구입니다. 주로 백엔드 서비스, 데이터베이스 쿼리, HTTP 요청, 비동기 작업 등의 성능을 분석하는 데 사용됩니다. ELK …","fields":{"slug":"/Elastic-APM/"},"frontmatter":{"date":"January 23, 2025","title":"Elastic APM","tags":["BackEnd","DevOps"]},"rawMarkdownBody":"![](image1.png)\n## Elastic APM\n\nElastic APM (Application Performance Monitoring)은 Elastic Stack(ELK 스택)의 일부로, 애플리케이션의 성능을 모니터링하고 성능 병목 현상과 오류를 추적하는 도구입니다. 주로 백엔드 서비스, 데이터베이스 쿼리, HTTP 요청, 비동기 작업 등의 성능을 분석하는 데 사용됩니다.\n\n### ELK 스택이란? \n\nELK 스택은 Elasticsearch, Logstash, Kibana의 약자로, 로그 및 데이터 분석을 위한 오픈소스 도구 모음입니다.\n\n- Elasticsearch: 검색 및 분석을 위한 강력한 검색 엔진\n\n- Logstash: 로그 및 데이터 수집·가공 도구\n\n- Kibana: 데이터를 시각화하는 대시보드 도구\n\n주로 서버 로그, 애플리케이션 로그를 수집하고 검색·분석하는 데 사용됩니다. 최근에는 Beats(경량 데이터 수집기)를 포함하여 ELK → Elastic Stack으로 확장되었습니다.\n\n### 주요 기능\n\n1. 트랜잭션 추적\n\n    - 애플리케이션의 특정 요청(예: API 호출)이 시작부터 끝까지 어떻게 실행되는지 추적\n\n    - 각 요청이 얼마나 걸리는지, 어느 부분에서 병목이 발생하는지 확인 가능\n\n1. 분산 추적 (Distributed Tracing)\n\n    - 여러 서비스 간의 호출을 추적하여 마이크로서비스 아키텍처에서 요청이 어떻게 전달되는지 분석 가능\n\n    - OpenTelemetry 및 Jaeger와 유사한 기능 제공\n\n1. 에러 및 예외 감지\n\n    - 애플리케이션에서 발생하는 오류(예: 500 에러, 데이터베이스 연결 오류)를 자동으로 수집 및 분석\n\n1. 메트릭 수집\n\n    - CPU, 메모리 사용량, 요청 수, 응답 시간 등의 메트릭을 자동으로 수집하여 시각화\n\n1. 로그 통합\n\n    - Elastic Stack의 Elasticsearch 및 Kibana와 연동하여 로그를 함께 분석 가능\n\n### 구조\n\nElastic APM은 보통 아래와 같은 구성으로 동작합니다.\n\n1. APM Agent (애플리케이션 내부에서 실행)\n\n    - 애플리케이션 코드에 삽입되어 요청, 오류, 메트릭을 수집하는 역할\n\n    - 언어별 지원: Python, Java, Node.js, Go, Ruby, .NET 등\n\n1. APM Server (수집된 데이터를 Elasticsearch로 전달)\n\n    - APM Agent가 보낸 데이터를 받아서 Elasticsearch에 저장\n\n1. Elasticsearch (데이터 저장소)\n\n    - 수집된 성능 데이터를 저장하고 검색하는 역할\n\n1. Kibana (시각화 도구)\n\n    - APM 데이터를 대시보드 형태로 분석하고 시각화하는 역할\n\n### 장점\n\n1. Elasticsearch 기반 → 빠르고 강력한 검색 및 분석 기능 제공\n\n1. 마이크로서비스 친화적 → 분산 트랜잭션 분석 지원\n\n1. 자동 인스트루먼테이션 → 코드 수정 없이도 기본적인 성능 데이터 수집 가능\n\n1. 로그 + 메트릭 + APM 통합 → ELK Stack과 결합하여 강력한 모니터링 시스템 구축 가능\n\n1. 오픈소스 → 기본적인 기능을 무료로 사용할 수 있음\n\n### Elastic APM을 도입해야 하는 경우\n\n1. 마이크로서비스 구조에서 서비스 간의 요청을 추적하고 싶은 경우\n\n1. 성능 저하 원인을 빠르게 찾고 싶은 경우\n\n1. 애플리케이션의 에러와 예외를 자동으로 모니터링하고 싶은 경우\n\n1. ELK Stack을 이미 사용하고 있는 경우\n\n1. Elastic APM은 New Relic, Datadog, AWS X-Ray 같은 APM 도구와 비교할 수 있으며, ELK 스택과의 강력한 연동이 가장 큰 장점입니다.\n\n## Elastic APM을 설치만 하면 request/response 분석이 자동?!\n\n기본적인 요청(Request), 응답(Response), 오류(Error), 메트릭(CPU, 메모리 등) 은 별도 설정 없이 자동으로 수집됩니다. Elastic APM 에이전트를 애플리케이션에 설치하고 실행하면 다음과 같은 데이터를 자동으로 캡처합니다.\n\n트랜잭션(Transaction)\n\n- HTTP 요청 (URL, 응답 시간, 상태 코드)\n\n- DB 쿼리 실행 시간\n\n- 외부 API 호출 시간\n\n스팬(Span)\n\n- 트랜잭션 내부의 세부 작업 (예: SQL 쿼리, HTTP 요청 등)\n\n에러(Error)\n\n- Unhandled Exception, HTTP 500 오류 등\n\n메트릭(Metric)\n\n- CPU, 메모리 사용량\n\n성능 오버헤드를 최소화하기 위해 전체 Body를 저장하지 않으며, URL, 상태 코드, 응답 시간, 헤더 등의 정보만 캡처합니다. 다만 저장하려면 추가 설정을 통해 저장할 수 있습니다. \n\n## 분산 트레이싱(Distributed Tracing)\n\nElastic APM에서는 \"분산 트레이싱(Distributed Tracing)\" 이라는 기능을 사용하여 각 서비스가 독립적이더라도 같은 요청을 추적할 수 있습니다. 이 과정에서 Trace ID와 Transaction ID라는 개념이 사용됩니다.\n\n### 요청 흐름 예시 (MSA 환경)\n\n1. 클라이언트 → `API Gateway`로 요청 보냄\n\n1. API Gateway → `User Service` 호출 (Trace ID 생성)\n\n1. User Service → `Order Service` 호출 (같은 Trace ID 사용)\n\n1. Order Service → `Payment Service` 호출 (같은 Trace ID 사용)\n\nTrace ID가 동일한 요청이라면 여러 서비스에서 발생한 트랜잭션이 하나의 흐름으로 연결됩니다.\n\n각 마이크로서비스는 독립적으로 실행되지만, 각 요청이 \"같은 요청\"임을 Trace ID를 통해 추적하는 방식입니다.\n\n### 분산 트레이싱이 동작하는 방식\n\n- 최초 요청이 들어오면 APM 에이전트가 `Trace ID`를 생성\n\n- 서비스 간의 HTTP 요청 헤더에 `Trace ID`를 자동으로 추가\n\n- 다른 서비스에서도 이 `Trace ID`를 읽고 이어서 로깅\n\n### 헤더 예시 (Trace Context)\n\n```makefile\nelastic-apm-traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\n```\n\n각 MSA가 이 헤더를 참조하여 \"같은 요청\"임을 식별합니다. OpenTelemetry 및 W3C Trace Context 표준을 지원하기 때문에, 다른 APM 툴과도 연동할 수 있습니다. Elastic APM을 사용하면, 마이크로서비스 간 각 요청이 어떻게 전달되었는지를 Kibana에서 시각적으로 분석할 수 있습니다.\n\n### Trace ID 발행 방식\n\nAPM 에이전트가 요청을 처음 만나면, Trace ID를 생성하여 HTTP 헤더에 포함합니다.\n\n1. 요청이 처음 들어오면 (Trace ID가 없음)\n\n    → APM 에이전트가 새로운 Trace ID를 생성\n\n    → HTTP 헤더(`elastic-apm-traceparent`)에 추가\n\n    → APM 로그를 저장할 때 Trace ID를 키값으로 저장\n\n1. 이미 Trace ID가 존재하는 경우 (다른 서비스에서 발행됨)\n\n    → 기존 요청 헤더에서 `Trace ID`를 가져와 그대로 사용\n\n    → 같은 Trace ID를 기반으로 새로운 트랜잭션(Transaction)을 생성\n\n    → APM 로그 저장 시 기존 Trace ID를 유지\n\nAPM이 적용된 서비스는 Trace ID가 없는 요청을 만나면 자동으로 생성하고, 이미 있으면 그대로 유지하면서 로깅합니다.\n\n### 로그 저장 및 추적\n\nElastic APM은 Trace ID를 기반으로 모든 서비스의 로그를 그룹화하여 저장합니다.\n\n- Trace ID를 기준으로 각 요청의 흐름을 추적\n\n- 개별 서비스의 실행 시간을 트랜잭션(Transaction) 단위로 기록\n\n- 요청 내에서 실행된 특정 동작(DB 쿼리, HTTP 요청 등)은 스팬(Span)으로 기록\n\n- 모든 기록은 Elasticsearch에 저장되어 Kibana에서 Trace ID별로 시각화 가능\n\n📌 예제\n\n| Trace ID | 서비스 | 요청 URL | 응답 시간 | 상태 코드 |\n| --- | --- | --- | --- | --- |\n| `abc123` | API Gateway | `/user/profile` | 120ms | 200 |\n| `abc123` | User Service | `/get-user` | 80ms | 200 |\n| `abc123` | Order Service | `/get-orders` | 150ms | 200 |\n| `abc123` | Payment Service | `/payment-status` | 250ms | 500 |\n\n이런 식으로 하나의 요청이 어떤 서비스들을 거쳤고, 어디에서 병목이 발생했는지 확인 가능합니다.\n\n따라서 Trace ID를 기반으로 로그를 그룹화하려면 모든 요청이 지나는 마이크로서비스(MSA)에 APM 에이전트를 설치하는 것이 중요합니다. 만약 특정 서비스에 APM이 설치되지 않았다면, Kibana에서는 특정 서비스를 거친 요청이 누락되거나 정확한 실행 시간을 알 수 없습니다.\n\n## Kibana\n\nElastic APM + Kibana를 활용하면 Trace ID별 요청 흐름을 시각적으로 분석할 수 있습니다.\n\n- 트랜잭션 별 요청 흐름을 한눈에 확인\n\n- 서비스 간 실행 시간 비교 (병목 지점 분석)\n\n- 에러가 발생한 서비스, 실행 시간 초과 등을 쉽게 탐색\n\nKibana의 APM UI에서는 각 서비스에서 실행된 트랜잭션과 관련된 스팬을 트리 형태로 볼 수 있어 트랜잭션이 어느 서비스에서 가장 오래 걸리는지 확인 가능합니다. \n\n```plain text\nTrace ID: abc123\n├── API Gateway (120ms)\n│   ├── User Service (80ms)\n│   ├── Order Service (150ms)\n│   ├── Payment Service (250ms) → ⛔ 500 에러 발생\n```\n\n\n\n"},{"excerpt":"서론 현재 회사에서 MSA 형태의 백엔드 서비스를 운용중입니다. 다만 지금은 프론트에서 API Gateway 없이 각 마이크로서비스를 직접 호출하고 있습니다.  엔드포인트 통합 작업을 위해 인프라를 고민중입니다. 유지보수가 편해야 하며, 작업공수가 가장 적게 들어가는 방법을 선택하려고 합니다.  또한 비용의 추가는 가능한 한 줄이려고 합니다.  AWS A…","fields":{"slug":"/우리-팀에-맞는-API-Gateway-선택하기/"},"frontmatter":{"date":"January 22, 2025","title":"우리 팀에 맞는 API Gateway 선택하기","tags":["AWS","DevOps","BackEnd"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n현재 회사에서 MSA 형태의 백엔드 서비스를 운용중입니다. 다만 지금은 프론트에서 API Gateway 없이 각 마이크로서비스를 직접 호출하고 있습니다. \n\n엔드포인트 통합 작업을 위해 인프라를 고민중입니다. 유지보수가 편해야 하며, 작업공수가 가장 적게 들어가는 방법을 선택하려고 합니다. \n\n또한 비용의 추가는 가능한 한 줄이려고 합니다. \n\n## AWS API Gateway\n\n### 소개\n\nAWS API Gateway는 RESTful API, WebSocket API 및 HTTP API를 생성, 배포 및 관리할 수 있도록 지원하는 완전 관리형 서비스입니다.\n\n#### API 라우팅 및 관리\n\n- 클라이언트 요청을 AWS Lambda, EC2, ECS, EKS, DynamoDB, 다른 API 등으로 라우팅\n\n- 요청 검증, 요청 변환, 응답 변환 등의 기능 제공.\n\n- REST API, HTTP API, WebSocket API 지원.\n\n#### 보안 및 인증\n\n- AWS IAM, Cognito, Lambda Authorizer를 통한 인증 및 권한 관리 가능.\n\n- JWT, OAuth 2.0 지원.\n\n#### 트래픽 관리 및 스케일링\n\n- 자동 스케일링 지원 (트래픽 증가 시 자동 확장).\n\n#### 로깅 및 모니터링\n\n- AWS CloudWatch, AWS X-Ray, OpenTelemetry 등을 활용한 API 성능 모니터링 가능.\n\n- API 호출량, 응답 시간, 오류율 분석 가능.\n\n### 비용\n\n[https://aws.amazon.com/ko/api-gateway/pricing/](https://aws.amazon.com/ko/api-gateway/pricing/)\n\n#### 비용 비교 (아시아 태평양 리전 기준)\n\n- REST API\n\n    - 백만 건당 $3.50\n\n    - 캐싱(0.5GB 기준) 사용 시 월 $14.40 추가 비용 발생\n\n- HTTP API\n\n    - 백만 건당 $1.23 (REST API 대비 65% 저렴)\n\n    - 캐싱 미지원 (CloudFront 활용 필요)\n\n#### REST API와 HTTP API\n\nHTTP API는 저렴하지만 (백만 건당 $1) 캐싱 기능을 지원하지 않고 VPC 연결이 불가능합니다. 또 JWT 기반 인증이 가능하지만 Cognito, Lambda Authorizer를 활용한 고급 인증기능은 불가능합니다. \n\nVPC 연결이 불가능하다는 점이 치명적으로, 보안을 위해 이상적인 아키텍쳐는 AWS API Gateway만 Public으로 유지한 채 다른 마이크로 서비스는 private 으로 설정하는 것인데, 이렇게 아키텍쳐를 꾸밀 경우 AWS PrivateLink 또는 VPC 링크를 지원하지 않는 HTTP API로는 대응이 불가능합니다. \n\nREST API는 VPC 링크(VPC Link)를 지원하므로, Private 네트워크 내의 ECS/EKS/EC2와 연결할 수 있습니다.\n\n그럼에도 현재 아키텍쳐를 변경할 계획이 없어 Private 네트워크 접근이 필요 없거나, 인증 서버를 따로 구현해 단순히 API Gateway에서는 JWT 인증만 필요한 경우에는 훨씬 저렴한 HTTP API를 이용하는 것도 좋은 선택이 될 수 있습니다. \n\n다만 HTTP API에서 REST API로 변환하는 것은 자동 변환 기능이 없어 번거로운 작업이 될 수 있습니다. \n\n#### REST API vs HTTP API 선택 기준 정리\n\n✅ REST API를 선택해야 하는 경우\n\n- Private 네트워크(VPC)에 배포된 ECS/EKS/EC2와 연결해야 한다면\n\n- Cognito, Lambda Authorizer 등 고급 인증 기능이 필요하다면\n\n- API Gateway의 캐싱 기능을 사용해야 한다면\n\n✅ HTTP API를 선택할 수 있는 경우\n\n- 기존 아키텍처를 변경할 계획이 없고, Private 네트워크 접근이 필요 없다면\n\n- JWT 기반 인증만 필요하고, Cognito 등 추가 인증이 필요 없다면\n\n- 비용을 최대한 절감해야 한다면 (REST API 대비 65% 저렴)\n\n\n\n❗ 주의: HTTP API에서 REST API로 변경하는 자동 변환 기능이 없으므로, 추후 변경이 필요할 경우 번거로운 작업이 될 수 있음.\n\n### Response 응답 꼴의 가공\n\nREST API에서 제공하는 Mapping Templates 기능을 사용하지 않으면 백엔드에서 내보내는 응답 꼴 그대로 리턴합니다. \n\n✔ HTTP API 사용 시 → 추가 설정 없이 기존 요청/응답 그대로 전달됨\n✔ REST API 사용 시 → Mapping Templates 사용 금지\n✔ API Gateway 오류 응답을 기존 백엔드 응답과 동일하게 설정\n\n이 조건을 만족하면 프론트에서 엔드포인트만 바꾸면 문제없이 작동합니다. \n\n#### Mapping Templates\n\nMapping Templates은 API Gateway에서 요청과 응답을 변환하는 기능으로, REST API에서만 지원하며, HTTP API에서는 사용할 수 없습니다. 기존 API의 응답 구조를 변경하지 않고 통일된 형식으로 응답을 제공할 수 있는 고급 기능으로, 현재 저희 상황에서는 오히려 필요하지 않은 기능입니다. \n\n### Swagger\n\nAPI Gateway의 OpenAPI Export 기능을 사용하면 Swagger 적용이 가능합니다. API Gateway에서 OpenAPI 3.0 형식으로 API 명세를 내보낼 수 있으며, 이를 Swagger UI에서 그대로 활용할 수 있습니다.\n\nAWS S3 + CloudFront를 이용하여 Swagger UI를 정적 웹사이트로 배포할 수 있습니다. API Gateway 수정 이벤트를 감지해 CI/CD 파이프라인을 활용하면 API 변경 시 자동으로 Swagger 문서를 업데이트할 수 있습니다.\n\n### 로깅\n\nElastic APM 적용은 쉽지 않습니다. Elastic APM 에이전트를 API Gateway 자체에 직접 설치하는 것은 불가능하므로, 만약 적용을 하려면 Lambda, OpenTelemetry, X-Amzn-Trace-Id 등을 활용해 우회적으로 API Gateway 트레이싱이 가능하긴 합니다. \n\n다만 API Gateway는 기본적으로 요청을 Lambda, ECS, EC2 등으로 라우팅하는 역할만 하며, 애플리케이션 로직을 처리하지 않고 성능 병목이나 오류는 보통 API Gateway 이후의 서비스에서 발생하기 때문에 APM을 적용할 필요성이 낮습니다.\n\n그럼에도 API Gateway의 로그는 활용성이 높을 수 있지만 CloudWatch를 활용하면 API Gateway 호출량 및 응답 시간 분석이 가능하기 때문에 결국 API Gateway에 직접 APM을 적용할 필요는 거의 없다고 생각됩니다. \n\n### 구현 및 배포\n\nterraform 또는 AWS CloudFormation을 사용하면 API Gateway 설정을 코드로 정의 가능하고, YAML 파일을 수정할 때에 CI/CD를 통해 자동으로 배포 가능합니다. \n\n추후 api가 수정된다면 yaml 파일만 수정해서 푸시하면 자동 배포 되도록 CI/CD 구성이 가능합니다. \n\n#### 예시 yaml\n\n<details>\n<summary>예제</summary>\n\n```yaml\nopenapi: \"3.0.1\"\ninfo:\n  title: \"API Gateway\"\n  version: \"1.0\"\npaths:\n  /api/users/:\n    post:\n      summary: \"Create a new user\"\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              properties:\n                name:\n                  type: string\n                email:\n                  type: string\n      responses:\n        \"200\":\n          description: \"User created successfully\"\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  success:\n                    type: boolean\n                  message:\n                    type: string\n      x-amazon-apigateway-integration:\n        type: \"http_proxy\"\n        httpMethod: \"POST\"\n        uri: \"https://api.example.com/users/\"\n        passthroughBehavior: \"when_no_match\"\n\n```\n\n\n</details>\n\n### AWS CDK(파이썬)를 사용한  API Gateway 관리\n\nAWS CDK는 인프라(AWS 리소스)를 코드로 정의하고 배포할 수 있는 Infrastructure as Code(IaC) 도구입니다. AWS 리소스를 YAML/JSON 같은 설정 파일이 아니라, Python, TypeScript, Java 등 프로그래밍 언어로 정의 가능하게 해줍니다. AWS CloudFormation을 기반으로 동작하며, 코드만 작성하면 자동으로 AWS 리소스를 생성하고 관리가 가능합니다. \n\n<details>\n<summary>REST API 생성 예시 코드 (요청 모델, 응답 모델 포함)</summary>\n\n이 예시 코드는 문서화를 위해 요청/응답 모델을 완벽하게 포함한 것입니다. \n\n```python\nfrom aws_cdk import core\nimport aws_cdk.aws_apigateway as apigateway\n\nclass ApiGatewayStack(core.Stack):\n    def __init__(self, scope: core.Construct, id: str, **kwargs):\n        super().__init__(scope, id, **kwargs)\n\n        # API Gateway 생성\n        api = apigateway.RestApi(\n            self, \"MyApi\",\n            rest_api_name=\"My API\",\n            description=\"API Gateway with Request & Response Models\",\n        )\n\n        # 요청 모델 (Request Model)\n        user_request_model = api.add_model(\n            \"UserRequestModel\",\n            content_type=\"application/json\",\n            schema=apigateway.JsonSchema(\n                schema=apigateway.JsonSchemaVersion.DRAFT4,\n                title=\"UserRequest\",\n                type=apigateway.JsonSchemaType.OBJECT,\n                properties={\n                    \"name\": apigateway.JsonSchema(type=apigateway.JsonSchemaType.STRING),\n                    \"email\": apigateway.JsonSchema(type=apigateway.JsonSchemaType.STRING),\n                },\n                required=[\"name\", \"email\"]\n            ),\n        )\n\n        # 응답 모델 (Response Model)\n        user_response_model = api.add_model(\n            \"UserResponseModel\",\n            content_type=\"application/json\",\n            schema=apigateway.JsonSchema(\n                schema=apigateway.JsonSchemaVersion.DRAFT4,\n                title=\"UserResponse\",\n                type=apigateway.JsonSchemaType.OBJECT,\n                properties={\n                    \"success\": apigateway.JsonSchema(type=apigateway.JsonSchemaType.BOOLEAN),\n                    \"message\": apigateway.JsonSchema(type=apigateway.JsonSchemaType.STRING),\n                }\n            ),\n        )\n\n        # 요청 검증기 (Request Validator)\n        request_validator = api.add_request_validator(\n            \"RequestValidator\",\n            validate_request_body=True,\n            validate_request_parameters=False,\n        )\n\n        # Users 리소스 생성\n        users_resource = api.root.add_resource(\"api\").add_resource(\"users\")\n\n        # POST 요청 (Create User) + 요청/응답 스키마 추가\n        users_resource.add_method(\n            \"POST\",\n            apigateway.Integration(\n                type=apigateway.IntegrationType.HTTP_PROXY,\n                integration_http_method=\"POST\",\n                uri=\"https://api.example.com/users/\"\n            ),\n            request_validator=request_validator,  # 요청 검증기 추가\n            request_models={\"application/json\": user_request_model},  # 요청 모델 추가\n            method_responses=[\n                apigateway.MethodResponse(\n                    status_code=\"200\",\n                    response_models={\"application/json\": user_response_model}\n                )\n            ]\n        )\n\n        # API Gateway 엔드포인트 출력\n        core.CfnOutput(self, \"ApiEndpoint\", value=api.url)\n\n```\n\n\n</details>\n\n<details>\n<summary>REST API 생성 예시 코드 (모델 정의 제외하고 단순화)</summary>\n\n요청 모델, 응답 모델을 문서화를 위해 명확히 정의하면 유지보수 부담이 커질 수 있습니다. 아래처럼 정의하면 문서화를 조금 포기하고 대신 효율적인 유지보수가 가능합니다. \n\n```python\nfrom aws_cdk import core\nimport aws_cdk.aws_apigateway as apigateway\n\nclass ApiGatewayStack(core.Stack):\n    def __init__(self, scope: core.Construct, id: str, **kwargs):\n        super().__init__(scope, id, **kwargs)\n\n        # API Gateway 생성 (간단한 설정)\n        api = apigateway.RestApi(\n            self, \"MyApi\",\n            rest_api_name=\"Simple API\",\n            description=\"Minimal API Gateway Configuration\"\n        )\n\n        # ======================\n        # Users API\n        # ======================\n        users_resource = api.root.add_resource(\"api\").add_resource(\"users\")\n\n        # /api/users/ - POST (Create User)\n        users_resource.add_method(\n            \"POST\",\n            apigateway.Integration(\n                type=apigateway.IntegrationType.HTTP_PROXY,\n                integration_http_method=\"POST\",\n                uri=\"https://api.example.com/users/\"\n            )\n        )\n\n        # ======================\n        # Orders API \n        # ======================\n        orders_resource = api.root.add_resource(\"api\").add_resource(\"orders\")\n\n        # /api/orders/ - POST (Create Order)\n        orders_resource.add_method(\n            \"POST\",\n            apigateway.Integration(\n                type=apigateway.IntegrationType.HTTP_PROXY,\n                integration_http_method=\"POST\",\n                uri=\"https://api.example.com/orders/\"\n            )\n        )\n\n        # API Gateway 엔드포인트 출력\n        core.CfnOutput(self, \"ApiEndpoint\", value=api.url)\n\n```\n\n\n</details>\n\n외부에 제공해야 하는 api의 경우라면 요청 모델, 응답 모델까지 명확하게 정의해야 합니다. 그런 경우가 아니라면 선택의 여지가 있습니다. \n\n현재 예시 코드는 REST API 배포를 위한 것으로 HTTP API의 경우에는 코드가 달라집니다. \n\n<details>\n<summary>HTTP API 생성 예시 코드</summary>\n\n```python\nimport aws_cdk.aws_apigatewayv2 as apigatewayv2\nimport aws_cdk.aws_apigatewayv2_integrations as integrations\n\nclass ApiGatewayStack(core.Stack):\n    def __init__(self, scope: core.Construct, id: str, **kwargs):\n        super().__init__(scope, id, **kwargs)\n\n        # HTTP API 생성\n        api = apigatewayv2.HttpApi(self, \"MyHttpApi\", api_name=\"Simple HTTP API\")\n\n        # Users 엔드포인트 추가\n        api.add_routes(\n            path=\"/api/users\",\n            methods=[apigatewayv2.HttpMethod.POST],\n            integration=integrations.HttpUrlIntegration(\n                \"UsersService\",\n                integration_url=\"https://api.example.com/users/\"\n            )\n        )\n\n```\n\n\n</details>\n\n#### AWS CDK의 배포\n\nAWS CLI를 활용해 수동으로 배포할 수 있습니다. \n\n```bash\ncdk deploy\n```\n\n를 사용해 배포합니다.\n\n`cdk synth` 명령어를 사용해 CloudFormation 템플릿을 미리 확인할 수 있습니다. \n\n#### 예상되는 어려움\n\n위의 예시 코드를 래핑을 해서 최대한 유지보수가 쉽게 코드는 작성하겠지만, 결국 각 마이크로 서비스를 수정한 이후에 이 프로젝트를 수정하는 별도의 작업을 추가로 해줘야 합니다. \n\n### Django DRF + `drf-spectacular` + CI/CD를 활용한 API Gateway 관리\n\n`drf-spectacular`는 Django DRF에서 OpenAPI 3.0 문서를 자동 생성해주는 라이브러리입니다. Swagger UI, Redoc, Postman 등에서 활용할 수 있는 API 문서를 제공하는데, 이 OpenAPI YAML 파일을 AWS API Gateway가 OpenAPI Import 기능을 지원하기 때문에 활용할 수 있습니다. \n\n#### S3를 활용한 OpenAPI 문서의 관리\n\n각 마이크로서비스에서 `drf-spectacular` 를 사용해 YAML 파일을 생성합니다. \n\n```bash\npython service1/manage.py spectacular --file service1_openapi.yaml\n```\n\n예를 들어 service1에서 api의 수정/삭제 등등이 일어나면 service1\\_openapi.yaml을 생성합니다. \n\nS3 버킷(`my-api-docs`)에 각 서비스의 OpenAPI 문서를 저장합니다. \n\n```bash\naws s3 cp service1_openapi.yaml s3://my-api-docs/service1_openapi.yaml\n```\n\n각 마이크로서비스의 문서를 병합해 최종 문서를 완성합니다. \n\n```bash\nopenapi-merge-cli -i s3://my-api-docs/service1_openapi.yaml s3://my-api-docs/service2_openapi.yaml -o merged_openapi.yaml\n```\n\n병합된 문서로 API Gateway를 업데이트 합니다. \n\n```bash\naws apigateway import-rest-api --body file://merged_openapi.yaml --rest-api-id ${{ secrets.API_GATEWAY_ID }}\n```\n\n#### 예상되는 어려움\n\n1. 문서 병합의 까다로움\n\n    엔드포인트가 같을 경우 병합에서 반드시 문제가 발생하여 특별히 신경써야 합니다. 또 각 API의 인증 방법이 다를 경우 해당 방법은 적용이 불가능하며, 이 외에도 응답 스키마, 공통 컴포넌트 등에서도 병합시 문제가 발생할 가능성이 있습니다. \n\n1. 각 서비스에서 추가 작업이 필요할 수 있음\n\n    혹시 swagger 생성을 위한 밑작업이 잘 안되어있다면 해당 작업을 위해 추가 작업이 필요할 수 있습니다. 또 엔드포인트가 현재 것을 유지하면서 추가 또는 수정 되는 것이 아니라 AWS API Gateway로 결국 엔드포인트가 변경이 되는 것으로 이로 인한 추가 공수가 다시 발생할 수 있습니다. \n\n위의 문제로 인해 실제 작업시에는 AWS CDK를 사용하는 것보다 예상하지 못한 문제가 발생할 가능성이 커보입니다. \n\n## AWS API Gateway 구현 및 배포 결론\n\n마이크로 서비스 수정 후 추가적으로 코드 작업이 필요하다는 단점이 있지만, 그럼에도 불구하고 `drf-spectacular`을 활용한 OpenAPI 병합 방식은 가능하지만, 유지보수성과 API 변경 관리를 고려할 때 AWS CDK가 더 적합한 선택이라고 생각합니다. \n\n## 직접 구현과의 비교\n\n| 비교 항목 | **직접 구현 (Django + DRF)** | **AWS API Gateway** |\n| --- | --- | --- |\n| **트래픽 처리** | uWSGI/Gunicorn 조정 필요 | 자동 확장(Auto Scaling) |\n| **성능 최적화** | Keep-Alive, Connection Pooling 직접 설정 필요 | AWS에서 자동 최적화 |\n| **로깅 & 모니터링** | Elastic APM 활용 | CloudWatch, X-Ray |\n\n## Kong API Gateway과의 비교\n\n### Kong API Gateway 소개\n\nKong은 오픈소스로 제공되는 고성능 API Gateway 및 마이크로서비스 관리 플랫폼입니다. 분산된 API 트래픽을 제어하고, 로드 밸런싱, 인증, 모니터링, 캐싱 등의 기능을 제공합니다.\n\n### **Kong을 사용하면 좋은 경우**\n\n- 온프레미스 환경에서 API Gateway가 필요할 때\n\n- AWS 이외의 멀티 클라우드 환경을 고려할 때\n\n"},{"excerpt":"GraphQL의 특징 클라이언트 주도형 데이터 요청 REST API는 백엔드가 정해준 엔드포인트를 그대로 사용해야 하지만, GraphQL은 클라이언트가 원하는 데이터만 요청할 수 있습니다.  REST API 요청 예시 (불필요한 데이터 포함) GraphQL 요청 예시 (원하는 데이터만 선택) REST API는 필요하지 않은 데이터까지 포함되지만, Grap…","fields":{"slug":"/GraphQL과-Django/"},"frontmatter":{"date":"January 20, 2025","title":"GraphQL과 Django","tags":["Python","Django","GraphQL"]},"rawMarkdownBody":"![](image1.png)\n## GraphQL의 특징\n\n### 클라이언트 주도형 데이터 요청\n\nREST API는 백엔드가 정해준 엔드포인트를 그대로 사용해야 하지만, GraphQL은 클라이언트가 원하는 데이터만 요청할 수 있습니다. \n\n#### REST API 요청 예시 (불필요한 데이터 포함)\n\n```plain text\nGET /users/1\n```\n\n```json\n{\n  \"id\": 1,\n  \"name\": \"Alice\",\n  \"email\": \"alice@example.com\",\n  \"phone\": \"123-456-7890\"\n}\n```\n\n#### GraphQL 요청 예시 (원하는 데이터만 선택)\n\n```plain text\n{\n  user(id: 1) {\n    name\n    email\n  }\n}\n```\n\n```json\n{\n  \"data\": {\n    \"user\": {\n      \"name\": \"Alice\",\n      \"email\": \"alice@example.com\"\n    }\n  }\n}\n```\n\nREST API는 필요하지 않은 데이터까지 포함되지만, GraphQL은 원하는 필드만 반환합니다. \n\n### 하나의 엔드포인트로 모든 요청 처리\n\nREST API는 엔드포인트가 여러 개(GET /users, GET /users/1/posts 등) 필요하지만, GraphQL은 단 하나의 엔드포인트(/graphql)만 사용하여 모든 요청을 처리할 수 있습니다.\n\nGraphQL에서는 Mutation이 데이터 변경(Create, Update, Delete) 작업이지만, HTTP 요청 방식은 기본적으로 POST를 사용합니다. 왜냐하면 요청 본문(Body)에 쿼리를 포함해야 하기 때문입니다. \n\n### 타입 시스템 & 스키마 정의\n\nGraphQL은 스키마 기반으로 동작하며, 데이터의 타입을 명확하게 정의합니다.\n\n```plain text\ntype User {\n  id: ID!\n  name: String!\n  email: String!\n  posts: [Post!]!\n}\ntype Post {\n  id: ID!\n  title: String!\n  content: String!\n}\ntype Query {\n  user(id: ID!): User\n  posts: [Post]\n}\n```\n\n### 정리\n\nGraphQL은 원하는 데이터만 요청이 가능하고, 하나의 엔드포인트로 모든 요청을 처리하며 강력한 타입 시스템으로 별도의 문서화가 필요 없어 프론트엔드에 최적화되어 있습니다. \n\n다만 쿼리가 복잡해질 경우 성능 이슈가 발생할 수 있음에 유의해야 합니다. \n\n## Django의 GraphQL\n\nDjango에서 GraphQL을 사용하려면 보통 `graphene-django` 라이브러리를 활용합니다.\n\n### 설치\n\n다음의 명령어를 통해 설치합니다. \n\n```bash\npip install graphene-django\n```\n\n그리고 Django 프로젝트의 `settings.py`에 GraphQL를 추가합니다. \n\n```python\nINSTALLED_APPS = [\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"graphene_django\",  # GraphQL 추가\n    \"myapp\",\n]\n\nGRAPHENE = {\n    \"SCHEMA\": \"myapp.schema.schema\"  # GraphQL 스키마 경로 지정\n}\n```\n\n\n\n### `graphene.ObjectType`\n\nDjango에서 GraphQL을 사용할 때, `graphene.ObjectType`을 상속받아 **GraphQL에서 응답 데이터의 구조를 정의**합니다.\n\n```python\nimport graphene\n\nclass UserType(graphene.ObjectType):\n    id = graphene.ID()\n    name = graphene.String()\n    email = graphene.String()\n```\n\n- GraphQL에서 `UserType`을 정의.\n\n- `id`, `name`, `email`이 응답에 포함될 필드임을 지정.\n\n즉, GraphQL API에서 클라이언트가 어떤 데이터를 받을 수 있는지 정의하는 것입니다.\n\n### `DjangoObjectType`\n\nDjango 모델을 GraphQL 타입으로 변환할 때는 `graphene_django.types.DjangoObjectType`을 사용합니다.\n\n```python\nfrom graphene_django.types import DjangoObjectType\nfrom .models import User\n\nclass UserType(DjangoObjectType):\n    class Meta:\n        model = User  # Django 모델과 자동 매핑\n```\n\n- `User` 모델과 자동으로 연결됨.\n\n- Django 모델 필드를 자동으로 GraphQL 필드로 변환 (`CharField → String`, `IntegerField → Int`  등).\n\n### GraphQL과 CSRF\n\n#### CSRF 보호란?\n\nDjango는 CSRF(Cross-Site Request Forgery) 공격 방지를 위해 POST 요청을 보호합니다. 하지만 GraphQL은 기본적으로 POST 요청을 사용하기 때문에 CSRF 보호 정책과 충돌할 수 있습니다. 따라서 `csrf_exempt`를 사용하면 Django의 CSRF 미들웨어가 해당 뷰를 검사하지 않도록 설정할 수 있습니다. \n\nGraphQL은 일반적으로 REST API처럼 클라이언트가 직접 요청을 보내는 방식입니다. 즉, 웹 브라우저에서 실행되는 폼(form) 기반 요청이 아니므로 CSRF 토큰을 사용할 필요가 없음.\n\n#### 보안 강화를 위한 대체 방법\n\n`csrf_exempt` 대신 `@csrf_protect`를 사용하는 방법이 있습니다. \n\n```python\nfrom django.views.decorators.csrf import csrf_protect\n\nurlpatterns = [\n    path(\"graphql/\", csrf_protect(GraphQLView.as_view(graphiql=True))),  # CSRF 보호 적용\n]\n```\n\n이렇게 하면 CSRF 보호를 유지하면서도 GraphQL API를 안전하게 사용할 수 있습니다. \n\n\n\n또, JWT 또는 Token 기반 인증을 사용할 수 있습니다. \n\nGraphQL API는 보통 JWT(Json Web Token) 또는 OAuth 토큰을 사용하여 인증합니다. 이렇게 하면 CSRF 보호 없이도 API 요청을 안전하게 관리할 수 있습니다.\n\n```python\nfrom graphql_jwt.decorators import login_required\nimport graphene\n\nclass Query(graphene.ObjectType):\n    user = graphene.Field(UserType)\n\n    @login_required\n    def resolve_user(self, info):\n        return info.context.user\n```\n\nCSRF 공격은 \"브라우저가 자동으로 요청을 보내는 방식\"을 악용하는 공격입니다. 그러나 JWT 인증 방식은 CSRF 보호가 필요 없는 구조입니다. 즉, GraphQL + JWT 인증을 사용하면 `csrf_exempt` 없이도 보안 문제를 예방할 수 있습니다. \n\n### Query\n\nGraphQL에서 데이터를 조회할 때는 `Query`를 사용합니다. Django에서는 다음과 같이 구현할 수 있습니다. \n\n```python\nimport graphene\nfrom graphene_django.types import DjangoObjectType\nfrom .models import User\n\nclass UserType(DjangoObjectType):\n    class Meta:\n        model = User\n\nclass Query(graphene.ObjectType):\n    user = graphene.Field(UserType, id=graphene.Int())\n\n    def resolve_user(self, info, id):\n        return User.objects.get(pk=id)\n\nschema = graphene.Schema(query=Query)\n```\n\n### Mutation\n\nGraphQL은 조회뿐만 아니라, 데이터를 추가/수정/삭제할 수도 있습니다. 이를 위해 Mutation을 사용합니다.\n\n요청을 이렇게 보낸다면, \n\n```plain text\nmutation {\n  createUser(name: \"Alice\", email: \"alice@example.com\") {\n    user {\n      id\n      name\n      email\n    }\n  }\n}\n```\n\n아래와 같이 구현할 수 있습니다. \n\n```python\nclass CreateUser(graphene.Mutation):\n    class Arguments:\n        name = graphene.String()\n        email = graphene.String()\n\n    user = graphene.Field(UserType)\n\n    def mutate(self, info, name, email):\n        user = User.objects.create(name=name, email=email)\n        return CreateUser(user=user)\n\nclass Mutation(graphene.ObjectType):\n    create_user = CreateUser.Field()\n```\n\n이런식으로 `Update`, `Delete`도 구현이 가능합니다. \n\n### `resolve_<필드명>` 규칙\n\nGraphQL에서 `Query`나 `Mutation`을 처리할 때, `graphene`은 자동으로 해당 필드를 처리하는 `resolve_<필드명>` 메서드를 찾습니다. 따라서 필드명이 `user`라면 `resolve_user()`라는 메서드를 만들면 자동으로 연결됩니다.\n\n```python\nimport graphene\nfrom graphene_django.types import DjangoObjectType\nfrom .models import User\n\nclass UserType(DjangoObjectType):\n    class Meta:\n        model = User\n\nclass Query(graphene.ObjectType):\n    user = graphene.Field(UserType, id=graphene.Int())  # GraphQL 필드 정의\n\n    def resolve_user(self, info, id):  # 필드명과 매칭된 resolver 함수\n        return User.objects.get(pk=id)\n\nschema = graphene.Schema(query=Query)\n```\n\n이 코드에서 GraphQL의 `user` 필드를 요청하면 `resolve_user()`가 자동 실행됩니다. `resolve_user(self, info, id)`를 만들지 않으면, `user` 필드의 데이터를 가져올 방법이 없기 때문에 오류가 발생합니다. \n\n`resolve_`를 사용하지 않고, `Field()`에서 직접 `resolver`를 지정할 수도 있지만, 일반적으로는 `resolve_`를 사용하는 것이 가독성이 좋고 유지보수가 쉽습니다. \n\n```python\nclass Query(graphene.ObjectType):\n    user = graphene.Field(UserType, id=graphene.Int(), resolver=lambda self, info, id: User.objects.get(pk=id))\n\n```\n\n### GraphQL의 성능 최적화 (N+1 문제 해결)\n\nGraphQL은 기본적으로 [N+1 문제](https://sharknia.github.io/N1-문제)가 발생할 가능성이 큽니다. Django ORM에서는 `select_related()`와 `prefetch_related()`를 활용해 최적화할 수 있습니다.\n\n예를 들어, \n\n```python\nclass Query(graphene.ObjectType):\n    users = graphene.List(UserType)\n\n    def resolve_users(self, info):\n        return User.objects.all()  # 각 user마다 profile을 조회하는 추가 쿼리 발생!\n```\n\n위와 같이 작성하면 `users`를 조회할 때 각 `profile`을 개별 쿼리로 조회해 N+1 문제가 발생합니다. \n\n```python\nclass Query(graphene.ObjectType):\n    users = graphene.List(UserType)\n\n    def resolve_users(self, info):\n        return User.objects.select_related(\"profile\").all()  # SQL JOIN 사용\n```\n\n이렇게 하면 한 번의 SQL 쿼리로 해결 가능합니다. ManyToMany 또는 1:N의 경우에는 `prefetch_related()`를 사용해 최적화 할 수 있습니다. \n\n## 예고편\n\n다음에는 GraphQL의 캐싱 전략, batch queries, Persisted Queries와 함께 GraphQL의 강점 중 하나인 Subscription(실시간 데이터 스트리밍) 기능에 대해 알아보겠습니다. 또 django의 `graphene-file-upload`를 이용한 파일 업로드 방법이나 `relay-style pagination` 에 대해서도 알아보려고 합니다. \n\n\n\n"},{"excerpt":"React Native Paper 소개 는 React Native로 앱을 개발할 때 Material Design 가이드를 준수하는 UI 컴포넌트 라이브러리입니다. Google의 Material Design 철학을 기반으로 한 다양한 UI 요소를 제공합니다. React Native를 처음 사용하거나, 디자인을 크게 신경 쓰지 않고 표준적인 UI를 빠르게 구…","fields":{"slug":"/React-Native-Navigation과-Paper-기본부터-사용하기/"},"frontmatter":{"date":"January 13, 2025","title":"React Native Navigation과 Paper: 기본부터 사용하기","tags":["React Native","Typescript"]},"rawMarkdownBody":"![](image1.png)\n## React Native Paper\n\n### 소개\n\n`react-native-paper`는 React Native로 앱을 개발할 때 Material Design 가이드를 준수하는 UI 컴포넌트 라이브러리입니다. Google의 Material Design 철학을 기반으로 한 다양한 UI 요소를 제공합니다. React Native를 처음 사용하거나, 디자인을 크게 신경 쓰지 않고 표준적인 UI를 빠르게 구축하려는 경우에 적합합니다. \n\n### 특징\n\n- Material Design 준수\n\n    - Google Material Design의 스타일 가이드를 따르며, 디자인 일관성을 유지하기에 적합합니다.\n\n- 사용하기 쉬운 컴포넌트\n\n    - 버튼, 카드, 텍스트 입력, 다이얼로그, 앱바 등 자주 사용하는 UI 컴포넌트를 제공합니다.\n\n    - **프리셋 스타일**과 **테마**를 이용해 빠르게 개발할 수 있습니다.\n\n- 커스터마이징 가능\n\n    - 제공되는 컴포넌트를 프로젝트에 맞게 커스터마이징할 수 있으며, 테마를 적용하여 색상, 글꼴 등을 변경 가능합니다.\n\n- 반응형 디자인 지원\n\n    - 다양한 화면 크기 및 플랫폼(iOS, Android)에서 잘 동작하는 반응형 디자인을 지원합니다.\n\n- React Native 친화적\n\n    - React Native의 철학에 맞춰 제작되었으며, `react-native-vector-icons`와 잘 통합됩니다.\n\n### 주요 컴포넌트\n\n- Button: 기본 버튼, 아이콘 버튼 등 다양한 스타일의 버튼 제공.\n\n- Card: Material Design 스타일의 카드 컴포넌트.\n\n- AppBar: 상단바(AppBar) 컴포넌트.\n\n- TextInput: 다양한 텍스트 입력 UI.\n\n- FAB: 플로팅 액션 버튼(Floating Action Button).\n\n- Dialog: 알림 대화상자.\n\n- Snackbar: 사용자에게 간단한 알림을 제공하는 스낵바.\n\n### 설치\n\n```bash\nnpm install nativewind react-native-paper react-native-vector-icons\n```\n\n### 예제\n\n```typescript\nimport * as React from 'react';\nimport { Button } from 'react-native-paper';\n\nconst MyComponent = () => (\n    <Button icon=\"camera\" mode=\"contained\" onPress={() => console.log('Pressed')}>\n        Press me\n    </Button>\n);\n\nexport default MyComponent;\n\n```\n\n위처럼 버튼 컴포넌트를 쉽게 사용할 수 있습니다. \n\n### 아이콘이 제대로 로드되지 않는 문제\n\nreact-native-vector-icons를 사용해 아이콘을 쓰고 싶은데, 아이콘 이름을 제대로 입력해도 아이콘이 제대로 출력되지 않는 문제가 발생했습니다. 이 문제는 다음의 방법을 사용해 해결할 수 있습니다. \n\nreact-native-vector-icons.d.ts 파일을 생성합니다. \n\n```typescript\ndeclare module 'react-native-vector-icons/MaterialCommunityIcons' {\n    import { Component } from 'react';\n    import { IconProps } from 'react-native-vector-icons/Icon';\n\n    export default class MaterialCommunityIcons extends Component<IconProps> {}\n}\n```\n\nApp.tsx의 상단부에 선언합니다. \n\n```typescript\nimport Icon from 'react-native-vector-icons/MaterialCommunityIcons';\n\nIcon.loadFont();\n```\n\n아래와 같이 사용합니다. \n\n```typescript\nimport React from 'react';\nimport { StyleSheet } from 'react-native';\nimport { Button } from 'react-native-paper';\nimport Icon from 'react-native-vector-icons/MaterialCommunityIcons';\n\ninterface SocialButtonProps {\n    text: string;\n    icon: string;\n    color: string;\n    onPress: () => void;\n}\n\nconst SocialButton: React.FC<SocialButtonProps> = ({ text, icon, color, onPress }) => {\n    return (\n        <Button\n            mode=\"contained\"\n            onPress={onPress}\n            style={[styles.button, { backgroundColor: color }]}\n            labelStyle={styles.label}\n            icon={() => <Icon name={icon} size={24} color=\"white\" />}\n        >\n            {text}\n        </Button>\n    );\n};\n```\n\n[이 페이지](https://pictogrammers.com/library/mdi/)에서 아이콘을 미리 볼 수 있습니다. \n\n## React Navigation\n\n상단 헤더는 `react-native-paper` 를 사용해 간단하게 구현이 가능하지만 하단탭은 React Navigation 라이브러리를 설치하고 사용해야 합니다.\n\n```typescript\nnpm install @react-navigation/native @react-navigation/bottom-tabs react-native-screens react-native-safe-area-context react-native-gesture-handler react-native-reanimated react-native-vector-icons\n```\n\nAppNavigator를 만들 때 사용했던 `AppNavigator.tsx` 파일에 다음의 내용을 추가 작성합니다. \n\n```typescript\nimport { createBottomTabNavigator } from '@react-navigation/bottom-tabs';\nimport React from 'react';\nimport Icon from 'react-native-vector-icons/MaterialCommunityIcons';\nimport FeedScreen from '../screens/FeedScreen';\nimport HomeScreen from '../screens/HomeScreen';\nimport WriteScreen from '../screens/WriteScreen';\n\nconst Tab = createBottomTabNavigator();\n\nconst BottomTabNavigator: React.FC = () => {\n    return (\n        <Tab.Navigator\n            screenOptions={({ route }) => ({\n                tabBarIcon: ({ color, size }) => {\n                    let iconName;\n                    if (route.name === 'Home') {\n                        iconName = 'home';\n                    } else if (route.name === 'Write') {\n                        iconName = 'pencil';\n                    }\n                    return <Icon name={iconName} size={size} color={color} />;\n                },\n                tabBarActiveTintColor: '#F4A261',\n                tabBarInactiveTintColor: 'gray',\n                // tabBarShowLabel: false, // 메뉴 이름 숨기기\n                headerShown: false,\n            })}\n        >\n            <Tab.Screen name=\"Home\" component={HomeScreen} />\n            <Tab.Screen name=\"Write\" component={WriteScreen} />\n        </Tab.Navigator>\n    );\n};\n```\n\n원하는대로 생성된 하단바의 모습을 볼 수 있습니다. \n\n\n\n"},{"excerpt":"React Native 환경 설정 현재 맥북을 사용하고 있고, IDE는 VScode를 사용하고 있습니다. NVM은 이미 설치되어 있다고 가정하겠습니다.  먼저, iOS 개발을 위한 환경을 확인하세요. Xcode 설치 App Store에서 Xcode를 설치하세요. 설치 후 Xcode를 실행하고, \"Preferences > Locations\"에서 Comman…","fields":{"slug":"/React-Native-설치/"},"frontmatter":{"date":"January 12, 2025","title":"React Native 설치","tags":["React Native","iOS","Node.js","Typescript"]},"rawMarkdownBody":"![](image1.png)\n## **React Native 환경 설정**\n\n현재 맥북을 사용하고 있고, IDE는 VScode를 사용하고 있습니다. NVM은 이미 설치되어 있다고 가정하겠습니다. \n\n먼저, iOS 개발을 위한 환경을 확인하세요.\n\n### **Xcode 설치**\n\n- App Store에서 Xcode를 설치하세요.\n\n- 설치 후 Xcode를 실행하고, \"Preferences > Locations\"에서 Command Line Tools를 설정합니다.\n\n    ![](image2.png)\n### **Node.js 설치**\n\nNVM을 이미 사용중이므로 다음의 명령어를 사용해 node 버전 명시만 해주겠습니다. 20.13.1을 사용하겠습니다. \n\n```bash\necho \"20.13.1\" > .nvmrc\n```\n\n## Watchman 설치\n\n#### **Watchman의 역할**\n\n- React Native CLI를 사용할 때, 파일 변경 사항을 감지해 앱을 즉각적으로 업데이트.\n\n- 대규모 프로젝트에서도 빠르게 파일 변경을 감지.\n\n- 선택 사항이지만, 맥에서 개발할 때 성능을 개선하는 데 유용.\n\n```bash\nbrew install watchman\n```\n\n설치 후 아래의 명령어로 설치를 확인합니다. \n\n```bash\nwatchman --version\n```\n\n## React Native 프로젝트 생성\n\nReact Native CLI를 사용해 프로젝트를 생성합니다.\n\n```bash\nnpx @react-native-community/cli init MyFirstApp\ncd MyFirstApp\n```\n\n설치를 하다보면 CocoaPods를 설치하겠냐고 물어봅니다. \n\n### CocoaPods\n\n**CocoaPods**는 macOS 및 iOS 앱 개발에서 사용되는 **의존성 관리 도구**입니다. React Native 프로젝트의 iOS 빌드와 관련된 라이브러리를 관리하기 위해 필수적인 도구입니다.\n\n#### **CocoaPods의 역할**\n\niOS 네이티브 라이브러리 관리\n\n- 외부 라이브러리(예: React Native Paper, Firebase 등)를 프로젝트에 추가하고 관리합니다.\n\n- 필요할 때마다 최신 버전으로 업데이트를 쉽게 수행할 수 있습니다.\n\n자동 구성\n\n- 네이티브 라이브러리를 프로젝트에 포함시킬 때 필요한 설정(예: Xcode 빌드 설정)을 자동으로 처리합니다.\n\n- CocoaPods 없이 이런 설정을 수동으로 관리하면 시간이 많이 걸리고 오류가 발생할 가능성이 높습니다.\n\nPodfile 관리\n\n- CocoaPods는 프로젝트의 의존성을 Podfile이라는 파일로 관리합니다.\n\n- Podfile은 어떤 라이브러리를 사용할지 정의하는 파일입니다.\n\n#### 설치\n\n```bash\npod --version\n```\n\nCocoaPods가 설치되지 않았다면 다음의 명령어를 사용해 설치합니다. \n\n```bash\nbrew install cocoapods\n```\n\ncocoapods를 지금 설치했다면 CocoaPods 의존성 재설치를 해야 합니다. \n\n```bash\ncd ios\npod install\n```\n\n## iOS 프로젝트 빌드\n\n```bash\nnpx react-native run-ios\n```\n\niOS 시뮬레이터가 자동으로 실행되며, 기본 React Native 앱이 표시됩니다.\n\n### 시뮬레이터\n\n해당 명령어를 실행했을 때, 다음과 같은 오류가 발생할 수 있습니다. \n\n```bash\nerror iOS devices or simulators not detected. Install simulators via Xcode or connect a physical iOS device\n```\n\n이 오류는 시뮬레이터가 설치되지 않은 것으로,\n\n```bash\nxcrun simctl list devices\n```\n\n이 명령어를 사용해 설치된 시뮬레이터를 확인할 수 있습니다. 만약 시뮬레이터가 설치되어 있지 않다면 설치해야 합니다. 저는 최초 Xcode 설치 시 iOS를 선택하지 않아 해당 오류가 발생했습니다. \n\nXcode의 설정에서 Components 탭으로 이동해 iOS 시뮬레이터를 설치해주면 됩니다. \n\n![](image3.png)\n### 오류!\n\n```bash\nError: EMFILE: too many open files, watch\n    at FSWatcher._handle.onchange (node:internal/fs/watchers:207:21)\nEmitted 'error' event on NodeWatcher instance at:\n    at FSWatcher._checkedEmitError (/Users/furychick/Develop/tuum/node_modules/metro-file-map/src/watchers/NodeWatcher.js:82:12)\n    at FSWatcher.emit (node:events:519:28)\n    at FSWatcher._handle.onchange (node:internal/fs/watchers:213:12) {\n  errno: -24,\n  syscall: 'watch',\n  code: 'EMFILE',\n  filename: null\n}\n```\n\n이런 오류가 발생할 수 있습니다. 이 에러는 파일 감시(watch)와 관련된 문제이며, macOS에서 `EMFILE: too many open files`는 시스템이 동시에 열 수 있는 파일 핸들의 수가 초과되었을 때 발생합니다. Watchman을 사용해 이 오류를 피할 수 있습니다. 이미 설치가 되어있는데도 해당 오류가 발생한다면 Watchman 서비스 재시작 - 캐시 정리 - 다시 실행을 시도합니다. \n\n```bash\nwatchman shutdown-server\nwatchman watch-del-all\nnpx react-native start --reset-cache\n```\n\n## 결론\n\n![](image4.png)\n이 화면이 확인되면 React Native 프로젝트가 **정상적으로 빌드 및 실행된 것**입니다. 이제 React Native의 기본 구조와 기능을 수정하거나 추가하여 프로젝트를 진행할 수 있습니다.\n\n\n\n"},{"excerpt":"일반적인 React Native 프로젝트의 구조 각 디렉토리의 역할  이미지, 폰트 등 정적 파일을 저장. 예: 아이콘, 배경 이미지, 로고 등.  재사용 가능한 UI 요소를 정의. 예: 버튼, 텍스트 입력 필드, 카드, 모달 등.  화면 간의 라우팅과 네비게이션 설정. 예: React Navigation을 사용하여 스택, 탭, 드로어 네비게이션 설정. …","fields":{"slug":"/React-Native의-프로젝트-구조와-화면-이동-구현하기/"},"frontmatter":{"date":"January 12, 2025","title":"React Native의 프로젝트 구조와 화면 이동 구현하기","tags":["React Native","Typescript"]},"rawMarkdownBody":"![](image1.png)\n## 일반적인 React Native 프로젝트의 구조 \n\n```plain text\nroot/\n├── android/                   # Android 네이티브 코드\n├── ios/                       # iOS 네이티브 코드\n├── src/                       # 주요 소스 코드 디렉토리\n│   ├── assets/                # 이미지, 폰트 등 정적 파일\n│   ├── components/            # 재사용 가능한 UI 컴포넌트\n│   ├── navigation/            # 네비게이션 관련 코드\n│   ├── screens/               # 화면(페이지) 컴포넌트\n│   │   ├── LoginScreen.tsx    # 로그인 화면\n│   │   ├── HomeScreen.tsx     # 홈 화면\n│   │   └── ...                # 기타 화면\n│   ├── services/              # API 요청 및 백엔드 연동\n│   ├── utils/                 # 유틸리티 함수와 헬퍼 코드\n│   ├── context/               # 글로벌 상태 관리 (React Context 등)\n│   └── App.tsx                # 메인 엔트리 파일\n├── package.json               # 프로젝트 정보 및 의존성 관리\n├── tsconfig.json              # 타입스크립트 설정 파일\n├── .gitignore                 # Git 무시 파일\n└── README.md                  # 프로젝트 설명\n```\n\n## 각 디렉토리의 역할\n\n### `src/assets/`\n\n- 이미지, 폰트 등 정적 파일을 저장.\n\n- 예: 아이콘, 배경 이미지, 로고 등.\n\n### `src/components/`\n\n- 재사용 가능한 UI 요소를 정의.\n\n- 예: 버튼, 텍스트 입력 필드, 카드, 모달 등.\n\n    ```plain text\n    src/components/\n    ├── CustomButton.tsx\n    ├── InputField.tsx\n    └── Modal.tsx\n    ```\n\n### `src/navigation/`\n\n- 화면 간의 라우팅과 네비게이션 설정.\n\n- 예: React Navigation을 사용하여 스택, 탭, 드로어 네비게이션 설정.\n\n    ```plain text\n    src/navigation/\n    ├── AppNavigator.tsx  # 앱 전체 네비게이션\n    ├── AuthStack.tsx     # 로그인/회원가입 스택\n    └── MainStack.tsx     # 로그인 후 메인 스택\n    ```\n\n###   `src/screens/`\n\n- 각 페이지(화면) 컴포넌트를 정의.\n\n- 페이지별로 파일을 생성하고, 필요하면 서브 디렉토리를 사용.\n\n    ```plain text\n    src/screens/\n    ├── LoginScreen.tsx\n    ├── SignupScreen.tsx\n    ├── HomeScreen.tsx\n    ├── ProfileScreen.tsx\n    └── SettingsScreen.tsx\n    ```\n\n###   `src/services/`\n\n- API 요청 및 백엔드와의 연동 로직 작성.\n\n- 예: `axios`나 `fetch`를 사용한 API 호출 함수 정의.\n\n    ```plain text\n    src/services/\n    ├── api.ts           # 공통 API 설정\n    ├── authService.ts   # 인증 관련 API\n    └── userService.ts   # 사용자 데이터 API\n    ```\n\n###   `src/utils/`\n\n- 재사용 가능한 유틸리티 함수, 상수, 헬퍼 코드.\n\n- 예: 날짜 포맷팅, 폼 유효성 검사 함수 등.\n\n    ```plain text\n    src/utils/\n    ├── dateUtils.ts\n    ├── validation.ts\n    ├── constants.ts\n    └── helpers.ts\n    ```\n\n### `src/context/`\n\n- React Context API를 사용해 글로벌 상태를 관리.\n\n- 예: 사용자 인증 상태, 테마 설정 등.\n\n    ```plain text\n    src/context/\n    ├── AuthContext.tsx\n    ├── ThemeContext.tsx\n    └── AppProvider.tsx\n    ```\n\n### `App.tsx`\n\n- 프로젝트의 메인 엔트리 파일.\n\n- 네비게이션 초기화 및 최상위 컴포넌트 렌더링.\n\n    ```typescript\n    import React from 'react';\n    import { NavigationContainer } from '@react-navigation/native';\n    import AppNavigator from './src/navigation/AppNavigator';\n    \n    const App: React.FC = () => {\n      return (\n        <NavigationContainer>\n          <AppNavigator />\n        </NavigationContainer>\n      );\n    };\n    \n    export default App;\n    ```\n\n## AppNavigator를 활용한 화면의 추가\n\nApp.tsx가 main.py 같은 역할을 하는 파일임을 짐작할 수 있습니다. 여기서 AppNavigator를 사용하고 있는 것도 확인할 수 있습니다. \n\n이 페이지는 React Navigation을 사용하여 앱의 화면 전환(네비게이션)을 관리하는 역할을 합니다. React Native 앱에서는 여러 화면이 있고, 이러한 화면 간의 이동을 효과적으로 관리하기 위해 네비게이션 라이브러리를 사용하는 것이 일반적입니다.\n\n`AppNavigator`(또는 React Navigation과 같은 네비게이션 시스템)를 사용하지 않으면, 화면 전환 및 뒤로가기 동작을 직접 관리해야 합니다. 이는 상당히 번거롭고, 유지보수성이 떨어질 수 있습니다. React Navigation을 사용하면 뒤로가기를 포함한 화면 전환과 네비게이션 흐름을 자동으로 관리할 수 있어 훨씬 간편합니다.\n\nReact Navigation 같은 라이브러리를 사용하지 않으면,\n\n1. 각 화면 전환을 수동으로 처리해야 합니다.\n\n1. 안드로이드의 뒤로가기 버튼(하드웨어 버튼)도 직접 구현해야 합니다.\n\n1. 상태를 유지하기 위한 글로벌 상태 관리나 Context API를 추가적으로 사용해야 할 수 있습니다.\n\n### 설치\n\n```bash\nnpm install @react-navigation/native\nnpm install react-native-screens react-native-safe-area-context react-native-gesture-handler react-native-reanimated react-native-vector-icons\nnpm install @react-navigation/stack\ncd ios && pod install && cd ..\n```\n\n###  **주요 역할**\n\n1. **스택 네비게이션 구성**\n\n    - `createStackNavigator`를 통해 화면 간의 전환 방식을 스택(Stack) 방식으로 관리.\n\n    - 스택 방식은 사용자가 한 화면에서 다른 화면으로 이동할 때, 이전 화면이 \"뒤로가기\"로 돌아갈 수 있도록 스택 구조에 저장되는 방식.\n\n1. **화면 정의**\n\n    - `SplashScreen`: 앱이 처음 로드될 때 사용자에게 표시되는 스플래시 화면.\n\n    - `LoginScreen`: 사용자가 로그인할 수 있는 화면.\n\n    - `HomeScreen`: 로그인 후 메인 화면으로 이동.\n\n1. **초기 화면 설정**\n\n    - `initialRouteName=\"Splash\"`는 앱 실행 시 첫 화면을 `SplashScreen`으로 설정.\n\n    - 앱 로드 후, `SplashScreen`에서 로그인 상태를 확인한 뒤 적절한 화면(`LoginScreen` 또는 `HomeScreen`)으로 리디렉션.\n\n1. **헤더 숨기기**\n\n    - `screenOptions={{ headerShown: false }}`를 통해 화면 상단의 기본 헤더를 숨김.\n\n### 코드\n\n/src/navigation 경로에 다음의 파일을 작성해줍니다. \n\n```typescript\nimport { createStackNavigator } from '@react-navigation/stack';\nimport React from 'react';\nimport HomeScreen from '../screens/HomeScreen';\nimport LoginScreen from '../screens/LoginScreen/LoginScreen';\nimport SplashScreen from '../screens/SplashScreen';\n\nconst Stack = createStackNavigator();\n\nconst AppNavigator: React.FC = () => {\n    return (\n        <Stack.Navigator initialRouteName=\"Splash\" screenOptions={{ headerShown: false }}>\n            <Stack.Screen name=\"Splash\" component={SplashScreen} />\n            <Stack.Screen name=\"Login\" component={LoginScreen} />\n            <Stack.Screen name=\"Home\" component={HomeScreen} />\n        </Stack.Navigator>\n    );\n};\n\nexport default AppNavigator;\n```\n\n#### **네비게이션 초기화**\n\n`createStackNavigator`를 통해 스택 네비게이터를 생성합니다.\n\n```typescript\nconst Stack = createStackNavigator();\n```\n\n#### **스택 구성**\n\n`<Stack.Navigator>`에 앱에서 사용할 화면들을 정의\n\n- `name`: 네비게이션 이름. 다른 화면으로 이동할 때 이 이름으로 참조.\n\n- `component`: 연결된 화면 컴포넌트\n\n```typescript\n<Stack.Navigator initialRouteName=\"Splash\" screenOptions={{ headerShown: false }}>\n    <Stack.Screen name=\"Splash\" component={SplashScreen} />\n    <Stack.Screen name=\"Login\" component={LoginScreen} />\n    <Stack.Screen name=\"Home\" component={HomeScreen} />\n</Stack.Navigator>\n```\n\n## useNavigation 훅을 사용한 화면 전환\n\nReact Navigation가 설치되었고 AppNavigator에 화면을 정의했다면, `useNavigation` 훅을 사용해 `navigation` 객체를 간단하게 가져올 수 있습니다.\n\n다만 이 전에 타입스크립트의 특징 때문에 경로와 관련된 타입을 명시해야 오류를 방지할 수 있습니다.\n\n### 타입 정의 추가 \n\nReact Navigation의 `createStackNavigator`에서 정의한 경로 이름(`Home`, `Login` 등)에 대한 타입을 추가해야 합니다. types/navigation.tsx 파일을 생성하고 다음의 내용을 작성합니다. 이는 선택사항으로 모든 네비게이션 타입 정의를 한 곳에서 관리하기 위함입니다. \n\n```typescript\nimport { StackNavigationProp } from '@react-navigation/stack';\n\ntype RootStackParamList = {\n  Home: undefined; // 'Home' 경로에 매개변수가 없는 경우\n  Login: undefined; // 'Login' 경로\n};\n\ntype NavigationProp = StackNavigationProp<RootStackParamList, 'Login'>;\n\nexport default NavigationProp;\n```\n\n`RootStackParamList` 정의\n\n- `Home`, `Login`, `Profile` 등 네비게이션 경로의 이름과 매개변수 타입을 정의.\n\n`useNavigation`에 타입 적용\n\n- `useNavigation<StackNavigationProp<RootStackParamList, 'Login'>>()`와 같이 타입 지정.\n\n### `useNavigation`에 타입 적용\n\n`useNavigation` 훅에 방금 정의한 `NavigationProp` 타입을 적용합니다.\n\n```typescript\nimport { useNavigation } from '@react-navigation/native';\nimport React from 'react';\nimport { StyleSheet, View } from 'react-native';\nimport SocialButton from '../../components/SocialButton';\nimport NavigationProp from '../../types/navigation'; // 방금 정의한 타입 가져오기\n\nconst LoginScreen: React.FC = () => {\n    const navigation = useNavigation<NavigationProp>(); // 타입 적용\n\n    const handleGoogleLogin = () => {\n        console.log('Google Login Clicked');\n        navigation.navigate('Home'); // 네비게이션 수행\n    };\n\n    return (\n        <View style={styles.container}>\n            <SocialButton\n                text=\"Google 계정으로 로그인\"\n                icon=\"google\"\n                color=\"#F4A261\"\n                onPress={handleGoogleLogin}\n            />\n        </View>\n    );\n};\n\nexport default LoginScreen;\n```\n\n### `reset` 메서드\n\n이대로 구현하면 로그인 후 홈 화면으로 이동할 때 **뒤로 가기 버튼**을 누르면 로그인 화면으로 돌아갈 수 있습니다. 이는 일반적으로 기대되는 동작이 아니며, 로그인 화면은 **로그인 후** 접근할 수 없도록 하는 것이 적절합니다.\n\n`navigation.reset`을 사용하면 네비게이션 스택을 재설정하여 뒤로가기를 방지할 수 있습니다. 이는 웹의 **리다이렉트**와 유사한 동작을 합니다.\n\n```typescript\nconst handleGoogleLogin = () => {\n    console.log('Google Login Clicked');\n\n    // 네비게이션 스택을 재설정\n    navigation.reset({\n        index: 0, // 스택의 첫 번째 화면으로 설정\n        routes: [{ name: 'Home' }], // 새로 설정할 스택의 화면\n    });\n};\n```\n\n이렇게 구현하면 로그인 화면은 네비게이션 스택에서 제거되며 \"뒤로가기\" 버튼을 눌러도 로그인 화면으로 돌아갈 수 없습니다.\n\n## 마무리\n\n오늘은 리액트 네이티브를 설치하고, 프로젝트 구조를 뜯어보고 간단한 화면을 만들어 화면을 이동시켜봤습니다. \n\n다음 시간에는 하단 탭을 구현하고 하단 탭으로 화면 전환을 하는 방법, 왼쪽/오른쪽에서 열리는 메뉴를 추가해보고 역시 이를 통해 화면을 이동해보겠습니다. \n\n\n\n"},{"excerpt":"서론 오랜만에 타입스크립트를 다루고 있습니다. 특히 코딩 테스트를 위해서 다루고 있는데, 오랜만에 타입스크립트의 배열을 다루다보니 얕은 복사와 깊은 복사에 관한 문제가 발생했습니다. 예전에 공부했던 내용이지만 정리한 적은 없는 것 같아 한 번 정리하고 넘어가려고 합니다.  TypeScript의 값 분류 TypeScript(JavaScript)는 \"객체 기…","fields":{"slug":"/Typescript의-얇은-복사-vs-깊은-복사/"},"frontmatter":{"date":"January 09, 2025","title":"Typescript의 얇은 복사 vs 깊은 복사","tags":["Typescript"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n오랜만에 타입스크립트를 다루고 있습니다. 특히 코딩 테스트를 위해서 다루고 있는데, 오랜만에 타입스크립트의 배열을 다루다보니 얕은 복사와 깊은 복사에 관한 문제가 발생했습니다. 예전에 공부했던 내용이지만 정리한 적은 없는 것 같아 한 번 정리하고 넘어가려고 합니다. \n\n## **TypeScript의 값 분류**\n\nTypeScript(JavaScript)는 \"객체 기반 언어\"(Object-Oriented Programming Language)입니다. \n\n이는 객체를 활용하여 데이터를 구성하고 조작하는 데 최적화되어 있음을 의미합니다. 배열도 객체로 다루어지며, 대부분의 연산은 객체 참조를 기반으로 동작합니다. 대신 모든 값이 객체인 것은 아닙니다. \n\n### Primitive Value (기본형 값)\n\n- 불변형(Immutable) 값입니다.\n\n- 값을 직접 복사하며, 다른 변수에 영향을 미치지 않습니다.\n\n- 기본형 값에는 다음이 포함됩니다.\n\n    - `number`, `string`, `boolean`, `null`, `undefined`, `symbol`, `bigint`\n\n- **메모리 저장 방식**\n\n    - Primitive 값은 스택(Stack)에 직접 저장됩니다.\n\n    - 값 자체가 변수에 저장되며, 독립적으로 동작합니다.\n\n### Reference Value (참조형 값)\n\n- 객체처럼 동작하며, 값이 아닌 참조를 복사합니다.\n\n- 참조형 값에는 다음이 포함됩니다.\n\n    - `object`, `array`, `function`, `Map`, `Set`, `Date` 등\n\n- 메모리 저장 방식\n\n    - 참조형 값은 힙(Heap)에 저장되며, 변수에는 객체의 메모리 주소(참조)가 저장됩니다.\n\n    - 따라서, 여러 변수가 동일한 객체를 참조할 수 있습니다.\n\n### Primitive vs Reference 차이\n\n#### Primitive Value 예시 (Immutable)\n\n```typescript\nlet a = 42;\nlet b = a;\nb = 100;\nconsole.log(a); // 42 (원본 값은 변경되지 않음)\nconsole.log(b); // 100\n```\n\n`a`와 `b`는 독립적인 값을 가지고 있으며, 하나를 변경해도 다른 값에 영향을 미치지 않습니다.\n\n#### Reference Value 예시 (Mutable)\n\n```typescript\nconst arr1 = [1, 2, 3];\nconst arr2 = arr1; // 참조 복사\narr2[0] = 100;\nconsole.log(arr1); // [100, 2, 3] (원본 배열도 변경됨)\nconsole.log(arr2); // [100, 2, 3]\n```\n\n`arr1`과 `arr2`는 동일한 배열 객체를 참조합니다. 하나를 수정하면 다른 것도 영향을 받습니다.\n\n## 얕은 복사와 깊은 복사 \n\n### 얕은 복사란? \n\n- 얕은 복사(Shallow Copy)는 **객체의 1차원 수준만 복사**하는 것을 의미합니다.\n\n- 배열이나 객체를 복사할 때, 최상위 수준의 값만 복사되며, 만약 내부 요소가참조형 값(Reference Value)이라면, 이 값들은 참조(주소)만 복사됩니다.\n\n- 따라서, 얕은 복사된 객체나 배열은 원본과 일부 데이터를 공유하게 되어, **한쪽을 수정하면 다른 쪽도 영향을 받을 수 있습니다.**\n\n### 깊은 복사란?\n\n- **깊은 복사(Deep Copy)**는 객체의 모든 수준을 복사하여, 원본과 완전히 독립적인 객체를 생성합니다.\n\n- 중첩된 데이터(예: 객체 안의 객체, 배열 안의 객체)까지 새로운 메모리를 할당합니다.\n\n## 얕은 복사의 동작\n\n얕은 복사가 어떻게 작동하는지 예시를 통해 메모리 구조와 함께 자세히 알아보겠습니다. \n\n### 예제 1: Primitive Value\n\n```typescript\nconst arr1 = [1, 2, 3];\nconst arr2 = [...arr1]; // 얕은 복사\narr2[0] = 100;\nconsole.log(arr1); // [1, 2, 3]\nconsole.log(arr2); // [100, 2, 3]\n```\n\n위는 언뜻 보기에는 깊은 복사로 동작하는 것으로 보이지만, `const arr2 = [...arr1];`는 얕은 복사를 시행합니다. \"깊은 복사처럼 작동한다\"는 것은 단지 불변형 값 덕분에 두 배열이 독립적이기 때문입니다.\n\n#### **메모리 구조**\n\n1. `arr1`이 생성되면, 배열 자체는 힙(Heap)에 저장되고, 각 요소(`1`, `2`, `3`)는 Primitive Value이므로 스택(Stack)에 저장됩니다.\n\n1. `const arr2 = [...arr1]`\n\n    - 스프레드 연산자(`...`)는 `arr1`의 각 요소를 복사하여 새로운 배열 `arr2`를 만듭니다.\n\n    - 배열 자체는 새로운 객체로 힙(Heap)에 저장되지만, 복사된 Primitive 값들은 각각 스택(Stack)에 저장됩니다.\n\n1. `arr2[0] = 100`\n\n    - `arr2`의 첫 번째 값을 스택(Stack)에서 변경합니다.\n\n    - `arr1`은 독립적인 배열이므로 영향을 받지 않습니다.\n\n### 예제 2: Reference Value\n\n```typescript\nconst arr1 = [{ a: 1 }, { b: 2 }, { c: 3 }];\nconst arr2 = [...arr1]; // 얕은 복사\n\narr2[0].a = 100;\nconsole.log(arr1); // [ { a: 100 }, { b: 2 }, { c: 3 } ]\nconsole.log(arr2); // [ { a: 100 }, { b: 2 }, { c: 3 } ]\n```\n\n이 예제에서는 \"얕은 복사\"의 문제가 발생하며, 참조가 공유됩니다.\n\n#### **메모리 구조**\n\n1. `arr1`이 생성되면, 배열 자체는 힙(Heap)에 저장되고, 배열의 각 요소(`{ a: 1 }`, `{ b: 2 }`, `{ c: 3 }`)도 힙(Heap)에 저장됩니다.\n\n    - `arr1` 배열은 힙에 저장된 객체들의 참조(주소)를 가집니다.\n\n1. `const arr2 = [...arr1]`:\n\n    - 스프레드 연산자(`...`)는 `arr1`의 각 요소의 참조(주소)를 복사하여 새로운 배열 `arr2`를 만듭니다.\n\n    - 배열 `arr2`는 새로운 객체로 힙(Heap)에 저장되지만, 각 요소는 `arr1`과 동일한 객체를 참조합니다.\n\n1. `arr2[0].a = 100`:\n\n    - `arr2[0]`이 `arr1[0]`과 동일한 객체를 참조하므로, 변경이 두 배열에 영향을 미칩니다.\n\n### 결론\n\n얕은 복사는 항상 1차원만 복사하는 것이 핵심입니다. 하지만, 배열의 요소가 불변형 값(Primitive Value)인지, 참조형 값(Reference Value)인지에 따라 결과가 달라질 뿐입니다.\n\n얕은 복사는 항상 동일한 작업을 수행합니다.\n\n- 1차원 요소만 복사하고, 요소가 불변형이면 값 복사, 참조형이면 참조 복사.\n\n결과 차이는 배열 요소의 유형(불변형 vs 참조형)에 따라 다릅니다.\n\n- 불변형 값 → 깊은 복사처럼 작동.\n\n- 참조형 값 → 얕은 복사로 인해 참조 문제 발생.\n\n#### **요약**\n\n1. **얕은 복사의 본질**\n\n    - 배열이나 객체의 **1차원 요소만 복사**합니다.\n\n    - 요소가 불변형 값이면 **값을 복사**하고, 참조형 값이면 **참조를 복사**합니다.\n\n1. **결과의 차이**\n\n    - 배열의 요소가 **불변형 값**이면, **\"깊은 복사처럼\"** 보일 뿐입니다.\n\n    - 배열의 요소가 **참조형 값**이면, **\"얕은 복사처럼\"** 참조를 공유합니다.\n\n## 얕은 복사와 깊은 복사를 언제 사용해야 할까?\n\n### **1. 얕은 복사(Shallow Copy)**\n\n- **언제 사용하나요?**\n\n    - 데이터가 불변형 값(Primitive Value)로만 이루어진 경우.\n\n    - 객체나 배열을 복사할 때, **수정하지 않는 1차원 데이터**만 필요할 때.\n\n- **장점**:\n\n    - 수행 속도가 빠르고, 메모리를 덜 사용.\n\n    - 간단한 데이터 구조에서는 충분히 적합.\n\n### **2. 깊은 복사(Deep Copy)**\n\n- **언제 사용하나요?**\n\n    - 데이터에 참조형 값(Reference Value)이 포함된 경우.\n\n    - 데이터가 **중첩된 구조**(객체 안의 객체, 배열 안의 배열)를 가지는 경우.\n\n    - 복사본이 원본과 완전히 독립적이어야 할 때.\n\n- **장점**:\n\n    - 원본 데이터와 완전히 독립적이므로, 수정해도 영향을 주지 않음.\n\n### **결론**\n\n- 복사 작업에서 얕은 복사와 깊은 복사의 차이를 명확히 이해하고, 데이터 구조와 사용 목적에 따라 올바른 방법을 선택해야 합니다.\n\n- 단순히 \"복사다!\"라고 덤비면 참조 문제나 의도치 않은 결과가 발생할 가능성이 큽니다.\n\n- 문제가 복잡하거나 확신이 없을 때는 깊은 복사를 기본으로 고려하는 것이 안전합니다. \n\n"},{"excerpt":"의존성 간 관계 자동 해결 FastAPI는 의존성 간의 관계를 자동으로 해결해 주기 때문에 개발자는 복잡한 로직을 작성하지 않아도 FastAPI가 필요한 리소스를 적절히 연결해줍니다.  FastAPI 의존성 주입의 기본 개념 의존성은 서로 다른 의존성을 참조할 수 있으며, FastAPI는 이를 바탕으로 의존성 그래프를 생성해 자동으로 해결합니다. : DB…","fields":{"slug":"/FastAPI-의존성-주입의-심화-활용법과-주의점/"},"frontmatter":{"date":"December 30, 2024","title":"FastAPI 의존성 주입의 심화 활용법과 주의점","tags":["FastAPI","Python"]},"rawMarkdownBody":"![](image1.png)\n## 의존성 간 관계 자동 해결\n\nFastAPI는 의존성 간의 관계를 자동으로 해결해 주기 때문에 개발자는 복잡한 로직을 작성하지 않아도 FastAPI가 필요한 리소스를 적절히 연결해줍니다. \n\n### FastAPI 의존성 주입의 기본 개념\n\n의존성은 서로 다른 의존성을 참조할 수 있으며, FastAPI는 이를 바탕으로 **의존성 그래프**를 생성해 자동으로 해결합니다.\n\n```python\nfrom fastapi import Depends, FastAPI\n\napp = FastAPI()\n\nasync def get_db_session() -> str:\n    return \"DB 세션\"\n\nasync def get_redis_client(db_session: Annotated[str, Depends(get_db_session)]) -> str:\n    return f\"Redis 클라이언트 (DB: {db_session})\"\n\n@app.get(\"/\")\nasync def read_data(redis_client: Annotated[str, Depends(get_redis_client)]) -> str:\n    return {\"redis_client\": redis_client}\n```\n\n- `get_db_session`: DB 세션 문자열을 반환하는 의존성.\n\n- `get_redis_client`: `get_db_session`의 반환값을 의존성으로 사용.\n\n- `read_data` 엔드포인트: `get_redis_client`의 반환값을 의존성으로 사용.\n\nFastAPI는 `read_data` → `get_redis_client` → `get_db_session` 순서로 의존성을 파악해 의존성 그래프를 생성합니다. 가장 하위 의존성(`get_db_session`)부터 실행하여 결과를 상위로 전달합니다. 최종적으로 `get_redis_client`의 결과를 `read_data` 함수로 전달합니다.\n\n### 의존성 간 관계 자동 해결의 장점\n\n- 복잡한 의존성 관리 단순화\n\n    - FastAPI는 의존성 간의 관계를 자동으로 파악하고 올바른 순서로 호출합니다.\n\n    - 개발자는 의존성 간의 호출 순서를 걱정하지 않아도 됩니다.\n\n- 유연한 확장 가능성\n\n    - 새로운 의존성을 추가하거나 기존 의존성을 수정해도 FastAPI가 자동으로 관계를 재구성합니다.\n\n### 의존성 간 관계 자동 해결시 주의점\n\n의존성 함수가 서로를 참조하는 경우(`A → B → A`), FastAPI는 순환 참조를 해결하지 못합니다.\n\n```python\n\nasync def get_a(b: Annotated[str, Depends(get_b)]):\n    return \"A\"\n\nasync def get_b(a: Annotated[str, Depends(get_a)]):\n    return \"B\"\n```\n\n중간 레벨의 의존성을 생성하거나 의존성을 리팩토링해 순환을 제거해야 합니다.\n\n## 의존성에 매개변수 전달하기\n\nFastAPI에서 의존성 함수에 직접 값을 전달하는 기능은 없지만, 아래와 같은 방법으로 유사한 동작을 구현할 수 있습니다.\n\n### 람다(Lambda) 함수로 전달\n\n의존성 함수가 동적으로 다른 값을 받을 수 있도록 람다를 사용합니다.\n\n```python\nfrom fastapi import Depends, FastAPI\n\napp = FastAPI()\n\ndef get_value(value: int):\n    return f\"Value is {value}\"\n\n@app.get(\"/\")\ndef read_root(custom_value: Annotated[str, Depends(lambda: get_value(42))]):\n    return {\"custom_value\": custom_value}\n```\n\n- `lambda: get_value(42)`를 통해 `42`를 매개변수로 전달.\n\n- `Depends()`는 동적으로 생성된 값을 사용할 수 있습니다.\n\n### 의존성 팩토리 함수\n\n의존성 함수를 팩토리 함수로 감싸서 동적으로 값을 전달합니다.\n\n```python\nfrom fastapi import Depends, FastAPI\n\napp = FastAPI()\n\ndef get_value_factory(value: int):\n    def get_value():\n        return f\"Value is {value}\"\n    return get_value\n\n@app.get(\"/\")\ndef read_root(custom_value: Annotated[str, Depends(get_value_factory(42))]):\n    return {\"custom_value\": custom_value}\n```\n\n- `get_value_factory(42)`는 `get_value`를 반환하며, 내부적으로 `42`를 사용할 수 있습니다.\n\n- 이 방법은 더 복잡한 로직에서 유용합니다.\n\n### 의존성 컨텍스트 전달 (Request State 활용)\n\nFastAPI의 `Request.state`를 사용하여 동적으로 데이터를 전달할 수 있습니다.\n\n```python\nfrom fastapi import Depends, FastAPI, Request\n\napp = FastAPI()\n\nasync def get_custom_value(request: Request):\n    return request.state.custom_value\n\n@app.middleware(\"http\")\nasync def add_custom_value(request: Request, call_next):\n    request.state.custom_value = 42\n    response = await call_next(request)\n    return response\n\n@app.get(\"/\")\nasync def read_root(custom_value: Annotated[str, Depends(get_custom_value)]):\n    return {\"custom_value\": custom_value}\n```\n\n- 미들웨어를 사용해 `Request.state`에 값을 추가.\n\n- 의존성 함수에서 `Request.state` 값을 참조.\n\n### 의존성 클래스 활용\n\n의존성 클래스를 사용해 동적으로 상태를 전달할 수 있습니다.\n\n```python\nclass ValueProvider:\n    def __init__(self, value: int):\n        self.value = value\n\n    def __call__(self):\n        return f\"Value is {self.value}\"\n\n@app.get(\"/\")\ndef read_root(custom_value: Annotated[str, Depends(ValueProvider(42))]):\n    return {\"custom_value\": custom_value}\n```\n\n- `ValueProvider` 클래스는 생성자에서 값을 받고, 호출 시 이 값을 반환.\n\n- FastAPI의 `Depends`는 호출 가능한 객체를 의존성으로 사용할 수 있으므로 동적으로 값을 전달 가능.\n\n### 주의점\n\n일반적으로 FastAPI의 의존성 주입 시스템을 우회해서 매개변수를 전달하는 방식(람다 함수나 팩토리 함수 등)은 유지보수성과 가독성 측면에서 좋지 않을 수 있습니다. 이러한 방법은 특정 상황에서는 유용하지만, 남용할 경우 프로젝트의 구조를 복잡하게 만들고 디버깅을 어렵게 할 수 있습니다.\n\n람다 함수, 팩토리 함수 등 우회 방법은 특수한 상황에서만 신중히 사용해야 합니다.\n\n가능한 경우, 동적 값을 처리하는 로직은 상위 레벨(미들웨어, 라우터)에서 관리하고 의존성 함수는 단순화하는 것을 권장합니다.\n\n## `Annotated` 를 활용한 의존성의 선언\n\n의존성의 선언에서 `Annotated`는 0.95 버전 이후에 추가된 기능으로, 타입 힌트를 더 명확히 표현하고 코드를 더 읽기 쉽게 만들어줍니다. FastAPI에서 `Annotated`를 사용하는 것은 가독성 외에도 타입 안정성, 코드 재사용성, 확장 가능성 등 여러 측면에서 이점을 제공합니다.\n\n### 코드 가독성 및 명확성\n\n`Annotated`를 사용하면 의존성의 역할을 타입 힌트를 통해 명확히 나타낼 수 있습니다. 코드의 의도를 더 직관적으로 파악할 수 있으므로, 팀원 간 협업이나 유지보수 시 유리합니다.\n\n```python\nfrom typing import Annotated\nfrom fastapi import Depends\n\nAdminUser = Annotated[str, Depends(admin_permissions)]\n\n@app.get(\"/admin\")\ndef admin_only_route(user: AdminUser):\n    return {\"message\": f\"Hello, {user}\"}\n```\n\n- `AdminUser`: 타입 힌트를 통해 이 변수가 관리자의 인증과 권한 검증을 포함한다는 점을 명확히 전달\n\n- 가독성과 의미 전달이 좋아져 코드 리뷰와 유지보수가 용이\n\n### 타입 안정성 및 도구 지원\n\n`Annotated`는 타입 힌팅을 강화하여, IDE의 자동 완성과 정적 분석 도구(`mypy` 등)가 더 정확하게 동작하도록 도와줍니다. 복잡한 의존성 체계를 사용할 때 코드 안정성을 높이는 데 기여합니다.\n\n```python\nAdminUser = Annotated[str, Depends(admin_permissions)]\n```\n\n- IDE에서 `AdminUser`를 사용할 때 자동 완성 기능으로 `Depends(admin_permissions)`가 연결됨.\n\n- `mypy`와 같은 도구는 타입 충돌을 방지하고 코드 품질을 유지하는 데 도움을 줌.\n\n### 확장 가능성\n\n타입 힌트와 의존성을 결합하여 더 복잡한 구조나 동작을 설계할 수 있습니다. 데이터를 검증하거나 추가적인 메타데이터를 전달하는 데 사용할 수 있습니다.\n\n```python\nfrom pydantic import BaseModel\nfrom typing import Annotated\nfrom fastapi import Depends\n\nclass AdminPayload(BaseModel):\n    action: str\n\nAdminRequest = Annotated[AdminPayload, Depends(admin_permissions)]\n\n@app.post(\"/admin/action\")\ndef perform_admin_action(payload: AdminRequest):\n    return {\"action_performed\": payload.action}\n```\n\n- `AdminRequest`: 관리자의 인증/인가와 함께 데이터를 검증하는 구조로 확장 가능.\n\n- 인증/인가와 데이터 검증을 결합하여 더욱 강력한 의존성 관리 가능.\n\n### 모든 의존성을 Annotated로 바꿔야 할까? \n\n아니요, 기존 방식(`Depends`)도 완벽히 동작합니다. Annotated는 더 명확한 타입 힌팅과 가독성을 제공하지만, 필수는 아닙니다.\n\n\n\n"},{"excerpt":"서론 이번에 FastAPI 과제를 진행하면서 아무생각없이 기존에 짜던대로 코드를 짜고 피드백을 받은 후, 제가 의존성 주입을 할 때 Depends()만 사용하는 방식이 구식 방법이라는 것을 뒤늦게 알게 됐습니다. 심지어 공식문서에도 권장하지 않는 방법이라고 적혀있었습니다.  아니 정말? 이라는 생각이 들어서 공식문서의 커밋 년도까지 확인을 했는데, 공식 …","fields":{"slug":"/FastAPI의-의존성-주입Dependency-Injection/"},"frontmatter":{"date":"December 26, 2024","title":"FastAPI의 의존성 주입(Dependency Injection)","tags":["FastAPI","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n이번에 FastAPI 과제를 진행하면서 아무생각없이 기존에 짜던대로 코드를 짜고 피드백을 받은 후, 제가 의존성 주입을 할 때 Depends()만 사용하는 방식이 구식 방법이라는 것을 뒤늦게 알게 됐습니다. 심지어 공식문서에도 권장하지 않는 방법이라고 적혀있었습니다. \n\n아니 정말? 이라는 생각이 들어서 공식문서의 커밋 년도까지 확인을 했는데, 공식 문서가 해당 내용으로 작성된 것은 2023년 중순으로 제 작업 이전이었습니다. \n\n변명의 여지가 없는 것으로, 곰곰이 생각해보니 의존성 주입이라는 친구와 제가 그다지 친하지 않다는 사실을 깨달았습니다. 예를 들면 매개변수는 어떻게 들어가는지, 이게 다른 의존성이 스윽 들어가고 이런 것들 그냥 아무 생각 없이 쓰던 것들이 사실은 제가 정확히 모르고 있다는 사실을 알게 됐습니다. \n\n그래서 이번 기회에 의존성 주입에 대해 한 번 정리하고 넘어가려고 합니다. \n\n## 의존성 주입(Dependency Injenction) 이란?\n\n의존성(dependency)이라는 단어를 한국어로 풀이하면 \"다른 것에 기대어 의존하는 상태\"를 의미합니다. FastAPI 같은 프로그램에서는 의존성은 어떤 코드가 특정 기능이나 데이터를 수행하기 위해 다른 객체, 함수, 또는 리소스를 필요로 하는 상태를 말합니다.\n\n예를 들면,\n\n```python\nfrom fastapi import Depends\nfrom typing import Annotated\n\ndef get_db():\n    return \"데이터베이스 세션\"\n\n@app.get(\"/items/\")\ndef read_items(db: Annotated[str, Depends(get_db)]):\n    return {\"db\": db}\n```\n\n여기서 \"read\\_items\" 함수가 의존성(`get_db`)에 의존합니다.\n\n프로그램에서 데이터베이스 연결이 필요한 코드가 있다고 합시다. 이때 그 코드는 데이터베이스 연결(리소스)에 의존합니다. 즉, \"내가 작동하려면 데이터베이스 연결이 있어야 해!\"라고 말하는 상황입니다. FastAPI의 의존성 주입(Dependency Injection)은 이러한 의존성을 명확하게 관리하고, 코드의 재사용성과 유지보수성을 높이는 데 목적을 둡니다.\n\n## FastAPI의 의존성 주입의 작동\n\n### 작동 순서\n\n1. 종속성 확인\n\n    - 함수의 매개변수에 `Depends()`가 있는지 확인합니다. \n\n    - `Depends()`는 호출될 함수를 정의하거나, 리소스를 반환하는 팩토리 역할을 합니다.\n\n1. 함수 호출 전 평가(evaluate)\n\n    - FastAPI는 필요한 의존성을 모두 호출하거나 평가 하여 결과값을 준비합니다.\n\n    - 예를 들어, 데이터베이스 세션을 생성하거나, 인증 정보를 확인합니다.\n\n1. 값 전달\n\n    - 준비된 의존성을 매개변수로 주입하여 실제 라우터, 미들웨어, 이벤트 핸들러 등에서 사용합니다.\n\n1. 범위(Scope) 관리\n\n    - FastAPI는 의존성의 **범위(scope)**를 관리합니다. (`request`, `session`, `singleton` 등)\n\n    - 예를 들어, `request` 범위의 의존성은 요청이 끝나면 자동으로 정리됩니다.\n\n### \"의존성을 평가한다\"는 의미\n\nFastAPI에서 의존성을 평가(evaluate)한다는 것은 `Depends()`에 지정된 함수나 객체를 실행하거나 처리하여, 해당 결과값을 준비한다는 뜻입니다.\n\n- 평가(evaluation)는 단순히 함수를 호출하는 것을 넘어서, 해당 의존성이 반환하는 값을 준비하고, 필요한 경우 예외를 처리하거나 적절한 스코프를 설정하는 과정을 포함합니다.\n\n#### 예시1: 단순한 함수 의존성\n\n```python\ndef get_db():\n    return \"데이터베이스 세션\"\n\n@app.get(\"/items/\")\ndef read_items(db: Annotated[str, Depends(get_db)]):\n    return {\"db\": db}\n```\n\n- FastAPI는 `get_db`를 호출하여 결과값 `\"데이터베이스 세션\"`을 반환합니다.\n\n- “평가”는 여기서 단순히 `get_db()`를 호출하고, 반환값을 준비하는 과정을 의미합니다.\n\n#### 예시2: 의존성 내부에 추가 로직이 있는 경우\n\n```python\ndef check_auth_header(auth: str = Header(...)):\n    if auth != \"secret-token\":\n        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n    return auth\n```\n\n- 이 경우 FastAPI는 `check_auth_header`를 호출하며, 헤더 값이 유효한지 검증합니다.\n\n- \"평가\"는 검증 로직이 실행되어 예외가 발생할 수도 있는 전체 과정을 포함합니다.\n\n### 의존성은 반드시 함수여야 할까? \n\n의존성은 반드시 함수일 필요는 없으며, 호출 가능한 객체도 사용할 수 있습니다. FastAPI의 `Depends()`는 **함수뿐 아니라 호출 가능한 객체(callable)**도 의존성으로 사용할 수 있습니다. 호출 가능한 객체란, `__call__` 메서드를 구현한 클래스 인스턴스를 의미합니다.\n\n#### 예시1: 호출 가능한 클래스\n\n```python\nclass DBSession:\n    def __call__(self):\n        return \"데이터베이스 세션\"\n\n@app.get(\"/\")\ndef read_root(db: Annotated[str, Depends(DBSession())]):\n    return {\"db\": db}\n```\n\n- 여기서 `DBSession` 클래스는 호출 가능한 객체로 동작하며, `__call__` 메서드가 실행됩니다.\n\n- FastAPI는 `DBSession`의 인스턴스를 평가하여, `__call__`의 반환값 `\"데이터베이스 세션\"`을 준비합니다.\n\n#### 예시2: 상수 값 (Callable이 아님)\n\n일반적으로 의존성은 **함수 또는 호출 가능한 객체**를 사용하는 것이 일반적이지만, 다음과 같은 객체도 사용할 수 있습니다.\n\n```python\ndb_config = {\"host\": \"localhost\", \"port\": 5432}\n\n@app.get(\"/\")\ndef read_root(config: Annotated[str, Depends(lambda: db_config)]):\n    return config\n```\n\n- 여기서 `db_config`는 상수 값이지만, 람다 함수 `lambda: db_config`로 감싸서 의존성으로 주입할 수 있습니다.\n\n### FastAPI 의존성의 범위(Scope)\n\n#### 범위란? \n\n범위는 의존성이 언제 생성되고 언제 소멸되는지를 정의합니다. FastAPI는 요청(Request) 기반의 애플리케이션이므로 일반적으로는 요청이 처리되는 동안 의존성이 유지되다가 요청이 끝날 때 정리됩니다.\n\n#### 주요 범위(scope) 종류\n\n1. `request` 범위 (기본값)\n\n    - 요청이 들어오면 의존성이 생성되고, 요청이 끝나면 소멸됩니다.\n\n    - 대부분의 의존성은 기본적으로 `request` 범위를 가집니다.\n\n    ```python\n    def get_db():\n        db = \"데이터베이스 세션 생성\"\n        try:\n            yield db\n        finally:\n            print(\"데이터베이스 세션 종료\")\n    \n    @app.get(\"/\")\n    def read_items(db: Annotated[str, Depends(get_db)]):\n        return {\"db\": db}\n    ```\n\n    - `get_db`는 요청이 들어올 때 실행되고, 요청이 끝날 때 `finally` 블록이 실행되어 세션이 종료됩니다.\n\n    - 요청이 끝난 후 해당 의존성(`db`)은 더 이상 사용되지 않습니다.\n\n1. `singleton` **** 범위\n\n    - 애플리케이션이 시작된 후 종료될 때까지 의존성을 단 한 번만 생성하고 유지합니다.\n\n    - FastAPI에서 특정 경우 `@lru_cache`를 사용해 싱글톤 동작을 흉내낼 수 있습니다.\n\n    ```python\n    from functools import lru_cache\n    \n    @lru_cache\n    def get_config():\n        return {\"key\": \"value\"}  # 설정 정보\n    \n    @app.get(\"/\")\n    def read_items(config: Annotated[str, Depends(get_config)]):\n        return config\n    ```\n\n    - `get_config`는 애플리케이션 생명주기 동안 단 한 번 실행됩니다.\n\n    - 이후 모든 요청은 같은 객체를 재사용합니다.\n\n1. `session` 범위\n\n    - 특정 세션 동안 유지되는 의존성을 정의할 때 사용됩니다.\n\n    - FastAPI에서 기본적으로 제공하지 않지만, `Dependency Injection Containers`(의존성 관리 라이브러리)를 사용해 구현할 수 있습니다.\n\n    - 하지만 FastAPI는 기본적으로 RESTful 아키텍처를 따르기 때문에, 요청(Request) 단위를 기본 범위(scope)로 처리합니다. 각 요청은 독립적으로 처리되며, 요청이 끝나면 관련 자원(예: DB 세션, 인증 정보 등)이 정리됩니다. 따라서 일반적인 FastAPI 애플리케이션에서는 세션 범위(scope)를 따로 고려할 필요가 없습니다.\n\n    - 다만, WebSocket, 장기 실행 작업, 상태를 유지해야 하는 특수한 경우에는 세션 스코프를 고려할 수 있습니다.\n\n#### lru\\_cache\n\n`lru_cache`는 Python의 내장 데코레이터로, 함수의 반환값을 캐싱하여 이후 호출 시 동일한 결과를 반환하는 데 사용됩니다. 이를 활용해 싱글톤 객체를 쉽게 구현할 수 있습니다.\n\n```python\nfrom functools import lru_cache\n\n@lru_cache\ndef get_config():\n    return {\"db_host\": \"localhost\", \"db_port\": 5432}\n\n@app.get(\"/\")\ndef read_config(config: Annotated[str, Depends(get_config)]):\n    return config\n```\n\nFastAPI에서 설정 정보나 리소스와 같이 **상태를 유지하지 않는 객체**를 공유할 때는 `lru_cache`를 사용하는 것이 훨씬 더 일반적입니다.\n\n직접 싱글톤을 구현하는 경우는 다음과 같은 상황에서 주로 사용됩니다.\n\n1. 복잡한 초기화 로직: 초기화 과정에서 외부 리소스를 다루거나 복잡한 설정을 적용해야 할 때.\n\n1. 상태를 유지해야 하는 경우: 싱글톤 객체가 내부적으로 상태를 관리해야 할 경우.\n\n### 의존성 사용시 주의점\n\n보통 의존성은 Router Function에서 사용합니다. Request의 시작과 동시에 의존성을 사용하는게 일반적입니다. 하지만 의존성 주입이 라우터에서만 가능한 것은 아닙니다. \n\n따라서 라우터가 아닌 서비스 레이어에서 직접 의존성을 주입해 사용(클래스등을 활용)하는 것이 기술적으로 가능하지만, FastAPI의 **의존성 관리 철학**과 **응집성/분리성**의 원칙에 따라 판단해야 합니다. \n\n일반적으로, **라우터에서 의존성을 주입한 후 서비스에 전달하는 방식**이 권장됩니다. 이는 의존성을 명확히 관리하고, 서비스 레이어를 테스트하기 쉽고, FastAPI와의 결합도를 줄이는 데 유리하기 때문입니다.\n\n## 결론\n\n의존성 주입은 반복되는 코드를 획기적으로 줄이면서도 결합도가 떨어져 유지보수가 굉장히 쉬워지고, 테스트코드를 짤 때에도 독립된 코드이므로 훨씬 수월해지므로 장점이 많습니다. \n\n다만 의존성이 지나치게 중첩되면 코드의 복잡도가 증가할 수 있어 주의가 필요합니다. \n\n"},{"excerpt":"서론 테스트 코드를 작성하다 보면 을 자주 사용하게 됩니다. 특히 의 는 외부 의존성을 대체하고 독립적인 테스트를 작성할 때 매우 유용합니다. 그런데 최근에 테스트를 작성하면서 Mocking을 적용했음에도 불구하고 원래 함수가 호출되는 문제를 겪었습니다. 결론부터 말하면, Mocking할 함수의 \"사용 경로\"와 \"선언 경로\"를 혼동한 것이 문제였습니다. …","fields":{"slug":"/Python-Mocking시-patch-경로-문제-해결하기/"},"frontmatter":{"date":"December 19, 2024","title":"Python Mocking시 patch 경로 문제 해결하기","tags":["Python","FastAPI","TestCode"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n테스트 코드를 작성하다 보면 `Mocking`을 자주 사용하게 됩니다. 특히 `pytest-mock`의 `mocker.patch`는 외부 의존성을 대체하고 독립적인 테스트를 작성할 때 매우 유용합니다. 그런데 최근에 테스트를 작성하면서 Mocking을 적용했음에도 불구하고 원래 함수가 호출되는 문제를 겪었습니다.\n\n결론부터 말하면, Mocking할 함수의 \"사용 경로\"와 \"선언 경로\"를 혼동한 것이 문제였습니다. 이 블로그에서는 이 문제의 원인과 해결 방법을 공유하겠습니다.\n\n## 문제 상황\n\n### 테스트 대상 코드\n\n테스트하려는 서비스 함수는 아래와 같이 특정 `Diary`와 관련된 자산 정보를 조회합니다.\n\n```python\nfrom app.domains.diaries.repositories import get_diary_by_id\nfrom app.domains.diary_assets.repositories import get_assets_by_diary_id\nfrom app.domains.diary_assets.schemas import AssetDetailResponse\n\ndef get_assets_by_diary_service(db: Session, diary_id: int, user_id: int) -> list[AssetDetailResponse]:\n    diary = get_diary_by_id(db, diary_id)\n    if not diary:\n        raise HTTPException(status_code=404, detail=\"Diary not found\")\n\n    if diary.user_id != user_id:\n        raise HTTPException(status_code=403, detail=\"You do not have permission to access this diary\")\n\n    diary_assets = get_assets_by_diary_id(db, diary_id)\n    return [\n        AssetDetailResponse(\n            id=asset.id,\n            diary_id=asset.diary_id,\n            asset_id=asset.asset_id,\n            amount=asset.amount,\n            buy_price=asset.buy_price,\n            created_at=asset.created_at,\n            updated_at=asset.updated_at,\n        )\n        for asset in diary_assets\n    ]\n\n```\n\n이 함수에서 `get_diary_by_id`와 `get_assets_by_diary_id`는 외부 레포지토리를 호출합니다. 이를 Mocking하여 독립적인 테스트를 작성하려고 했습니다.\n\n### **잘못된** Mocking\n\n처음에는 아래와 같이 Mocking을 작성했습니다.\n\n```python\nmocker.patch(\"app.domains.diaries.repositories.get_diary_by_id\", return_value=mock_diary)\nmocker.patch(\"app.domains.diary_assets.repositories.get_assets_by_diary_id\", return_value=mock_assets)\n\n```\n\n이 코드가 올바르게 작동해 `get_diary_by_id` 메소드가 mock\\_diary을 리턴할 것이라고 예상했지만, 테스트 중 mocker가 아니라 실제 `get_diary_by_id` 함수가 호출되었습니다(이 부분은 몇 번의 실패 끝에 실제 함수에 로그를 남겨 확인할 수 있었습니다.) \n\n### **문제의 원인**\n\nMocking의 대상은 함수가 선언된 위치가 아니라, 테스트 대상 함수에서 해당 함수를 가져오는 경로여야 합니다.\n\n- `get_assets_by_diary_service` 함수는 `app.domains.diary_assets.services` 모듈에서 `get_diary_by_id`를 import하여 사용합니다.\n\n- 따라서, Mocking할 때도 `services` 모듈에서 `get_diary_by_id`를 사용하는 경로를 지정해야 합니다.\n\n## 해결: 올바른 Mocking\n\n아래와 같이 경로를 수정해야 Mocking이 제대로 적용됩니다.\n\n```python\nmocker.patch(\"app.domains.diary_assets.services.get_diary_by_id\", return_value=mock_diary)\nmocker.patch(\"app.domains.diary_assets.services.get_assets_by_diary_id\", return_value=mock_assets)\n```\n\n이제 테스트 대상 함수가 사용하는 의존성을 올바르게 Mocking할 수 있습니다.\n\n## 정리\n\n### Mocking의 핵심: \"사용 경로를 기준으로\"\n\n1. 테스트 대상 함수에서 import된 경로를 기준으로 설정해야 합니다.\n\n1. 함수가 선언된 원래 위치를 Mocking하면, 테스트 대상 함수에서는 여전히 원래 함수를 호출하게 됩니다.\n\n### **왜 이런 문제가 발생할까?**\n\nPython에서 `import`는 모듈 간의 의존성을 설정합니다. 테스트 대상 함수가 의존성을 import한 순간, Python은 해당 모듈의 네임스페이스에 함수를 바인딩합니다. 따라서 Mocking은 실제 사용된 모듈의 네임스페이스에서 함수 대체를 수행해야 효과가 있습니다.\n\n이 문제는 `unittest.mock`, `pytest-mock` 등 모든 Mocking 라이브러리에서 동일하게 발생할 수 있습니다.\n\n## 마무리\n\n이 문제로 몇 시간을 헤매면서 Mocking의 원리를 제대로 이해하게 되었습니다. 지금은 당연하게 느껴지지만, 처음 겪을 때는 정말 답답했습니다. 알게된 후에도 자주 하게 되는 실수로, 제일 먼저 잘못된건 아닌지 확인하게 되는 습관이 들었습니다. \n\n이 경우에는 실제 함수에 로그를 남겨보는 것이 가장 알기 쉬웠습니다. \n\n"},{"excerpt":"문제 상황 최근 Python 프로젝트에서 Docker를 사용해 컨테이너를 실행하면서 환경 변수가 제대로 반영되지 않는 문제를 겪었습니다. Docker의  옵션을 사용해 환경 변수를 전달했지만, 코드 실행 결과는 Docker가 전달한 값이 아닌 로컬  파일의 값이 사용되는 이상한 상황이 발생했습니다. Docker Run에서 사용한 .env 파일에서 를 설정…","fields":{"slug":"/Docker에서-Python-환경-변수-관리/"},"frontmatter":{"date":"November 29, 2024","title":"Docker에서 Python 환경 변수 관리","tags":["Python","Docker"]},"rawMarkdownBody":"## 문제 상황\n\n최근 Python 프로젝트에서 Docker를 사용해 컨테이너를 실행하면서 환경 변수가 제대로 반영되지 않는 문제를 겪었습니다. Docker의 `--env-file` 옵션을 사용해 환경 변수를 전달했지만, 코드 실행 결과는 Docker가 전달한 값이 아닌 로컬 `.env` 파일의 값이 사용되는 이상한 상황이 발생했습니다.\n\n```shell\ndocker run -d --env-file .env --name my_app my_image\n```\n\nDocker Run에서 사용한 .env 파일에서 `KEYWORD=스타일러`를 설정했지만, 프로그램은 여전히 `KEYWORD=콜라`를 사용하고 있었습니다.\n\n## 원인 분석\n\nPython에서 환경 변수를 관리하기 위해 `python-dotenv` 라이브러리의 `load_dotenv`를 사용하고 있었습니다. 이 라이브러리는 `.env` 파일의 내용을 Python 환경 변수로 로드해주는 편리한 도구입니다.\n\n하지만, Docker와 `load_dotenv`를 함께 사용할 때 충돌이 발생했습니다.\n\n### 원인의 핵심\n\n- Docker 환경 변수: `-env-file` 옵션으로 전달된 값은 컨테이너 실행 시 시스템 환경 변수로 설정됩니다.\n\n- `load_dotenv`의 동작:\n\n    - `.env` 파일의 내용을 Python 환경 변수로 설정.\n\n    - 기본적으로 이미 존재하는 환경 변수는 덮어쓰지 않음.\n\n    - 하지만 코드가 `.env` 파일을 명시적으로 읽어, Docker에서 설정된 환경 변수를 덮어쓰는 문제가 발생.\n\n이로 인해 프로그램은 Docker가 전달한 `KEYWORD=스타일러`가 아닌 `.env` 파일의 `KEYWORD=콜라`를 사용했습니다.\n\n## 해결 방법\n\n### `load_dotenv` 제거\n\n`load_dotenv`는 로컬 `.env` 파일을 로드하기 위한 도구로, Docker와 함께 사용할 때는 불필요합니다. Docker는 `--env-file`로 전달한 환경 변수를 컨테이너 내부에 자동으로 설정하므로, Python 코드에서는 `os.getenv`만 사용하면 됩니다.\n\n```python\nimport os\n\n# 환경 변수 읽기\nkeyword = os.getenv(\"KEYWORD\", \"default_value\")\nprint(f\"Keyword: {keyword}\")\n```\n\n이렇게 `load_dotenv`를 제거하니, Docker에서 전달한 환경 변수(`KEYWORD=스타일러`)가 정상적으로 동작했습니다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n### Docker에서 환경 변수 확인\n\nDocker 컨테이너가 환경 변수를 올바르게 전달했는지 확인하려면 아래 명령어를 사용합니다.\n\n```shell\ndocker exec -it my_app env | grep KEYWORD\n```\n\n결과:\n\n```plain text\nKEYWORD=스타일러\n```\n\n이 명령어를 통해 컨테이너 내부에서 설정된 환경 변수를 확인할 수 있습니다.\n\n## 로컬에서는..?\n\n로컬에서 실행할 때는 `.env` 파일을 사용해야 합니다. \n\n```python\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nkeyword = os.getenv(\"KEYWORD\", \"default_value\")\nprint(f\"Keyword: {keyword}\")\n```\n\n## 교훈\n\n1. Docker와 Python 환경 변수 관리의 중복은 주의해야 합니다:\n\n    - Docker에서 환경 변수를 관리한다면, Python 코드에서 추가로 `.env` 파일을 읽지 않아야 합니다.\n\n1. 환경 변수 우선순위를 명확히 해야 합니다:\n\n    - Docker > 시스템 환경 변수 > `.env` 파일 순으로 환경 변수가 설정되도록 설계하면 혼란을 줄일 수 있습니다.\n\n"},{"excerpt":"서론 최근 ODROID SBC(Single Board Computer)를 사용하는 환경에서 루트 파일 시스템()이 읽기 전용(read-only) 모드로 전환되는 문제를 경험했습니다. 이는 특정 상황에서 발생할 수 있는 디스크 오류로, SD 카드나 eMMC와 같은 저장 장치를 사용하는 시스템에서 흔히 발생할 수 있습니다. 이번 글에서는 문제 발생 원인과 증…","fields":{"slug":"/SD-카드-기반-디스크-오류-및-해결-과정/"},"frontmatter":{"date":"November 27, 2024","title":"SD 카드 기반 디스크 오류 및 해결 과정","tags":["Odroid","Ubuntu"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n최근 ODROID SBC(Single Board Computer)를 사용하는 환경에서 루트 파일 시스템(`/`)이 읽기 전용(read-only) 모드로 전환되는 문제를 경험했습니다. 이는 특정 상황에서 발생할 수 있는 디스크 오류로, SD 카드나 eMMC와 같은 저장 장치를 사용하는 시스템에서 흔히 발생할 수 있습니다. 이번 글에서는 문제 발생 원인과 증상, 그리고 실제로 문제를 해결한 과정을 공유하고자 합니다.\n\n## 문제 증상\n\n문제는 다음과 같은 상황에서 처음 발생했습니다:\n\n- 시스템에서 디렉토리에 접근하거나 파일을 생성하려고 했을 때 \"Read-only file system\" 오류가 발생.\n\n- 예를 들어, `/HDD/develop` 디렉토리에서 Docker 이미지를 빌드하려고 했지만 권한 문제로 실패했습니다.\n\n    ```shell\n    ERROR: mkdir /home/USER/.docker: read-only file system\n    ```\n\n- 이후, 권한 문제를 해결하려고 `chmod`, `chgrp`, `usermod` 명령어를 시도했지만 여전히 동일한 오류가 발생했습니다.\n\n    ```shell\n    chmod: changing permissions of '/etc/passwd': Read-only file system\n    ```\n\n`mount | grep ' / '` 명령어를 통해 파일 시스템 상태를 확인한 결과, 루트 파일 시스템이 읽기 전용 모드로 전환된 것을 확인했습니다. ro 라고 써있는 것을 확인할 수 있습니다. \n\n```shell\n/dev/mmcblk1p2 on / type ext4 (ro,noatime,errors=remount-ro,stripe=32753)\n```\n\n## 원인 분석\n\n`ro`(read-only) 모드는 파일 시스템에서 심각한 오류가 발생했을 때 시스템이 자동으로 읽기 전용 모드로 전환하는 보호 메커니즘입니다. `/etc/fstab` 파일의 설정에서 `errors=remount-ro` 옵션이 이를 담당합니다. 일반적으로 이는 다음과 같은 이유로 발생할 수 있습니다:\n\n1. 파일 시스템 손상: 예상치 못한 종료나 전원 문제로 파일 시스템이 손상됨.\n\n1. SD 카드 불량: SD 카드나 eMMC 저장 장치의 물리적 손상.\n\n1. 디스크 용량 부족: 루트 파티션에 남은 공간이 없을 때.\n\n1. I/O 오류: 저장 장치와의 통신 문제.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n## 해결 과정\n\n### 파일 시스템 복구\n\n루트 파일 시스템 복구를 위해 `fsck`(파일 시스템 체크) 도구를 사용했습니다. 이를 위해 먼저 루트 파일 시스템이 마운트된 디스크를 식별했습니다. 제 경우 `/dev/mmcblk1p2`가 루트 디스크였습니다.\n\n다음 명령어로 파일 시스템 복구를 실행했습니다:\n\n```shell\nsudo fsck -y /dev/mmcblk1p2\n```\n\n- `y` 옵션은 모든 수정 사항에 대해 자동으로 \"Yes\"를 선택하게 합니다.\n\n- `fsck` 실행 중, 손상된 블록 및 파일 시스템 불일치를 수정했습니다.\n\n### 시스템 재부팅\n\n복구 후 시스템을 재부팅하여 파일 시스템이 정상적으로 작동하는지 확인했습니다:\n\n```shell\nsudo reboot\n```\n\n## 결과\n\n재부팅 후 `mount` 명령어로 다시 파일 시스템 상태를 확인한 결과, 루트 파일 시스템이 읽기-쓰기 모드로 마운트된 것을 확인할 수 있었습니다:\n\n```shell\n/dev/mmcblk1p2 on / type ext4 (rw,noatime,errors=remount-ro,stripe=32753)\n```\n\n이제 권한 변경 및 Docker 이미지 빌드 작업도 정상적으로 동작했습니다:\n\n```shell\nsudo chmod g+w /HDD/develop\nsudo docker build -t hotdeal_alarm .\n```\n\n\n\n다만, 해당 문제는 곧 다시 발생했으며 SD 카드 자체에 심각한 물리적인 손상이 생긴것으로 짐작되어 새로운 SD카드를 사기로 결정했습니다. \n\n이 글을 읽는 여러분은 위의 방법으로 해결되기를 기원합니다.. \n\n"},{"excerpt":"서론: 이란? startapp은 Django에서 새로운 앱(application)을 생성하기 위한 명령어입니다. Django는 프로젝트를 기능별로 나누어 독립적인 모듈(앱)로 구성하는 것을 권장합니다. startapp 명령어를 사용하면 앱을 생성하고, 필요한 기본 디렉토리와 파일 구조를 자동으로 생성해줍니다. Django의 이란? Django 앱은 특정 …","fields":{"slug":"/Django의-startapp/"},"frontmatter":{"date":"November 20, 2024","title":"Django의 startapp","tags":["Django","Python","BackEnd"]},"rawMarkdownBody":"![](image1.png)\n## 서론: `startapp`이란? \n\nstartapp은 Django에서 새로운 앱(application)을 생성하기 위한 명령어입니다. Django는 프로젝트를 기능별로 나누어 독립적인 모듈(앱)로 구성하는 것을 권장합니다. startapp 명령어를 사용하면 앱을 생성하고, 필요한 기본 디렉토리와 파일 구조를 자동으로 생성해줍니다.\n\n## **Django의** `앱`**이란?**\n\nDjango 앱은 특정 기능이나 도메인을 처리하는 독립적인 모듈입니다. 예를 들어, 블로그 기능이 필요한 경우 `blog`라는 앱을 생성하고, 사용자 관리 기능은 `users`라는 앱으로 나눌 수 있습니다.\n\n하나의 프로젝트에서 여러 앱을 생성하고 조합하여 큰 시스템을 구성합니다.\n\n### **왜 앱을 나눌까?**\n\n- 모듈화: 기능을 분리하여 유지보수가 쉬워집니다.\n\n- 재사용성: 다른 프로젝트에서도 독립적으로 앱을 가져다 쓸 수 있습니다.\n\n- 확장성: 프로젝트가 커지더라도 각 앱이 독립적으로 관리되므로 확장성이 높아집니다.\n\n## `startapp` 명령어의 역할\n\n`python manage.py startapp <앱_이름>` 명령을 실행하면, 아래와 같은 기본 파일과 디렉토리를 포함하는 앱 구조가 생성됩니다.\n\n```plain text\n앱_이름/\n    ├── migrations/\n    │   └── __init__.py  # 마이그레이션 파일 관리\n    ├── __init__.py      # Python 패키지 파일\n    ├── admin.py         # 관리자 페이지 설정\n    ├── apps.py          # 앱 구성 정보\n    ├── models.py        # 데이터베이스 모델 정의\n    ├── tests.py         # 테스트 케이스 작성\n    └── views.py         # 뷰 로직 작성\n```\n\n## `startapp` 사용 방법\n\n### 명령어 실행\n\n프로젝트 디렉토리에서 다음 명령어를 실행합니다.\n\n```bash\npython manage.py startapp myapp\n```\n\n### `settings.py`에 앱 등록\n\n생성된 앱을 프로젝트에서 사용하려면 `settings.py` 파일의 `INSTALLED_APPS`에 추가해야 합니다:\n\n```python\nINSTALLED_APPS = [\n    # 기존 앱들\n    'myapp',  # 새로 생성한 앱 추가\n]\n```\n\n### 앱 내에서 기능 추가\n\n생성된 파일들에 모델, 뷰, URL 등 앱의 로직을 작성합니다.\n\n"},{"excerpt":"서론 Django의 설치가 끝났고 DB 연결까지 완료됐다면 Django의 관리자 페이지를 활성화 할 수 있습니다.  Django 관리자 페이지 소개 Django 관리자 페이지는 웹 애플리케이션에서 데이터를 손쉽게 관리할 수 있도록 제공되는 자동화된 백오피스입니다. Django에서 기본적으로 제공하며, 별도의 코딩 없이도 모델 데이터를 추가, 수정, 삭제할…","fields":{"slug":"/Django의-관리자-페이지-만들기/"},"frontmatter":{"date":"October 29, 2024","title":"Django의 관리자 페이지 만들기","tags":["Django","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nDjango의 설치가 끝났고 DB 연결까지 완료됐다면 Django의 관리자 페이지를 활성화 할 수 있습니다. \n\n### Django 관리자 페이지 소개 \n\nDjango 관리자 페이지는 웹 애플리케이션에서 데이터를 손쉽게 관리할 수 있도록 제공되는 **자동화된 백오피스**입니다. Django에서 기본적으로 제공하며, 별도의 코딩 없이도 **모델 데이터를 추가, 수정, 삭제**할 수 있는 UI를 제공합니다. 이 페이지는 개발자와 관리자가 데이터베이스를 직접 접근하지 않아도 웹 상에서 데이터 관리를 효율적으로 할 수 있도록 돕습니다.\n\n### Django 관리자 페이지의 주요 기능\n\n- 모델 관리: 데이터베이스에 정의된 모델 데이터를 관리할 수 있습니다.\n\n- 사용자 관리: 관리자, 스태프 등의 사용자 권한을 설정하고 관리할 수 있습니다.\n\n- 보안 기능: Django의 인증 및 권한 관리 기능과 연동되어 보안이 보장됩니다.\n\n- 확장성: 커스터마이징이 가능해 필요한 기능을 추가하여 쉽게 확장할 수 있습니다.\n\nDjango 관리자 페이지는 기본적으로 제공되기 때문에, 간단한 설정만으로도 바로 사용할 수 있어 개발 속도를 높이고 데이터 관리의 효율성을 극대화할 수 있는 장점이 있습니다.\n\n## 관리자 페이지 설정\n\n관리자 페이지를 사용하기 위해서는 슈퍼유저를 생성해야 합니다. 슈퍼유저를 생성하기 위해서는 슈퍼유저 관리를 위한 기본 인증 테이블이 DB에 존재해야 합니다. \n\n만약 인증 테이블이 생성되지 않은 상태에서 createsuperuser를 실행한다면 \n\n```bash\ndjango.db.utils.ProgrammingError: relation \"auth_user\" does not exist\nLINE 1: ...user\".\"is_active\", \"auth_user\".\"date_joined\" FROM \"auth_user...\n```\n\n의 오류가 발생합니다. \n\n따라서 `createsuperuser` 명령을 실행하기 전에 반드시 `migrate` 명령을 먼저 실행해 기본 테이블을 생성해줘야 합니다.\n\n### 마이그레이션 적용\n\ndocker compse 환경에서의 명령어입니다. \n\n```bash\ndocker-compose -f {docker-compose yml 파일 경로} run web python manage.py migrate\n```\n\n### Superuser 설정\n\n```bash\ndocker-compose -f {docker-compose yml 파일 경로} run web python manage.py createsuperuser\n```\n\n명령을 실행하면, 아래와 같이 관리자 계정에 필요한 정보를 입력하라는 메시지가 표시됩니다.\n\n- 사용자명: 관리자 페이지에 로그인할 때 사용할 사용자명입니다.\n\n- 이메일 주소: 계정과 연결할 이메일 주소입니다.\n\n- 비밀번호: 로그인 시 사용할 비밀번호입니다. 두 번 입력하여 확인합니다.\n\n각 정보를 입력한 후, 성공 메시지가 나타나면 슈퍼유저 생성이 완료된 것입니다.\n\n## 관리자 페이지 접속\n\n여기까지 마치면 생성한 계정을 통해 [http://localhost:8000/admin/](http://localhost:8000/admin/) 에 접속해서 로그인 할 수 있습니다. \n\n## 관리자 페이지의 깨짐\n\n로그인은 되는데, 페이지 모양이 완전히 깨져서 나옵니다. 개발자 도구를 켜서 확인해보니 아래와 같은 오류들이 잔뜩 떠있습니다. \n\n![](image2.png)\n이는 정적파일(static files)이 올바르게 수집되지 않아서 발생하는 문제입니다. \n\n### 정적 파일 수집 및 설정\n\nDjango는 기본적으로 관리자 페이지에 필요한 JavaScript와 CSS 같은 정적 파일을 서빙하기 위해 collectstatic 명령을 사용합니다. 이 명령어를 통해 모든 정적 파일을 프로젝트의 STATIC\\_ROOT 디렉토리에 모으고, 웹 서버가 이를 제공할 수 있도록 합니다.\n\n#### STATIC\\_ROOT 설정 확인\n\n`settings.py`에서 `STATIC_ROOT`가 다음과 같이 설정되어 있는지 확인합니다. 이 경로로 모든 정적 파일이 모입니다.\n\n```python\nimport os\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nSTATIC_URL = '/static/'\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n```\n\n#### 정적 파일 수집\n\n아래의 명령어를 사용해 정적 파일을 수집합니다. \n\n```bash\ndocker-compose -f ./docker/docker-compose.local.yml run web python manage.py collectstatic --noinput\n```\n\n#### urls.py 설정\n\n개발 환경에서는 Django가 `/static/` 경로의 파일들을 직접 제공하도록 `urls.py`에 정적 파일 URL을 추가할 수도 있습니다.\n\n```python\n\nfrom django.conf import settings\nfrom django.conf.urls.static import static\nfrom django.contrib import admin\nfrom django.urls import path\n\nurlpatterns = [\n    path(\"admin/\", admin.site.urls),\n]\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n```\n\n위 설정이 완료되면 Django가 개발 모드에서 `/static/` 경로에 있는 파일들을 서빙할 수 있게 됩니다.\n\nDEBUG == True 일때에만 작동하게 되어있음을 확인할 수 있습니다. \n\nDjango에서 `urls.py` 설정을 개발 환경에서만 사용하는 이유는 정적 파일 서빙은 일반적으로 프로덕션 환경의 웹 서버(예: Nginx, Apache)가 담당하기 때문입니다. Django는 웹 애플리케이션을 제공하는 데 최적화되어 있으며, 정적 파일을 효율적으로 서빙하는 데는 적합하지 않습니다. 따라서, 개발 환경이 아닌 프로덕션 환경에서는 Django가 아닌 웹 서버가 정적 파일을 제공하도록 구성하는 것이 성능 면에서 더 효율적입니다.\n\n\n\n"},{"excerpt":"ORM(Object-Relational Mapping) 생각해보니 ORM을 사용한 적이 많았는데, 이게 어떤 것인지 명확하게 짚고 넘어갔던 적은 없던 것 같습니다. Spring의 ORM을 써보려고 하니, 이번 기회에 한 번 정리하고 넘어가려고 합니다.  ORM의 정의 ORM은 객체 지향 프로그래밍(OOP) 언어에서 사용하는 객체와 관계형 데이터베이스의 테…","fields":{"slug":"/JPAJava-Persistence-API-1/"},"frontmatter":{"date":"September 05, 2024","title":"JPA(Java Persistence API) (1)","tags":["Spring","DataBase","Kotlin","ORM"]},"rawMarkdownBody":"![](image1.png)\n## ORM(Object-Relational Mapping)\n\n생각해보니 ORM을 사용한 적이 많았는데, 이게 어떤 것인지 명확하게 짚고 넘어갔던 적은 없던 것 같습니다. Spring의 ORM을 써보려고 하니, 이번 기회에 한 번 정리하고 넘어가려고 합니다. \n\n### ORM의 정의\n\nORM은 객체 지향 프로그래밍(OOP) 언어에서 사용하는 객체와 관계형 데이터베이스의 테이블을 자동으로 매핑해주는 기법입니다. 이를 통해 데이터베이스와의 상호작용을 SQL이 아닌 객체를 통해 처리할 수 있습니다. ORM을 사용하면 데이터베이스의 테이블을 객체로 추상화하여 비즈니스 로직을 더 직관적으로 표현할 수 있으며, 코드의 재사용성과 유지보수성을 높일 수 있습니다.\n\n### 객체 지향 프로그래밍과 관계형 데이터베이스의 차이 \n\n- 객체 지향 프로그래밍(OOP)에서는 데이터를 객체로 모델링하고, 이 객체들 간의 상호작용을 통해 프로그램을 구성합니다. 객체는 속성(필드)과 행동(메서드)을 가집니다.\n\n- 관계형 데이터베이스(RDB)에서는 데이터를 테이블로 구조화하여 저장합니다. 테이블은 행과 열로 이루어져 있으며, 관계를 통해 데이터를 연결합니다.\n\n- 주요 차이점은 객체는 메모리에서 동작하는 반면, 데이터베이스는 저장 공간에 데이터를 영구적으로 저장한다는 점입니다. 객체 간의 관계는 참조로 연결되지만, 테이블 간의 관계는 외래 키(Foreign Key)로 연결됩니다. 또한 객체는 다형성(polymorphism)을 지원하지만, 테이블은 그렇지 않습니다.\n\n    \n        다형성(polymorphism)은 객체 지향 프로그래밍(OOP)에서 중요한 개념 중 하나로, 같은 메서드나 연산자가 여러 가지 형태로 동작할 수 있는 성질을 말합니다. 이를 통해 코드의 유연성과 확장성을 높일 수 있습니다.\n\n### 장점과 단점\n\n#### 장점\n\n- 생산성 향상: SQL을 직접 작성하지 않고도 데이터베이스 작업을 할 수 있으므로 개발자가 비즈니스 로직에 집중할 수 있습니다. 복잡한 SQL 쿼리를 줄이고, 간결한 코드로 데이터 접근 로직을 처리할 수 있습니다.\n\n- 유지보수성: 객체 지향 프로그래밍의 구조를 따르기 때문에 비즈니스 로직과 데이터베이스 로직을 분리할 수 있어 코드의 재사용성 및 유지보수성이 높아집니다.\n\n- 데이터베이스 독립성: ORM은 데이터베이스에 종속되지 않으며, 동일한 코드를 사용해 여러 데이터베이스와 상호작용할 수 있습니다. 즉, 데이터베이스가 변경되어도 코드 수정이 최소화됩니다.\n\n- 자동화된 매핑: 객체와 테이블의 매핑이 자동으로 이루어지기 때문에 데이터베이스의 스키마를 수동으로 매핑하는 부담이 줄어듭니다.\n\n#### 단점\n\n- 성능 저하: 복잡한 쿼리나 대량의 데이터를 다룰 때는 ORM이 생성하는 쿼리가 비효율적일 수 있습니다. 특히, [N+1 문제](https://sharknia.github.io/N1-문제)와 같은 성능 이슈가 발생할 수 있으며, 이를 해결하기 위해서는 추가적인 최적화가 필요합니다.\n\n- 추상화의 복잡성: ORM은 데이터베이스와의 상호작용을 추상화하지만, 너무 많은 추상화는 특정한 데이터베이스 특성을 활용하지 못하게 하거나 디버깅을 어렵게 만들 수 있습니다.\n\n- 복잡한 쿼리의 한계: ORM은 단순한 CRUD 작업에는 효율적이지만, 복잡한 쿼리나 고성능이 요구되는 작업에서는 SQL이 더 유리할 수 있습니다. 이때는 Native Query를 사용해야 할 수 있습니다.\n\n### ORM을 사용하지 않았을 때의 문제점\n\n- 반복되는 코드: SQL 쿼리 작성과 결과 매핑을 직접 처리해야 하기 때문에 비슷한 패턴의 코드가 반복적으로 나타날 수 있습니다. 이는 코드의 양을 증가시키고 유지보수를 어렵게 만듭니다.\n\n- 비즈니스 로직과 데이터베이스 로직의 혼합: SQL 쿼리가 비즈니스 로직에 직접 포함될 경우, 코드의 가독성과 유지보수성이 저하됩니다. 이는 복잡한 프로젝트에서 특히 문제로 작용할 수 있습니다.\n\n- 데이터베이스 종속성: 특정 데이터베이스에 종속된 SQL 쿼리를 사용하면, 데이터베이스가 변경될 때마다 코드를 수정해야 할 필요가 생깁니다. ORM을 사용하면 데이터베이스와 독립적으로 애플리케이션을 설계할 수 있습니다.\n\n- 수동 매핑의 불편함: 데이터베이스에서 가져온 데이터를 객체에 수동으로 매핑하는 과정에서 발생하는 실수나 비효율성이 존재할 수 있으며, 이를 관리하는 데 많은 시간이 소모될 수 있습니다.\n\n## JPA\n\n### JPA란? \n\n- JPA(Java Persistence API)는 자바에서 객체 지향 프로그래밍을 기반으로 관계형 데이터베이스를 다룰 수 있게 해주는 표준 인터페이스입니다. 즉, 객체와 데이터베이스 테이블 간의 매핑을 제공하는 프레임워크 표준입니다.\n\n- JPA를 사용하면 SQL을 직접 작성하는 대신, 객체를 통해 데이터베이스의 데이터를 조회, 삽입, 수정, 삭제하는 작업을 수행할 수 있습니다.\n\n- JPA는 데이터베이스와의 상호작용을 추상화하여 데이터베이스 종속성을 줄이고, 객체 지향적인 방식으로 애플리케이션을 설계할 수 있게 도와줍니다.\n\n### JPA와 Hibernate의 관계\n\n- JPA는 표준 인터페이스이기 때문에, 실제로 데이터베이스와 상호작용을 하려면 JPA의 구현체가 필요합니다. Hibernate는 JPA의 대표적인 구현체 중 하나로, 가장 널리 사용됩니다.\n\n- JPA가 ORM 표준을 정의하고 있다면, Hibernate는 이 표준을 기반으로 JPA의 동작을 구현하는 라이브러리입니다. 즉, JPA는 추상적인 규칙을 제공하고, Hibernate는 이를 실제로 동작하게 만듭니다.\n\n- 그 외에도 EclipseLink, OpenJPA 등 다른 JPA 구현체도 있지만, Spring Data JPA 환경에서는 일반적으로 Hibernate를 사용합니다.\n\n### JPA의 주요 어노테이션\n\nJPA에서는 객체와 데이터베이스 테이블을 매핑하기 위해 여러 가지 어노테이션을 제공합니다. 이 어노테이션들을 통해 엔티티 클래스가 테이블과 어떻게 연결되는지를 정의할 수 있습니다.\n\n- @Entity: 이 어노테이션이 붙은 클래스는 JPA가 관리하는 엔티티임을 나타냅니다. 즉, 해당 클래스는 데이터베이스 테이블과 매핑된다는 의미입니다.\n\n- @Table: 엔티티가 매핑될 데이터베이스 테이블의 이름을 지정합니다. 지정하지 않으면 기본적으로 엔티티 클래스 이름이 테이블 이름으로 사용됩니다.\n\n    ```kotlin\n    @Entity\n    @Table(name = \"users\")\n    class User {\n        // 엔티티 클래스의 필드 정의\n    }\n    ```\n\n- @Id: 해당 필드가 테이블의 기본 키(primary key)임을 나타냅니다.\n\n- @GeneratedValue: 기본 키가 자동으로 생성되는 방식을 정의합니다. 주로 `AUTO`, `IDENTITY`, `SEQUENCE`, `TABLE` 전략이 사용됩니다.\n\n- @Column: 엔티티의 필드를 데이터베이스의 컬럼과 매핑합니다. 컬럼 이름, 길이, null 허용 여부 등을 설정할 수 있습니다. 지정하지 않으면 필드 이름이 컬럼 이름으로 매핑됩니다.\n\n    ```kotlin\n    @Entity\n    class User {\n        @Id\n        @GeneratedValue(strategy = GenerationType.IDENTITY)\n        var id: Long? = null\n    \n        @Column(name = \"username\", nullable = false)\n        var username: String = \"\"\n    }\n    ```\n\n### 엔티티(Entity)와 테이블(Table) 매핑\n\n- JPA에서 엔티티(Entity)는 데이터베이스의 테이블(Table)과 매핑되는 자바 객체입니다. 즉, 엔티티는 데이터베이스에서 하나의 레코드(Row)를 표현하고, 엔티티의 각 필드는 테이블의 컬럼(Column)과 매핑됩니다.\n\n- 예를 들어, `User`라는 엔티티는 데이터베이스의 `users` 테이블과 매핑되고, 엔티티의 `username` 필드는 `users` 테이블의 `username` 컬럼과 연결됩니다. 이를 통해 자바 객체를 사용하여 데이터베이스의 데이터를 쉽게 다룰 수 있게 됩니다.\n\n- 또한, 엔티티 간의 관계도 JPA를 통해 정의할 수 있습니다. 예를 들어, `@OneToMany`, `@ManyToOne`, `@ManyToMany` 같은 애노테이션을 사용하여 테이블 간의 외래 키 관계를 객체 지향적으로 표현할 수 있습니다.\n\n    ```kotlin\n    @Entity\n    @Table(name = \"users\")\n    class User {\n        @Id\n        @GeneratedValue(strategy = GenerationType.IDENTITY)\n        var id: Long? = null\n    \n        @Column(name = \"username\", nullable = false)\n        var username: String = \"\"\n    }\n    ```\n\n이러한 매핑을 통해 JPA는 자바 애플리케이션의 객체 모델과 데이터베이스의 테이블 간의 일관된 관계를 유지하게 해줍니다. ORM의 핵심은 이런 매핑을 통해 개발자가 SQL에 의존하지 않고 객체 지향적인 방식으로 데이터베이스와 상호작용할 수 있게 하는 데 있습니다.\n\n## **Spring Data JPA**\n\n### Spring과 JPA의 통합\n\n- Spring은 강력한 의존성 주입(DI)과 트랜잭션 관리 기능을 제공하는 프레임워크로, 이를 통해 JPA와의 통합이 용이합니다.\n\n- Spring Data JPA는 Spring과 JPA를 통합하여 ORM 기능을 더 간단하고 쉽게 사용할 수 있게 해줍니다. 이를 통해 개발자는 JPA의 복잡한 설정 없이도 간단한 설정만으로 데이터베이스 연동을 할 수 있습니다.\n\n- Spring Data JPA는 JPA의 표준을 기반으로 하며, JPA 엔티티 매핑을 활용하면서도, 다양한 리포지토리(repository) 계층의 자동화된 구현을 지원합니다. 즉, 개발자는 직접 SQL 쿼리를 작성할 필요 없이, 메서드 이름만으로도 데이터베이스 작업을 쉽게 처리할 수 있습니다.\n\n### Spring Data JPA의 구조 및 역할\n\n- Spring Data JPA는 데이터베이스와의 상호작용을 쉽게 처리할 수 있도록 다양한 인터페이스와 클래스들을 제공합니다.\n\n- 주요 계층 구조\n\n    - Repository: Repository는 가장 상위 레벨의 인터페이스입니다. Spring Data JPA의 기본적인 인터페이스로, 이 자체로는 아무 기능도 제공하지 않습니다. 그저 기본적인 리포지토리 패턴을 정의한 추상적인 인터페이스일 뿐입니다.\n\n        이 인터페이스는 메서드가 정의되어 있지 않으며, 데이터 액세스 계층을 추상화하는 기반 역할을 합니다.\n\n        ```kotlin\n        public interface Repository<T, ID>\n        ```\n\n    - CrudRepository: 리포지토리의 기본 인터페이스로, CRUD 작업을 위한 메서드들을 제공합니다.\n\n    - JpaRepository: CrudRepository를 확장한 인터페이스로, JPA에 특화된 추가 기능(페이징, 정렬 등)을 제공합니다.\n\n### JpaRepository, CrudRepository 인터페이스 활용\n\n- Spring Data JPA는 JpaRepository와 CrudRepository 인터페이스를 제공하여 개발자가 직접 구현하지 않고도 기본적인 데이터 액세스 기능을 사용할 수 있게 합니다.\n\n- CrudRepository는 기본적인 CRUD(Create, Read, Update, Delete) 작업을 위한 메서드를 제공하는 가장 기본적인 인터페이스입니다.\n\n    이 인터페이스는 데이터를 저장, 조회, 수정, 삭제하는 작업을 처리하기 위한 메서드들이 제공됩니다. 즉, 실제 CRUD 작업을 수행할 수 있는 기능을 가지고 있습니다.\n\n    예를 들어, `save()`, `findById()`, `findAll()`, `deleteById()` 등의 메서드들이 정의되어 있어, 이 인터페이스를 상속받으면 별도로 구현 없이 기본적인 CRUD 작업을 사용할 수 있습니다.\n\n    ```kotlin\n    public interface CrudRepository<T, ID> extends Repository<T, ID> {\n        <S extends T> S save(S entity);\n        Optional<T> findById(ID id);\n        Iterable<T> findAll();\n        void deleteById(ID id);\n        void delete(T entity);\n    }\n    ```\n\n- JpaRepository는 CrudRepository를 확장하여 더 많은 기능을 제공합니다. 예를 들어, 페이징 처리, 정렬 등의 기능을 추가로 사용할 수 있습니다.\n\n    ```kotlin\n    interface JpaRepository<T, ID> : CrudRepository<T, ID> {\n        fun findAll(pageable: Pageable): Page<T>\n        fun findAll(sort: Sort): List<T>\n    }\n    ```\n\n### 기본적인 CRUD 작업 처리\n\nSpring Data JPA를 통해 직접 SQL을 작성하지 않고도 CRUD 작업을 쉽게 처리할 수 있습니다. `JpaRepository`나 `CrudRepository` 인터페이스를 상속받은 리포지토리를 사용하여 아래와 같은 작업을 자동으로 처리할 수 있습니다.\n\n- Create (저장): `save()` 메서드를 사용해 엔티티를 저장합니다. 새 엔티티라면 INSERT 쿼리를 생성하고, 기존 엔티티라면 UPDATE 쿼리를 실행합니다.\n\n    ```kotlin\n    val user = User(username = \"John\")\n    userRepository.save(user)\n    ```\n\n- Read (조회): `findById()`를 사용하여 특정 엔티티를 조회하고, `findAll()`을 사용하여 모든 엔티티를 조회할 수 있습니다. 또한, `findBy...` 형태의 메서드를 작성하여 다양한 조건으로 데이터를 조회할 수 있습니다.\n\n    ```kotlin\n    val user: Optional<User> = userRepository.findById(1L)\n    val allUsers: List<User> = userRepository.findAll()\n    ```\n\n- Update (수정): `save()` 메서드는 기본적으로 엔티티가 존재하면 수정 작업을 처리합니다. 특정 엔티티를 조회한 후 필드를 변경하고 다시 저장하면 UPDATE 쿼리가 실행됩니다.\n\n    ```kotlin\n    val user = userRepository.findById(1L).orElseThrow()\n    user.username = \"Jane\"\n    userRepository.save(user)\n    ```\n\n- Delete (삭제): `deleteById()`를 사용하여 특정 엔티티를 삭제하거나, `delete()` 메서드를 사용해 엔티티를 직접 삭제할 수 있습니다.\n\n    ```kotlin\n    userRepository.deleteById(1L)\n    ```\n\n\n\n"},{"excerpt":"서론 해당 프로젝트의 역할은 Rest API Backend 서버입니다. 본연의 역할을 위해 먼저 간단하게 Get 메소드에 대한 요청을 처리하는 컨트롤러를 정의해보겠습니다.  user 컨트롤러 프로젝트 구조는 DDD를 따라 정의할 예정입니다. 따라서 먼저 user 패키지를 정의하고, 그 안에  파일을 생성해주고 다음과 같이 GET 요청을 처리할 컨트롤러를 …","fields":{"slug":"/Kotlin에서의-GET-API-선언-후-Swagger-설정-하기/"},"frontmatter":{"date":"September 05, 2024","title":"Kotlin에서의 GET API 선언 후 Swagger 설정 하기","tags":["Kotlin","Spring"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n해당 프로젝트의 역할은 Rest API Backend 서버입니다. 본연의 역할을 위해 먼저 간단하게 Get 메소드에 대한 요청을 처리하는 컨트롤러를 정의해보겠습니다. \n\n## user 컨트롤러\n\n프로젝트 구조는 [DDD](https://sharknia.github.io/FastAPI와-DDD)를 따라 정의할 예정입니다. 따라서 먼저 user 패키지를 정의하고, 그 안에 `Controller.kt` 파일을 생성해주고 다음과 같이 GET 요청을 처리할 컨트롤러를 작성했습니다. \n\n```kotlin\npackage world.nolog.nolog_world.user\n\nimport org.springframework.web.bind.annotation.GetMapping\nimport org.springframework.web.bind.annotation.RequestMapping\nimport org.springframework.web.bind.annotation.RestController\n\ndata class HelloResponse(val result: String)\n\n@RestController\n@RequestMapping(\"/user\")  // 공통 경로 설정\nclass HelloController {\n    // /user 경로를 처리\n    @GetMapping\n    fun userRoot(): HelloResponse {\n        return HelloResponse(result = \"Welcome to the user page!\")\n    }\n\n    // /user/hello 경로를 처리\n    @GetMapping(\"/hello\")\n    fun hello(): HelloResponse {\n        return HelloResponse(result = \"hello world!~~\")\n    }\n\n}\n```\n\n### 코드 분석\n\n```kotlin\npackage world.nolog.nolog_world.user\n\nimport org.springframework.web.bind.annotation.GetMapping\nimport org.springframework.web.bind.annotation.RequestMapping\nimport org.springframework.web.bind.annotation.RestController\n```\n\n- `package world.nolog.nolog_world.user`: `HelloController` 클래스가 속해 있는 패키지 경로를 정의합니다. 일반적으로 프로젝트 구조에 따라 패키지가 설정됩니다.\n\n- `import` 구문: 스프링 웹 어노테이션인 `@GetMapping`, `@RequestMapping`, `@RestController`를 가져옵니다. 이는 HTTP 요청을 처리하기 위한 스프링 웹의 주요 어노테이션입니다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n\n\n```kotlin\ndata class HelloResponse(val result: String)\n```\n\n- `data class HelloResponse`: JSON 응답으로 반환될 데이터를 담는 간단한 코틀린 데이터 클래스입니다. `result`라는 이름의 문자열 필드 하나를 가집니다. 스프링 부트는 이 `data class`를 자동으로 JSON 형식으로 변환하여 클라이언트에게 반환합니다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n\n\n```kotlin\n@RestController\n@RequestMapping(\"/user\")  // 공통 경로 설정\nclass HelloController {\n```\n\n- `@RestController`: 이 클래스가 스프링의 RESTful 웹 API를 처리하는 컨트롤러임을 명시합니다.\n\n- `@RequestMapping(\"/user\")`: `/user` 경로로 들어오는 HTTP 요청을 이 컨트롤러가 처리하게 합니다. 즉, 이 컨트롤러 내의 모든 메소드가 기본적으로 `/user` 경로를 가집니다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n\n\n```kotlin\n    // /user 경로를 처리\n    @GetMapping\n    fun userRoot(): HelloResponse {\n        return HelloResponse(result = \"Welcome to the user page!\")\n    }\n```\n\n- `@GetMapping`: 경로를 명시하지 않으면 기본 경로인 `/user`로 들어오는 GET 요청을 처리합니다.\n\n- `userRoot`: 함수 이름은 임의로 지정된 이름입니다. 이 함수가 `/user` 경로로 들어오는 요청을 처리하며, `HelloResponse` 객체를 반환합니다.\n\n- `return HelloResponse(result = \"Welcome to the user page!\")`: `\"Welcome to the user page!\"`라는 값을 담은 `HelloResponse` 객체를 반환하여, 이를 JSON 응답으로 반환합니다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n\n\n```kotlin\n    // /user/hello 경로를 처리\n    @GetMapping(\"/hello\")\n    fun hello(): HelloResponse {\n        return HelloResponse(result = \"hello world\")\n    }\n```\n\n- `@GetMapping(\"/hello\")`: `/user/hello` 경로로 들어오는 GET 요청을 처리합니다.\n\n- `hello`: 마찬가지로 함수 이름은 임의로 지정되었습니다. 이 함수가 `/user/hello` 경로로 들어오는 요청을 처리하며, `HelloResponse` 객체를 반환합니다.\n\n- `return HelloResponse(result = \"hello world!~~\")`: `\"hello world\"`라는 값을 담은 `HelloResponse` 객체를 반환하여, 클라이언트에게 JSON 응답으로 보냅니다.\n\n## API 테스트\n\n`/user` 경로로 접속하면 다음과 같은 화면을 볼 수 있습니다. \n\n![](image2.png)\n## Springdoc OpenAPI\n\nAPI 문서는 협업때도 그렇고, 테스트를 위해서도 중요한 작업입니다. Django를 할 때에는 Swagger를 사용해서 API를 구현하면 자동으로 문서화가 되는 기능을 사용했었는데, Spring에도 비슷한 게 있습니다. Springdoc OpenAPI 입니다. \n\n### 소개\n\nSpringdoc OpenAPI는 Swagger/OpenAPI 3.0 사양을 지원하는 라이브러리로, 스프링 부트 애플리케이션과 잘 통합됩니다.\n설정이 매우 간단하고, 최신 OpenAPI 3.0 사양을 사용할 수 있습니다.\nREST API 문서와 UI를 자동으로 생성해 주며, API 테스트도 가능합니다.\n\n### 설치\n\n다음의 의존성을 추가합니다. \n\n```kotlin\ndependencies {\n\t\timplementation(\"org.springdoc:springdoc-openapi-starter-webmvc-ui:2.0.2\")\n}\n```\n\n그리고 다음의 설정 클래스를 선언합니다. config 패키지를 생성해서 그 안에 넣어주었습니다. \n\n```kotlin\npackage world.nolog.package.config\n\nimport io.swagger.v3.oas.models.Components\nimport io.swagger.v3.oas.models.OpenAPI\nimport io.swagger.v3.oas.models.info.Info\nimport org.springframework.context.annotation.Bean\nimport org.springframework.context.annotation.Configuration\n\n\n@Configuration\nclass SwaggerConfig {\n    @Bean\n    fun openAPI(): OpenAPI {\n        return OpenAPI()\n            .components(Components())\n            .info(apiInfo())\n    }\n\n    private fun apiInfo(): Info {\n        return Info()\n            .title(\"Springdoc 테스트\")\n            .description(\"Springdoc을 사용한 Swagger UI 테스트\")\n            .version(\"1.0.0\")\n    }\n}\n```\n\n이후  [http://localhost:8080/swagger-ui/index.html](http://localhost:8080/swagger-ui/index.html) 경로에서 Swagger UI로 API를 확인할 수 있습니다.\n\n![](image3.png)\n### 작동 원리\n\nSpringdoc OpenAPI는 Spring Web MVC 또는 Spring WebFlux와 통합되어, 컨트롤러 클래스와 그 안의 메서드에 있는 어노테이션을 기준으로 API 엔드포인트를 수집합니다. 이를 통해, 정의된 API 명세서를 자동으로 생성하고 Swagger UI에 보여줄 수 있습니다.\n\n다음은 Springdoc OpenAPI가 API 엔드포인트를 수집하는 기준입니다. 기본적으로 모든 컨트롤러 클래스와 매핑된 모든 메서드가 자동으로 API 문서에 포함됩니다. 특정 API를 문서화에서 제외하고 싶다면, `@Hidden` 어노테이션을 사용하여 OpenAPI 명세에서 해당 엔드포인트를 숨길 수 있습니다.\n\n#### 스프링 컨트롤러 클래스 (`@RestController`, `@Controller`)\n\n- `@RestController` 또는 `@Controller` 어노테이션이 붙은 클래스에서 API 엔드포인트를 수집합니다.\n\n- 일반적으로 이 클래스에서 HTTP 요청 경로를 정의하는 메서드를 찾습니다.\n\n#### HTTP 요청 매핑 어노테이션\n\n- `@GetMapping`, `@PostMapping`, `@PutMapping`, `@DeleteMapping`, `@PatchMapping`, `@RequestMapping`과 같은 스프링의 HTTP 요청 매핑 어노테이션이 있는 메서드를 API로 수집합니다.\n\n- **이 어노테이션들에 의해 정의된 HTTP 메서드와 경로 정보가 OpenAPI 문서로 변환됩니다.**\n\n### 더 상세한 문서 만들기\n\n이렇게만 해도 정의된 REST API들을 수집하여 자동으로 문서화합니다. 그러나 원활한 협업을 위해서는 더 자세한 문서를 작성해야 하는 경우가 많습니다. 다음은 그 문서를 작성하는 방법입니다. Springdoc OpenAPI에서 제공하는 대표적인 어노테이션들은 다음과 같습니다. \n\n#### `@Operation` 어노테이션\n\nAPI 엔드포인트에 대한 핵심 정보를 문서화하는 데 사용됩니다. 각 엔드포인트에 대한 간단한 설명(summary)과 자세한 설명(description)을 추가할 수 있으며, 이 설명은 Swagger UI에 표시되어 API 사용자를 위한 안내로 제공됩니다.\n\n- `summary`: API 엔드포인트의 간략한 설명을 제공.\n\n- `description`: API의 상세 설명을 제공하여 어떤 용도로 사용되는지 안내.\n\n- `tags`: API를 그룹화할 수 있는 태그를 설정. 이 태그를 이용해 API 문서를 논리적으로 분리할 수 있습니다.\n\n#### `@ApiResponse` 어노테이션\n\nAPI의 응답 결과에 대해 상세히 설명하는 데 사용됩니다. 각 API가 어떤 응답 코드와 응답 메시지를 반환하는지 문서화할 수 있습니다. 이 정보는 성공 시 응답뿐만 아니라 오류 응답도 함께 정의하여, API 사용자가 오류 상황에 대한 정보를 명확히 알 수 있게 해줍니다.\n\n- `responseCode`: 응답 코드 (HTTP 상태 코드)를 명시.\n\n- `description`: 해당 응답 코드에 대한 설명을 제공.\n\n- `content`: 반환되는 데이터의 타입이나 스키마를 정의할 수 있습니다.\n\n#### `@Parameter` 어노테이션\n\nAPI 메서드의 파라미터를 문서화할 때 사용됩니다. 주로 URL 경로에 포함된 경로 변수나 쿼리 파라미터, 요청 본문에서 사용하는 데이터를 더 명확히 설명하는 데 유용합니다. 이를 통해 API 사용자는 각 파라미터가 어떤 역할을 하는지, 필수인지 여부를 쉽게 이해할 수 있습니다.\n\n- `name`: 파라미터의 이름을 명시.\n\n- `description`: 파라미터에 대한 설명을 제공.\n\n- `required`: 필수 여부를 정의.\n\n- `example`: 파라미터의 예시값을 설정하여 문서에 추가.\n\n#### `@Schema`\n\nDTO 클래스나 특정 필드의 구조를 문서화할 때 사용합니다. 이 어노테이션을 사용하면 각 필드에 대해 설명을 추가할 수 있으며, 데이터 타입이나 예시값을 명시할 수 있습니다.\n\n- `description`: 각 필드의 의미를 설명.\n\n- `example`: 필드의 예시값을 제공하여 API 사용자가 쉽게 이해할 수 있도록 합니다.\n\n#### `@``RequestBody`\n\n요청 본문에 포함되는 데이터를 설명하는 데 사용됩니다.\n\n- `description`: 각 필드의 의미를 설명.\n\n- `example`: 필드의 예시값을 제공하여 API 사용자가 쉽게 이해할 수 있도록 합니다.\n\n### 어노테이션을 모두 활용한 API 문서 예제 \n\n```kotlin\nimport io.swagger.v3.oas.annotations.Operation\nimport io.swagger.v3.oas.annotations.Parameter\nimport io.swagger.v3.oas.annotations.responses.ApiResponse\nimport io.swagger.v3.oas.annotations.responses.ApiResponses\nimport io.swagger.v3.oas.annotations.media.Schema\nimport org.springframework.http.ResponseEntity\nimport org.springframework.web.bind.annotation.GetMapping\nimport org.springframework.web.bind.annotation.PathVariable\nimport org.springframework.web.bind.annotation.RequestParam\nimport org.springframework.web.bind.annotation.RestController\n\n@RestController\nclass UserController {\n\n    @Operation(\n        summary = \"Get user by ID\",\n        description = \"Returns user details based on the user ID provided.\",\n        tags = [\"User\"]\n    )\n    @ApiResponses(\n        value = [\n            ApiResponse(responseCode = \"200\", description = \"Successfully retrieved user\"),\n            ApiResponse(responseCode = \"400\", description = \"Invalid user ID supplied\"),\n            ApiResponse(responseCode = \"404\", description = \"User not found\"),\n            ApiResponse(responseCode = \"500\", description = \"Internal server error\")\n        ]\n    )\n    @GetMapping(\"/user/{id}\")\n    fun getUserById(\n        @Parameter(\n            name = \"id\",\n            description = \"ID of the user to be retrieved\",\n            required = true,\n            example = \"1\"\n        )\n        @PathVariable id: Long,\n        \n        @Parameter(\n            name = \"includeAddress\",\n            description = \"Flag to include user's address in the response\",\n            required = false,\n            example = \"true\"\n        )\n        @RequestParam(required = false, defaultValue = \"false\") includeAddress: Boolean\n    ): ResponseEntity<User> {\n        // 비즈니스 로직: 사용자 정보 조회\n        return ResponseEntity.ok(\n            User(\n                id = id,\n                name = \"John Doe\",\n                email = \"john.doe@example.com\",\n                address = if (includeAddress) \"123 Main St, Springfield\" else null\n            )\n        )\n    }\n}\n\ndata class User(\n    @Schema(description = \"User's unique ID\", example = \"123\")\n    val id: Long,\n\n    @Schema(description = \"User's full name\", example = \"John Doe\")\n    val name: String,\n\n    @Schema(description = \"User's email address\", example = \"john.doe@example.com\")\n    val email: String,\n\n    @Schema(description = \"User's physical address\", example = \"123 Main St, Springfield\", nullable = true)\n    val address: String? = null\n)\n\n```\n\n\n\n"},{"excerpt":"N+1 문제란? N+1 문제는 주로 ORM(Object-Relational Mapping)과 관련된 성능 문제입니다. 이를 간단하게 설명하자면, N+1 문제는 하나의 쿼리를 실행한 후, 그 결과에 대해 추가적으로 N개의 쿼리를 실행하게 되어, 총 N+1번의 쿼리가 발생하는 상황을 말합니다. Spring에서는 JPA나 Hibernate를 사용할 때 이 문제…","fields":{"slug":"/N1-문제/"},"frontmatter":{"date":"September 05, 2024","title":"N+1 문제","tags":["DataBase","Spring","ORM"]},"rawMarkdownBody":"\n\n![](image1.png)\n## N+1 문제란? \n\nN+1 문제는 주로 ORM(Object-Relational Mapping)과 관련된 성능 문제입니다. 이를 간단하게 설명하자면, N+1 문제는 하나의 쿼리를 실행한 후, 그 결과에 대해 추가적으로 N개의 쿼리를 실행하게 되어, 총 N+1번의 쿼리가 발생하는 상황을 말합니다.\n\nSpring에서는 JPA나 Hibernate를 사용할 때 이 문제가 자주 발생합니다. 예를 들어, `1:N` 관계를 가진 두 개의 엔티티(예: `User`와 `Order`)가 있다고 가정해봅시다. `User` 리스트를 조회하면서 각 `User`의 `Order` 리스트를 조회해야 한다면, 처음에 `User` 엔티티를 조회하는 쿼리가 1번 발생하고, 각 `User`마다 `Order` 리스트를 조회하는 쿼리가 N번 발생하여 총 N+1번의 쿼리가 실행됩니다.\n\nORM을 사용하는 개발자가 쿼리 최적화에 대한 이해가 부족하거나, 기본 설정 그대로 사용할 경우 발생하는 문제라고 볼 수 있습니다. ORM은 기본적으로 객체 지향적인 방식으로 데이터베이스를 다루기 때문에, 개발자는 비즈니스 로직에 집중할 수 있습니다. 그러나 데이터베이스와의 상호작용 방식을 깊이 이해하지 못하면, 의도치 않게 비효율적인 쿼리를 작성하게 될 수 있는 거죠. N+1 문제도 이런 맥락에서 발생하는 대표적인 문제입니다.\n\n## 원인\n\n이 문제가 발생하는 주된 원인은 JPA에서 연관된 엔티티를 지연 로딩(Lazy Loading) 방식으로 가져올 때, 각 엔티티마다 별도의 쿼리를 실행하기 때문입니다. 이러한 방식은 데이터가 많아질수록 성능에 악영향을 끼칠 수 있습니다.\n\n지연로딩은 연관된 엔티티를 실제로 필요할 때까지 로딩하지 않고, 그 시점에 쿼리를 실행하여 데이터를 불러오는 전략입니다. 이는 성능 최적화를 위해 설계된 방식이지만, 잘못 사용하면 오히려 성능 문제를 일으킬 수 있습니다.\n\n```kotlin\n@Entity\npublic class User {\n    @OneToMany(mappedBy = \"user\", fetch = FetchType.LAZY)\n    private List<Order> orders;\n}\n```\n\n위와 같이 설정하면, `User` 엔티티를 조회할 때는 `Order` 리스트는 로딩되지 않습니다. `orders` 필드에 접근하는 시점에 Hibernate가 `Order` 데이터를 가져오기 위한 쿼리를 실행합니다. 즉, 필요한 순간에만 쿼리를 실행하는 방식입니다.\n\n하지만 이 방식으로 여러 `User` 엔티티를 조회하고 각각의 `Order`에 접근하게 되면, 각 `User`에 대해 `Order`를 가져오는 쿼리가 N번 실행되어 N+1 문제가 발생할 수 있습니다. \n\n## 지연로딩(Lazy Loading)이란? \n\n지연로딩은 데이터베이스에서 실제로 데이터를 가져오는 시점을 지연시켜, 필요할 때만 데이터를 로드하는 방식입니다. 이 과정은 성능을 최적화하려는 목적에서 사용되지만, 상황에 따라 성능 저하를 유발할 수도 있습니다. 이제 지연로딩이 어떻게 작동하는지 좀 더 깊이 살펴보겠습니다.\n\n### 프록시 객체(Proxy Object)\n\n지연 로딩의 핵심은 프록시 객체입니다. Hibernate나 JPA와 같은 ORM 프레임워크는 데이터베이스에서 실제로 데이터를 가져오는 대신, 먼저 프록시 객체를 반환합니다. 이 프록시 객체는 실제 엔티티 클래스의 서브클래스 또는 대리 객체로, 데이터베이스에 접근하는 기능이 추가된 형태입니다.\n\n프록시 객체는 해당 엔티티에 대한 모든 정보를 가지고 있지는 않지만, 메모리에 로드된 엔티티와 동일한 메서드와 필드를 갖고 있습니다. 필요할 때만 데이터베이스에서 실제 데이터를 가져오는 역할을 합니다.\n\n### 필드 접근 시점에 쿼리 실행\n\n지연 로딩의 핵심은 필드에 실제로 접근하는 시점에 데이터베이스 쿼리를 실행한다는 점입니다. 즉, 엔티티를 처음 로드할 때는 연관된 데이터는 로드되지 않고, 연관된 데이터에 접근하려고 할 때 비로소 데이터베이스에 쿼리를 실행하여 필요한 데이터를 가져옵니다.\n\n예를 들어, `User` 엔티티와 `Order` 엔티티가 `1:N` 관계로 연결되어 있고, `Order` 필드가 지연 로딩으로 설정된 경우를 생각해봅시다.\n\n```kotlin\n@Entity\npublic class User {\n    @OneToMany(fetch = FetchType.LAZY)\n    private List<Order> orders;\n}\n```\n\n이 경우, `User` 객체를 조회할 때 `orders`는 바로 로드되지 않습니다. 대신 프록시 객체가 생성되고, 이 `orders` 필드에 접근하는 순간 `SELECT` 쿼리가 실행되어 데이터베이스에서 `Order` 리스트를 가져옵니다.\n\n```kotlin\nUser user = userRepository.findById(1L); // 여기서는 orders에 대한 쿼리 없음\nList<Order> orders = user.getOrders();   // 이 시점에서 쿼리 실행\n```\n\n여기서 핵심은 **데이터에 접근하는 순간**에 쿼리가 실행된다는 점입니다.\n\n#### Hibernate와 프록시 객체\n\nHibernate에서 프록시 객체는 데이터베이스에서 실제로 데이터를 로드할 때 메서드 호출을 가로챕니다. 예를 들어, `getOrders()` 메서드를 호출하면, Hibernate는 이 메서드 호출을 감지하고 해당 엔티티의 프록시가 아닌 실제 데이터를 로드하도록 동작합니다. 이를 통해 개발자는 메서드 호출 시점에 자연스럽게 데이터베이스와 상호작용하는 것처럼 느낍니다.\n\n### 장점\n\n- 불필요한 데이터 로드를 피함: 필요한 데이터만 로드하여 메모리 사용량을 줄일 수 있습니다. 특히, 연관된 데이터가 많을 때 불필요한 로드를 피할 수 있어 성능 향상에 도움이 됩니다.\n\n- 성능 최적화: 모든 데이터를 한 번에 가져오는 대신, 실제로 필요한 시점에만 데이터를 가져오므로 초기 로드 시간이 줄어들 수 있습니다. 예를 들어, 대규모 연관 데이터를 갖는 엔티티를 매번 로드할 필요가 없을 때 유용합니다.\n\n### 단점\n\n- N+1 문제: 앞서 설명한 것처럼, 지연 로딩을 잘못 사용하면 하나의 엔티티를 가져오는 쿼리 외에 추가적으로 여러 번의 쿼리가 실행되어 성능 저하를 초래할 수 있습니다. `1:N` 관계에서 `N`개의 엔티티가 각각 추가적인 쿼리를 실행하게 되어 N+1 문제가 발생할 수 있습니다.\n\n- 영속성 컨텍스트(Persistence Context)와의 연관성: 지연 로딩은 엔티티가 영속성 컨텍스트 안에 있을 때만 동작합니다. 만약 영속성 컨텍스트 밖에서(예: 트랜잭션이 종료된 후) 프록시 객체에 접근하면 `LazyInitializationException`이 발생할 수 있습니다. 이는 데이터를 로드할 수 있는 컨텍스트가 이미 종료되었기 때문입니다.\n\n- 복잡한 쿼리와 성능 저하: 많은 연관 엔티티가 지연 로딩으로 설정되어 있을 때, 각 엔티티에 접근할 때마다 별도의 쿼리가 실행되므로 성능 저하를 초래할 수 있습니다. 이를 피하기 위해서는 상황에 맞게 `Fetch Join` 등을 사용해 즉시 로딩으로 전환하거나, 필요한 데이터를 한 번에 가져오는 전략이 필요합니다.\n\n## 즉시 로딩(Eager Loading)\n\n그렇다면 지연 로딩을 사용하지 않으면 모든 문제가 해결될까요? \n\n지연 로딩은 필요할 때만 데이터를 로드하는 반면, 즉시 로딩(Eager Loading)은 엔티티가 처음 로드될 때 관련된 모든 데이터를 즉시 로드합니다.\n\n```kotlin\n@Entity\npublic class User {\n    @OneToMany(fetch = FetchType.EAGER)\n    private List<Order> orders;\n}\n```\n\n위와 같이 `FetchType.EAGER`로 설정된 경우, `User`를 조회할 때 `Order` 리스트도 함께 즉시 로딩됩니다. 이 방식은 N+1 문제를 피할 수 있지만, 불필요한 데이터까지 모두 로드하게 되어 메모리 사용량이 늘어날 수 있습니다.\n\n## 즉시로딩 vs 지연로딩\n\n로우 쿼리 레벨에서 이야기 한다면 즉시 로딩은 불필요하게 User 테이블을 조회할 떄마다 Join 해서 Order를 항상 다 가져오는거고, 지연 로딩은 User만 가지고 왔다가 Order가 필요할 때 뒤늦게 N개의 User에 대해 for 문을 돌면서 User별 Order를 가져오는 쿼리를 N번 실행해서 생기는 문제라고 이해할 수 있습니다. \n\n### 즉시로딩\n\n`User` 테이블을 조회할 때마다 항상 `JOIN`을 사용해서 연관된 `Order` 테이블의 데이터를 함께 가져옵니다. 이 방식은 한 번의 쿼리로 필요한 데이터를 모두 가져올 수 있지만, `Order` 데이터를 필요로 하지 않는 경우에도 항상 `JOIN`이 발생해 불필요한 데이터까지 로드하게 되어 메모리나 성능상 오버헤드가 생길 수 있습니다.\n\n이를 쿼리문으로 표시하면 다음과 같습니다. \n\n```sql\nSELECT u.*, o.*\nFROM User u\nLEFT JOIN Order o ON u.id = o.user_id\n```\n\n### 지연로딩\n\n처음에는 `User` 테이블만 조회하고, 연관된 `Order`는 실제로 필요할 때 그때서야 개별적으로 쿼리를 실행하여 데이터를 가져옵니다. 이로 인해 `User`를 조회한 후 `Order`가 필요해지면, 각 `User`별로 `Order`를 조회하는 추가적인 쿼리가 발생하게 됩니다. 만약 `N`명의 `User`가 있다면, `N`번의 추가 쿼리가 실행되며, 이로 인해 N+1 문제가 발생합니다.\n\n이를 쿼리문으로 표시하면 다음과 같습니다. \n\n```sql\n-- 처음에 User만 조회\nSELECT * FROM User;\n\n-- 나중에 각 User마다 Order를 조회 (N번 실행)\nSELECT * FROM Order WHERE user_id = ?;\n```\n\n\n\n[N+1 문제의 해결방법](https://sharknia.github.io/N1-문제의-해결방법)으로 이어집니다. \n\n"},{"excerpt":"서론 지난 시간의 N+1 문제 소개에 이어 이번 시간에는 N+1 문제를 해결하기 위해서는 어떤 방법들이 있는지 알아보려고 합니다.  레포지토리에 User와 Order를 조인해서 가져오는 메소드와 User만 가져오는 메소드를 따로 만들기 이렇게 하면, 상황에 맞게 필요한 데이터만 가져올 수 있고, 불필요한 쿼리나 데이터 로드를 방지할 수 있습니다. 예시 레…","fields":{"slug":"/N1-문제의-해결방법/"},"frontmatter":{"date":"September 05, 2024","title":"N+1 문제의 해결방법","tags":["DataBase","Spring","ORM"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n지난 시간의 [N+1 문제 소개](https://sharknia.github.io/N1-문제)에 이어 이번 시간에는 N+1 문제를 해결하기 위해서는 어떤 방법들이 있는지 알아보려고 합니다. \n\n## **레포지토리에 User와 Order를 조인해서 가져오는 메소드**와 **User만 가져오는 메소드**를 따로 만들기\n\n이렇게 하면, 상황에 맞게 필요한 데이터만 가져올 수 있고, 불필요한 쿼리나 데이터 로드를 방지할 수 있습니다.\n\n### 예시\n\n레포지토리에 각각의 메소드를 따로 작성할 수 있습니다.\n\n#### `User`만 조회하는 메소드\n\n이 메소드는 `User`만 가져오고, `Order`는 지연 로딩을 통해 나중에 필요할 때 가져오도록 설정할 수 있습니다.\n\n```kotlin\n@Repository\npublic interface UserRepository extends JpaRepository<User, Long> {\n    // User만 조회하는 메소드\n    @Query(\"SELECT u FROM User u\")\n    List<User> findAllUsers();\n}\n```\n\n#### `User`와 `Order`를 함께 조회하는 메소드\n\n이 메소드는 `JOIN FETCH`를 사용하여 `User`와 연관된 `Order`를 한 번에 가져옵니다.\n\n```kotlin\n@Repository\npublic interface UserRepository extends JpaRepository<User, Long> {\n    // User와 Order를 조인해서 조회하는 메소드\n    @Query(\"SELECT u FROM User u JOIN FETCH u.orders\")\n    List<User> findAllUsersWithOrders();\n}\n```\n\n### 장점\n\n#### 필요한 데이터만 로드\n\n- `User`만 필요한 경우 불필요한 `Order` 데이터를 가져오지 않기 때문에 쿼리가 간결하고 성능이 최적화됩니다.\n\n- 반대로, `User`와 `Order`가 모두 필요한 경우에는 조인을 통해 데이터를 한 번에 가져와서 N+1 문제를 방지할 수 있습니다.\n\n#### 쿼리 제어 가능\n\n- 각 메소드에서 쿼리의 동작 방식을 명시적으로 설정할 수 있으므로, ORM의 기본 동작(지연 로딩, 즉시 로딩 등)에 의존하지 않고 최적화된 방식으로 데이터 조회가 가능합니다.\n\n### 단점\n\n#### **메소드의 복잡성 증가**\n\n- 레포지토리에 여러 종류의 메소드를 추가하다 보면 코드가 복잡해질 수 있습니다. 특히, 다양한 쿼리 요구사항을 처리해야 하는 경우 메소드 수가 급격히 늘어나 관리가 어려워질 수 있습니다.\n\n- 매번 어떤 메소드를 사용할지 개발자가 직접 선택해야 하므로 실수로 적절하지 않은 메소드를 사용할 가능성도 있습니다.\n\n#### 중복 코드 발생 가능성\n\n- `User`만 가져오는 메소드와 `Order`까지 조인하는 메소드처럼 비슷한 기능을 가진 메소드들이 생기면서 중복 코드가 발생할 수 있습니다. 중복된 코드를 유지보수해야 하므로 코드가 장기적으로 복잡해질 수 있습니다.\n\n#### 확장성의 한계\n\n- 만약 엔티티의 관계가 많아지거나 복잡해지면, 조인해야 할 엔티티가 많아지므로 각 관계마다 최적화된 메소드를 만들어야 하는데, 이 과정에서 코드 관리가 어려워질 수 있습니다.\n\n- 예를 들어 `User`와 `Order`뿐만 아니라 `Payment`나 `Delivery` 같은 연관된 엔티티가 추가된다면 그때마다 새로운 메소드를 만들어야 할 수 있습니다.\n\n## Fetch Join\n\n### 예시\n\nFetch Join을 사용하여 연관된 엔티티를 한 번에 가져오는 방법입니다. JPA에서 `JOIN FETCH`를 사용하면 기본적으로 Lazy Loading으로 설정된 연관 엔티티를 즉시 로딩으로 전환하여, 필요한 데이터를 모두 한 번에 가져옵니다.\n\n```kotlin\n@Repository\npublic interface UserRepository extends JpaRepository<User, Long> {\n    @Query(\"SELECT u FROM User u JOIN FETCH u.orders\")\n    List<User> findAllUsersWithOrders();\n}\n```\n\n### 장점\n\n- 한 번의 쿼리로 관련 데이터를 모두 조회: 여러 테이블을 조인해 한 번의 쿼리로 데이터를 조회하므로 N+1 문제를 해결할 수 있습니다.\n\n- 성능 최적화: 불필요한 추가 쿼리가 발생하지 않기 때문에 성능이 향상됩니다.\n\n### 단점\n\n- 데이터 중복: 조인된 테이블의 데이터가 중복되어 반환될 수 있습니다. 예를 들어, `User` 1명에 `Order` 10개가 있으면, `User`가 10번 중복되어 조회될 수 있습니다.\n\n- 페이징 불가: JPA에서는 `Fetch Join`을 사용할 때 페이징 기능을 사용할 수 없습니다. 대량의 데이터에서 페이징이 필요하면 다른 해결책을 찾아야 합니다.\n\n## EntityGraph\n\n### 예시\n\n`@EntityGraph`는 특정 엔티티의 연관된 엔티티들을 즉시 로딩으로 설정할 수 있는 기능입니다. 이 방법은 Fetch Join보다 더 명시적으로 필요한 엔티티들만 선택적으로 로드할 수 있습니다.\n\n```kotlin\n@Repository\npublic interface UserRepository extends JpaRepository<User, Long> {\n    @EntityGraph(attributePaths = {\"orders\"})\n    List<User> findAllWithOrders();\n}\n```\n\n### 장점\n\n- 선택적 즉시 로딩: 특정 연관 엔티티만 선택적으로 즉시 로딩할 수 있어 성능 최적화에 유리합니다.\n\n- 더 간결한 쿼리: Fetch Join보다 코드가 간결하며 쿼리 가독성이 좋습니다.\n\n### 단점\n\n- 유연성 부족: 복잡한 조인 관계를 설정할 때는 Fetch Join보다 유연성이 떨어질 수 있습니다. 특히 다중 테이블 조인이나 특정 조건의 조인에는 적합하지 않을 수 있습니다.\n\n- 쿼리 튜닝의 한계: 데이터베이스 레벨에서 세밀한 쿼리 튜닝을 하기는 어렵습니다.\n\n## Batch Size\n\n### 예시\n\n`@BatchSize`는 연관 엔티티를 Lazy Loading으로 가져올 때, 한 번에 가져올 데이터의 수를 조정하여 성능을 최적화하는 방법입니다.\n\n```kotlin\n@Entity\npublic class User {\n    @OneToMany(fetch = FetchType.LAZY)\n    @BatchSize(size = 10)\n    private List<Order> orders;\n}\n```\n\n### 장점\n\n- Lazy Loading을 유지하면서 성능 최적화: 한 번에 여러 개의 엔티티를 가져오므로, Lazy Loading의 장점을 살리면서 N+1 문제를 줄일 수 있습니다.\n\n- 메모리 효율: 필요한 만큼의 데이터를 가져오므로 메모리 사용을 최적화할 수 있습니다.\n\n### 단점\n\n- 설정 복잡성: 각 연관 관계마다 적절한 배치 크기를 설정해야 하므로, 모든 상황에 대해 적절한 값을 찾기가 어려울 수 있습니다.\n\n- 대규모 데이터에서 한계: 데이터가 대규모일 경우에도 배치로 묶어 가져오기 때문에 여전히 성능 문제나 메모리 부담이 발생할 수 있습니다.\n\n## DTO Projection 사용\n\n### 예시\n\nDTO(데이터 전송 객체)를 사용해 필요한 데이터만 쿼리해서 가져오는 방식입니다. 엔티티 전체를 로딩하는 대신, 특정 필드들만 쿼리해서 가져오기 때문에 성능 최적화가 가능합니다.\n\n```kotlin\npublic class UserOrderDTO {\n    private String userName;\n    private LocalDateTime orderDate;\n\n    public UserOrderDTO(String userName, LocalDateTime orderDate) {\n        this.userName = userName;\n        this.orderDate = orderDate;\n    }\n}\n\n@Repository\npublic interface UserRepository extends JpaRepository<User, Long> {\n    @Query(\"SELECT new com.example.UserOrderDTO(u.name, o.orderDate) FROM User u JOIN u.orders o\")\n    List<UserOrderDTO> findUserOrderData();\n}\n```\n\n### 장점\n\n- 필요한 데이터만 조회: 엔티티 전체를 가져오지 않고, 필요한 필드만 가져와 성능을 최적화할 수 있습니다.\n\n- 성능 최적화: 연관 엔티티가 많아도 DTO로 필요한 정보만 가져오므로 데이터 로딩 비용을 줄일 수 있습니다.\n\n### 단점\n\n- 유연성 부족: DTO는 엔티티와 다르게 데이터베이스의 영속성 컨텍스트에서 관리되지 않으므로, 비즈니스 로직에서 엔티티처럼 활용하기 어려울 수 있습니다.\n\n- 복잡한 쿼리 작성 필요: 다양한 필드를 조합해야 할 경우, 복잡한 쿼리와 DTO 설계가 필요해질 수 있습니다.\n\n## Secondary Query Cache\n\n### 예시\n\nORM에서 제공하는 2차 캐시 기능을 활용하여, 쿼리 결과를 캐싱하고 필요할 때 캐시된 데이터를 재사용하는 방식입니다. Hibernate에서는 설정을 통해 2차 캐시를 사용할 수 있습니다.\n\n```bash\nhibernate.cache.use_second_level_cache=true\nhibernate.cache.region.factory_class=org.hibernate.cache.jcache.JCacheRegionFactory\n```\n\n### 장점\n\n- 데이터 재사용: 데이터베이스를 매번 조회하지 않고 캐시에 저장된 데이터를 재사용할 수 있으므로 성능 향상에 도움이 됩니다.\n\n- N+1 문제 완화: 동일한 쿼리가 반복적으로 실행될 때 캐시된 데이터를 활용하여 성능 저하를 방지할 수 있습니다.\n\n### 단점\n\n- 데이터 일관성 문제: 캐시된 데이터가 실시간으로 변경되지 않기 때문에 데이터 일관성 문제가 발생할 수 있습니다. 자주 변경되는 데이터를 캐싱하는 데는 부적합할 수 있습니다.\n\n- 캐시 관리의 복잡성: 캐시 만료 시간, 데이터 동기화 등의 설정을 적절히 관리해야 하며, 복잡한 애플리케이션에서는 캐시 관리가 어려워질 수 있습니다.\n\n## 요약\n\n로우쿼리로 직접 작업할 때에도 스키마가 복잡해지면 결국 쿼리문이 복잡해져서 작업이 힘들어지는데 ORM도 결국 같은 문제를 갖고 있다고 생각됩니다. 각 방법은 상황에 맞게 적용해야 하며, 각각의 장점과 단점이 있으므로 성능과 코드 복잡성을 모두 고려한 최적화 전략을 선택하는 것이 중요합니다.\n\n- Fetch Join: 한 번의 쿼리로 모든 데이터를 가져올 수 있지만, 페이징이 어렵고 중복 데이터가 발생할 수 있음.\n\n- EntityGraph: 더 세밀한 제어가 가능하지만, 유연성이 부족하고 쿼리 튜닝에 한계가 있음.\n\n- Batch Size: Lazy Loading을 유지하면서 최적화를 시도할 수 있지만, 설정의 복잡성과 대규모 데이터 처리에 한계가 있음.\n\n- DTO Projection: 필요한 데이터만 조회할 수 있어 성능이 좋지만, DTO 설계가 복잡하고 영속성 관리가 어려움.\n\n- Secondary Cache: 쿼리 성능을 크게 향상시키지만, 캐시 관리의 복잡성과 데이터 일관성 문제가 발생할 수 있음.\n\n"},{"excerpt":"서론 JVM(Java Virtual Machine) 버전을 관리할 수 있는 도구로는 SDKMAN!이라는 것이 있습니다. SDKMAN!은 다양한 JVM 버전뿐만 아니라 여러 Java 관련 도구(예: Maven, Gradle, Scala 등)도 쉽게 설치하고 관리할 수 있게 도와줍니다. 설치 터미널에서 다음 명령어를 실행하여 SDKMAN!을 설치합니다. 설치…","fields":{"slug":"/SDKMAN과-bootRun/"},"frontmatter":{"date":"September 04, 2024","title":"SDKMAN!과 bootRun","tags":["Kotlin","Spring"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nJVM(Java Virtual Machine) 버전을 관리할 수 있는 도구로는 **SDKMAN!**이라는 것이 있습니다. SDKMAN!은 다양한 JVM 버전뿐만 아니라 여러 Java 관련 도구(예: Maven, Gradle, Scala 등)도 쉽게 설치하고 관리할 수 있게 도와줍니다.\n\n## 설치\n\n터미널에서 다음 명령어를 실행하여 SDKMAN!을 설치합니다.\n\n```bash\ncurl -s \"https://get.sdkman.io\" | bash\n```\n\n설치가 완료되면, 새로운 쉘을 열거나 다음 명령어를 실행하여 SDKMAN!을 활성화합니다.\n\n```bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\n```\n\nJVM 버전을 설치하고 관리할 수 있습니다. 예를 들어, 특정 Java 버전을 설치하려면 다음과 같이 입력합니다.\n\n```bash\nsdk install java 11.0.11.hs-adpt\n```\n\n설치된 JVM 버전을 전환할 때는 다음 명령어를 사용합니다.\n\n```bash\nsdk use java 11.0.11.hs-adpt\n```\n\n## bootRun\n\n### booRun?\n\n`bootRun`은 Spring Boot 애플리케이션을 Gradle을 통해 직접 실행할 수 있도록 하는 Gradle task입니다. 이는 Gradle 빌드 도구를 사용하는 Spring Boot 프로젝트에서, 애플리케이션을 컴파일하고 즉시 실행할 수 있도록 해주는 편리한 명령어입니다.\n\n### `bootRun`의 역할\n\n1. 코드를 컴파일: `bootRun` 명령어는 프로젝트의 코드를 컴파일한 후, 애플리케이션을 실행합니다.\n\n1. 애플리케이션 실행: Spring Boot 애플리케이션을 직접 실행하여 `localhost:8080` 등에서 결과를 확인할 수 있습니다.\n\n1. 개발 단계에서 주로 사용: `bootRun`은 애플리케이션을 개발할 때 빠르게 실행하고 테스트하기에 적합한 도구입니다. `bootJar`와 같이 패키징하는 과정 없이도 바로 실행이 가능하기 때문에, 코드를 수정하고 결과를 빠르게 확인할 수 있습니다.\n\n### `bootRun` 명령어 실행 방법\n\n![](image2.png)\nIntellij에서 Gradle 메뉴를 선택 후 `bootRun`을 더블클릭 합니다. \n\n### 주의 사항\n\n- `bootRun`은 개발 환경에서 주로 사용되며, 배포용 빌드에는 적합하지 않습니다. 실제로 애플리케이션을 배포할 때는 `bootJar` 같은 태스크로 패키징된 JAR 파일을 만들어서 배포하는 것이 좋습니다.\n\n- 환경 변수 및 프로파일 설정을 통해 개발과 배포 환경을 구분해서 사용할 수 있습니다.\n\n### 요약\n\n- `bootRun`은 Spring Boot 애플리케이션을 즉시 실행하는 Gradle의 편리한 태스크입니다.\n\n- 개발 중에 빠르게 애플리케이션을 실행하고 테스트할 수 있습니다.\n\n### 결과\n\nbootRun까지 마치면 [http://localhost:8080](http://localhost:8080/)에서 서버를 테스트 해볼 수 있습니다.\n\n## 코드 수정 후 자동 컴파일-자동 재실행 설정하기\n\n코드를 수정할 때마다 bootRun을 할 생각을 하니 끔찍합니다. 이를 피하기 위해 코드 변경 시 서버가 자동으로 재시작되도록 설정하는 것이 훨씬 편리하게 개발할 수 있는 방법입니다.\n\n### IntelliJ에서 자동 빌드 설정\n\n설정에서 다음을 찾아 `프로젝트 자동 빌드` 를 체크합니다.\n\n![](image3.png)\n이번안 다음을 찾아 `개발된 애플리케이션이 현재 실행 중인 경우에도 auto-make가 시작되도록 허용` 을 체크합니다. \n\n![](image4.png)\n### 자동 서버 재실행: spring boot devtools\n\n### Spring Boot Devtools의 주요 기능\n\n#### **자동 재시작 (Automatic Restart)**\n\n애플리케이션의 클래스나 리소스 파일이 변경되면, Devtools가 이를 감지하고 애플리케이션을 자동으로 재시작합니다.\n\nGradle 또는 Maven 프로젝트에 의존성을 추가하면 활성화되며, Spring Boot 프로젝트를 재실행하지 않아도 변경된 코드가 반영됩니다.\n\n#### **LiveReload 통합**\n\nLiveReload는 브라우저를 자동으로 새로 고침하는 기능을 제공하며, 코드를 수정하면 브라우저에서 변경 사항이 즉시 반영됩니다.\n\n이를 통해 프론트엔드 개발 시에도 수정한 내용을 즉시 확인할 수 있습니다.\n\n#### **캐싱 비활성화**\n\nDevtools는 개발 중 데이터 캐싱을 비활성화하여, 변경된 설정이나 데이터를 즉시 반영할 수 있도록 돕습니다. 프로덕션 환경에서는 캐싱을 사용하는 것이 일반적이지만, 개발 중에는 캐싱으로 인해 변경 사항이 제대로 반영되지 않는 상황을 방지합니다.\n\n#### **프로퍼티 파일 자동 로딩**\n\n`application.properties` 또는 `application.yml` 파일을 수정하면, Devtools가 이를 감지하고 다시 로딩해 변경된 설정을 반영합니다.\n\n### 설치\n\n자동으로 서버를 재실행 하기 위해서는 spring boot devtools가 필요합니다. 다음의 의존성을 추가하고 Gradle을 새로고침합니다. \n\n```bash\ndevelopmentOnly(\"org.springframework.boot:spring-boot-devtools\")\n```\n\n`developmentOnly` 를 사용해 **개발 환경에서만 사용**하도록 설계되었습니다. 배포 시 Devtools가 포함되지 않으며, 이를 위한 별도의 설정 없이도 프로덕션 빌드에서 Devtools가 자동으로 제외됩니다.\n\n### 환경 설정\n\n`application.properties` 에 다음의 설정을 추가해줬습니다. 이로써 이 프로젝트가 어느 환경인지를 명시할 수 있습니다. \n\n```kotlin\n${enviroment:dev}\n```\n\n### 결과\n\n여기까지 마치면 클래스 파일을 수정을 하면 브라우저를 통해서 테스트를 해보면 변경사항이 별도의 작업 없이 반영되는 것을 확인할 수 있습니다. 다만 파이썬이나 Node.js처럼 즉각적으로 반영은 되지 않고 약간의 시간 소요가 필요합니다. \n\n## 윈도우에선: jabba\n\n윈도우에서도 SDKMAN!을 사용할 수 있는지 찾아보면 WSL을 통해 설치할 수 있다는 글이 꽤 나옵니다. 물론 이렇게 해서 설치를 할 수는 있지만, WSL은 윈도우 환경과 분리되어 있어 WSL에 설치된 프로그램은 기본적으로 WSL 내부에서만 동작합니다. \n\n단적으로 이야기해서 인텔리제이에서는 이렇게 설치된 SDKMAN!을 통한 JDK는 사용할 수 없습니다. 따라서 윈도우에서 JDK 버전 관리를 위해서는 다른 프로그램을 사용해야 합니다. 바로 Jabba입니다. \n\n### Jabba 설치\n\nPowerShell을 관리자 권한으로 실행해 다음의 명령어를 실행합니다. \n\n```powershell\niwr -useb https://raw.githubusercontent.com/shyiko/jabba/master/install.ps1 | iex\n```\n\n### 설치 오류\n\n관리자 권한으로 실행을 했는데도 다음과 같은 오류가 난다면\n\n```powershell\n. : 이 시스템에서 스크립트를 실행할 수 없으므로 C:\\Users\\ID\\.jabba\\jabba.ps1 파일을 로드할 수 없습니다. 자세한\n내용은 about_Execution_Policies(https://go.microsoft.com/fwlink/?LinkID=135170)를 참조하십시오.\n위치 줄:80 문자:3\n+ . $jabbaHome\\jabba.ps1\n+   ~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : 보안 오류: (:) [], PSSecurityException\n    + FullyQualifiedErrorId : UnauthorizedAccess\n```\n\nPowerShell의 실행 정책이 기본적으로 제한되어 있어 발생합니다. 이를 허용하려면 실행 정책을 변경하면 됩니다. PowerShell을 관리자 권한으로 실행해 다음의 명령어를 입력합니다.\n\n```powershell\nSet-ExecutionPolicy RemoteSigned -Scope CurrentUser\n```\n\n이 명령어는 로컬에서 생성된 스크립트는 실행을 허용하고, 원격에서 다운로드된 스크립트는 서명이 있을 때만 실행하도록 합니다.\n\n다시 시도를 하면 설치가 정상적으로 된 것을 확인할 수 있습니다. \n\n### JDK 설치\n\n다음의 명령어를 사용해 설치 가능한 리스트를 확인할 수 있습니다. \n\n```powershell\njabba ls-remote\n```\n\n원하는 버전을 선택해 다음의 명령어로 설치합니다. \n\n```powershell\njabba install {선택한버전}\n```\n\n아래의 명령어로 설치된 JDK 리스트를 확인하고\n\n```powershell\njabba ls\n```\n\n다음의 명령어로 사용할 자바를 선택할 수 있습니다. \n\n```powershell\njabba use {선택한버전}\n```\n\n## 오류\n\n### 잘못된 Gradle JDK 구성을 발견했습니다.\n\n**IntelliJ IDEA**에서 Gradle이 사용하고 있는 JDK 버전이 프로젝트에 필요한 JDK 버전과 일치하지 않기 때문에 발생할 수 있습니다. 이를 해결하려면 IntelliJ에서 Gradle이 사용하는 JDK를 올바르게 설정해 주어야 합니다.\n\nIntelliJ IDEA에서 \"잘못된 Gradle JDK 구성을 발견했습니다\" 메시지 옆에 있는 \"Gradle 설정 열기\" 버튼을 클릭하거나, 상단 메뉴에서 File > Settings (macOS의 경우 IntelliJ IDEA > Preferences)로 이동합니다.\n\n왼쪽 메뉴에서 Build, Execution, Deployment > Build Tools > Gradle을 선택합니다.\n\n![](image5.png)\n현재 사용하는 버전의 JVM이 올바르게 선택되어 있는지 확인합니다. 만약 필요한 버전이 목록에 없으면, 해당 버전을 설치 또는 추가 해줘야합니다.\n\n오류를 해결 한 후 \n\n![](image6.png)\nRefresh를 실행합니다. \n\n"},{"excerpt":"서론 한국에서 자바와 스프링은 대세중의 대세입니다.  구인풀도 넓고, 자료도 많고, 검증된 안정성으로 다른 언어로 시작한 회사들도 스프링 기반으로 백엔드 서버를 바꾸는 일도 많습니다.  처음에 자바는 뭔가 C랑 다르게 생기고 그래서 거부감도 많았었는데, 최근 이것저것 다른 언어나 프레임워크를 써보다 보니 상대적으로 낯을 덜 가리게 되어서 한 번 그냥 써볼…","fields":{"slug":"/KotlinSpring-프로젝트-설치-및-실행/"},"frontmatter":{"date":"September 03, 2024","title":"Kotlin/Spring 프로젝트 설치 및 실행","tags":["Kotlin","Spring"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n한국에서 자바와 스프링은 대세중의 대세입니다. \n\n구인풀도 넓고, 자료도 많고, 검증된 안정성으로 다른 언어로 시작한 회사들도 스프링 기반으로 백엔드 서버를 바꾸는 일도 많습니다. \n\n처음에 자바는 뭔가 C랑 다르게 생기고 그래서 거부감도 많았었는데, 최근 이것저것 다른 언어나 프레임워크를 써보다 보니 상대적으로 낯을 덜 가리게 되어서 한 번 그냥 써볼까 합니다. \n\n또 코틀린이 자바보다 현대적이고 간결하다 하니, 굳이 묘한 거부감에서 여태까지 부리던 고집을 유지할 필요가 없을 것 같습니다. \n\n그래서 이번 프로젝트는 정말 끝내는걸 목표로! 코틀린-스프링으로 진행해보려고 합니다. \n\n## 프로젝트 초기화\n\n스프링 부트(Sprint Boot)는 빠르게 프로젝트를 설정할 수 있는 다양한 방법을 제공합니다. 가장 쉬운 방법은 Spring Initializr를 사용하는 것입니다. 다음은 Spring Initializr를 사용하여 코틀린 프로젝트를 설정하는 방법입니다. \n\n### Spring Initializr\n\n![](image2.png)\n[Spring Initializr](https://start.spring.io/)는 스프링 부트 프로젝트를 생성할 수 있는 웹 도구입니다. 여기서 필요한 의존성과 설정을 선택하면, 프로젝트가 자동으로 생성됩니다.\n\n### 각 항목에 대한 설명\n\n#### Project\n\n- Gradle - Groovy: Gradle는 빌드 도구로, Groovy는 그 빌드 스크립트를 작성하는 언어입니다. Groovy 기반의 Gradle은 전통적인 방식으로 널리 사용됩니다.\n\n- Gradle - Kotlin: Gradle을 사용하는데, 빌드 스크립트를 Kotlin 언어로 작성합니다. Kotlin DSL은 타입 안전성과 더 나은 IDE 지원을 제공합니다.\n\n- Maven: 또 다른 빌드 도구로, XML 기반의 설정 파일을 사용합니다. Maven은 오랫동안 Java 프로젝트에서 널리 사용되어 왔으며, 설정이 직관적이라는 장점이 있습니다.\n\n#### Language\n\n- Java: 가장 널리 사용되는 JVM 언어로, Spring Boot 프로젝트에서 기본적으로 사용됩니다.\n\n- Kotlin: JetBrains에서 개발한 현대적인 JVM 언어로, 간결하고 강력한 기능을 제공합니다. Spring Boot는 Kotlin과도 잘 통합되어 있으며, Kotlin의 간결한 문법을 활용할 수 있습니다.\n\n- Groovy: 동적 타이핑을 지원하는 JVM 언어로, Gradle의 기본 스크립트 언어입니다. 코드가 간결하고 유연하며, 스크립팅 및 간단한 애플리케이션에 자주 사용됩니다.\n\n#### Spring Boot Version\n\n- 3.4.0 (SNAPSHOT): 최신 개발 중인 버전으로, 아직 안정화되지 않은 기능이 포함될 수 있습니다. 최신 기능을 테스트하거나 개발 단계에서 사용할 수 있습니다.\n\n- 3.4.0 (M2): 마일스톤(Milestone) 버전으로, 안정화가 진행 중인 버전입니다. 새로운 기능이 도입될 예정이지만, 여전히 테스트와 피드백이 필요한 단계입니다.\n\n- 3.3.4 (SNAPSHOT): 3.3.x 라인의 최신 개발 버전입니다. 마찬가지로 새로운 기능을 포함하지만, 아직 안정화되지 않았습니다.\n\n- 3.3.3: 3.3.x 라인의 최신 안정 릴리스입니다. 버그 수정 및 안정화된 기능들이 포함되어 있습니다.\n\n- 3.2.10 (SNAPSHOT): 3.2.x 라인의 최신 개발 버전으로, 3.2.x 라인에 대한 최신 기능을 포함하고 있지만, 안정화가 이루어지지 않았습니다.\n\n- 3.2.9: 3.2.x 라인의 안정 릴리스로, 안정성과 신뢰성을 바탕으로 한 기능들이 포함되어 있습니다.\n\n\n        안정된 개발을 원한다면 안정 릴리스(예: 3.2.9, 3.3.3)를 선택하는 것이 좋습니다.\n\n    최신 기능을 테스트하고 싶다면 SNAPSHOT이나 Milestone 버전을 선택할 수 있습니다. 하지만 이러한 버전은 불안정할 수 있으므로, 프로덕션 환경에는 권장되지 않습니다.\n\n#### Group\n\n- Group ID는 프로젝트의 고유 식별자로, 주로 도메인 이름을 역순으로 사용합니다. 예를 들어, 도메인이 `example.com`이라면 `com.example`와 같이 설정됩니다. 이 값은 Maven 또는 Gradle에서 프로젝트를 식별하는 데 사용됩니다.\n\n#### Artifact\n\n- Artifact ID는 생성된 프로젝트의 이름입니다. 이 이름은 빌드된 결과물(JAR 파일 등)의 이름이 됩니다. 보통 프로젝트의 이름으로 설정하며, 고유해야 합니다.\n\n#### Name\n\n- 프로젝트의 이름을 설정하는 항목으로, 일반적으로 Artifact와 동일하게 설정합니다. IDE나 빌드 도구에서 프로젝트 이름으로 표시됩니다.\n\n#### Description\n\n- 프로젝트에 대한 간단한 설명을 작성하는 곳입니다. 이 설명은 주로 프로젝트의 목적이나 내용을 요약합니다.\n\n#### Package Name\n\n- 패키지 이름은 Java/Kotlin 파일의 기본 패키지 경로를 지정합니다. 보통 Group과 Artifact를 결합하여 만듭니다. 예를 들어, `com.example.demo`와 같이 설정합니다. 모든 소스 파일이 이 패키지 하위에 생성됩니다.\n\n#### Packaging\n\n프로젝트의 결과물 형식을 선택하는 항목입니다.\n\n- Jar: 대부분의 스프링 부트 프로젝트에서 사용되는 기본 형식으로, 애플리케이션을 실행 가능한 단일 JAR 파일로 패키징합니다.\n\n- War: 기존의 서블릿 컨테이너(예: Tomcat, Jetty)에 배포하기 위한 웹 애플리케이션 아카이브(WAR) 파일 형식입니다.\n\n#### **Java Version**\n\n- 프로젝트에서 사용할 자바의 버전을 선택하는 항목입니다. 최신 자바 기능을 사용하거나 기존 호환성을 유지하기 위해 버전을 선택합니다. Spring Boot 3.x 이상에서는 자바 17 이상이 요구됩니다.\n\n- 17버전을 사용하겠습니다. \n\n### **Dependencies**\n\nDependencies는 프로젝트에서 필요로 하는 외부 라이브러리나 프레임워크를 의미합니다. 프로젝트가 의존하는 모든 외부 코드를 관리하고, 해당 의존성들이 올바르게 설치되고 사용될 수 있도록 돕습니다. 이것은 NPM(Node Package Manager)에서 JavaScript 라이브러리를 설치하고 관리하는 것과 매우 유사합니다.\n일단 저는 다음의 의존성을 설치하려고 합니다. \n\n#### Spring Web\n\nRESTful API 개발을 위한 기본적인 웹 기능을 제공합니다. 이 의존성을 추가하면 Spring MVC와 함께 HTTP 요청을 처리할 수 있습니다.\n\n#### Spring Data JPA\n\n데이터베이스 접근을 쉽게 할 수 있도록 도와줍니다. JPA를 사용해 객체 지향적으로 데이터베이스를 다룰 수 있으며, 이와 함께 추가적으로 사용할 데이터베이스 드라이버가 필요합니다.\n\n#### PostgreSQL Driver\n\nPostgreSQL 데이터베이스와 연결하기 위한 드라이버입니다. Spring Data JPA와 함께 사용되어 데이터베이스 연동이 가능합니다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n추가로 다음의 JWT 인증을 사용할 예정이므로 다음의 의존성을 설치합니다. \n\n#### Spring Security\n\n애플리케이션에 보안 기능을 추가하고자 할 때 유용합니다. 기본적인 인증 및 권한 부여 기능을 손쉽게 설정할 수 있습니다.\n\n#### Jackson Module Kotlin\n\nJSON을 코틀린 데이터 클래스에 매핑할 때 유용한 라이브러리입니다. 코틀린에서 JSON 데이터를 처리할 때 직렬화/역직렬화 작업을 쉽게 할 수 있습니다.\n\n해당 의존성은 Spring Initializr에서는 기본적으로 표시되지 않으므로, 추후 `build.gradle.kts` 에서 추가하겠습니다. \n\n### 의존성 관리\n\nSpring Initializr를 통해 프로젝트를 생성할 때, 위의 의존성을 선택하여 프로젝트를 생성하면 자동으로 `build.gradle.kts`에 해당 의존성들이 추가됩니다. 또는, 이미 생성된 프로젝트에 의존성을 추가하고 싶다면, `build.gradle.kts` 파일에 다음과 같이 추가할 수 있습니다.\n\n```kotlin\ndependencies {\n    implementation(\"org.springframework.boot:spring-boot-starter-web\")\n    implementation(\"org.springframework.boot:spring-boot-starter-data-jpa\")\n    implementation(\"org.postgresql:postgresql\")\n    implementation(\"com.fasterxml.jackson.module:jackson-module-kotlin\")\n    developmentOnly(\"org.springframework.boot:spring-boot-devtools\")\n    testImplementation(\"org.springframework.boot:spring-boot-starter-test\")\n}\n```\n\n### 생성 및 프로젝트 구조 확인\n\nGENERATE를 누르면 파일이 다운로드됩니다. \n\nVS Code에서 프로젝트를 열면 다음과 같은 기본 폴더와 파일들이 있습니다. \n\n- `src/main/kotlin/`: 이 폴더 아래에 Kotlin 소스 코드가 위치합니다.\n\n- `src/main/resources/`: 설정 파일 및 리소스 파일들이 위치합니다.\n\n- `build.gradle.kts`: 프로젝트의 빌드 설정을 관리하는 파일입니다.\n\n- `application.properties` 또는 `application.yml`: 애플리케이션 설정을 관리하는 파일입니다.\n\n## 서버 실행\n\n로컬에 JVM을 설치하지 않고 도커와 `docker-compose`를 활용하여 스프링 부트 서버와 PostgreSQL 데이터베이스를 컨테이너로 실행해보려고 합니다. \n\n이렇게 하면 애플리케이션을 완전 격리된 환경에서 실행할 수 있어 이로 인해 개발, 테스트, 배포 환경 간의 일관성을 유지할 수 있으며, \"내 로컬에서는 잘 되는데 서버에서는 왜 안 될까?\" 같은 문제를 줄일 수 있습니다.\n\n다만, 이렇게 할 경우에는 애플리케이션 디버깅이 좀 더 어려워지는 문제가 생길 수 있는데, 일단 이렇게 해보려고 합니다. \n\n### 환경 변수 설정\n\n#### .env 파일\n\n환경 변수 관리를 위해 .env 파일을 사용하겠습니다. \n\n```makefile\nPOSTGRES_DB=yourdbname\nPOSTGRES_USER=yourusername\nPOSTGRES_PASSWORD=yourpassword\n\nDB_URL=jdbc:postgresql://db:5432/${POSTGRES_DB}\nDB_USER=${POSTGRES_USER}\nDB_PASSWORD=${POSTGRES_PASSWORD}\n```\n\n#### application.properties 파일\n\napplication.properties에 DB 연결을 위한 다음의 내용을 추가합니다. \n\n```bash\n# 데이터베이스 연결 설정\nspring.datasource.url=${DB_URL}\nspring.datasource.username=${DB_USER}\nspring.datasource.password=${DB_PASSWORD}\nspring.datasource.driver-class-name=org.postgresql.Driver\n```\n\n### Dockerfile 작성\n\n스프링 부트 애플리케이션을 도커 이미지로 빌드할 수 있도록 Dockerfile을 작성합니다.\n\n```docker\n# Gradle 빌드 단계\nFROM gradle:7.6.1-jdk17 as build\nWORKDIR /app\nCOPY . /app\nRUN gradle bootJar\n\n# 런타임 단계\nFROM openjdk:17-slim\nWORKDIR /app\nCOPY --from=build /app/build/libs/*.jar app.jar\nENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n```\n\n이 Dockerfile은 두 가지 단계로 구성됩니다:\n\n- 빌드 단계: Gradle을 사용하여 애플리케이션을 빌드합니다.\n\n- 런타임 단계: 빌드된 JAR 파일을 경량의 OpenJDK 이미지에서 실행합니다.\n\n#### openjdk:17-alpine과 맥북\n\n`openjdk:17-alpine` 를 사용할 경우 애플 실리콘 기반의 맥북에서 docker-compose build를 실행할 경우 \n\n```bash\nfailed to solve: openjdk:21-alpine: docker.io/library/openjdk:21-alpine: not found\n```\n\n오류가 발생합니다. 이는 `alpine`의 특성으로 `alpine` 기반 이미지는 특히 경량화되어 있어, 특정 아키텍처에 맞춘 바이너리만 포함하고 있을 수 있습니다. 따라서 ARM 기반 시스템에서 `alpine` 이미지를 사용할 때 호환성 문제가 발생할 수 있습니다.\n\n따라서 `slim` 이미지로 교체 해주었습니다. `slim` 이미지는 `alpine`보다 약간 더 크지만, 다양한 플랫폼에서의 호환성이 더 좋습니다.\n\n### docker-compose.yml 작성\n\n`docker-compose.yml` 파일을 사용하여 스프링 부트 애플리케이션과 PostgreSQL을 각각 컨테이너로 실행합니다.\n\n```yaml\nversion: '3.8'\n\nservices:\n    app:\n        build: .\n        ports:\n            - '3000:8080'\n        env_file:\n            - .env\n        depends_on:\n            - db\n\n    db:\n        image: postgres:15\n        env_file:\n            - .env\n        ports:\n            - '5432:5432'\n        volumes:\n            - db_data:/var/lib/postgresql/data\n\nvolumes:\n    db_data:\n\n```\n\n이 파일은 두 개의 서비스를 정의합니다:\n\n- app: 스프링 부트 애플리케이션을 실행하는 컨테이너\n\n- db: PostgreSQL 데이터베이스를 실행하는 컨테이너\n\n## 컨테이너 실행 및 테스트\n\n"},{"excerpt":"서론 컨테이너 기술이 점점 더 중요한 개발 도구로 자리 잡으면서, Docker 이미지를 효율적으로 관리할 수 있는 레지스트리 선택도 중요해졌습니다. 많은 개발자들이 Docker Hub와 GitHub Container Registry(GHCR) 사이에서 어떤 서비스를 사용할지 고민하는데요, 각 서비스의 특징과 차이점을 살펴보고, 개인 프로젝트에 어떤 레지스…","fields":{"slug":"/GitHub-Container-Registry-vs-Docker-Hub/"},"frontmatter":{"date":"August 22, 2024","title":"GitHub Container Registry vs Docker Hub","tags":["GitHub","Docker"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n컨테이너 기술이 점점 더 중요한 개발 도구로 자리 잡으면서, Docker 이미지를 효율적으로 관리할 수 있는 레지스트리 선택도 중요해졌습니다. 많은 개발자들이 Docker Hub와 GitHub Container Registry(GHCR) 사이에서 어떤 서비스를 사용할지 고민하는데요, 각 서비스의 특징과 차이점을 살펴보고, 개인 프로젝트에 어떤 레지스트리가 더 적합한지 알아보겠습니다.\n\n## 기본 사용 목적과 생태계 통합\n\n**Docker Hub**는 Docker 이미지를 저장하고 관리하는 가장 널리 알려진 레지스트리 서비스입니다. Docker CLI와의 직접적인 통합 덕분에 Docker 이미지를 쉽게 빌드하고 푸시할 수 있습니다. 또한, CI/CD 도구와도 잘 통합되어 있어 다양한 프로젝트에서 유용하게 사용할 수 있습니다.\n\n반면 **GHCR**은 GitHub 리포지토리와의 긴밀한 통합을 자랑합니다. GitHub Actions를 활용하여 코드와 이미지를 한 곳에서 관리할 수 있으며, GitHub 생태계를 적극적으로 활용하는 개발자에게는 특히 편리한 선택이 될 수 있습니다.\n\n## 프라이빗 이미지 관리와 접근 제어\n\nDocker Hub는 무료 사용자의 경우 프라이빗 저장소를 하나만 제공하지만, GHCR은 무료 사용자에게도 무제한 프라이빗 저장소를 제공합니다. 따라서 개인 프로젝트에서 여러 프라이빗 이미지를 관리하려면 GHCR이 더 유리합니다. 또한, GHCR은 GitHub 리포지토리의 접근 제어와 연동되어 있어 기존 팀 및 조직 설정을 그대로 사용할 수 있습니다.\n\n## 사용의 용이성 및 학습 곡선\n\nDocker Hub는 오래된 서비스인 만큼 사용자 인터페이스와 명령어가 직관적이며, 이미 Docker 생태계에 익숙한 사용자에게는 친숙합니다. 반면, GHCR은 GitHub를 이미 사용하고 있다면 쉽게 적응할 수 있지만, GitHub Actions와의 연동 등 추가적인 학습이 필요할 수 있습니다.\n\n## 퍼포먼스 및 안정성\n\n두 서비스 모두 글로벌 인프라를 바탕으로 안정적이고 빠른 성능을 제공합니다. 다만, GHCR은 GitHub Actions와의 연동 시 뛰어난 퍼포먼스를 보여줍니다.\n\n## 비용\n\nDocker Hub는 프라이빗 저장소에 대한 제한이 있으며, 추가 저장소를 원할 경우 유료 플랜을 선택해야 합니다. 반면, GHCR은 무료 플랜 내에서도 무제한 프라이빗 저장소를 제공하므로, 비용 면에서는 GHCR이 더 나은 선택일 수 있습니다.\n\n## 결론\n\n만약 GitHub를 자주 사용하고, GitHub Actions를 활용해 코드와 이미지를 한 곳에서 관리하고자 한다면 GHCR이 개인 용도로 적합한 선택입니다. 하지만 Docker 생태계와 도구에 더 익숙하고 퍼블릭 이미지를 많이 활용하고 싶다면 Docker Hub가 더 적합할 수 있습니다.\n\n"},{"excerpt":"서론 NAS 서버의 영상을 보는 방법엔 여러가지가 있습니다. 그 중 LG WebOS에서는 서버에 직접적으로 연결할만한 충분한 방법을 제공하지 않기 때문에 어떤 방법이 좋을까 잠깐 고민하다가 DLNA가 떠올랐습니다.  서버 설정도 간단하기 때문에 짧게 하고 넘어가려고 합니다.  DLNA 장점 호환성: DLNA는 다양한 제조사에서 채택한 표준 프로토콜로, 서…","fields":{"slug":"/Ubuntu-DLNA-서버-설정/"},"frontmatter":{"date":"August 18, 2024","title":"Ubuntu DLNA 서버 설정","tags":["NAS","Odroid","Ubuntu"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nNAS 서버의 영상을 보는 방법엔 여러가지가 있습니다. 그 중 LG WebOS에서는 서버에 직접적으로 연결할만한 충분한 방법을 제공하지 않기 때문에 어떤 방법이 좋을까 잠깐 고민하다가 DLNA가 떠올랐습니다. \n\n서버 설정도 간단하기 때문에 짧게 하고 넘어가려고 합니다. \n\n## DLNA\n\n### 장점\n\n- 호환성: DLNA는 다양한 제조사에서 채택한 표준 프로토콜로, 서로 다른 기기 간에도 원활하게 미디어를 공유할 수 있습니다. 예를 들어, 스마트폰, 태블릿, 스마트 TV, 게임 콘솔 등에서 모두 사용할 수 있습니다.\n\n- 편리한 미디어 공유: 복잡한 설정 없이 네트워크에 연결된 기기 간에 미디어 파일을 손쉽게 공유하고 스트리밍할 수 있습니다. 미디어 서버에 파일을 업로드하면 즉시 다른 기기에서 재생이 가능합니다.\n\n- 자동 탐지: DLNA 지원 기기는 네트워크 상에 연결된 미디어 서버를 자동으로 탐지하여 사용자가 별도의 설정 없이도 미디어를 쉽게 이용할 수 있습니다.\n\n- 저전력 및 경량 운영: minidlna와 같은 경량 서버 프로그램은 서버의 자원을 효율적으로 사용하여 저전력 환경에서도 안정적으로 동작합니다.\n\n### 단점\n\n- 제한된 파일 형식 지원 - DLNA는 모든 파일 형식을 지원하지 않습니다. 특정 비디오, 오디오, 자막 파일 형식은 DLNA를 통해 스트리밍할 때 제대로 지원되지 않을 수 있습니다. 이로 인해 파일을 변환하거나 별도의 코덱을 설치해야 하는 번거로움이 발생할 수 있습니다.\n\n- 인증 및 보안 부족 - 앞서 언급한 것처럼 DLNA는 인증 메커니즘이 거의 없기 때문에 보안이 취약합니다. 이를 통해 연결된 장치가 네트워크 내에서 자유롭게 미디어에 접근할 수 있지만, 그만큼 보안 위험도 큽니다. 이는 외부에서 접근 시 더욱 큰 문제가 됩니다.\n\n- 제한된 장치 호환성- DLNA는 다양한 기기에서 사용할 수 있지만, 모든 기기에서 동일한 수준의 지원을 제공하지는 않습니다. 일부 기기는 DLNA를 완전히 지원하지 않거나, 특정 기능(예: 자막 지원, 특정 미디어 포맷 재생)이 제대로 작동하지 않을 수 있습니다.\n\n- DLNA 서버는 미디어 파일을 자동으로 인덱싱하지만, 이를 수동으로 관리하기가 어려울 수 있습니다. 미디어 라이브러리가 크거나, 파일이 다양한 폴더에 분산되어 있는 경우, DLNA 서버에서 이를 제대로 탐지하지 못하거나, 잘못된 메타데이터로 인식할 수 있습니다.\n\n## minidlna\n\nminidlna는 DLNA(Digital Living Network Alliance) 표준을 지원하는 경량 미디어 서버 소프트웨어입니다. Linux 기반의 시스템에서 쉽게 설정할 수 있으며, 동영상, 음악, 사진과 같은 다양한 미디어 파일을 네트워크 상에서 DLNA를 지원하는 기기들로 스트리밍할 수 있습니다. 특히 minidlna는 설정이 간편하고, 서버의 자원을 효율적으로 사용하는 것이 큰 장점입니다.\n\n## 설치 및 설정\n\n다음의 명령어로 minidlna를 설치합니다. \n\n```bash\nsudo apt install minidlna\n```\n\n설정파일도 간단합니다. \n\n```bash\nmedia_dir=A,/var/lib/minidlna/Music\nmedia_dir=V,/var/lib/minidlna/Video\nmedia_dir=P,/var/lib/minidlna/Picture\n```\n\nA가 음악, V가 동영상, P가 사진 폴더입니다. 필요없다면 지정하지 않아도 됩니다. \n\n설정 적용을 위해 프로그램을 다시 시작합니다. \n\n```bash\nsudo systemctl restart minidlna.service\n```\n\n## 확인\n\n`내부아이피:8200`으로 접속하면 현황을 볼 수 있습니다. WebOS에서도 미디어 플레이어를 실행하면 같은 망이라면 자동으로 확인이 됩니다. \n\n자막 파일도 정상 지원합니다. \n\n## [중요] 보안 문제\n\nDLNA를 포트포워딩을 사용해 외부에서 접근 가능하도록 하는 것은 추천하지 않습니다. \n\nDLNA 프로토콜은 기본적으로 보안에 취약합니다. 데이터 전송 시 암호화가 이루어지지 않기 때문에, 민감한 정보가 노출될 수 있습니다. 이는 SMB 프로토콜에서 발생하는 보안 문제와 유사합니다.\n\n또한 단점에서 언급되었듯이 DLNA 서버는 인증 없이 미디어 파일에 접근할 수 있기 때문에 악의적인 사용자가 네트워크에 침투할 위험이 있습니다.\n\n\n\n"},{"excerpt":"Tailwind CSS 소개 Tailwind CSS는 유틸리티 기반의 CSS 프레임워크로, 미리 정의된 클래스를 사용하여 빠르고 쉽게 스타일링할 수 있게 해줍니다. 전통적인 CSS 프레임워크와 달리, Tailwind는 사용자가 CSS를 작성하는 방식에 혁신을 가져왔습니다. 각 클래스는 단일 속성과 값을 가지고 있어, 더 나은 유지보수성과 높은 재사용성을 …","fields":{"slug":"/Install-Tailwind-CSS-with-Nextjs/"},"frontmatter":{"date":"August 04, 2024","title":"Install Tailwind CSS with Next.js","tags":["Next.js"]},"rawMarkdownBody":"![](image1.png)\n## Tailwind CSS 소개\n\nTailwind CSS는 유틸리티 기반의 CSS 프레임워크로, 미리 정의된 클래스를 사용하여 빠르고 쉽게 스타일링할 수 있게 해줍니다. 전통적인 CSS 프레임워크와 달리, Tailwind는 사용자가 CSS를 작성하는 방식에 혁신을 가져왔습니다. 각 클래스는 단일 속성과 값을 가지고 있어, 더 나은 유지보수성과 높은 재사용성을 제공합니다.\n\n#### 장점\n\n1. 빠른 개발 속도: 미리 정의된 유틸리티 클래스를 사용하여 빠르게 스타일링할 수 있습니다.\n\n1. 작은 파일 크기: 사용되지 않는 CSS를 제거하여 최종 CSS 파일 크기를 최소화합니다.\n\n1. 일관성: 클래스 이름이 직관적이고 일관성이 있어 코드를 쉽게 읽고 이해할 수 있습니다.\n\n#### 단점\n\n1. 초기 학습 곡선: Tailwind의 유틸리티 클래스 시스템에 익숙해지기까지 시간이 걸릴 수 있습니다.\n\n1. HTML 코드의 가독성 저하: 많은 클래스를 사용할 경우 HTML 코드가 복잡해질 수 있습니다.\n\n## Tailwind UI\n\nTailwind UI는 Tailwind CSS를 기반으로 구축된 고급 UI 컴포넌트 라이브러리입니다. Tailwind UI는 다양한 범주의 컴포넌트를 제공하여, 개발자가 빠르고 쉽게 아름다운 사용자 인터페이스를 만들 수 있도록 돕습니다. Tailwind UI는 유료 라이브러리지만, 시간과 비용을 절약할 수 있는 강력한 도구입니다.\n\n#### 장점\n\n1. 디자인 품질: 고품질의 디자인을 제공하여, 디자이너 없이도 아름다운 UI를 만들 수 있습니다.\n\n1. 생산성 향상: 미리 준비된 컴포넌트를 사용하여 개발 시간을 단축할 수 있습니다.\n\n1. 커스터마이징 가능: Tailwind CSS의 모든 유틸리티 클래스를 사용하여 쉽게 커스터마이징할 수 있습니다.\n\n#### 단점\n\n1. 비용: Tailwind UI는 유료 라이브러리입니다.\n\n1. 의존성: Tailwind CSS에 대한 의존성이 있어, Tailwind를 사용하지 않는 프로젝트에서는 사용이 어렵습니다.\n\n## Tailwind CSS 설치\n\nTailwind CSS를 Next.js 프로젝트에 설치하려면 다음 명령어를 사용합니다.\n\n```shell\nnpm install -D tailwindcss postcss autoprefixer\n```\n\n그런 다음 Tailwind CSS와 PostCSS의 초기 설정을 수행합니다.\n\n```shell\nnpx tailwindcss init -p\n```\n\n### PostCSS?\n\nPostCSS는 CSS를 변환하기 위한 도구로, 다양한 플러그인을 통해 CSS 코드의 처리를 자동화할 수 있습니다. Tailwind CSS는 PostCSS 플러그인을 통해 CSS 파일을 처리하고, 필요한 유틸리티 클래스들을 생성합니다. Tailwind CSS는 PostCSS를 사용하여 컴파일 타임에 유틸리티 클래스들을 자동으로 생성하고, 사용되지 않는 CSS를 제거하는 등 여러 작업을 수행합니다.\n\n### Autoprefixer?\n\nAutoprefixer는 PostCSS 플러그인 중 하나로, CSS에 자동으로 공급 업체 접두사를 추가하여 브라우저 간의 호환성을 보장합니다. 예를 들어, CSS에서 `display: flex`를 사용하면 Autoprefixer가 이를 다양한 브라우저에 맞게 `webkit-` 또는 `ms-`와 같은 접두사를 추가하여 변환해줍니다.\n\n### globals.css 수정\n\nNext.js에서 globals.css 파일은 전역 스타일을 정의하는 데 사용됩니다. 이 파일에 작성된 CSS는 Next.js 애플리케이션의 모든 페이지와 컴포넌트에 적용됩니다. 주로 기본적인 스타일 설정, 공통 레이아웃, 기본 타이포그래피, 공통 유틸리티 클래스 등을 정의하는 데 사용됩니다.\n\nTailwind CSS를 사용하기 위해 `globals.css` 파일에 다음 내용을 추가합니다.\n\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n### `tailwind.config.js`  수정\n\n또한 `tailwind.config.js` 파일을 수정하여 Tailwind가 CSS 클래스 생성을 위해 검색할 파일 경로를 지정합니다.\n\n```javascript\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\n    \"./app/**/*.{js,ts,jsx,tsx,mdx}\",\n    \"./pages/**/*.{js,ts,jsx,tsx,mdx}\",\n    \"./components/**/*.{js,ts,jsx,tsx,mdx}\",\n\n    // Or if using `src` directory:\n    \"./src/**/*.{js,ts,jsx,tsx,mdx}\",\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n```\n\n## VS Code 확장 설치\n\n프로젝트 개발을 더 편리하게 하기 위해 다음 VS Code 확장을 설치합니다:\n\n- PostCSS Language Support: PostCSS 파일을 편집할 때 문법 강조와 같은 기능을 제공합니다.\n\n- Tailwind CSS IntelliSense: Tailwind CSS 클래스 이름 자동 완성 및 툴팁을 제공합니다.\n\n## 참조\n\n[https://tailwindcss.com/docs/guides/nextjs](https://tailwindcss.com/docs/guides/nextjs)\n\n\n\n"},{"excerpt":"Fixture Fixture는 테스트 실행 전에 필요한 상태나 객체를 설정하는 데 사용됩니다. 주로 데이터베이스 초기화, 파일 시스템 설정, 특정 객체 생성 등의 작업을 포함합니다. 예제 위의 예제는 Django 테스트 환경에서 실제로  객체를 데이터베이스에 생성합니다. 이 fixture는 테스트 함수가 실행될 때마다 호출되며, 테스트가 끝나면 데이터베이…","fields":{"slug":"/Mock과-Stub-Fixture/"},"frontmatter":{"date":"June 20, 2024","title":"Mock과 Stub, Fixture","tags":["TDD","Django","Python"]},"rawMarkdownBody":"## Fixture\n\n**Fixture**는 테스트 실행 전에 필요한 상태나 객체를 설정하는 데 사용됩니다. 주로 데이터베이스 초기화, 파일 시스템 설정, 특정 객체 생성 등의 작업을 포함합니다.\n\n#### 예제\n\n```python\n@pytest.fixture\ndef user(db):\n    return User.objects.create_user(\n        email=\"user@example.com\",\n        dob=\"1990-01-01\",\n        gender=1,\n        nickname=\"testnickname\",\n    )\n```\n\n위의 예제는 Django 테스트 환경에서 실제로 `User` 객체를 데이터베이스에 생성합니다. 이 fixture는 테스트 함수가 실행될 때마다 호출되며, 테스트가 끝나면 데이터베이스 상태를 롤백합니다.\n\n## Mock\n\n**Mock**은 테스트 중에 외부 의존성을 대체하는 객체입니다. 실제 객체의 동작을 모방하지만, 더 가볍고 빠르며 예측 가능한 결과를 반환합니다. Mock 객체는 호출된 메서드와 그 인자를 기록할 수 있어서, 테스트 중에 호출 여부와 호출된 인자를 검증할 수 있습니다.\n\n#### 예제\n\n```python\nfrom unittest.mock import MagicMock, patch\n\ndef test_s3_upload():\n    mock_s3_client = MagicMock()\n    mock_s3_client.upload_file.return_value = \"mocked_url\"\n\n    with patch('common.S3Client', return_value=mock_s3_client):\n        s3_client = S3Client()\n        result = s3_client.upload_file(\"fake_path\")\n        assert result == \"mocked_url\"\n```\n\n위의 예제에서는 `S3Client`의 `upload_file` 메서드를 모킹하여 실제로 파일을 업로드하지 않고 가짜 URL을 반환하도록 합니다.\n\n## Stub\n\n**Stub**은 테스트 중에 의존성을 대체하는 간단한 구현체입니다. 주로 특정한 메서드 호출에 대해 미리 정의된 값을 반환하도록 설정됩니다. Mock과 비슷하지만, 호출된 메서드와 인자를 기록하지 않으며, 더 단순한 형태입니다.\n\n#### 예제\n\n```python\nclass S3ClientStub:\n    def upload_file(self, file_path):\n        return \"stubbed_url\"\n\ndef test_s3_upload():\n    s3_client = S3ClientStub()\n    result = s3_client.upload_file(\"fake_path\")\n    assert result == \"stubbed_url\"\n```\n\n위의 예제에서는 `S3ClientStub`을 사용하여 `upload_file` 메서드를 간단하게 스텁(stub) 처리했습니다.\n\n## 차이점 정리\n\n1. **Fixture**: 테스트를 위해 필요한 상태나 객체를 설정합니다. 실제 객체나 데이터베이스 상태를 사용합니다.\n\n1. **Mock**: 외부 의존성을 대체하여 실제 객체 대신 가짜 객체를 사용합니다. 호출된 메서드와 인자를 기록할 수 있습니다.\n\n1. **Stub**: 특정한 메서드 호출에 대해 미리 정의된 값을 반환하는 간단한 구현체입니다. Mock보다 더 단순한 형태입니다.\n\n\n\n"},{"excerpt":"서론 Django REST Framework(DRF)를 사용하다 보면, 모델에 없는 가상의 필드를 시리얼라이저에 추가해야 할 때가 있습니다. 이때 유용하게 사용할 수 있는 것이 입니다. 를 통해 동적으로 계산된 값을 시리얼라이저에 포함할 수 있습니다. 이번 글에서는 이를 구현하는 방법을 간단히 살펴보겠습니다. 예제: 가상의 필드 추가하기 예를 들어,  모…","fields":{"slug":"/Django-REST-Framework에서-가상-필드-추가하기/"},"frontmatter":{"date":"June 17, 2024","title":"Django REST Framework에서 가상 필드 추가하기","tags":["Django","DRF","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nDjango REST Framework(DRF)를 사용하다 보면, 모델에 없는 가상의 필드를 시리얼라이저에 추가해야 할 때가 있습니다. 이때 유용하게 사용할 수 있는 것이 `SerializerMethodField`입니다. `SerializerMethodField`를 통해 동적으로 계산된 값을 시리얼라이저에 포함할 수 있습니다. 이번 글에서는 이를 구현하는 방법을 간단히 살펴보겠습니다.\n\n## 예제: 가상의 필드 추가하기\n\n예를 들어, `Photos` 모델이 아래와 같이 정의되어 있다고 가정해봅시다:\n\n```python\nfrom django.db import models\nimport uuid\n\nclass Photos(models.Model):\n    id = models.UUIDField(default=uuid.uuid4, primary_key=True, editable=False)\n    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name=\"photos\")\n    match_type = models.CharField(max_length=10)\n    file_name = models.TextField()\n    updated_at = models.DateTimeField(auto_now=True)\n    created_at = models.DateTimeField(auto_now_add=True)\n```\n\n이 모델을 기반으로 하는 시리얼라이저에 `id`와 `match_type`을 조합한 가상의 필드 `id_match_type`을 추가해보겠습니다.\n\n## `SerializerMethodField` 사용하기\n\n먼저, `SerializerMethodField`를 사용하여 시리얼라이저에 가상의 필드를 추가합니다:\n\n```python\nfrom rest_framework import serializers\nfrom .models import Photos\n\nclass PhotoSerializer(serializers.ModelSerializer):\n    id_match_type = serializers.SerializerMethodField()  # 가상의 필드 정의\n\n    class Meta:\n        model = Photos\n        fields = [\"id\", \"match_type\", \"file_name\", \"updated_at\", \"created_at\", \"id_match_type\"]\n\n    def get_id_match_type(self, obj):  # 가상의 필드를 계산하는 메서드 정의\n        return f\"{obj.id}-{obj.match_type}\"\n\n\n```\n\n위 코드에서 `SerializerMethodField`는 `id_match_type`이라는 이름의 필드를 시리얼라이저에 추가합니다. DRF는 자동으로 `get_id_match_type` 메서드를 찾아 이 필드의 값을 계산합니다. 이 메서드는 객체(`obj`)를 인자로 받아, `id`와 `match_type`을 조합한 문자열을 반환합니다.\n\n## 결과\n\n이제 `PhotoSerializer`를 사용하여 직렬화된 데이터에는 `id_match_type` 필드가 포함됩니다:\n\n```json\n{\n    \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n    \"match_type\": \"CUTE\",\n    \"file_name\": \"photo.jpg\",\n    \"updated_at\": \"2024-06-17T10:26:19Z\",\n    \"created_at\": \"2024-06-17T10:26:19Z\",\n    \"id_match_type\": \"123e4567-e89b-12d3-a456-426614174000-CUTE\"\n}\n\n\n```\n\n## 결론\n\n`SerializerMethodField`는 모델에 존재하지 않는 가상의 필드를 시리얼라이저에 추가할 때 매우 유용한 도구입니다. 이를 통해 동적으로 계산된 값을 클라이언트에게 전달할 수 있습니다. 위 예제에서는 `id`와 `match_type`을 조합한 가상의 필드를 추가하는 방법을 살펴보았습니다. \n\n"},{"excerpt":"서론 이번 글에서는 AWS에서 VPC를 생성하고, EC2 인스턴스를 설정하며, 터미널과 Visual Studio Code를 통해 원격으로 연결하는 과정을 기록하려고 합니다.  VPC 생성 1. VPC 생성 AWS Management Console에 로그인합니다. 상단 검색창에 \"VPC\"를 입력하고 VPC 대시보드로 이동합니다. \"VPCs\"를 선택한 후 \"…","fields":{"slug":"/AWS에서-EC2-인스턴스-생성-및-Visual-Studio-Code로-원격-연결하기/"},"frontmatter":{"date":"June 13, 2024","title":"AWS에서 EC2 인스턴스 생성 및 Visual Studio Code로 원격 연결하기","tags":["AWS","BackEnd","VSCode"]},"rawMarkdownBody":"## 서론\n\n이번 글에서는 AWS에서 VPC를 생성하고, EC2 인스턴스를 설정하며, 터미널과 Visual Studio Code를 통해 원격으로 연결하는 과정을 기록하려고 합니다. \n\n## VPC 생성\n\n### 1. VPC 생성\n\n1. AWS Management Console에 로그인합니다.\n\n1. 상단 검색창에 \"VPC\"를 입력하고 VPC 대시보드로 이동합니다.\n\n1. \"VPCs\"를 선택한 후 \"Create VPC\" 버튼을 클릭합니다.\n\n1. VPC 이름, IPv4 CIDR 블록(예: 10.0.0.0/16), IPv6 CIDR 블록 설정, 테넌시 등을 입력합니다.\n\n1. \"Create VPC\" 버튼을 클릭하여 VPC를 생성합니다.\n\n### 2 서브넷 생성\n\n1. VPC 대시보드에서 \"Subnets\"를 선택합니다.\n\n1. \"Create Subnet\" 버튼을 클릭합니다.\n\n1. 서브넷 이름, VPC 선택, 가용 영역(AZ) 선택, IPv4 CIDR 블록(예: 10.0.1.0/24)을 입력합니다.\n\n1. \"Create Subnet\" 버튼을 클릭하여 서브넷을 생성합니다.\n\n### 3. 인터넷 게이트웨이 생성 및 연결\n\n1. VPC 대시보드에서 \"Internet Gateways\"를 선택합니다.\n\n1. \"Create internet gateway\" 버튼을 클릭합니다.\n\n1. 인터넷 게이트웨이 이름을 입력하고 \"Create internet gateway\" 버튼을 클릭합니다.\n\n1. 생성된 인터넷 게이트웨이를 선택하고 \"Actions\" 메뉴에서 \"Attach to VPC\"를 선택합니다.\n\n1. 앞서 생성한 VPC를 선택하고 \"Attach internet gateway\"를 클릭합니다.\n\n### 4. 라우팅 테이블 생성 및 설정\n\n1. VPC 대시보드에서 \"Route Tables\"를 선택합니다.\n\n1. \"Create route table\" 버튼을 클릭합니다.\n\n1. 라우팅 테이블 이름과 VPC를 선택한 후 \"Create route table\" 버튼을 클릭합니다.\n\n1. 생성된 라우팅 테이블을 선택하고 \"Routes\" 탭을 클릭한 후 \"Edit routes\" 버튼을 클릭합니다.\n\n1. 기본 경로(0.0.0.0/0)에 인터넷 게이트웨이를 대상으로 추가합니다.\n\n1. \"Save routes\" 버튼을 클릭합니다.\n\n1. \"Subnet associations\" 탭을 클릭하고 \"Edit subnet associations\" 버튼을 클릭합니다.\n\n1. 앞서 생성한 서브넷을 선택하고 \"Save\" 버튼을 클릭합니다.\n\n## EC2 인스턴스 생성\n\n### 1. SSH 키 페어 생성\n\n1. 상단 검색창에 \"EC2\"를 입력하고 EC2 대시보드로 이동합니다.\n\n1. 왼쪽 사이드바에서 \"Key Pairs\"를 선택합니다.\n\n1. \"Create key pair\" 버튼을 클릭합니다.\n\n1. 키 페어 이름을 입력하고, 키 파일 형식을 선택한 후 \"Create key pair\" 버튼을 클릭합니다. 이때, PEM 형식을 선택하는 것을 권장합니다.\n\n1. 키 페어가 생성되면 자동으로 .pem 파일이 다운로드됩니다. 이 파일을 안전한 위치에 저장합니다.\n\n### 2. EC2 인스턴스 생성\n\n1. EC2 대시보드에서 \"Instances\"를 선택합니다.\n\n1. \"Launch instances\" 버튼을 클릭합니다.\n\n1. \"Name and tags\" 섹션에서 인스턴스 이름을 입력합니다.\n\n1. \"Application and OS Images (Amazon Machine Image)\" 섹션에서 사용할 AMI를 선택합니다.\n\n1. \"Instance type\" 섹션에서 원하는 인스턴스 유형을 선택합니다.\n\n1. \"Key pair (login)\" 섹션에서 앞서 생성한 키 페어를 선택합니다.\n\n1. \"Network settings\" 섹션에서 편집을 눌러 1에서 생성한 VPC와 서브넷을 선택합니다.\n\n1. 보안 그룹 설정에서 기존 보안 그룹을  새로 생성합니다. SSH (22번 포트)를 허용하는 규칙이 포함되어야 SSH로 접근할 수 있습니다. 허용할 IP는 내 아이피를 선택하거나 IPv4 Anywhere을 선택하는데, IPv4 Anywhere을 선택한 경우에는 모든 IP에서 접근이 가능함에 주의해야 합니다. \n\n1. \"Launch instance\" 버튼을 클릭하여 인스턴스를 생성합니다.\n\n## 터미널에서 연결\n\n### 1. SSH 키 파일 권한 설정\n\n1. 터미널을 엽니다 (Windows의 경우 Git Bash 또는 PuTTY 사용).\n\n1. 다운로드한 .pem 파일의 권한을 설정합니다.\n\n    ```shell\n    chmod 400 path/to/your-key-pair.pem\n    ```\n\n### 2. SSH를 사용하여 인스턴스에 연결\n\n1. SSH를 사용하여 인스턴스에 연결합니다. 아래 명령어를 사용하시면 됩니다. `instance-public-dns`는 EC2 인스턴스의 퍼블릭 DNS 이름입니다.\n\n    ```shell\n    ssh -i path/to/your-key-pair.pem ec2-user@instance-public-dns\n    ```\n\n### 3. EC2 인스턴스의 퍼블릭 DNS 확인 방법\n\n1. EC2 대시보드에서 \"Instances\"를 선택합니다.\n\n1. 연결하려는 인스턴스를 선택합니다.\n\n1. 하단에 인스턴스 세부 정보가 나타나며, \"Public DNS (IPv4)\" 항목에서 퍼블릭 DNS 이름을 확인할 수 있습니다.\n\n## VS Code에서 연결\n\n### 1. Visual Studio Code 설치 및 설정\n\n#### Remote - SSH 확장 설치\n\n- Visual Studio Code를 열고, 왼쪽 사이드바에서 확장 아이콘(네모 모양)을 클릭합니다.\n\n- 검색창에 `Remote - SSH`를 입력하고 `Remote - SSH` 확장을 설치합니다.\n\n### 2. Visual Studio Code에서 SSH 구성\n\n1. VS Code에서 Command Palette 열기\n\n    - `Ctrl+Shift+P` (또는 `Cmd+Shift+P` on Mac)을 눌러 Command Palette를 엽니다.\n\n1. SSH 구성 파일 편집\n\n    - Command Palette에서 `Remote-SSH: Open SSH Configuration File...`을 입력하고 선택합니다.\n\n    - 로컬의 SSH 설정 파일 경로를 선택합니다 (보통 `~/.ssh/config`).\n\n1. SSH 호스트 추가\n\n    - 구성 파일에 다음 내용을 추가합니다. `Host` 부분은 원하는 이름으로 설정하고, `HostName`, `User`, `IdentityFile` 부분을 자신의 정보로 변경합니다.\n\n        ```plain text\n        Host your-instance-name\n            HostName instance-public-dns\n            User ec2-user\n            IdentityFile /path/to/your-key-pair.pem\n        ```\n\n### 3. Visual Studio Code에서 원격 연결\n\n1. VS Code의 Remote Explorer 열기\n\n    - Visual Studio Code의 왼쪽 사이드바에서 \"Remote Explorer\" 아이콘을 클릭합니다.\n\n1. SSH Targets 섹션에서 호스트 선택\n\n    - `SSH Targets` 섹션에서 추가한 호스트를 선택하고 `Connect to Host in New Window`를 클릭합니다.\n\n1. 호스트 키 확인\n\n    - 연결할 때 처음으로 호스트 키를 신뢰할 것인지 묻는 창이 나타날 수 있습니다. \"Yes\"를 클릭하여 신뢰합니다.\n\n1. 원격 서버에 연결\n\n    - 연결이 성공하면 새로운 VS Code 창이 열리며, 원격 서버의 파일 시스템에 접근할 수 있습니다.\n\n"},{"excerpt":"서론 ngrok을 이용하는 등 로컬에서만 개발을 하다 테스트 서버를 꾸며야 하는 때가 왔습니다. AWS를 이용하기로 했고, 그 과정을 진행했던 기록을 남기려고 합니다.  외부에서는 API Gateway에만 접근이 가능하며, DB나 백엔드 서버는 직접 접근이 불가능하도록 구현하려고 합니다.  VPC (Virtual Private Cloud) VPC 생성 V…","fields":{"slug":"/AWS-활용-테스트-서버-구현-1/"},"frontmatter":{"date":"June 09, 2024","title":"AWS 활용 테스트 서버 구현 (1)","tags":["AWS","BackEnd"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n[ngrok](https://sharknia.github.io/ngrok-로컬-서버를-쉽게-공개하는-도구)을 이용하는 등 로컬에서만 개발을 하다 테스트 서버를 꾸며야 하는 때가 왔습니다. AWS를 이용하기로 했고, 그 과정을 진행했던 기록을 남기려고 합니다. \n\n외부에서는 API Gateway에만 접근이 가능하며, DB나 백엔드 서버는 직접 접근이 불가능하도록 구현하려고 합니다. \n\n## VPC (Virtual Private Cloud)\n\n### VPC 생성\n\n1. VPC 서비스로 이동: 상단 검색창에서 \"VPC\"를 검색하여 VPC 대시보드로 이동합니다.\n\n1. VPC 생성:\n\n    - VPC 대시보드에서 \"Your VPCs\"를 클릭합니다.\n\n    - \"Create VPC\" 버튼을 클릭합니다.\n\n    - VPC 이름을 입력합니다 (예: `MyVPC`).\n\n    - IPv4 CIDR 블록을 입력합니다 (예: `10.0.0.0/16`). \n\n    - 기타 옵션은 기본값으로 두고 \"Create\" 버튼을 클릭합니다.\n\n### IPv4 CIDR 블록이란?\n\n#### CIDR (Classless Inter-Domain Routing)\n\n- CIDR은 IP 주소를 효율적으로 할당하고 라우팅하기 위한 방법입니다.\n\n- CIDR 표기법은 IP 주소 뒤에 슬래시(`/`)와 숫자를 붙여서 사용합니다. 예를 들어, `10.0.0.0/16`입니다.\n\n#### 구성 요소\n\n- 네트워크 주소: `10.0.0.0`\n\n    - 네트워크의 시작 주소를 나타냅니다.\n\n- 서브넷 마스크: `/16`\n\n    - 네트워크와 호스트 부분을 구분하는 비트 수를 나타냅니다.\n\n    - `/16`은 처음 16비트가 네트워크 주소를, 나머지 16비트가 호스트 주소를 의미합니다.\n\n#### 의미\n\n- 10.0.0.0/16은 IP 주소 범위를 나타냅니다.\n\n    - 시작 주소는 `10.0.0.0`입니다.\n\n    - 끝 주소는 `10.0.255.255`입니다.\n\n    - 총 65,536개의 IP 주소를 포함합니다 (2^16).\n\n#### 예시\n\n- 공용 서브넷: `10.0.1.0/24`\n\n    - 시작 주소: `10.0.1.0`\n\n    - 끝 주소: `10.0.1.255`\n\n    - 총 256개의 IP 주소를 포함합니다 (2^8).\n\n    - `/24`는 처음 24비트가 네트워크 주소를, 나머지 8비트가 호스트 주소를 의미합니다.\n\n- 프라이빗 서브넷: `10.0.2.0/24`\n\n    - 시작 주소: `10.0.2.0`\n\n    - 끝 주소: `10.0.2.255`\n\n    - 총 256개의 IP 주소를 포함합니다.\n\n#### 설정 예시\n\n- VPC CIDR 블록: `10.0.0.0/16`\n\n- 공용 서브넷 CIDR 블록: `10.0.1.0/24`\n\n- 프라이빗 서브넷 CIDR 블록: `10.0.2.0/24`\n\n이와 같이 CIDR 블록을 설정하면 VPC 내에서 사용할 수 있는 IP 주소의 범위를 지정하고, 공용 및 프라이빗 서브넷을 나누어 관리할 수 있습니다.\n\n### 서브넷 생성\n\n- VPC 대시보드에서 \"Subnets\"를 클릭합니다.\n\n- \"Create Subnet\" 버튼을 클릭합니다.\n\n- 서브넷 이름을 입력합니다 (예: `PublicSubnet1`, `PrivateSubnet1`).\n\n- VPC를 선택합니다 (예: `MyVPC`).\n\n- 서브넷의 가용 영역을 선택합니다.\n\n- 각 서브넷의 IPv4 CIDR 블록을 입력합니다 (예: 공용 서브넷은 `10.0.1.0/24`, 프라이빗 서브넷은 `10.0.2.0/24`).\n\n- \"Create Subnet\" 버튼을 클릭합니다.\n\n#### 인터넷 게이트웨이 생성 및 연결\n\n1. 인터넷 게이트웨이 생성:\n\n    - VPC 대시보드에서 \"Internet Gateways\"를 클릭합니다.\n\n    - \"Create Internet Gateway\" 버튼을 클릭합니다.\n\n    - 이름을 입력합니다 (예: `MyInternetGateway`).\n\n    - \"Create\" 버튼을 클릭합니다.\n\n1. 인터넷 게이트웨이 연결:\n\n    - 생성한 인터넷 게이트웨이를 선택하고 \"Actions\"를 클릭한 후 \"Attach to VPC\"를 선택합니다.\n\n    - VPC를 선택하고 \"Attach\" 버튼을 클릭합니다.\n\n#### 라우팅 테이블 설정\n\n1. 라우팅 테이블 생성:\n\n    - VPC 대시보드에서 \"Route Tables\"를 클릭합니다.\n\n    - \"Create Route Table\" 버튼을 클릭합니다.\n\n    - 이름을 입력합니다 (예: `PublicRouteTable`).\n\n    - VPC를 선택하고 \"Create\" 버튼을 클릭합니다.\n\n1. 라우팅 테이블 편집:\n\n    - 생성한 라우팅 테이블을 선택하고 \"Routes\" 탭으로 이동합니다.\n\n    - \"Edit Routes\" 버튼을 클릭합니다.\n\n    - \"Add route\"를 클릭하여 다음을 추가합니다:\n\n        - 목적지: `0.0.0.0/0`\n\n        - 타겟: 인터넷 게이트웨이 (예: `MyInternetGateway`)\n\n    - \"Save routes\" 버튼을 클릭합니다.\n\n1. 서브넷 연결:\n\n    - 라우팅 테이블을 선택하고 \"Subnet associations\" 탭으로 이동합니다.\n\n    - \"Edit subnet associations\" 버튼을 클릭합니다.\n\n    - 공용 서브넷을 선택하고 \"Save\" 버튼을 클릭합니다.\n\n## 보안 그룹 및 네트워크 ACL\n\n### 보안 그룹 생성\n\n1. EC2 대시보드로 이동: 상단 검색창에서 \"EC2\"를 검색하여 EC2 대시보드로 이동합니다.\n\n1. 보안 그룹 생성:\n\n    - EC2 대시보드에서 \"Security Groups\" 클릭:\n\n        - \"Security Groups\"를 클릭합니다.\n\n        - \"Create security group\" 버튼을 클릭합니다.\n\n    - 보안 그룹 이름 및 설명 입력:\n\n        - 보안 그룹 이름을 입력합니다.\n\n        - 설명을 입력합니다.\n\n        - VPC를 선택합니다 (예: `MyVPC`).\n\n        - 보안 그룹은 서비스 별로 구분하여 만들어주는게 관리하기가 쉽습니다. \n\n1. 인바운드 규칙 설정:\n\n    - API Gateway 보안 그룹 (APIGatewaySG):\n\n        - Type: HTTP, Protocol: TCP, Port range: 80, Source: 0.0.0.0/0 (모든 IP 허용)\n\n        - Type: HTTPS, Protocol: TCP, Port range: 443, Source: 0.0.0.0/0 (모든 IP 허용)\n\n    - Auth Server 보안 그룹 (TestServerSG):\n\n        - Type: Custom TCP, Protocol: TCP, Port range: 8000 (예제 포트), Source: APIGatewaySG (API Gateway 보안 그룹에서만 접근 허용)\n\n    - DB 보안 그룹 (DBSG):\n\n        - **Type**: PostgreSQL, **Protocol**: TCP, **Port range**: 5432, **Source**: TestServerSG (TestServerSG 보안 그룹에서만 접근 허용)\n\n1. 아웃바운드 규칙 설정:\n\n    - 기본적으로 모든 아웃바운드 트래픽이 허용되지만, 필요에 따라 아웃바운드 규칙을 설정할 수 있습니다.\n\n#### 네트워크 ACL 설정\n\n1. VPC 대시보드로 이동: 상단 검색창에서 \"VPC\"를 검색하여 VPC 대시보드로 이동합니다.\n\n1. 네트워크 ACL 생성:\n\n    - VPC 대시보드에서 \"Network ACLs\" 클릭:\n\n        - \"Network ACLs\"를 클릭합니다.\n\n        - \"Create Network ACL\" 버튼을 클릭합니다.\n\n    - 네트워크 ACL 이름 및 설명 입력:\n\n        - 네트워크 ACL 이름을 입력합니다 (예: `PublicSubnetACL`, `PrivateSubnetACL`).\n\n        - 설명을 입력합니다.\n\n        - VPC를 선택합니다 (예: `MyVPC`).\n\n    - **서브넷 연결**:\n\n        - 생성한 네트워크 ACL을 선택하고 \"Subnet associations\" 탭으로 이동합니다.\n\n        - \"Edit subnet associations\" 버튼을 클릭합니다.\n\n        - 공용 서브넷 및 프라이빗 서브넷을 각각 적절한 네트워크 ACL에 연결합니다.\n\n1. 인바운드 및 아웃바운드 규칙 설정:\n\n    #### Public 서브넷 (PublicSubnetACL)\n\n    - **인바운드 규칙**:\n\n        - Rule #100: `Allow`, Protocol: `TCP`, Port range: `80`, Source: `0.0.0.0/0` (모든 IP)\n\n        - Rule #110: `Allow`, Protocol: `TCP`, Port range: `443`, Source: `0.0.0.0/0` (모든 IP)\n\n    - **아웃바운드 규칙**:\n\n        - Rule #100: `Allow`, Protocol: `TCP`, Port range: `80`, Destination: `0.0.0.0/0`\n\n        - Rule #110: `Allow`, Protocol: `TCP`, Port range: `443`, Destination: `0.0.0.0/0`\n\n        - Rule #120: `Allow`, Protocol: `TCP`, Port range: `1024-65535`, Destination: `0.0.0.0/0`\n\n    #### Private 서브넷 (PrivateSubnetACL)\n\n    - **인바운드 규칙**:\n\n        - Rule #100: `Allow`, Protocol: `TCP`, Port range: `5432`, Source: `10.0.0.0/16` (프라이빗 서브넷이 위치한 VPC의 CIDR 블록, 내부 통신 허용)\n\n        - Rule #110: `Allow`, Protocol: `TCP`, Port range: `1024-65535`, Source: `10.0.0.0/16` (이 규칙은 내부 통신을 위한 포트 범위 허용)\n\n    - **아웃바운드 규칙**:\n\n        - Rule #100: `Allow`, Protocol: `TCP`, Port range: `80`, Destination: `0.0.0.0/0`\n\n        - Rule #110: `Allow`, Protocol: `TCP`, Port range: `443`, Destination: `0.0.0.0/0`\n\n        - Rule #120: `Allow`, Protocol: `TCP`, Port range: `1024-65535`, Destination: `0.0.0.0/0`\n\n## RDS\n\n### PostgreSQL RDS 인스턴스 생성\n\n1. RDS 서비스로 이동: 상단 검색창에서 \"RDS\"를 검색하여 RDS 대시보드로 이동합니다.\n\n1. DB 인스턴스 생성:\n\n    - RDS 대시보드에서 \"Databases\"를 클릭합니다.\n\n    - \"Create database\" 버튼을 클릭합니다.\n\n    - **데이터베이스 생성 방법**: \"Standard Create\"를 선택합니다.\n\n    - **엔진 옵션**: \"PostgreSQL\"을 선택합니다.\n\n    - **버전**: 원하는 PostgreSQL 버전을 선택합니다.\n\n1. DB 인스턴스 설정:\n\n    - DB 인스턴스 식별자: 인스턴스의 이름을 입력합니다 (예: `MyPostgreSQLDB`).\n\n    - 마스터 사용자 이름: 기본 관리자 계정 이름을 입력합니다 (예: `admin`).\n\n    - 마스터 암호: 관리자 계정의 암호를 입력합니다.\n\n1. 인스턴스 사양:\n\n    - DB 인스턴스 클래스: 요구사항에 맞는 인스턴스 클래스를 선택합니다 (예: `db.t3.micro`).\n\n    - 스토리지 유형: 일반적인 용도로 \"General Purpose (SSD)\"를 선택합니다.\n\n    - 할당된 스토리지: 필요에 따라 스토리지 크기를 설정합니다 (예: 20GB).\n\n1. 가용성 및 내구성:\n\n    - 필요에 따라 Multi-AZ 배포를 설정합니다 (고가용성을 위해 권장).\n\n1. 네트워크 및 보안:\n\n    - VPC 선택: 생성한 VPC (`MyVPC`)를 선택합니다.\n\n    - 서브넷 그룹: RDS 서브넷 그룹을 선택합니다 (RDS 인스턴스를 배치할 서브넷 그룹).\n\n    - 퍼블릭 액세스 가능성: \"No\"를 선택하여 인스턴스를 프라이빗 서브넷에 배치합니다.\n\n    - 보안 그룹: 이전에 생성한 `DBSG` 보안 그룹을 선택합니다.\n\n1. 데이터베이스 옵션:\n\n    - 기본 설정을 사용하거나, 필요에 따라 데이터베이스 이름을 입력합니다.\n\n1. 백업 설정:\n\n    - 자동 백업: 자동 백업을 활성화하고 보관 기간을 설정합니다 (예: 7일).\n\n1. 모니터링:\n\n    - 필요에 따라 Amazon CloudWatch Enhanced Monitoring을 활성화합니다.\n\n1. 암호화:\n\n    - 필요에 따라 암호화를 활성화합니다.\n\n1. 추가 구성:\n\n    - 필요에 따라 로깅 및 유지관리 옵션을 설정합니다.\n\n1. DB 인스턴스 생성:\n\n    - 모든 설정을 확인한 후 \"Create database\" 버튼을 클릭합니다.\n\n## 마무리\n\n이로써 DB를 포함한 서버 인프라가 일단 구성됐습니다. 다음 글에서는 이미지를 빌드해서 Push한 후, ECS에 배포를 해보겠습니다. \n\n\n\n"},{"excerpt":"DRF에서 다양한 HTTP 메서드(, , , , )를 처리하기 위한 여러 가지 API 뷰 클래스들을 소개하고, 이를 효과적으로 사용하는 방법을 안내하겠습니다. 또한, 를 활용하여 직접 뷰를 구현하는 방법도 다룹니다. DRF의 기본 뷰 클래스들을 활용한 구현 공통 속성들에 대한 설명 serializer_class 는 뷰에서 사용할 시리얼라이저 클래스를 지정…","fields":{"slug":"/DRF의-API-View/"},"frontmatter":{"date":"June 05, 2024","title":"DRF의 API View","tags":["Django","Python","DRF"]},"rawMarkdownBody":"![](image1.png)\nDRF에서 다양한 HTTP 메서드(`GET`, `POST`, `PUT`, `PATCH`, `DELETE`)를 처리하기 위한 여러 가지 API 뷰 클래스들을 소개하고, 이를 효과적으로 사용하는 방법을 안내하겠습니다. 또한, `GenericAPIView`를 활용하여 직접 뷰를 구현하는 방법도 다룹니다.\n\n## DRF의 기본 뷰 클래스들을 활용한 구현\n\n### 공통 속성들에 대한 설명\n\n#### serializer\\_class\n\n`serializer_class`는 뷰에서 사용할 시리얼라이저 클래스를 지정합니다. 시리얼라이저는 Django 모델 인스턴스를 JSON으로 변환하거나, 클라이언트로부터 받은 JSON 데이터를 Django 모델 인스턴스로 변환하는 역할을 합니다.\n\n```python\nclass CustomUserSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = CustomUser\n        fields = ['id', 'username', 'nickname', 'dob', 'avatar']\n```\n\n#### queryset\n\n`queryset`은 뷰에서 처리할 기본 쿼리셋을 지정합니다. 이는 데이터베이스에서 특정 모델의 인스턴스를 선택하는 데 사용됩니다.\n\n```python\nqueryset = CustomUser.objects.all()\n```\n\n#### permission\\_classes\n\n`permission_classes`는 뷰에 대한 접근 권한을 제어합니다. 예를 들어, 인증된 사용자만 접근할 수 있도록 설정할 수 있습니다.\n\n```python\npermission_classes = [IsAuthenticated]\n```\n\n### UpdateAPIView: PUT, PATCH 요청 처리\n\n`UpdateAPIView`는 객체를 업데이트하는 데 사용됩니다. `PUT` 요청은 객체의 모든 필드를 업데이트하고, `PATCH` 요청은 일부 필드만 업데이트합니다.\n\n```python\nfrom rest_framework import generics\nfrom rest_framework.permissions import IsAuthenticated\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass UpdateCustomUserView(generics.UpdateAPIView):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get_object(self):\n        return self.request.user\n```\n\n### CreateAPIView: POST 요청 처리\n\n`CreateAPIView`는 새로운 객체를 생성하는 데 사용됩니다.\n\n```python\nfrom rest_framework import generics\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass CreateCustomUserView(generics.CreateAPIView):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n```\n\n### RetrieveAPIView: GET 요청 처리\n\n`RetrieveAPIView`는 단일 객체를 가져오는 데 사용됩니다.\n\n```python\nfrom rest_framework import generics\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass RetrieveCustomUserView(generics.RetrieveAPIView):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get_object(self):\n        return self.request.user\n```\n\n### DestroyAPIView: DELETE 요청 처리\n\n`DestroyAPIView`는 객체를 삭제하는 데 사용됩니다.\n\n```python\nfrom rest_framework import generics\nfrom .models import CustomUser\n\nclass DestroyCustomUserView(generics.DestroyAPIView):\n    queryset = CustomUser.objects.all()\n    permission_classes = [IsAuthenticated]\n\n    def get_object(self):\n        return self.request.user\n```\n\n### ListAPIView: 다수의 객체를 GET 요청으로 가져오기\n\n`ListAPIView`는 여러 객체를 리스트 형태로 가져오는 데 사용됩니다.\n\n```python\nfrom rest_framework import generics\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass ListCustomUserView(generics.ListAPIView):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n```\n\n## GenericAPIView와 Mixins를 활용한 구현\n\nDRF는 `GenericAPIView`와 여러 `mixins`를 제공하여 보다 유연하게 뷰를 구성할 수 있습니다. 이를 통해 기본 제공되는 뷰 클래스를 사용하지 않고도 비슷한 기능을 구현할 수 있습니다.\n\n### mixins?\n\nDjango REST Framework(DRF)에서 `mixins`는 뷰 클래스에 특정한 기능을 추가하는 작은 모듈입니다. 여러 `mixins`를 조합하여 하나의 뷰에서 다양한 기능을 구현할 수 있습니다. DRF의 `mixins`는 특히 `GenericAPIView`와 함께 사용되어 CRUD(Create, Read, Update, Delete) 작업을 수행하는 데 유용합니다.\n\n### UpdateAPIView를 GenericAPIView로 구현\n\n`GenericAPIView`와 `UpdateModelMixin`을 사용하여 `PUT`과 `PATCH` 요청을 처리합니다.\n\n```python\nfrom rest_framework import generics, mixins\nfrom rest_framework.permissions import IsAuthenticated\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass UpdateCustomUserView(generics.GenericAPIView, mixins.UpdateModelMixin):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get_object(self):\n        return self.request.user\n\n    def patch(self, request, *args, **kwargs):\n        return self.partial_update(request, *args, **kwargs)\n\n    def put(self, request, *args, **kwargs):\n        return self.update(request, *args, **kwargs)\n```\n\n### CreateAPIView를 GenericAPIView로 구현\n\n`GenericAPIView`와 `CreateModelMixin`을 사용하여 `POST` 요청을 처리합니다.\n\n```python\nfrom rest_framework import generics, mixins\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass CreateCustomUserView(generics.GenericAPIView, mixins.CreateModelMixin):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n\n    def post(self, request, *args, **kwargs):\n        return self.create(request, *args, **kwargs)\n```\n\n### RetrieveAPIView를 GenericAPIView로 구현\n\n`GenericAPIView`와 `RetrieveModelMixin`을 사용하여 `GET` 요청을 처리합니다.\n\n```python\nfrom rest_framework import generics, mixins\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass RetrieveCustomUserView(generics.GenericAPIView, mixins.RetrieveModelMixin):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get_object(self):\n        return self.request.user\n\n    def get(self, request, *args, **kwargs):\n        return self.retrieve(request, *args, **kwargs)\n```\n\n### DestroyAPIView를 GenericAPIView로 구현\n\n`GenericAPIView`와 `DestroyModelMixin`을 사용하여 `DELETE` 요청을 처리합니다.\n\n```python\nfrom rest_framework import generics, mixins\nfrom .models import CustomUser\n\nclass DestroyCustomUserView(generics.GenericAPIView, mixins.DestroyModelMixin):\n    queryset = CustomUser.objects.all()\n    permission_classes = [IsAuthenticated]\n\n    def get_object(self):\n        return self.request.user\n\n    def delete(self, request, *args, **kwargs):\n        return self.destroy(request, *args, **kwargs)\n```\n\n### ListAPIView를 GenericAPIView로 구현\n\n`GenericAPIView`와 `ListModelMixin`을 사용하여 여러 객체를 가져오는 `GET` 요청을 처리합니다.\n\n```python\nfrom rest_framework import generics, mixins\nfrom .models import CustomUser\nfrom .serializers import CustomUserSerializer\n\nclass ListCustomUserView(generics.GenericAPIView, mixins.ListModelMixin):\n    queryset = CustomUser.objects.all()\n    serializer_class = CustomUserSerializer\n\n    def get(self, request, *args, **kwargs):\n        return self.list(request, *args, **kwargs)\n```\n\n"},{"excerpt":"서론 Django에서 사용자 인증 시스템을 커스터마이징해야 하는 경우가 있습니다. 이때 Django는 두 가지 주요 클래스(와 )를 제공하여 커스텀 유저 모델을 쉽게 만들 수 있게 해줍니다.  이 글에서는 이 두 클래스의 차이점과 각각의 사용 예시를 소개하겠습니다. Django 사용자 인증 시스템 기본 모델과 필드 Django는 기본적으로  모델을 제공하…","fields":{"slug":"/Django의-사용자-인증-시스템-커스터마이징/"},"frontmatter":{"date":"June 03, 2024","title":"Django의 사용자 인증 시스템 커스터마이징","tags":["Django","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nDjango에서 사용자 인증 시스템을 커스터마이징해야 하는 경우가 있습니다. 이때 Django는 두 가지 주요 클래스(`AbstractUser`와 `AbstractBaseUser`)를 제공하여 커스텀 유저 모델을 쉽게 만들 수 있게 해줍니다.  이 글에서는 이 두 클래스의 차이점과 각각의 사용 예시를 소개하겠습니다.\n\n## Django 사용자 인증 시스템\n\n### 기본 모델과 필드\n\nDjango는 기본적으로 `User` 모델을 제공하여 사용자 정보를 저장하고 관리합니다. `User` 모델에는 다음과 같은 필드가 포함되어 있습니다:\n\n- `username`: 사용자 이름 (필수)\n\n- `password`: 암호화된 비밀번호 (필수)\n\n- `email`: 이메일 주소 (선택)\n\n- `first_name`: 이름 (선택)\n\n- `last_name`: 성 (선택)\n\n- 기타 인증 및 권한 관련 필드 (`is_staff`, `is_superuser`, `is_active` 등)\n\n### 인증 뷰와 URL\n\nDjango는 사용자 인증과 관련된 기본 뷰를 제공합니다. 이 뷰들은 `django.contrib.auth.views` 모듈에 포함되어 있으며, 다음과 같은 뷰들이 있습니다:\n\n- `LoginView`: 사용자가 로그인할 수 있는 뷰\n\n- `LogoutView`: 사용자가 로그아웃할 수 있는 뷰\n\n- `PasswordChangeView`: 사용자가 비밀번호를 변경할 수 있는 뷰\n\n- `PasswordResetView`: 사용자가 비밀번호를 재설정할 수 있는 뷰\n\n이 뷰들은 설정된 URL에 매핑하여 사용할 수 있습니다.\n\n### Form Class\n\nDjango는 사용자 인증과 관련된 여러 폼 클래스를 제공하여 사용자 입력을 처리합니다. 대표적인 폼 클래스로는 다음이 있습니다:\n\n- `UserCreationForm`: 새로운 사용자를 생성하는 폼\n\n- `AuthenticationForm`: 사용자가 로그인할 때 사용하는 폼\n\n- `PasswordChangeForm`: 사용자가 비밀번호를 변경할 때 사용하는 폼\n\n- `PasswordResetForm`: 사용자가 비밀번호를 재설정할 때 사용하는 폼\n\n## AbstractUser\n\n`AbstractUser`는 Django의 기본 User 모델을 확장하는 가장 간단한 방법입니다. 이 클래스는 기본 User 모델의 모든 필드와 메서드를 상속하므로, 추가적인 필드를 정의하거나 메서드를 추가하는 데 적합합니다.\n\n### 장점\n\n- 기본 필드 제공: `AbstractUser`는 Django의 기본 User 모델에서 제공하는 모든 필드를 포함합니다. 이를 통해 사용자 이름, 이메일, 비밀번호, 이름 및 성과 같은 필드를 쉽게 사용할 수 있습니다.\n\n- 편리한 확장성: 기존 필드를 유지하면서 필요한 추가 필드만 확장할 수 있어, 대부분의 기본적인 사용자 정보 요구 사항을 충족할 수 있습니다.\n\n- 간단한 설정: 커스터마이징이 쉬우며, 기본적인 인증 기능을 사용하기 위한 추가적인 설정이 거의 필요하지 않습니다.\n\n### 단점\n\n- 제한된 유연성: 기본 User 모델의 필드와 구조를 유지해야 하므로, 사용자 모델을 처음부터 완전히 재정의하고자 할 때는 적합하지 않습니다.\n\n- 불필요한 필드: 모든 기본 필드가 포함되어 있으므로, 불필요한 필드가 있을 수 있습니다. 예를 들어, 이메일을 기본 로그인 필드로 사용하고 싶을 때 사용자 이름 필드가 불필요하게 포함될 수 있습니다.\n\n### 예제\n\n```python\n# models.py\nfrom django.contrib.auth.models import AbstractUser\nfrom django.db import models\n\nclass CustomUser(AbstractUser):\n    date_of_birth = models.DateField(null=True, blank=True)\n    profile_picture = models.ImageField(upload_to='profiles/', null=True, blank=True)\n\n# settings.py\nAUTH_USER_MODEL = 'yourapp.CustomUser'\n```\n\n## AbstractBaseUser\n\n`AbstractBaseUser`는 사용자 모델을 처음부터 완전히 커스터마이징하고자 할 때 사용합니다. 이 클래스는 인증에 필요한 기본 필드와 메서드만 제공하며, 나머지 필드는 직접 정의해야 합니다.\n\n### 장점\n\n- 완전한 커스터마이징: `AbstractBaseUser`를 사용하면 사용자 모델의 모든 필드를 처음부터 완전히 정의할 수 있습니다. 이를 통해 로그인 필드, 필수 필드 등을 자유롭게 설정할 수 있습니다.\n\n- 유연성: 사용자 모델의 구조와 필드를 자유롭게 설정할 수 있어, 복잡한 요구 사항이나 특정 비즈니스 로직을 구현하는 데 매우 유용합니다.\n\n- 효율성: 필요한 필드만 포함시킬 수 있으므로, 모델의 간결성과 효율성을 유지할 수 있습니다.\n\n### 단점\n\n- 복잡한 설정: 모든 필드와 매니저를 직접 정의해야 하므로, 설정 과정이 복잡하고 시간이 많이 걸릴 수 있습니다. 특히, 커스텀 사용자 매니저를 작성해야 합니다.\n\n- 높은 진입 장벽: 초보자에게는 설정과 구현이 어려울 수 있으며, 기본 User 모델을 사용하는 것보다 더 많은 학습이 필요합니다.\n\n### 예제\n\n```python\n# models.py\nfrom django.contrib.auth.models import AbstractBaseUser, BaseUserManager\nfrom django.db import models\n\nclass CustomUserManager(BaseUserManager):\n    def create_user(self, email, password=None, **extra_fields):\n        if not email:\n            raise ValueError('The Email field must be set')\n        email = self.normalize_email(email)\n        user = self.model(email=email, **extra_fields)\n        user.set_password(password)\n        user.save(using=self._db)\n        return user\n\n    def create_superuser(self, email, password=None, **extra_fields):\n        extra_fields.setdefault('is_staff', True)\n        extra_fields.setdefault('is_superuser', True)\n\n        return self.create_user(email, password, **extra_fields)\n\nclass CustomUser(AbstractBaseUser):\n    email = models.EmailField(unique=True)\n    first_name = models.CharField(max_length=30, blank=True)\n    last_name = models.CharField(max_length=30, blank=True)\n    is_active = models.BooleanField(default=True)\n    is_staff = models.BooleanField(default=False)\n    is_superuser = models.BooleanField(default=False)\n\n    objects = CustomUserManager()\n\n    USERNAME_FIELD = 'email'\n    REQUIRED_FIELDS = []\n\n    def __str__(self):\n        return self.email\n\n# settings.py\nAUTH_USER_MODEL = 'yourapp.CustomUser'\n```\n\n\n\n"},{"excerpt":"Install 맥에서는 다음의 명령어로 간단하게 Go를 설치할 수 있습니다.  설치 확인 다음의 명령어로 설치를 확인합니다.  또는 다음의 명령어로 설치를 확인합니다.  Go의 Workspace Go의 워크스페이스와 프로젝트 관리는 다른 언어와 비교했을 때 독특한 면이 있습니다. Go의 워크스페이스는 Go 언어에서 소스 코드와 컴파일된 바이너리, 패키지를…","fields":{"slug":"/Go-입문/"},"frontmatter":{"date":"May 28, 2024","title":"Go 입문","tags":["Go","OAuth2"]},"rawMarkdownBody":"![](image1.png)\n## Install\n\n맥에서는 다음의 명령어로 간단하게 Go를 설치할 수 있습니다. \n\n```bash\nbrew install go\n```\n\n### 설치 확인\n\n다음의 명령어로 설치를 확인합니다. \n\n```bash\n$ go version\ngo version go1.22.3 darwin/arm64\n```\n\n또는 다음의 명령어로 설치를 확인합니다. \n\n```bash\n$ go env\nGO111MODULE=''\nGOARCH='arm64'\nGOBIN=''\nGOCACHE='/Users/furychick/Library/Caches/go-build'\nGOENV='/Users/furychick/Library/Application Support/go/env'\nGOEXE=''\nGOEXPERIMENT=''\nGOFLAGS=''\nGOHOSTARCH='arm64'\nGOHOSTOS='darwin'\nGOINSECURE=''\nGOMODCACHE='/Users/furychick/go/pkg/mod'\nGONOPROXY=''\nGONOSUMDB=''\nGOOS='darwin'\nGOPATH='/Users/furychick/go'\nGOPRIVATE=''\nGOPROXY='https://proxy.golang.org,direct'\nGOROOT='/opt/homebrew/Cellar/go/1.22.3/libexec'\nGOSUMDB='sum.golang.org'\nGOTMPDIR=''\nGOTOOLCHAIN='auto'\nGOTOOLDIR='/opt/homebrew/Cellar/go/1.22.3/libexec/pkg/tool/darwin_arm64'\nGOVCS=''\nGOVERSION='go1.22.3'\nGCCGO='gccgo'\nAR='ar'\nCC='cc'\nCXX='c++'\nCGO_ENABLED='1'\nGOMOD='/dev/null'\nGOWORK=''\nCGO_CFLAGS='-O2 -g'\nCGO_CPPFLAGS=''\nCGO_CXXFLAGS='-O2 -g'\nCGO_FFLAGS='-O2 -g'\nCGO_LDFLAGS='-O2 -g'\nPKG_CONFIG='pkg-config'\nGOGCCFLAGS='-fPIC -arch arm64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/55/rk2pgkpj32d494v1hkj7j5zc0000gn/T/go-build2326145907=/tmp/go-build -gno-record-gcc-switches -fno-common'\n```\n\n## Go의 Workspace\n\nGo의 워크스페이스와 프로젝트 관리는 다른 언어와 비교했을 때 독특한 면이 있습니다. Go의 워크스페이스는 Go 언어에서 소스 코드와 컴파일된 바이너리, 패키지를 관리하는 디렉터리 구조를 의미합니다. \n\n### **기본 워크스페이스 구조**\n\nGo의 워크스페이스는 보통 세 가지 주요 디렉터리로 구성됩니다:\n\n- src: 소스 코드 파일이 위치하는 디렉터리입니다. 각 프로젝트는 src 디렉터리 하위에 폴더로 존재하며, 폴더 구조는 패키지 경로를 반영합니다.\n\n- pkg: 패키지가 컴파일된 결과물이 저장되는 디렉터리입니다. 이는 src 디렉터리의 코드가 컴파일될 때 생성되는 파일들이 위치하는 곳입니다.\n\n- bin: 실행 파일이 저장되는 디렉터리입니다. src 디렉터리의 코드가 컴파일되어 생성된 실행 파일이 여기 저장됩니다.\n\n## **Go Modules**\n\nGo 1.11 이후로는 Go Modules가 도입되어, GOPATH의 중요성이 줄어들었습니다. Go Modules는 프로젝트별로 의존성을 관리할 수 있도록 하며, 프로젝트 디렉터리 내에서 독립적으로 동작합니다. Go Modules를 사용하면 go.mod 파일을 통해 의존성을 명시하고 관리할 수 있습니다. \n\n고로, 예전처럼 반드시 워크스페이스 안에 프로젝트가 위치할 필요가 없으며 여타 언어처럼 아무 곳에나 프로젝트를 생성할 수 있습니다. \n\n### 예제 프로젝트 생성\n\n#### 프로젝트 디렉토리 생성\n\n```bash\nmkdir myproject\ncd myproject\n```\n\n### 모듈 초기화\n\n```bash\ngo mod init myproject\n```\n\n### 의존성 관리\n\n```bash\ngo mod tidy\n```\n\n`go mod tidy` 명령어는 Go Modules와 관련된 명령어로, `go.mod` 파일과 `go.sum` 파일을 정리하고 최신 상태로 유지하는 데 사용됩니다. 이 명령어는 다음과 같은 작업을 수행합니다:\n\n- 사용되지 않는 의존성 제거: 프로젝트 코드에서 더 이상 사용되지 않는 패키지를 `go.mod`와 `go.sum` 파일에서 제거합니다.\n\n- 필요한 의존성 추가: 코드에서 사용하고 있지만 `go.mod`에 명시되지 않은 패키지를 찾아서 추가합니다.\n\n- 정확한 버전 관리: 모든 의존성이 올바른 버전으로 명시되어 있는지 확인하고, 이를 `go.sum` 파일에 기록합니다.\n\n### 실행\n\n```bash\ngo run main.go\n```\n\n`go run` 명령어는 지정된 Go 소스 파일을 컴파일하고 즉시 실행합니다. \n\n컴파일된 실행 파일을 디스크에 저장하지 않고, 메모리에서 직접 실행합니다. 따라서 빠르게 코드를 테스트하거나 간단한 스크립트를 실행하는 데 유용합니다.\n\n배포하거나 다른 시스템에서 실행할 프로그램을 만들 때에는 `go build` 명령어를 사용해야 합니다. \n\n## Hello World 찍어보기\n\n### `main.go`  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello, World!\")\n}\n```\n\n### 파일 실행\n\n```bash\n$ go run main.go\n```\n\n## Go의 메서드 선언 기본 형식\n\nGo 언어에서 메서드는 다음과 같은 형식을 가집니다. \n\n```go\nfunc (receiver ReceiverType) MethodName(parameters) (returnTypes) {\n    // 메서드 본문\n}\n```\n\nreceiver: 메서드가 호출될 때, 해당 타입의 인스턴스를 참조할 수 있는 변수입니다.\n\nReceiverType: 메서드가 속하는 타입입니다. 구조체 타입일 때가 많지만, 기본 타입일 수도 있습니다.\n\nMethodName: 메서드의 이름입니다.\n\nparameters: 메서드에 전달되는 인수입니다.\n\nreturnTypes: 메서드가 반환하는 값의 타입입니다.\n\n### receiver(수신자)\n\n수신자는 메서드가 어떤 타입에 속하는지 지정합니다. 수신자는 값 수신자와 포인터 수신자로 나눌 수 있습니다.\n\n#### **값 수신자(Value Receiver)**\n\n값 수신자는 메서드가 호출될 때, 수신자의 복사본을 사용합니다. 수신자의 필드를 변경하더라도 원본에는 영향을 미치지 않습니다.\n\n```go\ntype Person struct {\n    Name string\n}\n\n// 값 수신자 메서드\nfunc (p Person) Greet() string {\n    return \"Hello, \" + p.Name\n}\n```\n\n#### **포인터 수신자(Pointer Receiver)**\n\n포인터 수신자는 메서드가 호출될 때, 수신자의 포인터를 사용합니다. 수신자의 필드를 변경하면 원본에도 영향을 미칩니다.\n\n```go\ntype Person struct {\n    Name string\n}\n\n// 포인터 수신자 메서드\nfunc (p *Person) SetName(name string) {\n    p.Name = name\n}\n```\n\n#### 예시\n\n`Person` 구조체를 사용하여 값 수신자와 포인터 수신자의 예시입니다. \n\n```go\npackage main\n\nimport \"fmt\"\n\n// Person 구조체 정의\ntype Person struct {\n    Name string\n    Age  int\n}\n\n// 값 수신자 메서드\nfunc (p Person) Greet() string {\n    return \"Hello, \" + p.Name\n}\n\n// 포인터 수신자 메서드\nfunc (p *Person) SetAge(age int) {\n    p.Age = age\n}\n\nfunc main() {\n    // Person 인스턴스 생성\n    person := Person{Name: \"Alice\", Age: 30}\n\n    // 값 수신자 메서드 호출\n    greeting := person.Greet()\n    fmt.Println(greeting) // \"Hello, Alice\"\n\n    // 포인터 수신자 메서드 호출\n    person.SetAge(35)\n    fmt.Println(person.Age) // 35\n}\n```\n\n위 예시에서 `Greet` 메서드는 값 수신자를 사용하여 `Person`의 이름을 반환합니다. 반면, `SetAge` 메서드는 포인터 수신자를 사용하여 `Person`의 나이를 변경합니다.]\n\n### **메서드와 함수의 차이점**\n\nGo에서 메서드와 일반 함수의 주요 차이점은 메서드는 특정 타입에 속한다는 점입니다. 일반 함수는 특정 타입에 속하지 않으며, 메서드는 특정 타입의 인스턴스에서 호출됩니다.\n\n#### **일반 함수 예시**\n\n```go\nfunc Greet(name string) string {\n    return \"Hello, \" + name\n}\n```\n\n#### **메서드 예시**\n\n```go\ntype Person struct {\n    Name string\n}\n\nfunc (p Person) Greet() string {\n    return \"Hello, \" + p.Name\n}\n```\n\n## 파이썬과의 비교로 Go 이해하기\n\n클래스(구조체)의 메소드가 클래스(구조체) 바깥에서 정의되는 느낌이라고 이해했습니다. 파이썬과 비교하며 코드를 다시 살펴보겠습니다. \n\n### 구조체(클래스) 정의\n\n#### 파이썬\n\n클래스 내부에 초기화 메서드(`__init__`)를 정의합니다.\n\n```python\nclass Person:\n    def __init__(self, name):\n        self.name = name\n```\n\n#### Go\n\n구조체를 정의하여 데이터를 저장합니다.\n\n```go\ntype Person struct {\n    Name string\n}\n```\n\n### 메서드 정의\n\n#### 파이썬\n\n파이썬에서는 클래스 내부에서 메서드를 정의합니다. 메서드는 클래스의 인스턴스에서 호출되며, `self` 키워드를 사용하여 인스턴스 변수와 다른 메서드에 접근합니다.\n\n```python\nclass Person:\n    def __init__(self, name):\n        self.name = name\n\n    def greet(self):\n        return f\"Hello, {self.name}\"\n\nperson = Person(\"Alice\")\nprint(person.greet())  # \"Hello, Alice\"\n```\n\n#### Go\n\nGo에서는 메서드를 구조체 바깥에서 정의하지만, 해당 구조체와 연관시키기 위해 수신자(Receiver)를 사용합니다. 수신자는 메서드가 호출될 때, 해당 구조체의 인스턴스를 참조할 수 있게 해줍니다.\n\n```go\npackage main\n\nimport \"fmt\"\n\n// Person 구조체 정의\ntype Person struct {\n    Name string\n}\n\n// 수신자를 사용하는 메서드 정의\nfunc (p Person) Greet() string {\n    return \"Hello, \" + p.Name\n}\n\nfunc main() {\n    person := Person{Name: \"Alice\"}\n    fmt.Println(person.Greet())  // \"Hello, Alice\"\n}\n```\n\n### 메서드 호출\n\n#### 파이썬\n\n인스턴스를 생성하고 메서드를 호출합니다.\n\n```python\nperson = Person(\"Alice\")\nprint(person.greet())  # \"Hello, Alice\"\n```\n\n#### Go\n\n구조체 인스턴스를 생성하고 메서드를 호출합니다.\n\n```go\nperson := Person{Name: \"Alice\"}\nfmt.Println(person.Greet())  // \"Hello, Alice\"\n```\n\n"},{"excerpt":"필요 라이브러리 설치 Go 프로젝트를 생성한 후 OAuth 인증을 위한 라이브러리를 설치해야 합니다.  Server.go 작성 기본 그리고 나서 해당 파일을 run 합니다.  이 이후에 http://localhost:9096/token?grant_type=client_credentials&client_id=000000&client_secret=999999…","fields":{"slug":"/Go를-활용한-OAuth-20-구현/"},"frontmatter":{"date":"May 28, 2024","title":"Go를 활용한 OAuth 2.0 구현","tags":["Go","OAuth2"]},"rawMarkdownBody":"![](image1.png)\n## 필요 라이브러리 설치\n\n[Go 프로젝트를 생성](https://sharknia.github.io/Go-입문)한 후 OAuth 인증을 위한 라이브러리를 설치해야 합니다. \n\n```bash\n$ go get -u -v github.com/go-oauth2/oauth2/v4/...\n```\n\n## Server.go 작성\n\n### 기본\n\n```go\npackage main\n\nimport (\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/go-oauth2/oauth2/v4/errors\"\n\t\"github.com/go-oauth2/oauth2/v4/manage\"\n\t\"github.com/go-oauth2/oauth2/v4/models\"\n\t\"github.com/go-oauth2/oauth2/v4/server\"\n\t\"github.com/go-oauth2/oauth2/v4/store\"\n)\n\nfunc main() {\n\tmanager := manage.NewDefaultManager()\n\t// token memory store\n\tmanager.MustTokenStorage(store.NewMemoryTokenStore())\n\n\t// client memory store\n\tclientStore := store.NewClientStore()\n\tclientStore.Set(\"000000\", &models.Client{\n\t\tID:     \"000000\",\n\t\tSecret: \"999999\",\n\t\tDomain: \"http://localhost\",\n\t})\n\tmanager.MapClientStorage(clientStore)\n\n\tsrv := server.NewDefaultServer(manager)\n\tsrv.SetAllowGetAccessRequest(true)\n\tsrv.SetClientInfoHandler(server.ClientFormHandler)\n\n\tsrv.UserAuthorizationHandler = func(w http.ResponseWriter, r *http.Request) (userID string, err error) {\n\t\treturn \"000000\", nil\n\t}\n\n\tsrv.SetInternalErrorHandler(func(err error) (re *errors.Response) {\n\t\tlog.Println(\"Internal Error:\", err.Error())\n\t\treturn\n\t})\n\n\tsrv.SetResponseErrorHandler(func(re *errors.Response) {\n\t\tlog.Println(\"Response Error:\", re.Error.Error())\n\t})\n\n\thttp.HandleFunc(\"/authorize\", func(w http.ResponseWriter, r *http.Request) {\n\t\terr := srv.HandleAuthorizeRequest(w, r)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t}\n\t})\n\n\thttp.HandleFunc(\"/token\", func(w http.ResponseWriter, r *http.Request) {\n\t\tsrv.HandleTokenRequest(w, r)\n\t})\n\n\tlog.Fatal(http.ListenAndServe(\":9096\", nil))\n}\n```\n\n그리고 나서 해당 파일을 run 합니다. \n\n```bash\n$ go run server.go \n```\n\n이 이후에 [http://localhost:9096/token?grant\\_type=client\\_credentials&client\\_id=000000&client\\_secret=999999&scope=read](http://localhost:9096/token?grant_type=client_credentials&client_id=000000&client_secret=999999&scope=read)로 접속하면 token을 발급 받을 수 있습니다. \n\n### 심화 - JWT 토큰 생성 추가 \n\nServer.go를 다음과 같이 수정합니다.\n\n```go\npackage main\n\nimport (\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/go-oauth2/oauth2/v4/errors\"\n\t\"github.com/go-oauth2/oauth2/v4/generates\"\n\t\"github.com/go-oauth2/oauth2/v4/manage\"\n\t\"github.com/go-oauth2/oauth2/v4/models\"\n\t\"github.com/go-oauth2/oauth2/v4/server\"\n\t\"github.com/go-oauth2/oauth2/v4/store\"\n\t\"github.com/golang-jwt/jwt\"\n)\n\nfunc main() {\n\tmanager := manage.NewDefaultManager()\n\t// token memory store\n\tmanager.MustTokenStorage(store.NewMemoryTokenStore())\n\n\t// client memory store\n\tclientStore := store.NewClientStore()\n\tclientStore.Set(\"000000\", &models.Client{\n\t\tID:     \"000000\",\n\t\tSecret: \"999999\",\n\t\tDomain: \"http://localhost\",\n\t})\n\tmanager.MapClientStorage(clientStore)\n\n\tsrv := server.NewDefaultServer(manager)\n\tsrv.SetAllowGetAccessRequest(true)\n\tsrv.SetClientInfoHandler(server.ClientFormHandler)\n\n\tmanager.MapAccessGenerate(generates.NewJWTAccessGenerate(\"\", []byte(\"your_secret_key\"), jwt.SigningMethodHS512))\n\n\tsrv.UserAuthorizationHandler = func(w http.ResponseWriter, r *http.Request) (userID string, err error) {\n\t\treturn \"000000\", nil\n\t}\n\n\tsrv.SetInternalErrorHandler(func(err error) (re *errors.Response) {\n\t\tlog.Println(\"Internal Error:\", err.Error())\n\t\treturn\n\t})\n\n\tsrv.SetResponseErrorHandler(func(re *errors.Response) {\n\t\tlog.Println(\"Response Error:\", re.Error.Error())\n\t})\n\n\thttp.HandleFunc(\"/authorize\", func(w http.ResponseWriter, r *http.Request) {\n\t\terr := srv.HandleAuthorizeRequest(w, r)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t}\n\t})\n\n\thttp.HandleFunc(\"/token\", func(w http.ResponseWriter, r *http.Request) {\n\t\tsrv.HandleTokenRequest(w, r)\n\t})\n\n\tlog.Fatal(http.ListenAndServe(\":9096\", nil))\n}\n```\n\nMapAccessGenerate 메소드 부분이 추가되었습니다. `your_secret_key`는 jwt 토큰 발급에 사용되는 시크릿 키입니다. HS512 알고리즘을 사용하였습니다. \n\n이렇게 하면 access token이 HS512 알고리즘을 사용한 jwt 토큰으로 변경됩니다. \n\n### 심화 2 - JWT 토큰 커스텀\n\nServer.go를 다음과 같이 수정합니다. \n\n```go\npackage main\n\nimport (\n\t\"context\"\n\t\"log\"\n\t\"net/http\"\n\t\"time\"\n\n\t\"github.com/go-oauth2/oauth2/v4\"\n\t\"github.com/go-oauth2/oauth2/v4/errors\"\n\t\"github.com/go-oauth2/oauth2/v4/generates\"\n\t\"github.com/go-oauth2/oauth2/v4/manage\"\n\t\"github.com/go-oauth2/oauth2/v4/models\"\n\t\"github.com/go-oauth2/oauth2/v4/server\"\n\t\"github.com/go-oauth2/oauth2/v4/store\"\n\t\"github.com/golang-jwt/jwt\"\n)\n\n\ntype CustomJWTAccessGenerate struct {\n    generates.JWTAccessGenerate\n}\n\n// NewCustomJWTAccessGenerate creates a new CustomJWTAccessGenerate instance\nfunc NewCustomJWTAccessGenerate(secretKey []byte) *CustomJWTAccessGenerate {\n    return &CustomJWTAccessGenerate{\n        JWTAccessGenerate: generates.JWTAccessGenerate{\n\t\t\tSignedKeyID: \"\",\n            SignedKey: secretKey,\n            SignedMethod: jwt.SigningMethodHS512,\n        },\n    }\n}\n\n\nfunc (a *CustomJWTAccessGenerate) Token(ctx context.Context, data *oauth2.GenerateBasic, isGenRefresh bool) (string, string, error) {\n    now := time.Now()\n    exp := now.Add(data.TokenInfo.GetAccessExpiresIn())\n\n    // Create custom claims\n    claims := jwt.MapClaims{\n        \"iss\":    \"your_service_name\",\n        \"aud\":    data.Client.GetID(),\n        \"exp\":    exp.Unix(),\n        \"iat\":    now.Unix(),\n        \"sub\":    data.UserID,\n        \"custom\": \"custom_value\", // Add custom field here\n    }\n\n    token := jwt.NewWithClaims(a.SignedMethod, claims)\n    access, err := token.SignedString(a.SignedKey)\n    if err != nil {\n        return \"\", \"\", err\n    }\n\n    var refresh string\n    if isGenRefresh {\n        refresh = data.TokenInfo.GetRefresh()\n    }\n\n    return access, refresh, nil\n}\nfunc main() {\n\tmanager := manage.NewDefaultManager()\n\t// token memory store\n\tmanager.MustTokenStorage(store.NewMemoryTokenStore())\n\n\t// client memory store\n\tclientStore := store.NewClientStore()\n\tclientStore.Set(\"000000\", &models.Client{\n\t\tID:     \"000000\",\n\t\tSecret: \"999999\",\n\t\tDomain: \"http://localhost\",\n\t})\n\tmanager.MapClientStorage(clientStore)\n\n\tsrv := server.NewDefaultServer(manager)\n\tsrv.SetAllowGetAccessRequest(true)\n\tsrv.SetClientInfoHandler(server.ClientFormHandler)\n\n    manager.MapAccessGenerate(NewCustomJWTAccessGenerate([]byte(\"your_secret_key\")))\n\n\tsrv.UserAuthorizationHandler = func(w http.ResponseWriter, r *http.Request) (userID string, err error) {\n\t\treturn \"000000\", nil\n\t}\n\n\tsrv.SetInternalErrorHandler(func(err error) (re *errors.Response) {\n\t\tlog.Println(\"Internal Error:\", err.Error())\n\t\treturn\n\t})\n\n\tsrv.SetResponseErrorHandler(func(re *errors.Response) {\n\t\tlog.Println(\"Response Error:\", re.Error.Error())\n\t})\n\n\thttp.HandleFunc(\"/authorize\", func(w http.ResponseWriter, r *http.Request) {\n\t\terr := srv.HandleAuthorizeRequest(w, r)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusBadRequest)\n\t\t}\n\t})\n\n\thttp.HandleFunc(\"/token\", func(w http.ResponseWriter, r *http.Request) {\n\t\tsrv.HandleTokenRequest(w, r)\n\t})\n\n\tlog.Fatal(http.ListenAndServe(\":9096\", nil))\n}\n\n```\n\n`NewCustomJWTAccessGenerate` 를 선언하고, 해당 구조체를 사용해서 Token을 생성하는 것으로 코드를 수정해주었습니다. \n\n`Token` 메서드에서 원하는대로 JWT 토큰을 커스텀 할 수 있습니다. \n\n## 주의\n\n`oauth2` 라이브러리는 [이 라이브러리](https://github.com/golang-jwt/jwt)를 사용합니다. 해당 라이브러리는 최신 버전이 5.x로, 깃허브에서 설치법을 확인할 수 있습니다. 다만 oauth2 라이브러리에서는 3.2.2 버전을 사용하고 있어 저 설치법대로 설치를 한다면 충돌이 일어나 코드가 제대로 작동하지 않습니다. \n\noauth2 라이브러리를 설치한다면 3.2.2 버전이 제대로 설치되므로, 추가로 해당 라이브러리를 설치할 필요가 없습니다. \n\n## 참조\n\n[https://github.com/go-oauth2/oauth2/?tab=readme-ov-file](https://github.com/go-oauth2/oauth2/?tab=readme-ov-file)\n\n"},{"excerpt":"Serializers란? serializers는 DRF(Django REST Framwork)의 일부입니다. serializers는 Django 모델 인스턴스나 쿼리셋을 JSON과 같은 네이티브 데이터 타입으로 변환해주고(직렬화), 역으로 JSON 데이터를 Django 모델 인스턴스로 변환(역직렬화)하는 역할을 합니다. 왜 사용하나요? serializer…","fields":{"slug":"/Django의-Serializers/"},"frontmatter":{"date":"May 23, 2024","title":"Django의 Serializers","tags":["Django","Python"]},"rawMarkdownBody":"![](image1.png)\n## Serializers란?\n\nserializers는 DRF(Django REST Framwork)의 일부입니다. serializers는 Django 모델 인스턴스나 쿼리셋을 JSON과 같은 네이티브 데이터 타입으로 변환해주고(직렬화), 역으로 JSON 데이터를 Django 모델 인스턴스로 변환(역직렬화)하는 역할을 합니다.\n\n## 왜 사용하나요? \n\nserializers는 클라이언트와 서버 간의 데이터 교환을 쉽게 해줍니다. \n\n1. 데이터 직렬화: Django 모델 인스턴스나 쿼리셋을 JSON 등으로 변환하여 클라이언트에게 전송할 수 있습니다.\n\n1. 데이터 역직렬화: 클라이언트로부터 JSON 데이터를 받아 Django 모델 인스턴스로 변환하여 데이터베이스에 저장할 수 있습니다.\n\n1. 데이터 검증: 클라이언트로부터 받은 데이터를 저장하기 전에 검증하여 올바른 데이터만 데이터베이스에 저장하도록 합니다.\n\n## Serializers의 종류와 사용법\n\n### Serializer\n\n가장 기본적인 형태로, 필드와 메서드를 통해 직접 정의할 수 있습니다.\n\n각 필드를 명시적으로 정의하므로 데이터 구조가 매우 유연하지만, 코드의 양이 많아질 수 있습니다.\n\n```python\nfrom rest_framework import serializers\n\nclass UserSerializer(serializers.Serializer):\n    id = serializers.IntegerField(read_only=True)\n    email = serializers.EmailField()\n    dob = serializers.DateField()\n    gender = serializers.ChoiceField(choices=[(1, 'Male'), (2, 'Female')])\n    nickname = serializers.CharField(max_length=100)\n```\n\n### ModelSerializer\n\nDjango 모델과 자동으로 매핑되므로, 필드를 일일이 정의할 필요가 없습니다. 이를 통해 코드의 양을 줄이고, 가독성을 높일 수 있습니다. 모델의 모든 필드를 직렬화하고 싶다면,\n\n```python\nfrom rest_framework import serializers\nfrom .models import User\n\nclass UserProfileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = User\n        fields = \"__all__\"\n```\n\n이렇게 하면 모델에 정의된 모든 필드가 직렬화에 포함됩니다. 이 방법은 특히 모델의 필드가 자주 변경되거나 많은 필드를 포함하고 있을 때 유용합니다.\n\n물론 필드를 직접 지정해서 사용할 수도 있습니다. \n\n```python\nfrom rest_framework import serializers\nfrom .models import User\n\nclass UserProfileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\"]\n```\n\n### HyperlinkedModelSerializer\n\n`HyperlinkedModelSerializer`는 객체에 대한 하이퍼링크를 생성하여 응답에 포함시켜 리스트를 자동으로 생성해주는 역할을 할 수 있습니다. `ViewSet`과 `Router`를 함께 사용하면, Django REST Framework는 자동으로 API 엔드포인트를 생성해주며, 이를 통해 각 객체에 대한 하이퍼링크를 포함하는 JSON 응답을 만들 수 있습니다.\n\n#### Models.py\n\n먼저 다음과 같은 User Model이 있다고 합시다. \n\n```python\nfrom django.db import models\n\nclass User(models.Model):\n    email = models.EmailField(unique=True)\n    dob = models.DateField()\n    gender = models.IntegerField(choices=[(1, 'Male'), (2, 'Female')])\n    nickname = models.CharField(max_length=100)\n```\n\n#### **serializers.py**\n\n`HyperlinkedModelSerializer`를 사용하여 Serializer를 정의합니다.\n\n```python\nfrom rest_framework import serializers\nfrom .models import User\n\nclass UserProfileHyperlinkedSerializer(serializers.HyperlinkedModelSerializer):\n    class Meta:\n        model = User\n        fields = [\"url\", \"email\", \"dob\", \"gender\", \"nickname\"]\n        extra_kwargs = {\n            'url': {'view_name': 'user-detail', 'lookup_field': 'pk'}\n        }\n```\n\n여기서 `url` 필드는 각 `User` 객체를 참조하는 하이퍼링크를 포함합니다. `extra_kwargs`를 사용하여 `view_name`과 `lookup_field`를 지정할 수 있습니다. `view_name`은 이 URL이 참조하는 뷰의 이름을 지정하고, `lookup_field`는 URL을 생성할 때 사용할 필드를 지정합니다. 또한 `lookup_field`는 생략이 가능합니다. \n\n#### **views.py**\n\n`User` 모델에 대한 뷰셋을 정의합니다.\n\n```python\nfrom rest_framework import viewsets\nfrom .models import User\nfrom .serializers import UserProfileHyperlinkedSerializer\n\nclass UserViewSet(viewsets.ModelViewSet):\n    queryset = User.objects.all()\n    serializer_class = UserProfileHyperlinkedSerializer\n```\n\n#### **urls.py**\n\n적절한 URL 패턴을 설정합니다.\n\n```python\nfrom django.urls import path, include\nfrom rest_framework.routers import DefaultRouter\nfrom .views import UserViewSet\n\nrouter = DefaultRouter()\nrouter.register(r'users', UserViewSet)\n\nurlpatterns = [\n    path('', include(router.urls)),\n]\n```\n\n#### 결과\n\n이제 API를 시작하고 `http://example.com/users/`로 접속하면 다음과 같은 사용자 목록을 하이퍼링크로 포함한 JSON 응답을 받을 수 있습니다.\n\n```json\n[\n    {\n        \"url\": \"http://example.com/users/1/\",\n        \"email\": \"example1@example.com\",\n        \"dob\": \"1990-01-01\",\n        \"gender\": 1,\n        \"nickname\": \"example_user1\"\n    },\n    {\n        \"url\": \"http://example.com/users/2/\",\n        \"email\": \"example2@example.com\",\n        \"dob\": \"1992-02-02\",\n        \"gender\": 2,\n        \"nickname\": \"example_user2\"\n    }\n]\n```\n\n`http://example.com/users/1/`로 GET 요청을 보내면 다음과 같은 JSON 응답을 받습니다.\n\n```json\n{\n    \"url\": \"http://example.com/users/1/\",\n    \"email\": \"example1@example.com\",\n    \"dob\": \"1990-01-01\",\n    \"gender\": 1,\n    \"nickname\": \"example_user1\"\n}\n```\n\n이렇게 HyperlinkedModelSerializer를 활용해서 간단하게 리스트-상세페이지를 구현할 수 있습니다. \n\n## 다양한 관계 다루기\n\n먼저 모델을 미리 정의하겠습니다. \n\n```python\nfrom django.db import models\n\nclass User(models.Model):\n    email = models.EmailField(unique=True)\n    dob = models.DateField()\n    gender = models.IntegerField(choices=[(1, 'Male'), (2, 'Female')])\n    nickname = models.CharField(max_length=100)\n\nclass UserProfile(models.Model):\n    user = models.OneToOneField(User, on_delete=models.CASCADE, related_name='profile')\n    bio = models.TextField()\n    website = models.URLField()\n    \nclass Post(models.Model):\n    user = models.ForeignKey(User, related_name='posts', on_delete=models.CASCADE)\n    title = models.CharField(max_length=100)\n    content = models.TextField()\n    \nclass Group(models.Model):\n    name = models.CharField(max_length=100)\n    members = models.ManyToManyField(User, related_name='groups')\n```\n\nUser와 UserProfile은 1:1관계입니다. \n\nUser와 Post는 1:N 관계입니다. \n\nUser와 Group은 N:N 관계입니다. \n\n### 1:1 관계\n\n이는 다음과 같이 직렬화 할 수 있습니다. \n\n```python\nfrom rest_framework import serializers\nfrom .models import User, UserProfile\n\nclass UserProfileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserProfile\n        fields = [\"bio\", \"website\"]\n\nclass UserSerializer(serializers.ModelSerializer):\n    profile = UserProfileSerializer(read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\", \"profile\"]\n```\n\n`User`와 연결된 `UserProfile`이 있는 상황에서 `UserSerializer`를 사용하여 직렬화된 JSON 데이터는 다음과 같습니다:\n\n```json\n{\n    \"id\": 1,\n    \"email\": \"example@example.com\",\n    \"dob\": \"1990-01-01\",\n    \"gender\": 1,\n    \"nickname\": \"example_user\",\n    \"profile\": {\n        \"bio\": \"This is an example bio.\",\n        \"website\": \"https://example.com\"\n    }\n}\n```\n\n### 1:N 관계\n\n```python\nclass PostSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Post\n        fields = [\"id\", \"title\", \"content\"]\n\nclass UserSerializer(serializers.ModelSerializer):\n    posts = PostSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\", \"posts\"]\n```\n\n`Post` 모델과 `User` 모델이 1:N 관계를 가지고 있는 경우, `User`가 여러 `Post`를 가지고 있을 때 직렬화된 JSON의 예시는 다음과 같습니다.\n\n```json\n{\n    \"id\": 1,\n    \"email\": \"example@example.com\",\n    \"dob\": \"1990-01-01\",\n    \"gender\": 1,\n    \"nickname\": \"example_user\",\n    \"posts\": [\n        {\n            \"id\": 1,\n            \"title\": \"First Post\",\n            \"content\": \"This is the content of the first post.\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Second Post\",\n            \"content\": \"This is the content of the second post.\"\n        }\n    ]\n}\n```\n\n### N:N 관계\n\n```python\nclass GroupSerializer(serializers.ModelSerializer):\n    members = serializers.PrimaryKeyRelatedField(many=True, read_only=True)\n\n    class Meta:\n        model = Group\n        fields = [\"id\", \"name\", \"members\"]\n\nclass UserSerializer(serializers.ModelSerializer):\n    groups = GroupSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\", \"groups\"]\n```\n\n`Group` 모델과 `User` 모델이 N:N 관계를 가지고 있는 경우, `User`가 여러 `Group`에 속해 있을 때 직렬화된 JSON의 예시는 다음과 같습니다.\n\n```json\n{\n    \"id\": 1,\n    \"email\": \"example@example.com\",\n    \"dob\": \"1990-01-01\",\n    \"gender\": 1,\n    \"nickname\": \"example_user\",\n    \"groups\": [\n        {\n            \"id\": 1,\n            \"name\": \"Group One\",\n            \"members\": [1, 2, 3]\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Group Two\",\n            \"members\": [1, 4, 5]\n        }\n    ]\n}\n```\n\n### 결론\n\n이를 한 번에 정의하면 다음과 같습니다. \n\n```python\nfrom rest_framework import serializers\nfrom .models import User, UserProfile, Post, Group\n\nclass UserProfileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserProfile\n        fields = [\"bio\", \"website\"]\n\nclass PostSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Post\n        fields = [\"id\", \"title\", \"content\"]\n\nclass GroupSerializer(serializers.ModelSerializer):\n    members = serializers.PrimaryKeyRelatedField(many=True, read_only=True)\n\n    class Meta:\n        model = Group\n        fields = [\"id\", \"name\", \"members\"]\n\nclass UserSerializer(serializers.ModelSerializer):\n    profile = UserProfileSerializer(read_only=True)\n    posts = PostSerializer(many=True, read_only=True)\n    groups = GroupSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\", \"profile\", \"posts\", \"groups\"]\n```\n\n## **Depth 옵션을 활용한 중첩된 관계 직렬화**\n\n  `depth` 옵션을 통해 직렬화할 때 중첩된 관계의 깊이를 설정할 수 있습니다. `depth` 옵션은 기본적으로 외래 키(ForeignKey), 일대일 관계(OneToOneField), 다대다 관계(ManyToManyField) 등을 직렬화할 때 사용되며, 이를 통해 직렬화된 데이터에서 중첩된 객체의 필드를 포함할 수 있습니다.\n\n### 사용법\n\n`depth` 옵션은 `ModelSerializer`의 `Meta` 클래스 내에 설정할 수 있으며, 설정된 깊이만큼 중첩된 관계를 자동으로 직렬화합니다.\n\n```python\nfrom rest_framework import serializers\nfrom .models import User, UserProfile, Post, Group\n\nclass UserProfileSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserProfile\n        fields = [\"bio\", \"website\"]\n\nclass PostSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Post\n        fields = [\"id\", \"title\", \"content\"]\n\nclass GroupSerializer(serializers.ModelSerializer):\n    members = serializers.PrimaryKeyRelatedField(many=True, read_only=True)\n\n    class Meta:\n        model = Group\n        fields = [\"id\", \"name\", \"members\"]\n\nclass UserSerializer(serializers.ModelSerializer):\n    profile = UserProfileSerializer(read_only=True)\n    posts = PostSerializer(many=True, read_only=True)\n    groups = GroupSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\", \"profile\", \"posts\", \"groups\"]\n        depth = 1  # 여기서 depth 옵션을 설정합니다.\n\n```\n\n예를 들어, `depth`를 1로 설정하면 `UserSerializer`는 다음과 같은 JSON 응답을 생성합니다.\n\n```json\n{\n    \"id\": 1,\n    \"email\": \"example@example.com\",\n    \"dob\": \"1990-01-01\",\n    \"gender\": 1,\n    \"nickname\": \"example_user\",\n    \"profile\": {\n        \"bio\": \"This is an example bio.\",\n        \"website\": \"https://example.com\"\n    },\n    \"posts\": [\n        {\n            \"id\": 1,\n            \"title\": \"First Post\",\n            \"content\": \"This is the content of the first post.\"\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Second Post\",\n            \"content\": \"This is the content of the second post.\"\n        }\n    ],\n    \"groups\": [\n        {\n            \"id\": 1,\n            \"name\": \"Group One\",\n            \"members\": [1, 2, 3]\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Group Two\",\n            \"members\": [1, 4, 5]\n        }\n    ]\n}\n\n```\n\n이 JSON 응답에서 볼 수 있듯이, `profile`, `posts`, `groups` 필드에 대한 중첩된 관계가 자동으로 직렬화되었습니다. `depth` 옵션을 더 큰 값으로 설정하면 더 깊이 중첩된 관계까지 직렬화할 수 있습니다.\n\n예를 들어 다음과 같이 depth를 2로 설정하면,\n\n```python\n\nclass UserSerializer(serializers.ModelSerializer):\n    profile = UserProfileSerializer(read_only=True)\n    posts = PostSerializer(many=True, read_only=True)\n    groups = GroupSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\"id\", \"email\", \"dob\", \"gender\", \"nickname\", \"profile\", \"posts\", \"groups\"]\n        depth = 2  # depth 옵션을 2로 설정합니다.\n```\n\n다음과 같은 응답을 받을 수 있습니다. \n\n```json\n{\n    \"id\": 1,\n    \"email\": \"example@example.com\",\n    \"dob\": \"1990-01-01\",\n    \"gender\": 1,\n    \"nickname\": \"example_user\",\n    \"profile\": {\n        \"bio\": \"This is an example bio.\",\n        \"website\": \"https://example.com\",\n        \"user\": {\n            \"id\": 1,\n            \"email\": \"example@example.com\",\n            \"dob\": \"1990-01-01\",\n            \"gender\": 1,\n            \"nickname\": \"example_user\"\n        }\n    },\n    \"posts\": [\n        {\n            \"id\": 1,\n            \"title\": \"First Post\",\n            \"content\": \"This is the content of the first post.\",\n            \"user\": {\n                \"id\": 1,\n                \"email\": \"example@example.com\",\n                \"dob\": \"1990-01-01\",\n                \"gender\": 1,\n                \"nickname\": \"example_user\"\n            }\n        },\n        {\n            \"id\": 2,\n            \"title\": \"Second Post\",\n            \"content\": \"This is the content of the second post.\",\n            \"user\": {\n                \"id\": 1,\n                \"email\": \"example@example.com\",\n                \"dob\": \"1990-01-01\",\n                \"gender\": 1,\n                \"nickname\": \"example_user\"\n            }\n        }\n    ],\n    \"groups\": [\n        {\n            \"id\": 1,\n            \"name\": \"Group One\",\n            \"members\": [\n                {\n                    \"id\": 1,\n                    \"email\": \"example@example.com\",\n                    \"dob\": \"1990-01-01\",\n                    \"gender\": 1,\n                    \"nickname\": \"example_user\"\n                },\n                {\n                    \"id\": 2,\n                    \"email\": \"example2@example.com\",\n                    \"dob\": \"1992-02-02\",\n                    \"gender\": 2,\n                    \"nickname\": \"example_user2\"\n                }\n            ]\n        },\n        {\n            \"id\": 2,\n            \"name\": \"Group Two\",\n            \"members\": [\n                {\n                    \"id\": 1,\n                    \"email\": \"example@example.com\",\n                    \"dob\": \"1990-01-01\",\n                    \"gender\": 1,\n                    \"nickname\": \"example_user\"\n                },\n                {\n                    \"id\": 3,\n                    \"email\": \"example3@example.com\",\n                    \"dob\": \"1993-03-03\",\n                    \"gender\": 1,\n                    \"nickname\": \"example_user3\"\n                }\n            ]\n        }\n    ]\n}\n\n```\n\n위의 예제 응답에서 볼 수 있듯이, `depth` 옵션이 2로 설정되면 `profile`, `posts`, `groups` 필드에 연결된 객체들 뿐만 아니라, 그 객체들과 연결된 다른 객체들까지도 직렬화됩니다. 이는 중첩된 관계를 더욱 깊이 직렬화하여 더 많은 관련 정보를 포함할 수 있게 합니다.\n\n이처럼 `depth` 옵션을 통해 중첩된 관계의 깊이를 조절하여 직렬화된 JSON 응답에 포함되는 데이터를 세밀하게 조정할 수 있습니다.\n\n### 주의\n\n`depth` 옵션을 사용할 때 몇 가지 주의할 점이 있습니다:\n\n- 퍼포먼스: 너무 깊은 depth를 설정하면 직렬화 과정에서 성능 저하가 발생할 수 있습니다. 필요 이상의 깊이를 설정하지 않는 것이 좋습니다.\n\n- 상호 참조: 깊은 관계에서 상호 참조가 발생할 경우 순환 참조 문제가 생길 수 있습니다. 이를 방지하기 위해서는 depth 값을 신중하게 설정해야 합니다.\n\n이처럼 `depth` 옵션을 통해 중첩된 관계를 직렬화할 때 깊이를 조절할 수 있으며, 이를 통해 간단하고 직관적으로 중첩된 데이터를 다룰 수 있습니다.\n\n## Pydantic과의 차이점? \n\nFastAPI를 사용하면서 [Pydantic Model](https://sharknia.github.io/Pydantic-모델)을 적극 사용한 적이 있습니다. \n\n### 공통점\n\n- 직렬화 및 역직렬화: 둘 다 데이터 직렬화와 역직렬화를 지원합니다.\n\n- 데이터 검증: 입력 데이터의 유효성을 검사하는 기능을 제공합니다.\n\n- 필드 정의: 각 필드의 타입과 제약 조건을 정의할 수 있습니다.\n\n### 차이점\n\n- 프레임워크 의존성:\n\n    - DRF serializers는 Django와 강하게 결합되어 있으며, Django ORM과의 통합을 염두에 두고 설계되었습니다.\n\n    - Pydantic은 특정 프레임워크에 의존하지 않으며, 다양한 환경에서 사용할 수 있습니다.\n\n- 타입 힌팅:\n\n    - Pydantic은 Python의 타입 힌팅을 적극 활용하여 더 직관적이고 명시적인 데이터 검증을 제공합니다.\n\n    - DRF serializers는 Django 폼 스타일의 검증을 따릅니다.\n\n- 유연성 및 사용 범위:\n\n    - DRF serializers는 주로 Django 애플리케이션 내에서 사용됩니다.\n\n    - Pydantic은 FastAPI뿐만 아니라, 별도의 데이터 검증 라이브러리로도 사용할 수 있습니다.\n\n    \n\n결론적으로, Django REST Framework를 사용하는 경우 DRF serializers를 사용하는 것이 자연스럽고 편리합니다. \n\n"},{"excerpt":"서론 Django Rest Framework(DRF)는 Django에서 RESTful API를 쉽게 구축할 수 있도록 도와주는 도구입니다. JWT(Json Web Token)는 클라이언트와 서버 간의 인증을 안전하게 수행하기 위한 방법으로 널리 사용됩니다. 이번에는 DRF에서 JWT를 이용한 사용자 인증을 구현하고, 이를 위한 커스텀 퍼미션 클래스를 작성…","fields":{"slug":"/Django-Rest-Framework에서-JWT를-이용한-사용자-인증-구현/"},"frontmatter":{"date":"May 22, 2024","title":"Django Rest Framework에서 JWT를 이용한 사용자 인증 구현","tags":["Supabase","Django","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nDjango Rest Framework(DRF)는 Django에서 RESTful API를 쉽게 구축할 수 있도록 도와주는 도구입니다. JWT(Json Web Token)는 클라이언트와 서버 간의 인증을 안전하게 수행하기 위한 방법으로 널리 사용됩니다. 이번에는 DRF에서 JWT를 이용한 사용자 인증을 구현하고, 이를 위한 커스텀 퍼미션 클래스를 작성해보겠습니다.\n\n## **Django Rest Framework(DRF)란?**\n\nDjango Rest Framework(DRF)는 Django를 위한 강력하고 유연한 도구로, RESTful API를 쉽게 구축하고 관리할 수 있게 해줍니다. DRF는 직관적인 API 개발을 위해 다양한 기능을 제공하며, 대표적인 기능으로는 다음과 같은 것들이 있습니다.\n\n- Serializer: 복잡한 데이터 쿼리셋과 모델 인스턴스를 JSON 등으로 직렬화/역직렬화하는 도구.\n\n- Viewsets: 기본 CRUD 작업을 간편하게 구현할 수 있는 뷰 로직.\n\n- Authentication & Permissions: 다양한 인증 및 권한 부여 메커니즘을 제공하여 보안 설정을 강화.\n\n- Browsable API: 개발 중에 API를 쉽게 테스트하고 문서화할 수 있는 웹 브라우저 기반 인터페이스.\n\n이 모든 기능을 통해 DRF는 백엔드 개발자가 빠르고 효율적으로 웹 API를 구축할 수 있도록 도와줍니다.\n\n## **JWT란?**\n\nJWT는 JSON 형식의 데이터를 사용하여 정보의 무결성과 진위를 검증하는 토큰입니다. 주로 인증과 정보 교환에 사용되며, 클라이언트와 서버 간의 통신에서 사용자가 인증되었음을 보증하는 데 사용됩니다.\n\n## 구현\n\n### JWT\n\nJWT는 Supabase를 활용한 인증을 거쳐 로그인을 할 때 발급됩니다. \n\n[이 페이지](https://supabase.com/docs/guides/auth/jwts#jwts-in-supabase)를 참고하면 Supabase에서 생성되는 JWT의 꼴을 알 수 있습니다. \n\n```json\n{\n  \"aud\": \"authenticated\",\n  \"exp\": 1615824388,\n  \"sub\": \"0334744a-f2a2-4aba-8c8a-6e748f62a172\",\n  \"email\": \"d.l.solove@gmail.com\",\n  \"app_metadata\": {\n    \"provider\": \"email\"\n  },\n  \"user_metadata\": null,\n  \"role\": \"authenticated\"\n}\n\n```\n\n이 중에서 일단 sub, email을 활용해보려고 합니다. \n\n또한 HS256 알고리즘을 사용하는 것을 확인할 수 있습니다. \n\n### **테스트 코드 작성**\n\n잘 될지 모르겠지만, 지금 진행하는 프로젝트는 테스트코드를 먼저 작성하는 방향으로 가려고 합니다. \n\n토큰 검증을 할 것이므로, 예상 시나리오는 \n\n- 올바른 토큰\n\n- 토큰이 없는 경우 \n\n- 토큰이 잘못된 경우\n\n- 토큰이 만료된 경우\n\n의 네 가지입니다. 해당 케이스의 테스트 코드를 작성합니다. \n\n```python\nfrom datetime import datetime, timedelta, timezone\n\nimport jwt\nfrom django.conf import settings\nfrom django.test import RequestFactory, TestCase\nfrom rest_framework.exceptions import AuthenticationFailed\n\nfrom project.permission import IsAuthenticatedWithJWT\n\n\nclass TestIsAuthenticatedWithJWT(TestCase):\n    def setUp(self):\n        self.factory = RequestFactory()\n        self.secret = settings.SUPABASE_JWT_SECRET\n\n    def test_has_permission_with_valid_token(self):\n        request = self.factory.get(\"/\")\n        token = jwt.encode(\n            {\n                \"sub\": \"user123\",\n                \"email\": \"user@example.com\",\n            },\n            self.secret,\n            algorithm=\"HS256\",\n        )\n        request.META[\"HTTP_AUTHORIZATION\"] = f\"Bearer {token}\"\n        permission = IsAuthenticatedWithJWT()\n\n        self.assertTrue(permission.has_permission(request, None))\n        self.assertEqual(\n            request.user,\n            {\n                \"user_id\": \"user123\",\n                \"email\": \"user@example.com\",\n            },\n        )\n\n    def test_has_permission_with_invalid_token(self):\n        request = self.factory.get(\"/\")\n        request.META[\"HTTP_AUTHORIZATION\"] = \"Bearer invalidtoken\"\n        permission = IsAuthenticatedWithJWT()\n\n        with self.assertRaises(AuthenticationFailed):\n            permission.has_permission(request, None)\n\n    def test_has_permission_without_token(self):\n        request = self.factory.get(\"/\")\n        permission = IsAuthenticatedWithJWT()\n\n        with self.assertRaises(AuthenticationFailed):\n            permission.has_permission(request, None)\n\n    def test_has_permission_with_expired_token(self):\n        request = self.factory.get(\"/\")\n        token = jwt.encode(\n            {\n                \"sub\": \"user123\",\n                \"email\": \"user@example.com\",\n                \"exp\": datetime.now(timezone.utc) - timedelta(seconds=1),\n            },\n            self.secret,\n            algorithm=\"HS256\",\n        )\n        request.META[\"HTTP_AUTHORIZATION\"] = f\"Bearer {token}\"\n        permission = IsAuthenticatedWithJWT()\n\n        with self.assertRaises(AuthenticationFailed):\n            permission.has_permission(request, None)\n\n```\n\n### **Custom Permission 클래스 작성**\n\nDRF에서 JWT를 사용한 인증을 처리하기 위해 커스텀 퍼미션 클래스를 작성합니다. 이 클래스는 요청에 포함된 JWT 토큰을 검증하고, 유효한 경우 사용자 정보를 설정합니다.\n\n```python\n# project/permissions.py\n\nimport jwt\nfrom django.conf import settings\nfrom rest_framework import permissions\nfrom rest_framework.exceptions import AuthenticationFailed\n\nclass IsAuthenticatedWithJWT(permissions.BasePermission):\n    def has_permission(self, request, view):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header:\n            raise AuthenticationFailed(\"Authorization header missing\")\n\n        try:\n            token = auth_header.split(\" \")[1]\n            payload = jwt.decode(token, settings.SUPABASE_JWT_SECRET, algorithms=[\"HS256\"])\n            request.user = {\n                \"user_id\": payload.get(\"sub\"),\n                \"email\": payload.get(\"email\"),\n            }\n        except jwt.ExpiredSignatureError:\n            raise AuthenticationFailed(\"Token has expired\")\n        except jwt.InvalidTokenError:\n            raise AuthenticationFailed(\"Invalid token\")\n\n        return True\n\n```\n\n### View에 Permission Class 적용\n\n작성한 커스텀 퍼미션 클래스를 뷰에 적용하여, 해당 뷰에 접근하는 모든 요청이 JWT를 통해 인증되도록 설정합니다.\n\n```python\n# views.py\n\nfrom rest_framework import status\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\nfrom drf_yasg.utils import swagger_auto_schema\n\nfrom .models import UserProfile\nfrom .serializers import UserProfileSerializer\nfrom project.permissions import IsAuthenticatedWithJWT\n\nclass UserProfileUpdateView(APIView):\n    permission_classes = [IsAuthenticatedWithJWT]\n\n    @swagger_auto_schema(\n        request_body=UserProfileSerializer,\n        responses={200: UserProfileSerializer, 404: \"Not Found\", 400: \"Bad Request\"},\n    )\n    def put(self, request, *args, **kwargs):\n        user_id = request.user[\"user_id\"]\n        try:\n            user_profile = UserProfile.objects.get(pk=user_id)\n        except UserProfile.DoesNotExist:\n            return Response(\n                {\"error\": \"UserProfile not found\"}, status=status.HTTP_404_NOT_FOUND\n            )\n\n        serializer = UserProfileSerializer(\n            user_profile, data=request.data, partial=True\n        )\n        if serializer.is_valid():\n            serializer.save()\n            return Response(serializer.data, status=status.HTTP_200_OK)\n        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n\n```\n\n\n\n"},{"excerpt":"소개 Swagger란? Swagger는 RESTful API를 설계, 빌드, 문서화 및 소비하는 데 사용되는 오픈 소스 도구 모음입니다. Swagger는 API를 시각적으로 확인하고 테스트할 수 있는 인터페이스를 제공하여 개발자와 사용자 간의 소통을 용이하게 합니다. Swagger의 주요 기능: API 문서화: 자동으로 API 문서를 생성하여 API 사용…","fields":{"slug":"/Django-프로젝트에-Swagger-설정하기/"},"frontmatter":{"date":"May 21, 2024","title":"Django 프로젝트에 Swagger 설정하기","tags":["Django","Python"]},"rawMarkdownBody":"## 소개\n\n### Swagger란?\n\nSwagger는 RESTful API를 설계, 빌드, 문서화 및 소비하는 데 사용되는 오픈 소스 도구 모음입니다. Swagger는 API를 시각적으로 확인하고 테스트할 수 있는 인터페이스를 제공하여 개발자와 사용자 간의 소통을 용이하게 합니다.\n\nSwagger의 주요 기능:\n\n1. API 문서화: 자동으로 API 문서를 생성하여 API 사용법을 쉽게 이해할 수 있도록 합니다.\n\n1. API 테스트: 제공된 인터페이스를 통해 API 요청을 직접 테스트할 수 있습니다.\n\n1. 호환성: 다양한 언어와 프레임워크에서 사용 가능합니다.\n\n### drf-yasg란?\n\n`drf-yasg`(Django REST framework - Yet Another Swagger Generator)는 Django REST Framework(DRF)를 위한 자동 Swagger/OpenAPI 문서 생성을 제공하는 도구입니다. `drf-yasg`를 사용하면 API 엔드포인트를 자동으로 문서화할 수 있으며, Swagger UI와 ReDoc을 통해 웹에서 쉽게 볼 수 있는 인터페이스를 제공합니다.\n\n이를 통해 개발자는 API 개발 및 유지 보수를 보다 효율적으로 수행할 수 있습니다.\n\n## 설치\n\ndrf-yasg를 설치합니다. \n\n```bash\npoetry add drf-yasg\n```\n\n## settings.py\n\n`settings.py` 파일에 `INSTALLED_APPS`에 `drf_yasg`를 추가합니다.\n\n```python\nINSTALLED_APPS = [\n    # 기존 앱들\n    'rest_framework',\n    'drf_yasg',\n]\n```\n\nstatic 파일 관련 내용을 설정해야 합니다. \n\n`STATIC_URL` 및 `STATICFILES_DIRS`, `STATIC_ROOT`를 설정합니다.\n\n```python\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n# Static files (CSS, JavaScript, Images)\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, 'static'),\n]\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n```\n\n## urls.py\n\n`urls.py` 파일을 수정하여 Swagger UI 및 ReDoc을 위한 URL 패턴을 추가합니다.\n\n```python\nfrom django.contrib import admin\nfrom django.urls import path, re_path\nfrom rest_framework import permissions\nfrom drf_yasg.views import get_schema_view\nfrom drf_yasg import openapi\n\nschema_view = get_schema_view(\n   openapi.Info(\n      title=\"Your API Title\",\n      default_version='v1',\n      description=\"Test description\",\n      terms_of_service=\"https://www.google.com/policies/terms/\",\n      contact=openapi.Contact(email=\"contact@yourapi.local\"),\n      license=openapi.License(name=\"BSD License\"),\n   ),\n   public=True,\n   permission_classes=(permissions.AllowAny,),\n)\n\nurlpatterns = [\n\t\t... 추가되는 내용\n    re_path(r'^swagger(?P<format>\\.json|\\.yaml)$', schema_view.without_ui(cache_timeout=0), name='schema-json'),\n    path('swagger/', schema_view.with_ui('swagger', cache_timeout=0), name='schema-swagger-ui'),\n    path('redoc/', schema_view.with_ui('redoc', cache_timeout=0), name='schema-redoc'),\n\t  ... 추가되는 내용 끝\n]\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n\n```\n\n## 정적 파일 수집\n\n다음 명령어를 실행하여 정적 파일을 모읍니다. 이 명령어는 모든 앱의 정적 파일을 `STATIC_ROOT` 디렉터리에 복사합니다.\n\n```python\npython manage.py collectstatic --noinput\n```\n\n## 확인\n\n이제 `/swagger` endpoint로 접속해보면 \n\n![](image1.png)\n이 화면을 확인할 수 있습니다. \n\n"},{"excerpt":"서론 갈 길이 멀지만 하나씩 해보겠습니다. 만들고자 하는 프로젝트는 있지만 아직 확정은 나지 않았고, 어떤 프로젝트를 하게 되더라도 로그인은 필수이지 않을까 싶습니다.  기술스택? Supabase를 한동안 사용해본 결과 큰 규모의 프로젝트에는 맞지 않는다고 뼈저리게 느꼈습니다.  그런가하면, 소규모 1인 프로젝트에는 더할나위없이 적합하다고 느꼈습니다. 가…","fields":{"slug":"/Nextjs-Supabase-회원가입-구현/"},"frontmatter":{"date":"May 20, 2024","title":"Next.js + Supabase 회원가입 구현","tags":["Supabase","Next.js"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n\n        천리길도 한걸음부터. 시작이 반이다. \n\n갈 길이 멀지만 하나씩 해보겠습니다. 만들고자 하는 프로젝트는 있지만 아직 확정은 나지 않았고, 어떤 프로젝트를 하게 되더라도 로그인은 필수이지 않을까 싶습니다. \n\n### 기술스택?\n\nSupabase를 한동안 사용해본 결과 큰 규모의 프로젝트에는 맞지 않는다고 뼈저리게 느꼈습니다. \n\n그런가하면, 소규모 1인 프로젝트에는 더할나위없이 적합하다고 느꼈습니다. 가격에 비해서 기능이 강력하고 서버를 꾸미는 수고로움을 엄청나게 축소시켜줍니다. \n\nNext.js도 마찬가지로, 하나의 프로젝트에서 백과 프론트를 모두 구현할 수 있다는게 큰 장점으로 느껴졌고 개인적으로 Typescript가 이유를 모르게 호감이어서 선택했습니다. \n\n따라서 이번 사이드 프로젝트의 기술 스택은 Next.js + Supabase로 꾸며보려고 합니다. \n\nNext.js도 그렇지만, 특히 프론트엔드는 완전히 새로운 경험이므로 초짜나 다름없습니다. 천천히 빠르게(? 해보려고 합니다. \n\n## Google 설정\n\n구글 OAuth 2.0을 활용한 구글 로그인을 구현할 예정입니다. 추후 어떤 서비스가 되더라도 구글 로그인을 폐기할 일은 없을 것 같아 이것부터 구현해보려고 합니다. \n\n#### 프로젝트 생성\n\n[구글 클라우드 콘솔](https://console.cloud.google.com/)에서 프로젝트를 생성합니다. \n\n![](image2.png)\n상단에 oauth를 검색하면 OAuth 동의 화면 메뉴가 나옵니다. 외부를 선택하고 만들기를 실행합니다. \n\n필수 입력값(앱 이름, 사용자 지원 이메일, 개발자 이메일)만 입력한 후 다음을 선택합니다. 이후에도 계속 다음을 선택하고, 대시보드로 돌아가기를 클릭하면 완료됩니다. \n\n#### 각종 키 값 획득\n\n![](image3.png)\n바로 좌측의 사용자 인증 정보 메뉴로 이동합니다. \n\n![](image4.png)\n상단의 사용자 인증 정보 만들기를 선택하고 OAuth 클라이언트 ID를 선택합니다. \n\n![](image5.png)\n이름을 적당히 입력하고, 승인된 자바스크립트 원본에는 서버의 주소를, 승인된 리디렉션 URI에는 콜백으로 호출될 URI를 입력합니다. \n\n여기까지 완료하면 클라이언트 ID와 클라이언트 보안 비밀번호를 얻을 수 있습니다. \n\n## Supabase 설정 및 설치\n\n### 프로젝트 설정\n\n[Supabase](https://supabase.com/)에 프로젝트를 생성한 후, Authentication 메뉴에서 Provider를 선택해 구글 토글을 켜주고, 위에서 획득한 클라이언트 아이디와 클라이언트 보안 비밀번호를 입력합니다. \n\n아래쪽에서 `https://[project-id].supabase.co/auth/v1/callback` 꼴의 콜백 URI를 얻을 수 있습니다. 구글 OAuth2.0의 리디렉션 UI에 해당 주소를 넣어줍니다. \n\n### Install\n\nSupabase를 설치합니다. \n\n```bash\nnpm install @supabase/supabase-js\n```\n\n## 코드 작성\n\n### supabase.ts\n\n`common/supabase.ts` 파일을 생성하고 다음의 내용을 작성합니다. \n\n```typescript\nimport { createClient } from '@supabase/supabase-js';\n\nconst supabaseUrl = 'YOUR_SUPABASE_URL';\nconst supabaseAnonKey = 'YOUR_SUPABASE_ANON_KEY';\n\nexport const supabase = createClient(supabaseUrl, supabaseAnonKey);\n```\n\n`supabaseUrl`과 `supabaseAnonKey`는 Supabase의 Project Setting - API 메뉴에서 획득할 수 있습니다. \n\n### googleLoginButton\n\n`pages/main/googleLoginButton.tsx`파일을 생성하고 다음의 내용을 작성합니다. \n\n```typescript\nimport React from 'react';\nimport { supabase } from '../../common/supabase';\n\nconst GoogleLoginButton: React.FC = () => {\n    const handleGoogleLogin = async () => {\n        const { error } = await supabase.auth.signInWithOAuth({\n            provider: 'google',\n        });\n\n        if (error) {\n            console.error('Login Failed:', error.message);\n            // 로그인 실패 시 처리 로직을 여기에 추가하세요.\n        } else {\n            console.log('Login Success');\n            // 로그인 성공 시 처리 로직을 여기에 추가하세요.\n        }\n    };\n\n    return (\n        <div style={styles.container}>\n            <h1>Login</h1>\n            <button onClick={handleGoogleLogin} style={styles.button}>\n                Sign in with Google\n            </button>\n        </div>\n    );\n};\n\nconst styles = {\n    container: {\n        display: 'flex',\n        flexDirection: 'column' as const,\n        justifyContent: 'center',\n        alignItems: 'center',\n        height: '100vh',\n    },\n    button: {\n        padding: '10px 20px',\n        fontSize: '16px',\n        cursor: 'pointer',\n    },\n};\n\nexport default GoogleLoginButton;\n```\n\n### login.tsx\n\n`pages/main/login.tsx` 파일을 생성하고, 다음의 내용을 작성합니다. \n\n```typescript\nimport React from 'react';\nimport GoogleLoginButton from './GoogleLoginButton';\n\nconst Login: React.FC = () => {\n    return (\n        <div>\n            <GoogleLoginButton />\n        </div>\n    );\n};\n\nexport default Login;\n```\n\n## 회원가입 테스트\n\n[http://localhost:3000/main/login](http://localhost:3000/main/login) 으로 접속해봅니다. \n\n![](image6.png)\n버튼을 눌러서 구글 로그인을 해본 후에 Supabase의 Authentication - Users 메뉴로 들어가면 정상적으로 회원가입이 된 것을 확인할 수 있습니다. \n\n"},{"excerpt":"지난 시간에는 django를 설치하고 DB 세팅까지 완료했습니다. 이번에는 소셜 로그인 기능을 붙여보겠습니다.  django-allauth vs dj-rest-auth 와 는 모두 Django 애플리케이션에서 인증 및 사용자 관리를 쉽게 할 수 있도록 도와주는 패키지입니다. 하지만 이 두 패키지는 약간 다른 목적과 사용 사례를 가지고 있습니다. 둘을 비교…","fields":{"slug":"/Django-Discord-소셜-회원가입-및-로그인-구현/"},"frontmatter":{"date":"May 17, 2024","title":"Django Discord 소셜 회원가입 및 로그인 구현","tags":["Django","BackEnd","Python"]},"rawMarkdownBody":"![](image1.png)\n[지난 시간](https://sharknia.github.io/docker-compose를-활용한-postgresql과-django-커넥션-만들기)에는 django를 설치하고 DB 세팅까지 완료했습니다. 이번에는 소셜 로그인 기능을 붙여보겠습니다. \n\n## django-allauth vs dj-rest-auth\n\n`django-allauth`와 `dj-rest-auth`는 모두 Django 애플리케이션에서 인증 및 사용자 관리를 쉽게 할 수 있도록 도와주는 패키지입니다. 하지만 이 두 패키지는 약간 다른 목적과 사용 사례를 가지고 있습니다. 둘을 비교해보면 다음과 같습니다.\n\n## Discord 세팅\n\n[Discord Developer Portal](https://discord.com/developers/applications)에서 Application을 생성합니다. \n\n첫 페이지에서 APPLICATION ID, PUBLIC KEY를 기록합니다. \n\n좌측의 OAuth2 메뉴로 이동합니다. \n\nCLIENT SECRET을 얻고 기록합니다. \n\n일단 여기까지만 먼저 해두겠습니다. \n\n## 설치\n\n### 패키지 설치 \n\n먼저 패키지를 설치합니다.  위에서 언급한 두 패키지 이외에도 `allauth`는 jwt 토큰은 생성해주지 않기 때문에 `djangorestframework-simplejwt` 라이브리러도 필요합니다. \n\n```bash\npoetry add django-allauth djangorestframework-simplejwt\n```\n\n패키지를 설치한 후엔 반드시 이미지를 다시 빌드해줍니다. \n\n### settings.py\n\n`settings.py` 파일을 열고, 다음의 내용을 추가합니다. \n\n```bash\nINSTALLED_APPS = [\n    ...\n    \"allauth\",\n    \"allauth.account\",\n    \"allauth.socialaccount\",\n    \"allauth.socialaccount.providers.discord\",  # 디스코드 소셜 로그인 추가\n    \"rest_framework\",\n    \"rest_framework.authtoken\",\n    \"dj_rest_auth\",\n    \"dj_rest_auth.registration\",\n    \"django.contrib.sites\",\n    ...\n]\n\nSITE_ID = 1\n\nAUTHENTICATION_BACKENDS = [\n    'django.contrib.auth.backends.ModelBackend',\n    'allauth.account.auth_backends.AuthenticationBackend',\n]\n\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'rest_framework.authentication.TokenAuthentication',\n        'rest_framework.authentication.SessionAuthentication',\n    ],\n}\n\n# 이메일 인증 설정\nACCOUNT_EMAIL_VERIFICATION = \"none\"  # 이메일 확인 비활성화\nACCOUNT_EMAIL_REQUIRED = True  # 회원가입 시 이메일 주소 필수\n\n# 로그인 후 리디렉션 URL\nLOGIN_REDIRECT_URL = \"/\"\n\nMIDDLEWARE = [\n    ...\n    \"allauth.account.middleware.AccountMiddleware\",  # 이 줄을 추가\n    ...\n]\n\n```\n\n### 마이그레이션 적용\n\n여기까지 마친 후 마이그레이션을 진행합니다. \n\n```bash\ndocker-compose exec web python manage.py migrate\n```\n\n### urls.py\n\nurls.py를 열어 urlpattern에 다음의 endpoint를 추가합니다. \n\n```python\n\nurlpatterns = [\n\t\t...\n    path(\"accounts/\", include(\"allauth.urls\")),  # allauth URL 패턴 추가\n]\n```\n\n이제 로그인을 위한 Callback URL을 Discord에 정의할 수 있습니다. Callback URL은 `/accounts/discord/login/callback/` 입니다. \n\nDiscord의 Redirects에 해당 주소를 추가합니다. [http://127.0.0.1:8000/accounts/discord/login/callback/](http://127.0.0.1:8000/accounts/discord/login/callback/)로 적어주어도 좋고, 저는 ngrok을 이용중이므로 해당 주소를 입력해주었습니다. \n\n### 관리자 페이지 접속\n\n[관리자 페이지](http://localhost:8000/admin)에 Social Application을 생성해줘야 합니다. 해당 페이지에 접속하여 로그인합니다. 계정이 없다면 다음의 명령어를 사용하여 계정을 생성합니다. \n\n```bash\ndocker-compose -f docker/docker-compose.yml exec web python manage.py createsuperuser\n```\n\n#### Sites 추가\n\n먼저 Sites를 추가해야 합니다. Sites-sites에서 추가를 해주면 되는데, [localhost](http://localhost/) 또는 ngrok 주소 중 사용하는 것을 입력해줍니다. \n\n![](image2.png)\n#### Social Application 추가\n\n여기까지 완료해야 Social Application을 추가할 수 있습니다. \n\nSocial Accounts 메뉴의 Social Application를 선택하고, `ADD SOCIAL APPLICATION` 버튼을 눌러 추가합니다. \n\n![](image3.png)\n사이트는 여러개 등록해둘 수 있지만 일단 하나만 추가해줍니다. 아까 기록해두었던 키 들을 적절한 곳에 입력하면 됩니다. \n\n### 로그인 테스트\n\n이제 로그인을 테스트 해볼 수 있습니다. Discord의 OAuth2 메뉴에 가서 OAuth2 URL Generator에서\n\n![](image4.png)\n위와 같이 선택을 하고 Redirect URL을 선택하면 GENERATED URL이 생성됩니다. \n\n해당 URL로 접속 후, Discord로 로그인을 하면 \n\n![](image5.png)\n이렇게 로그인이 된 화면을 확인할 수 있습니다. Third-Party Login Failure라고 나오지만, 가장 위쪽의 Message에 주목하면 됩니다. 로그인이 잘 된 것이 맞습니다. ~~(여기서 100년 헤맸습니다. 누군가 왜 이런 화면이 나오는지 아시는 분 계신가요)~~\n\n이 화면을 보고 나면 관리자 페이지에서 User 또는 Social Users를 확인하면 제대로 가입이 된 것을 볼 수 있습니다. 성공 여부는 해당 페이지에서 확인하시면 됩니다. \n\n## Restful한 로그인을 위한 API 구현\n\n짠! 완료입니다! 면 좋겠지만 사실은 그렇지 않습니다. 위에서 짧게 언급했듯이 allauth는 JWT TOKEN 발급까지는 해주지 않습니다. 클라이언트와의 협의에 따라 달라지겠지만 REST한 JWT 토큰 생성을 위한 별도의 API가 필요합니다. \n\n### 모듈 생성\n\nauthentication 이라는 모듈을 만들어서 해당 앱에서 로그인을 관리하겠습니다. \n\n```bash\ndocker-compose -f docker/docker-compose.yml exec web python manage.py startapp authentication\n```\n\n### Settings.py\n\n생성한 앱을 추가해줍니다. \n\n```python\nINSTALLED_APPS = [\n    ...\n    \"authentication\", # 인증 관련 모듈\n    ...\n]\n```\n\n### views.py\n\nauthetication의 views.py에 다음의 내용을 작성합니다. \n\n```python\nimport os\n\nimport requests\nfrom allauth.socialaccount.helpers import complete_social_login\nfrom allauth.socialaccount.models import SocialLogin, SocialToken\nfrom allauth.socialaccount.providers.discord.views import DiscordOAuth2Adapter\nfrom rest_framework import status\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\nfrom rest_framework_simplejwt.tokens import RefreshToken\n\n\nclass DiscordRestCallback(APIView):\n    def get(self, request, *args, **kwargs):\n        code = request.GET.get(\"code\")\n        if not code:\n            return Response(\n                {\"error\": \"No code provided\"}, status=status.HTTP_400_BAD_REQUEST\n            )\n        access_token = self.get_access_token(code)\n        user_info = self.get_user_info(access_token)\n        jwt_tokens = self.perform_login(request, access_token, user_info)\n        return Response(jwt_tokens, status=status.HTTP_200_OK)\n\n    def perform_login(self, request, access_token, user_info):\n        adapter = DiscordOAuth2Adapter(request)\n        provider = adapter.get_provider()\n        app = provider.app\n        token = SocialToken(app=app, token=access_token)\n\n        # 유저 정보를 사용하여 소셜 로그인 객체를 생성\n        login = adapter.complete_login(request, app, token, response=user_info)\n        login.token = token\n        login.state = SocialLogin.state_from_request(request)\n        complete_social_login(request, login)\n\n        if not login.is_existing:\n            login.save(request, connect=True)\n\n        # JWT 토큰 생성\n        user = login.account.user\n        refresh = RefreshToken.for_user(user)\n        return {\n            \"refresh\": str(refresh),\n            \"access\": str(refresh.access_token),\n        }\n\n    def get_access_token(self, code):\n        API_ENDPOINT = \"https://discord.com/api/v10\"\n        CLIENT_ID = os.getenv(\"DISCORD_CLIENT_ID\")\n        CLIENT_SECRET = os.getenv(\"DISCORD_CLIENT_SECRET\")\n        REDIRECT_URI = os.getenv(\"DISCORD_REDIRECT_URI\")\n\n        data = {\n            \"grant_type\": \"authorization_code\",\n            \"code\": code,\n            \"redirect_uri\": REDIRECT_URI,\n        }\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        r = requests.post(\n            f\"{API_ENDPOINT}/oauth2/token\",\n            data=data,\n            headers=headers,\n            auth=(CLIENT_ID, CLIENT_SECRET),\n        )\n        r.raise_for_status()\n        access_token = r.json().get(\"access_token\")\n        return access_token\n\n    def get_user_info(self, access_token):\n        API_ENDPOINT = \"https://discord.com/api/v10\"\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n        }\n        r = requests.get(f\"{API_ENDPOINT}/users/@me\", headers=headers)\n        r.raise_for_status()\n        return r.json()\n```\n\nDiscord의 API들을 활용해 access token을 얻고, 해당 access\\_token을 사용해서 유저 정보를 Discord로부터 받아옵니다. 그리고 해당 정보들을 이용해 로그인(회원가입)을 한 후 jwt token을 만들어 클라이언트에게 응답합니다. \n\n### urls.py\n\n소셜 로그인 관련 URL을 추가해야 합니다. authentication 안에 urls.py를 생성하고 다음의 코드를 입력합니다. \n\n```python\nfrom django.urls import path\n\nfrom .views import DiscordRestCallback\n\nurlpatterns = [\n    path(\n        \"discord/login/\",\n        DiscordRestCallback.as_view(),\n        name=\"discord_rest_callback\",\n    ),\n]\n\n# JWT 토큰 설정\nSIMPLE_JWT = {\n    \"ACCESS_TOKEN_LIFETIME\": timedelta(minutes=5),\n    \"REFRESH_TOKEN_LIFETIME\": timedelta(days=1),\n    \"ROTATE_REFRESH_TOKENS\": False,\n    \"BLACKLIST_AFTER_ROTATION\": True,\n    \"UPDATE_LAST_LOGIN\": False,\n    \"ALGORITHM\": \"HS256\",\n    \"SIGNING_KEY\": SECRET_KEY,\n    \"VERIFYING_KEY\": None,\n    \"AUDIENCE\": None,\n    \"ISSUER\": None,\n    \"AUTH_HEADER_TYPES\": (\"Bearer\",),\n    \"AUTH_HEADER_NAME\": \"HTTP_AUTHORIZATION\",\n    \"USER_ID_FIELD\": \"id\",\n    \"USER_ID_CLAIM\": \"user_id\",\n    \"AUTH_TOKEN_CLASSES\": (\"rest_framework_simplejwt.tokens.AccessToken\",),\n    \"TOKEN_TYPE_CLAIM\": \"token_type\",\n    \"JTI_CLAIM\": \"jti\",\n    \"SLIDING_TOKEN_REFRESH_EXP_CLAIM\": \"refresh_exp\",\n    \"SLIDING_TOKEN_LIFETIME\": timedelta(minutes=5),\n    \"SLIDING_TOKEN_REFRESH_LIFETIME\": timedelta(days=1),\n}\n```\n\n그리고 프로젝트 메인의 urls.py를 넣어 authentication의 url을 추가해줍니다. \n\n```python\n\nurlpatterns = [\n\t\t...\n    path(\"authentication/\", include(\"authentication.urls\")),\n\t  ...\n]\n\n```\n\n이제 Discord OAuth2 페이지로 이동해서 Callback URL을 `/authentication/discord/login/`로 교체합니다. \n\n그 후 로그인을 시도하면,\n\n![](image6.png)\n아까처럼 로그인 성공 화면이 아니라 JWT 토큰을 발급 받을 수 있습니다. \n\n## 참조\n\n[https://discord.com/developers/docs/topics/oauth2](https://discord.com/developers/docs/topics/oauth2)\n\n[https://docs.allauth.org/en/latest/introduction/index.html](https://docs.allauth.org/en/latest/introduction/index.html)\n\n"},{"excerpt":"서론 로컬에서 개발할 때, 종종 외부에서 로컬 서버에 접근해야 하는 상황이 발생합니다. 예를 들어, 웹훅(Webhook)을 테스트하거나, 클라이언트에게 데모를 보여주기 위해 로컬 서버를 공개해야 할 때가 있습니다. 이러한 상황에서 매우 유용한 도구가 바로 ngrok입니다. ngrok 소개 ngrok은 로컬에서 실행 중인 서버를 공용 인터넷에 노출시켜주는 …","fields":{"slug":"/ngrok-로컬-서버를-쉽게-공개하는-도구/"},"frontmatter":{"date":"May 14, 2024","title":"ngrok: 로컬 서버를 쉽게 공개하는 도구","tags":["BackEnd"]},"rawMarkdownBody":"## 서론\n\n로컬에서 개발할 때, 종종 외부에서 로컬 서버에 접근해야 하는 상황이 발생합니다. 예를 들어, 웹훅(Webhook)을 테스트하거나, 클라이언트에게 데모를 보여주기 위해 로컬 서버를 공개해야 할 때가 있습니다. 이러한 상황에서 매우 유용한 도구가 바로 ngrok입니다.\n\n## ngrok 소개\n\nngrok은 로컬에서 실행 중인 서버를 공용 인터넷에 노출시켜주는 역방향 프록시 도구입니다. 이를 통해 로컬에서 개발 중인 애플리케이션을 외부에서도 접근할 수 있게 됩니다. ngrok은 다음과 같은 주요 기능을 제공합니다:\n\n- HTTPS 터널링: 로컬 서버를 HTTPS를 통해 안전하게 노출합니다.\n\n- 인스펙션 인터페이스: 요청과 응답을 실시간으로 확인할 수 있는 웹 인터페이스를 제공합니다.\n\n- 포트 포워딩: 특정 포트를 지정하여 외부에서 접근할 수 있게 합니다.\n\n- 자동 재연결: 연결이 끊어질 경우 자동으로 다시 연결합니다.\n\n## ngrok 설치 및 기본 사용법\n\nngrok을 사용하기 위해서는 먼저 설치가 필요합니다. 다음은 ngrok 설치 및 기본 사용법에 대한 단계별 설명입니다.\n\n#### 1. ngrok 다운로드 및 설치\n\n[ngrok 공식 웹사이트](https://ngrok.com/)에서 운영체제에 맞는 ngrok을 설치합니다. \n\n맥에서는 간단하게 다음의 명령어를 통해 설치할 수 있습니다. \n\n```shell\nbrew install ngrok/ngrok/ngrok\n```\n\n정상적인 실행을 위해서는 토큰을 등록해야 합니다. 사이트에 회원가입을 합니다. \n\n로그인 까지 마친 후 [이 페이지](https://dashboard.ngrok.com/get-started/setup/macos)의 installation 섹션에서 auth-token을 얻을 수 있습니다. \n\n```shell\nngrok config add-authtoken [token]\n```\n\n위의 명령어를 실행하면 토큰이 저장됩니다. \n\n#### 2. ngrok 실행\n\nngrok 설치가 완료되면, 다음 명령어를 사용하여 로컬 서버를 공개할 수 있습니다. 여기서 `8080`은 로컬 서버가 실행 중인 포트 번호입니다.\n\n```bash\nngrok http 8080\n```\n\n명령어를 실행하면 다음과 같은 출력이 나타납니다:\n\n```bash\nngrok by @inconshreveable                                      (Ctrl+C to quit)\n\nSession Status                online\nAccount                       YourAccount (Plan: Free)\nVersion                       2.3.40\nRegion                        United States (us)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    http://abcdefgh.ngrok.io -> http://localhost:8080\nForwarding                    https://abcdefgh.ngrok.io -> http://localhost:8080\n\nConnections                   ttl     opn     rt1     rt5     p50     p90\n                              0       0       0.00    0.00    0.00    0.00\n```\n\n여기서 `https://abcdefgh.ngrok.io` URL을 통해 외부에서 로컬 서버에 접근할 수 있습니다.\n\n## 마무리\n\n이제 ngrok을 사용하여 로컬 서버의 내용을 https로 접근할 수 있습니다. 무료 플랜의 단점(껐다 킬 때마다 주소가 바뀌는 등)도 있지만, 간단한 테스트를 수행하기에는 더 없이 적절합니다. \n\n\n\n"},{"excerpt":"Next.js? 개요 Next.js는 React 기반의 프레임워크로, 웹사이트와 애플리케이션을 구축하기 위해 사용됩니다. 서버 사이드 렌더링(SSR), 정적 사이트 생성(SSG), 그리고 클라이언트 사이드 렌더링을 지원하며, 개발자가 사용하기 편리한 기능들을 제공합니다. 특징 및 장점 파일 기반 라우팅:  디렉토리 내의 파일 구조를 기반으로 자동 라우팅을…","fields":{"slug":"/Nextjs-소개와-설치-간단-예제/"},"frontmatter":{"date":"May 13, 2024","title":"Next.js 소개와 설치, 간단 예제","tags":["Next.js","Javascript"]},"rawMarkdownBody":"![](image1.png)\n## Next.js?\n\n### 개요\n\nNext.js는 React 기반의 프레임워크로, 웹사이트와 애플리케이션을 구축하기 위해 사용됩니다. 서버 사이드 렌더링(SSR), 정적 사이트 생성(SSG), 그리고 클라이언트 사이드 렌더링을 지원하며, 개발자가 사용하기 편리한 기능들을 제공합니다.\n\n### 특징 및 장점\n\n- 파일 기반 라우팅: `pages` 디렉토리 내의 파일 구조를 기반으로 자동 라우팅을 생성합니다.\n\n- 자동 코드 분할과 서버 사이드 렌더링을 통해 빠른 페이지 로드를 제공합니다. \n\n- 또한 SSR을 통해 SEO에 최적화 되어 있어 검색 엔진 최적화가 용이합니다. \n\n- 데이터 가져오기: `getStaticProps`, `getServerSideProps` 같은 데이터 페칭 함수를 제공하여 데이터 관리를 간소화합니다.\n\n- API 라우트: 서버사이드 로직을 쉽게 구현할 수 있는 API 경로를 지원합니다.\n\n## 설치\n\n### Node.js 설치\n\nnvm은 설치되어있다고 가정합니다. \n\n오늘(2024년 5월 13일) 기준 가장 최신의 20.13.1 버전을 설치합니다. \n\n```bash\nnvm install 20.13.1\necho \"20.13.1\" > .nvmrc\nnvm use\n```\n\n### Next.js 설치\n\n```bash\nnpx create-next-app@latest\n```\n\n이후 설정은 기본값으로 해주었습니다.\n\n```bash\nuser@MacBook-Pro nextjs % npx create-next-app@latest\nNeed to install the following packages:\ncreate-next-app@14.2.3\nOk to proceed? (y) y\n✔ What is your project named? … my-app\n✔ Would you like to use TypeScript? … No / Yes\n✔ Would you like to use ESLint? … No / Yes\n✔ Would you like to use Tailwind CSS? … No / Yes\n✔ Would you like to use `src/` directory? … No / Yes\n✔ Would you like to use App Router? (recommended) … No / Yes\n✔ Would you like to customize the default import alias (@/*)? … No / Yes\nCreating a new Next.js app in /Users/user/Develop/nextjs/my-app.\n\nUsing npm.\n\nInitializing project with template: app-tw \n\n\nInstalling dependencies:\n- react\n- react-dom\n- next\n\nInstalling devDependencies:\n- typescript\n- @types/node\n- @types/react\n- @types/react-dom\n- postcss\n- tailwindcss\n- eslint\n- eslint-config-next\n\n\nadded 358 packages, and audited 359 packages in 32s\n\n133 packages are looking for funding\n  run `npm fund` for details\n\nfound 0 vulnerabilities\nInitialized a git repository.\n\nSuccess! Created my-app at /Users/user/Develop/nextjs/my-app\n```\n\n이제 my-app 디렉토리로 이동하고, \n\n```bash\ncd my-app\n```\n\n다음의 코드를 실행하면\n\n```bash\nnpm run dev\n```\n\n[http://localhost:3000](http://localhost:3000/)에서 바로 웹페이지를 확인할 수 있습니다. \n\n## 간단 예제\n\n### Hello World 출력하기 \n\n`/hello` 경로에 접속하면 `Hello World` 가 출력되도록 해보겠습니다. Next.js는 파일 기반 라우팅 시스템을 사용하기 때문에 간단히 `pages` 디렉토리 내에 `hello.js` 파일을 생성하고 그 안에 적절한 코드를 작성하면 됩니다.\n\n프로젝트의 `pages` 디렉토리 내에 `hello.js` 파일을 생성합니다. 이 파일이 `/hello` 경로에 대응됩니다. `hello.js` 파일을 열고 다음과 같은 코드를 작성합니다\n\n```javascript\nexport default function Hello() {\n    return <div>Hello World</div>;\n}\n```\n\n[http://localhost:3000/test/hello](http://localhost:3000/test/hello)로 접속해보면 Hello World 문구를 확인할 수 있습니다. \n\n![](image2.png)\n### API Route 방식을 사용해서 Hello World 출력하기\n\n#### API Route?\n\nNext.js의 API 라우트는 `pages/api` 디렉토리 내에 정의된 서버사이드 코드를 통해 구현됩니다. 이 기능을 통해 개발자는 RESTful API를 손쉽게 구현할 수 있으며, 이 API는 데이터 처리, 데이터베이스 쿼리, 외부 API 호출 등 서버사이드 로직을 실행하는 데 사용됩니다.\n\nAPI 라우트를 설정할 때 파일 위치는 반드시 `pages/api` 디렉토리 안에 있어야 합니다. 이 구조는 Next.js의 컨벤션을 따르는 것으로, Next.js는 이 디렉토리 내의 파일들을 서버사이드 로직으로 처리하고 자동으로 해당 파일 경로에 매칭되는 API 엔드포인트를 생성합니다.\n\n#### 왜 `pages/api` 디렉토리인가?\n\n- 자동화된 API 엔드포인트: `pages/api` 내부에 파일을 배치함으로써, Next.js는 해당 파일의 경로를 기반으로 자동으로 API 엔드포인트를 설정합니다. 예를 들어, `pages/api/user.js`는 `/api/user`로 접근할 수 있습니다.\n\n- 서버 사이드 실행: 이 디렉토리 내의 코드는 클라이언트 사이드가 아닌 서버 사이드에서 실행됩니다. 이는 보안 상 중요한 데이터를 클라이언트에 노출하지 않고 처리할 수 있게 합니다.\n\n- 분리된 로직: `pages/api`는 UI와 관련 없이 데이터 처리나 서버 로직을 관리하는 데 사용되므로, 프론트엔드와 백엔드 로직을 명확하게 분리할 수 있습니다.\n\n#### 경로 및 파일 구조\n\n- 파일 이름과 디렉토리 구조가 URL 경로로 직접 매핑됩니다. 예를 들어, `pages/api/products/list.js`는 `/api/products/list` 경로에 매핑됩니다.\n\n- 파일 내에서는 기본으로 내보내는 함수(`export default function`)를 사용하여 API의 로직을 처리합니다.\n\n#### API 생성\n\n/pages/api 경로에 hello.js 파일을 생성하고 다음의 내용을 작성합니다. \n\n```javascript\nexport default function handler(req, res) {\n  res.status(200).json({ message: 'Hello World' });\n}\n```\n\n이제 [http://localhost:3000/api/hello](http://localhost:3000/api/hello)로 접속하면 api가 정상적인 응답값을 내려주는 것을 확인할 수 있습니다. \n\n#### API의 호출\n\nAPI 라우트의 결과를 페이지에 표시하려면 클라이언트 측 JavaScript를 사용하여 API 라우트를 호출하고, 응답을 상태로 관리하여 렌더링해야 합니다. React의 `useState`와 `useEffect` 훅을 활용할 수 있습니다.\n\n`/test/hello.js` 를 다음과 같이 수정합니다. \n\n```javascript\nimport React, { useEffect, useState } from 'react';\n\nexport default function Hello() {\n    const [message, setMessage] = useState('Loading...');\n\n    useEffect(() => {\n        fetch('/api/hello')\n            .then((res) => res.json())\n            .then((data) => setMessage(data.message))\n            .catch((err) => setMessage('Failed to fetch data'));\n    }, []);\n\n    return <div>api response : {message}</div>;\n}\n```\n\n1. useState 사용: `message` 상태를 선언하여 API 호출의 결과를 저장합니다. 초기값은 'Loading...'으로 설정하여 API 응답을 기다리는 동안 사용자에게 보여집니다.\n\n1. useEffect 사용: 컴포넌트가 마운트될 때 `/api/hello2` 엔드포인트를 호출합니다. `fetch` 함수를 사용하여 API를 호출하고, 응답이 돌아오면 JSON으로 변환 후 `setMessage`를 통해 상태를 업데이트합니다. 에러가 발생하면 메시지를 'Failed to fetch data'로 설정합니다.\n\n이제 [http://localhost:3000/test/hello](http://localhost:3000/test/hello)로 접속하면 다음과 같은 화면을 볼 수 있습니다. \n\n![](image3.png)\n## getStaticProps\n\n`getStaticProps`는 빌드 시에 데이터를 불러와 페이지를 미리 생성하는 데 사용됩니다. 이는 주로 변경되지 않는 데이터가 필요한 페이지에 적합합니다.\n\n간단한 getStaticProps 예제를 구현해보겠습니다. \n\n먼저, data로 사용될 json 파일을 작성합니다. 위치는 `data/` 입니다. \n\n```json\n[\n    {\n        \"id\": 1,\n        \"title\": \"First Post\",\n        \"content\": \"This is the first post.\"\n    },\n    {\n        \"id\": 2,\n        \"title\": \"Second Post\",\n        \"content\": \"This is the second post.\"\n    }\n]\n```\n\n그리고 `pages/posts.js` **에 다음의 내용을 작성합니다.** \n\n```javascript\nimport React from 'react';\n\nexport async function getStaticProps() {\n    // 가정: 데이터는 로컬 JSON 파일에서 불러옴\n    const data = require('../data/post.json');\n    return {\n        props: {\n            posts: data,\n        },\n    };\n}\n\nexport default function Posts({ posts }) {\n    return (\n        <div>\n            <h1>Blog Posts</h1>\n            <ul>\n                {posts.map((post) => (\n                    <li key={post.id}>\n                        <h2>{post.title}</h2>\n                        <p>{post.content}</p>\n                    </li>\n                ))}\n            </ul>\n        </div>\n    );\n}\n\n```\n\n`getStaticProps` 함수는 Next.js 애플리케이션을 빌드할 때 호출됩니다. 이는 개발 모드에서는 페이지에 처음 접근할 때 한 번, 프로덕션 모드에서는 빌드 과정에서 실행됩니다.\n\n또한 이 함수는 서버 사이드에서만 실행됩니다. 즉, 클라이언트 사이드 또는 브라우저에서는 실행되지 않습니다. 이는 보안 상 중요한 작업을 수행하거나 외부 API, 데이터베이스 등 서버 측 리소스에 접근할 때 유용합니다.\n\n`getStaticProps`에서 반환된 데이터는 React 컴포넌트의 props로 전달됩니다. 이를 통해 사전에 정의된 데이터를 이용하여 페이지를 렌더링할 수 있으며, 이 페이지는 HTML 파일로 생성되어 클라이언트에게 제공됩니다.\n\n`getStaticProps`에서 반환하는 `props` 객체의 키 이름과, 페이지 컴포넌트의 `props` 매개변수에서 받는 이름이 일치해야 합니다.\n\n## **getServerSideProps**\n\n`getServerSideProps`는 각 페이지 요청 시 서버에서 데이터를 불러와 렌더링하는 데 사용됩니다. 동적인 데이터가 필요한 페이지에 적합합니다.\n\n아까 useEffect를 사용했던 코드를 개선해보겠습니다.  test/hello.js를 다음과 같이 수정합니다. \n\n```javascript\nimport React from 'react';\n\nexport async function getServerSideProps() {\n    try {\n        const res = await fetch('http://localhost:3000/api/hello'); // API 엔드포인트 호출\n        const data = await res.json();\n        return {\n            props: { message: data.message }, // 페이지 컴포넌트로 props 전달\n        };\n    } catch (err) {\n        return {\n            props: { message: 'Failed to fetch data' }, // 에러 처리\n        };\n    }\n}\n\nexport default function Hello({ message }) {\n    return <div>API response: {message}</div>; // 서버에서 받은 메시지 출력\n}\n\n```\n\n`getServerSideProps`는 페이지가 렌더링될 때마다 서버에서 실행되어, 서버 사이드에서 필요한 데이터를 사전에 불러오고, 이 데이터를 페이지 컴포넌트로 직접 전달할 수 있습니다. 이 방법은 페이지 로드 시 클라이언트 사이드에서 추가적인 API 요청을 방지하므로 초기 렌더링 속도를 개선할 수 있습니다.\n\n#### `getServerSideProps` 사용 시 고려할 점\n\n- 서버 부하: `getServerSideProps`는 페이지 요청이 있을 때마다 서버에서 실행되므로, 트래픽이 많은 사이트에서는 서버에 상당한 부하가 가해질 수 있습니다.\n\n- 응답 시간: 클라이언트 사이드에서 데이터를 가져오는 것보다는 빠를 수 있지만, 서버 사이드에서 모든 데이터를 처리하고 완성된 페이지를 전송해야 하므로 응답 시간이 길어질 수 있습니다.\n\n- SEO 최적화: `getServerSideProps`를 사용하면 서버에서 렌더링된 페이지가 클라이언트에게 전송되기 때문에, 검색 엔진 최적화(SEO)에 유리합니다. 모든 콘텐츠가 HTML로 서버에서 미리 생성되어 전달되기 때문입니다.\n\n사실 `/api/hello` API는 고정된 결과(Hello World)만들 반환하므로, getServerSideProps 메소드로 구현할 필요가 없습니다. \n\n데이터가 변경되지 않는다면, `getStaticProps`를 사용하여 빌드 시점에 데이터를 미리 불러와서 정적 페이지를 생성하는 것이 매우 효율적입니다. 이 방법은 로딩 시간을 단축시키고 서버 부하를 줄이는 데 도움을 줄 수 있습니다.\n\nAPI의 결과가 항상 동일하다면 해당 데이터를 빌드 시 한 번만 불러오고, 생성된 HTML 파일을 사용하여 모든 사용자 요청을 처리할 수 있기 때문에, 웹사이트의 성능을 크게 향상시킬 수 있습니다. 이런 접근 방식은 특히 고정된 콘텐츠를 다루는 블로그, 문서화 사이트, 제품 정보 페이지 등에서 유리합니다.\n\n\n\n"},{"excerpt":"이전 포스팅에서 간단하게 django 프로젝트를 설치하고 구동해봤습니다. 이번 시간에는 postgresql과 django 프로젝트를 연결해보려고 합니다.  .env 파일 작성 DB 설정을 위한 환경변수 파일을 작성합니다.  해당 파일을 docker-compose.yml 파일과 같은 곳에 위치시킵니다.  이 파일은 보안에 유의하여, git과 같은 곳에 올라…","fields":{"slug":"/docker-compose를-활용한-postgresql과-django-커넥션-만들기/"},"frontmatter":{"date":"May 11, 2024","title":"docker-compose를 활용한 postgresql과 django 커넥션 만들기","tags":["Django","Docker","Docker-compose","Postgresql"]},"rawMarkdownBody":"![](image1.png)\n[이전 포스팅](https://sharknia.github.io/Django-설치)에서 간단하게 django 프로젝트를 설치하고 구동해봤습니다. 이번 시간에는 postgresql과 django 프로젝트를 연결해보려고 합니다. \n\n## .env 파일 작성\n\nDB 설정을 위한 환경변수 파일을 작성합니다. \n\n```shell\nPOSTGRES_DB=\nPOSTGRES_USER=\nPOSTGRES_PASSWORD=\nDATABASE_URL=postgres://[username]:[password]@[db-url]:[port]/[db-name]\n```\n\n해당 파일을 docker-compose.yml 파일과 같은 곳에 위치시킵니다. \n\n이 파일은 보안에 유의하여, git과 같은 곳에 올라가지 않도록 주의합니다. \n\n## Makefile 작성\n\n원활한 테스트를 위해 Makefile을 미리 작성합니다. \n\n```makefile\nup:\n\tdocker-compose -f docker/docker-compose.yml up -d\n\ndown:\n\tdocker-compose -f docker/docker-compose.yml down\n\nps:\n\tdocker-compose -f docker/docker-compose.yml ps\n\nlog: \n\tdocker-compose -f docker/docker-compose.yml logs\n\n```\n\n이제 `make up`, `make down` 명령어를 통해 간단하게 컨테이너를 재시작 할 수 있습니다. \n\n## **docker-compose.yml 파일 수정**\n\n해당 파일을 다음과 같이 수정합니다. \n\n```yaml\nversion: '3.8'\n\nservices:\n    db:\n        image: postgres\n        volumes:\n            - postgres_data:/var/lib/postgresql/data\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB}\n            POSTGRES_USER: ${POSTGRES_USER}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n        ports:\n            - '5432:5432'\n    web:\n        build:\n            context: ..\n            dockerfile: docker/Dockerfile\n        command: gunicorn project.wsgi:application --bind 0.0.0.0:8000 --reload\n        volumes:\n            - ../:/app\n        ports:\n            - '8000:8000'\n        depends_on:\n            - db\n        environment:\n            DATABASE_URL: ${DATABASE_URL}\n            POSTGRES_DB: ${POSTGRES_DB}\n            POSTGRES_USER: ${POSTGRES_USER}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n\nvolumes:\n    postgres_data:\n\n```\n\nport는 원하는대로 수정하면 됩니다. \n\n## dj-database-url 설치\n\n`dj-database-url`는 Django 프로젝트에서 데이터베이스 설정을 간편하게 관리할 수 있도록 도와주는 파이썬 라이브러리입니다. 이 라이브러리를 사용하면, 표준 데이터베이스 연결 URL을 파싱하여 Django의 데이터베이스 설정 형식으로 변환할 수 있습니다. 즉, `DATABASE_URL` 환경 변수에서 데이터베이스 연결 문자열을 읽어 Django에서 사용할 수 있게 설정하는 역할을 합니다. \n\n간단한 연결을 위해 해당 라이브러리를 설치하겠습니다. \n\n```shell\npoetry shell\npoetry add dj-database-url\n```\n\n이렇게 설치를 한 후, 이미지를 다시 빌드합니다. \n\n## Setting.py 수정\n\nDATABASE 부분을 다음과 같이 수정합니다. \n\n```python\nimport dj_database_url\n\nDATABASES = {\"default\": dj_database_url.config()}\n```\n\n`DATABASE_URL` 환경변수를 사용하므로, 반드시 해당 환경변수가 설정되어 있어야 합니다. \n\n## 연결 확인\n\nDB 연결이 잘 됐는지 테스트 해볼 수 있습니다. \n\n다음의 명령어를 사용해 파이썬 Shell을 실행하고, \n\n```shell\ndocker-compose -f docker/docker-compose.yml run web python manage.py shell\n```\n\n다음의 파이썬 코드를 입력합니다. 오류가 없다면 연결이 잘 된 것입니다. \n\n```python\nfrom django.db import connection\ncursor = connection.cursor()\n```\n\npsql을 이용해 DB에 직접 연결해 DB를 테스트 해볼수도 있습니다. \n\n```bash\npsql -h [url] -U [username] -d [dbname] -p 5432\n```\n\n\n\n"},{"excerpt":"설치할 버전의 선택 오늘 날짜(2024년 5월 10일) 기준 Django 4.2 버전이 가장 최신의 LTS 버전이며, 해당 Django 버전에서는 4.2.8 버전부터 파이썬 3.12를 지원하며 그 미만 버전은 파이썬 3.11까지 지원합니다.  따라서 저는 파이썬 3.11, Django 4.2.13 버전을 설치하겠습니다.  파이썬 설치 장고를 설치할 디렉토…","fields":{"slug":"/Django-설치/"},"frontmatter":{"date":"May 10, 2024","title":"Django 설치","tags":["Django","Python","Pyenv","Docker","Docker-compose"]},"rawMarkdownBody":"## 설치할 버전의 선택\n\n오늘 날짜(2024년 5월 10일) 기준 [Django 4.2 버전](https://docs.djangoproject.com/en/5.0/releases/4.2/)이 가장 최신의 LTS 버전이며, 해당 Django 버전에서는 4.2.8 버전부터 파이썬 3.12를 지원하며 그 미만 버전은 파이썬 3.11까지 지원합니다. \n\n따라서 저는 파이썬 3.11, Django 4.2.13 버전을 설치하겠습니다. \n\n## 파이썬 설치\n\n장고를 설치할 디렉토리로 이동한 다음, [Pyenv](https://sharknia.github.io/Apple-Silicon과-pyenv)를 사용해서 파이썬을 설치하겠습니다. \n\n```shell\npyenv install 3.11.8\npyenv local 3.11.8\n```\n\n위의 명령어를 사용해 파이썬 3.11.8 버전을 설치 및 세팅합니다. \n\n만약 python 버전이 바뀌지 않았다면 [이 링크](https://sharknia.github.io/Pyenv-local-실행을-했는데-파이썬-버전이-바뀌지-않는다)를 참고합니다. \n\n```shell\nuser@MacBook-Pro django % python --version\nPython 3.11.8\n```\n\n파이썬이 설치되었습니다. \n\n## Django 설치\n\n[poetry](https://sharknia.github.io/Poetry)를 사용해 설치합니다. \n\npoetry에서 pyenv에서 설치된 파이썬 3.11을 사용한 가상환경을 만들도록 해야 합니다. \n\n```shell\npoetry env use $(pyenv which python3.11)\n```\n\n다음의 명령어를 사용해 poetry init 후 django를 설치합니다. \n\n```shell\npoetry init\npoetry add django^4.2.13\n```\n\n그 다음 poetry 가상 환경을 시작합니다. \n\n```typescript\npoetry shell\n```\n\n다음과 같이 확인할 수 있습니다. \n\n```shell\n(django-example-py3.11) user@MacBook-Pro django % python --version \nPython 3.11.8\n(django-example-py3.11) user@MacBook-Pro django % python -m django --version\n4.2.13\n```\n\n정상적으로 원하는 버전이 설치된 것을 확인할 수 있습니다. \n\n이제 기본 django 프로젝트를 세팅합니다. \n\n```shell\ndjango-admin startproject project .\n```\n\n## 기타 라이브러리 설치\n\nPostgresql을 사용할 것이므로, `psycopg2-binary`를 설치합니다. 또한 `gunicorn`도 설치합니다. \n\n```shell\npoetry add psycopg2-binary gunicorn\n```\n\n## Docker 이미지 및 컨테이너 생성과 실행\n\n여기까지만 해도 로컬에서 Django 프로젝트를 구동할 수 있습니다. 하지만 저는 일관된 개발 환경을 위해 Docker를 사용하려고 합니다. \n\n다음과 같이 Dockerfile을 작성합니다. \n\n```docker\n# 베이스 이미지를 Python 3.11로 설정\nFROM python:3.11-slim\n\n# 시스템 의존성 패키지 설치\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    libpq-dev \\\n    && apt-get clean\n\n# 환경 변수 설정\nENV PYTHONUNBUFFERED=1\n\n# 작업 디렉토리 설정\nWORKDIR /app\n\n# Poetry 설치\nRUN pip install --upgrade pip && pip install poetry\n\n# Poetry 설정 (가상환경을 컨테이너 외부로 생성하지 않도록)\nRUN poetry config virtualenvs.create false\n\n# 프로젝트 파일 복사\nCOPY pyproject.toml poetry.lock /app/\n\n# 의존성 설치\nRUN poetry install --no-root\n\n# 나머지 프로젝트 파일 복사\nCOPY . /app/\n\n# 장고 애플리케이션 실행\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n\n```\n\n이미지를 빌드합니다. \n\n```docker\ndocker build -t my-django-app .\n```\n\n컨테이너를 실행합니다. \n\n```docker\ndocker run -d -p 8000:8000 my-django-app\n```\n\n이제 [http://localhost:8000](http://localhost:8000/) 에서 장고 프로젝트가 실행된 것을 확인할 수 있습니다. \n\n![](image1.png)\n## **Docker Compose**\n\n짠! 하고 글이 종료됐다면 좋았겠지만 그렇지 않습니다. \n\n### Docker Compose란? \n\nDocker Compose는 멀티 컨테이너 Docker 애플리케이션을 정의하고 실행하기 위한 도구입니다. 각 서비스의 컨테이너 설정을 하나의 YAML 파일 (`docker-compose.yml`)에 정의하고, 단일 명령어로 모든 컨테이너를 쉽게 시작, 중지 및 관리할 수 있습니다.\n\n### 왜 사용하나요? \n\n환경의 일관성을 위해 컨테이너를 통해 서비스를 실행하게 되는데, 이렇게 하면 로컬에서 코드를 수정하고 테스트 할 때마다 이미지를 다시 말고 컨테이너를 다시 실행해야 하는 번거로움이 발생합니다. \n\n이를 방지하기 위해 docker-compose의 volumes 옵션으로 프로젝트 디렉토리를 마운트해 로컬의 코드 변경 사항이 컨테이너에 즉시 반영되도록 할 수 있습니다. \n\n### 개발 환경을 위한 docker-compose\n\n#### docker-compose.yml 파일 작성\n\n루트 디렉토리에 docker-compose.yml 파일을 생성하고 다음의 내용을 작성합니다. \n\n```yaml\nversion: '3.8'\n\nservices:\n    web:\n        build: .\n        command: gunicorn project.wsgi:application --bind 0.0.0.0:8000 --reload\n        volumes:\n            - .:/app\n        ports:\n            - '8000:8000'\n\n```\n\n- `volumes`의 `.:/app` 설정은 로컬 디렉토리 `.`(현재 디렉토리)를 컨테이너의 `/app` 디렉토리로 마운트합니다. 이렇게 하면 로컬 코드 변경 사항이 컨테이너 내부에서도 반영됩니다.\n\n- `gunicorn project.wsgi:application` 명령을 통해 애플리케이션 서버가 실행되며, 코드를 변경하면 해당 변경 사항이 컨테이너에 즉시 적용됩니다.\n\n- Note: 하지만 Gunicorn은 자체적으로 코드 변경 사항을 감지하고 다시 시작하지 않으므로, 개발 환경에서는 Gunicorn 대신 `runserver`를 사용하거나 Gunicorn에 `-reload` 옵션을 추가할 수 있습니다.\n\n#### **Docker Compose 실행**\n\n여기까지 완료되면 다음의 명령어를 사용해 서비스를 실행합니다. 위에서 테스트를 위해 실행했던 컨테이너는 중지해야 합니다. \n\n```bash\n# 빌드와 서비스 시작\ndocker-compose up --build -d\n\n# 서비스 상태 확인\ndocker-compose ps\n\n# 로그 확인\ndocker-compose logs\n\n# 모든 서비스 중지\ndocker-compose down\n```\n\n[http://localhost:8000](http://localhost:8000/) 에서 장고 프로젝트가 실행된 것을 확인할 수 있습니다. \n\n#### 코드 즉시 반영 테스트\n\n코드가 즉시 반영 되는지 테스트를 하기 위해 `project` 디렉토리 내에 `views.py` 파일을 생성하고 다음과 같이 내용을 작성합니다. \n\n```python\n# project/views.py\nfrom django.http import HttpResponse\n\n\ndef hello_world(request):\n    return HttpResponse(\"helloworld\")\n```\n\n`project/urls.py` 을 다음과 같이 수정합니다. \n\n```python\n# project/urls.py\nfrom django.contrib import admin\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('hello/', views.hello_world),  # /hello로 요청 시 helloworld 출력\n]\n```\n\n이후 바로 `http://localhost:8000/hello`로 접근하면 `helloworld`라는 메시지가 출력됩니다. 별도의 과정 없이 코드가 바로 반영됨을 확인할 수 있습니다. \n\n\n\n"},{"excerpt":"문제 이 링크를 참조해 Pyenv를 설치했습니다. 이후, python 3.11버전을 추가로 설치하고 새로 시작하는 프로젝트에서 python 3.11 버전을 사용하기 위해 pyenv local 명령어를 사용했는데, python 버전이 바뀌지 않은 것을 확인했습니다.  원인 이는  셈(shim) 경로 설정이 제대로 되어 있지 않아서 발생하는 문제입니다. 해결…","fields":{"slug":"/Pyenv-local-실행을-했는데-파이썬-버전이-바뀌지-않는다/"},"frontmatter":{"date":"May 09, 2024","title":"Pyenv local 실행을 했는데 파이썬 버전이 바뀌지 않는다","tags":["Pyenv","Homebrew","Python"]},"rawMarkdownBody":"## 문제\n\n[이 링크](https://sharknia.github.io/Apple-Silicon과-pyenv)를 참조해 Pyenv를 설치했습니다. 이후, python 3.11버전을 추가로 설치하고 새로 시작하는 프로젝트에서 python 3.11 버전을 사용하기 위해 pyenv local 명령어를 사용했는데,\n\n```shell\nuser@MacBook-Pro django % pyenv local 3.11.8\nuser@MacBook-Pro django % pyenv versions    \n  system\n  3.9.18\n* 3.11.8 (set by /Users/user/Develop/django/.python-version)\nuser@MacBook-Pro django % python3 --version \nPython 3.9.18\n```\n\npython 버전이 바뀌지 않은 것을 확인했습니다. \n\n## 원인\n\n이는 `pyenv` 셈(shim) 경로 설정이 제대로 되어 있지 않아서 발생하는 문제입니다.\n\n## 해결\n\n.zshrc에 다음의 내용을 추가합니다. \n\n```shell\n# pyenv 설정\nexport PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init --path)\"\n```\n\nshims는 사용자가 설정한 특정 Python 버전 및 가상환경을 가리키는 링크를 포함한 디렉토리입니다.\n이를 통해 python, pip, pytest 등의 명령어를 실행할 때 shims에 설정된 Python 버전을 자동으로 사용하게 됩니다.\n\n다음 명령어로 변경 사항을 적용합니다. \n\n```shell\nsource ~/.zshrc\n```\n\npyenv rehash 명령어를 사용해 재해시를 합니다. \n\n```shell\npyenv rehash\n```\n\n이제 다시 pyenv local을 사용하면 제대로 버전이 바뀐것을 확인할 수 있습니다. \n\n```shell\npyenv local 3.11.8\npython3 --version\n```\n\n\n\n"},{"excerpt":"서론 이제 작업이 막바지에 이르러 실서버를 꾸며야 하는 날이 왔습니다. 로컬에서 명령어를 입력해서 실서버에 배포를 하는 일은 절대로 사양이기 때문에 Bitbucket의 Pipelines를 활용해서 태그가 달리는 순간 자동적으로 Supabase 내용이 실서버에 적용되도록 CI/CD 워크플로우를 구축하려고 합니다.  이 포스팅은 해당 과정을 기록한 것입니다.…","fields":{"slug":"/Supabase를-위한-Bitbucket-Pipelines-CICD-워크플로우-구축하기/"},"frontmatter":{"date":"April 29, 2024","title":"Supabase를 위한 Bitbucket Pipelines CI/CD 워크플로우 구축하기","tags":["Supabase"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n이제 작업이 막바지에 이르러 실서버를 꾸며야 하는 날이 왔습니다. 로컬에서 명령어를 입력해서 실서버에 배포를 하는 일은 절대로 사양이기 때문에 Bitbucket의 Pipelines를 활용해서 태그가 달리는 순간 자동적으로 Supabase 내용이 실서버에 적용되도록 CI/CD 워크플로우를 구축하려고 합니다. \n\n이 포스팅은 해당 과정을 기록한 것입니다. \n\n## 워크플로우 파일 작성\n\n우선 워크플로우 파일을 작성해야 합니다. Github Action과 다르게 Bitbucket Pipelines를 위한 yml 파일은 루트 폴더에 바로 위치합니다. `bitbucket-pipelines.yml` 파일입니다. \n\n### 환경변수 정의\n\n[이 링크](https://supabase.com/docs/guides/cli/managing-environments?queryGroups=environment&environment=production) 를 보면 필요한 환경변수들이 정의되어 있습니다. Github Action을 위한 페이지이지만, Bitbucket에서도 해당 내용은 동일하게 적용됩니다. \n\n| SUPABASE_ACCESS_TOKEN | Supabase Access Token입니다.  | [이 페이지](https://supabase.com/dashboard/account/tokens) |  에서 발급받을 수 있습니다. |\n| --- | --- |\n| SUPABASE_DB_PASSWORD | DB 비밀번호입니다. 각 프로젝트의 Setting - Database 메뉴 - Database password 섹션에서 재발급 받을 수 있습니다.  |\n| SUPABASE_PROJECT_ID | 각 프로젝트의 Setting - General 메뉴 - Reference ID 섹션에서 확인할 수 있습니다.  |\n\n![](image2.png)\nBitbukcet 콘솔의 Pipelines를 정의하는 곳에서 위와 같이 설정해주면 됩니다. SUPABASE\\_ACCESS\\_TOKEN는 모든 환경에서 공통적으로 쓰이기 때문에 Repository 영역에 정의해주고, 나머지 두 환경변수는 각 환경 (dev, prod, staging 등.. ) 마다 다를 것이므로 Deployments 영역에 정의해주었습니다. \n\n### bitbucket-pipelines.yml\n\n완성된 코드는 다음과 같습니다. \n\n```yaml\nimage: node:20\n\npipelines:\n    tags:\n        'v*': # 태그 이름이 'v'로 시작하는 경우\n            - step:\n                  name: Edge function Deploy and DB Update\n                  caches:\n                      - node\n                  script:\n                      - npm i supabase\n                      - npx supabase link --project-ref $SUPABASE_PROJECT_ID\n                      - npx supabase functions deploy\n                      - yes | npx supabase db push\n                  services:\n                      - docker\n                  deployment: production\n```\n\ntags가 달렸을 때, 그 태그의 이름이 v로 시작한다면 production deployment의 워크플로우가 실행됩니다. \n\nsupabase를 설치해주고, 실서버 프로젝트에 연결해준 후, edge function을 배포하고, 마지막으로 DB의 변경사항을 체크해 db 스키마를 업데이트 합니다. \n\n## Troubleshooting\n\n처음에는 [해당 가이드](https://supabase.com/docs/guides/functions/cicd-workflow#bitbucket-pipelines)를 보고 코드를 작성하였습니다. \n\n### deployment의 정의\n\n<details>\n<summary>에러 코드 전문</summary>\n\n```yaml\nimage: node:20\n\npipelines:\n  branches:\n    main:\n      - step:\n          name: Setup\n          caches:\n            - node\n          script:\n            - npm i supabase\n      - step:\n          name: Functions Deploy\n          script:\n            - npx supabase functions deploy\n          services:\n            - docker\n          deployment: production\n      - step:\n          name: DB Update\n          script:\n            - npx supabase db push\n          services:\n            - docker\n          deployment: production\n```\n\n\n</details>\n\n처음 위와 같이 워크플로우를 작성하였을 때, 아래와 같은 오류가 발생했습니다. \n\n```bash\nConfiguration error\nThe deployment environment 'production' in your bitbucket-pipelines.yml file \noccurs multiple times in the pipeline. \nPlease refer to our documentation for valid environments and their ordering.\n```\n\nBitbucket Pipelines에서 발생하는 이 오류 메시지는 특정 단계에서 동일한 배포 환경(여기서는 'production')이 여러 번 지정되었을 때 나타납니다. 각 파이프라인 구성에서는 각 배포 환경을 한 번만 명시적으로 참조해야 합니다.\n\n여기서 문제는 'Functions Deploy' 단계와 'DB Update' 단계 모두에서 `deployment: production`이 사용되었다는 것입니다. 이러한 중복을 해결하기 위해 다음과 같은 방법들을 고려할 수 있습니다. \n\n어차피 모두 실행되어야 하는 코드 조각이므로, 여러 스텝을 단일 배포 단계로 통합해주었습니다. \n\n### npm i supabase를 했는데도 npx에서 다시 설치하려고 하는 문제\n\n<details>\n<summary>문제 코드 전문</summary>\n\n```yaml\nimage: node:20\n\npipelines:\n    tags:\n        'v*': # 태그 이름이 'v'로 시작하는 경우\n            - step:\n                  name: Setup\n                  caches:\n                      - node\n                  script:\n                      - npm i supabase\n                  services:\n                      - docker\n            - step:\n                  name: Edge function Deploy and DB Update\n                  caches:\n                      - node\n                  script:\n                      - npx supabase link --project-ref $SUPABASE_PROJECT_ID\n                      - npx supabase functions deploy\n                      - yes | npx supabase db push\n                  services:\n                      - docker\n                  deployment: production\n\n```\n\n\n</details>\n\n위와 같이 작성하면, 첫번째 스텝에서 npm i 를 통해 supabase를 저장하고 캐싱 해주었는데도 두번째 스텝에서 패키지를 찾지 못하고 npx에서 다시 인스톨을 합니다. \n\n```bash\nnpx supabase link --project-ref $SUPABASE_PROJECT_ID\n1s\n+ npx supabase link --project-ref $SUPABASE_PROJECT_ID\nnpm WARN exec The following package was not found and will be installed: supabase@1.163.6\n```\n\n결국 Step을 나누지 않고 한 Step에 합쳐주었습니다.\n\n### 프롬포트에 응답이 필요한 경우\n\n```yaml\nscript:\n  - npm i supabase\n  - npx supabase link --project-ref $PRODUCTION_PROJECT_ID --debug\n  - npx supabase db push --debug\n```\n\n이렇게 스크립트를 작성한 경우, 아래와 같이 출력된 상황에서 무한 로딩이 발생합니다. \n\n```bash\nDo you want to push these migrations to the remote database?\n • 20240329084939_remote_schema.sql\n • 20240415051605_remote_schema.sql\n • 20240415052055_remote_schema.sql\n • 20240415082434_remote_schema.sql\n • 20240415092355_storage.sql\n • 20240415092443_remote_schema.sql\n • 20240416060241_remote_schema.sql\n • 20240429012631_remote_schema.sql\n • 20240429061821_remote_schema.sql\n • 20240429062003_remote_schema.sql\n [Y/n] 2024/04/29 08:30:26 PG Recv: {\"Type\":\"ParameterStatus\",\"Name\":\"application_name\",\"Value\":\"Supavisor\"}\n```\n\n배포 작업이 진행되다보면, Y 또는 N 응답을 요구하는 경우가 있습니다. 이 경우가 바로 그 경우입니다. \n\nCI/CD 파이프라인에서 대화형 프롬프트에 자동으로 응답하는 방법은 일반적으로 스크립트를 사용하여 자동화하는 것입니다. 이 경우, `supabase db push` 명령이 사용자 입력을 요구하는데, 이를 자동화하기 위해 `yes` 명령을 사용할 수 있습니다.\n\n`yes` 명령은 반복적으로 `y`를 출력하여, 대화형 프롬프트에 자동으로 'yes'를 입력하는 데 사용됩니다. 이 명령을 `supabase db push`와 파이프라인하면 사용자 입력을 요구하는 모든 질문에 자동으로 `y`를 제공할 수 있습니다.\n\nBitbucket Pipelines의 스크립트를 다음과 같이 수정합니다. \n\n```yaml\nscript:\n  - npm i supabase\n  - npx supabase link --project-ref $PRODUCTION_PROJECT_ID --debug\n  - yes | npx supabase db push --debug\n```\n\n\n        물론, 무조건 Y를 연타하게 되기 때문에 주의해서 사용해야 합니다!\n\n### Supabase Migration 버전 맞추기\n\n```yaml\nyes | npx supabase db push\n2s\n+ yes | npx supabase db push\nConnecting to remote database...\nDo you want to push these migrations to the remote database?\n • 20240329084939_remote_schema.sql\n • 20240415051605_remote_schema.sql\n • 20240415052055_remote_schema.sql\n • 20240415082434_remote_schema.sql\n • 20240415092355_storage.sql\n • 20240415092443_remote_schema.sql\n • 20240416060241_remote_schema.sql\n • 20240429012631_remote_schema.sql\n • 20240429061821_remote_schema.sql\n • 20240429062003_remote_schema.sql\n [Y/n] y\nApplying migration 20240329084939_remote_schema.sql...\nERROR: type \"action_type_enum\" already exists (SQLSTATE 42710)    \nAt statement 24: CREATE TYPE \"public\".\"action_type_enum\" AS ENUM (\n    'pending',                                                    \n    'skipped',                                                    \n    'disliked',                                                   \n    'proposed',                                                   \n    'blocked'                                                     \n)                                                                 \nTry rerunning the command with --debug to troubleshoot the error.\n```\n\n이미 존재한다는 오류가 발생하고 있습니다. 로컬에서 remote의 마이그레이션 상태를 확인해봅시다. \n\n```yaml\nsupabase link --project-ref [ProjcetId]\n```\n\n를 통해 실서버 프로젝트와 연결한 후 \n\n```yaml\nsupabase migration list   \n```\n\n명령어를 사용하면\n\n```yaml\n        LOCAL      │ REMOTE │     TIME (UTC)\n  ─────────────────┼────────┼──────────────────────\n    20240329084931 │        │ 2024-03-29 08:49:39\n    20240415051602 │        │ 2024-04-15 05:16:05\n    20240415052053 │        │ 2024-04-15 05:20:55\n    20240415082434 │        │ 2024-04-15 08:24:34\n    20240415092355 │        │ 2024-04-15 09:23:55\n    20240415092447 │        │ 2024-04-15 09:24:43\n    20240416060241 │        │ 2024-04-16 06:02:41\n    20240429012631 │        │ 2024-04-29 01:26:31\n    20240429061821 │        │ 2024-04-29 06:18:21\n    20240429062003 │        │ 2024-04-29 06:20:03\n```\n\n이렇게, 마이그레이션 버전이 로컬과 원격이 맞지 않는 것을 확인할 수 있습니다. \n\n왜냐면 제가 실서버에 DB를 세팅할 때에 supabase migration 명령어를 사용한 것이 아니라 직접 생성된 쿼리문을 실서버에 실행시켰기 때문입니다. \n\nDB 상태가 이미 강제로 맞춰져 있을 것이므로(작업이 끝난 후 혹시 몰라 한 번 더 확인 하였습니다\\), 다음의 명령어를 통해 강제로 마이그레이션이 적용된 상태로 수리합니다. \n\n```yaml\nsupabase migration repair --status applied\n```\n\n이렇게 한 후, 다시 migration list 명령어를 사용하면 \n\n```yaml\nConnecting to remote database...\n\n  \n        LOCAL      │     REMOTE     │     TIME (UTC)\n  ─────────────────┼────────────────┼──────────────────────\n    20240329084939 │ 20240329084939 │ 2024-03-29 08:49:39\n    20240415051605 │ 20240415051605 │ 2024-04-15 05:16:05\n    20240415052055 │ 20240415052055 │ 2024-04-15 05:20:55\n    20240415082434 │ 20240415082434 │ 2024-04-15 08:24:34\n    20240415092355 │ 20240415092355 │ 2024-04-15 09:23:55\n    20240415092443 │ 20240415092443 │ 2024-04-15 09:24:43\n    20240416060241 │ 20240416060241 │ 2024-04-16 06:02:41\n    20240429012631 │ 20240429012631 │ 2024-04-29 01:26:31\n    20240429061821 │ 20240429061821 │ 2024-04-29 06:18:21\n    20240429062003 │ 20240429062003 │ 2024-04-29 06:20:03\n```\n\n정상적으로 마이그레이션이 적용되었고, CI/CD를 다시 태워보면\n\n```yaml\nyes | npx supabase db push\n2s\n+ yes | npx supabase db push\nConnecting to remote database...\nRemote database is up to date.\n```\n\n정상적으로 원격 DB가 최신화된 로그를 확인할 수 있습니다. \n\n## (Option) Edge Function의 배포\n\nEdge Function의 배포는 다음의 명령어를 통해 이뤄집니다. \n\n```bash\nsupabase functions deploy [Option: 함수명]\n```\n\n그런데 인증 jwt 토큰이 없어도 실행이 가능해야 하는 Edge function이 있을 수 있습니다. 이때에는 옵션값을 추가해줘야 합니다. 옵션은 다음과 같습니다. \n\n```bash\nsupabase functions deploy hello-world --no-verify-jwt\n```\n\n함수명을 입력하지 않고, deploy 까지만 했을 때에는 전체 함수를 한 번에 배포하게 되는데 기본적으로 jwt 토큰이 필요한 상태로 배포되게 됩니다. 따라서 특히 웹훅을 위한 함수 같은 경우 인증 오류가 발생하게 되므로, \n\n- Supabase Console에서 하나하나 토글을 해주거나\n\n- 다시 하나하나 옵션을 붙여 특정 함수를 배포해주거나 \n\n해야 하지만, [이 방법](https://supabase.com/docs/guides/functions/cicd-workflow#declarative-configuration)을 사용하면 supabase functions deploy 명령어만 사용해서 jwt 토큰 미사용 옵션값과 함께 한 번에 배포가 가능합니다. CI/CD를 위해서는 사실상 필수라고 할 수 있습니다. \n\nconfig.toml 파일에 다음과 같이 작성해주면 됩니다. \n\n```bash\n[functions.hello-world]\nverify_jwt = false\n```\n\n\n\n"},{"excerpt":"서론 Supabase에서 특정 edge function이나 db function, 또는 뷰나 테이블의 CRUD에 있어서도 계정에 따라 다른 결과물을 내놔야 하는 경우가 아주 많습니다. 쉽게 예를 들자면 특정 유저에게 내부 재화를 부여하는 기능이 관리자 계정은 가능하지만 일반 사용자 계정은 불가능해야 하는 경우가 있을 수 있겠네요.  User 데이터가 저장…","fields":{"slug":"/supabase-custom-claims를-활용한-관리자-권한-관리/"},"frontmatter":{"date":"April 25, 2024","title":"supabase-custom-claims를 활용한 관리자 권한 관리","tags":["Supabase","Postgresql"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nSupabase에서 특정 edge function이나 db function, 또는 뷰나 테이블의 CRUD에 있어서도 계정에 따라 다른 결과물을 내놔야 하는 경우가 아주 많습니다. 쉽게 예를 들자면 특정 유저에게 내부 재화를 부여하는 기능이 관리자 계정은 가능하지만 일반 사용자 계정은 불가능해야 하는 경우가 있을 수 있겠네요. \n\nUser 데이터가 저장된 테이블에 해당 유저의 권한을 저장해두고 이를 읽어서 사용한다면 쉽게 구현할 수 있겠지만, Supabase에서는 이를 더 효과적으로 구현하기 위한 방법을 준비해두었습니다. \n\n## supabase-custom-claims?\n\n### 소개\n\nsupabase-custom-claims를 사용해 인증된 사용자가 앱에 로그인할 때 받는 액세스 토큰에 JSON 데이터를 임의로 추가 또는 삭제할 수 있습니다. \n\n사용자의 계획, 레벨, 그룹, 가입 일자, 권한 등 다양한 정보를 포함시킬 수 있어 이를 사용해 애플리케이션의 특정 부분에 대한 Access 권한을 세밀하게 조정할 수 있습니다. \n\n문자열/숫자/Boolean/배열/JSON 객체등을 모두 저장할 수 있습니다. 저장된 데이터는 사용자 테이블의 raw\\_app\\_meta\\_data 컬럼에 저장됩니다. \n\n#### 장점\n\n사용자 정의 클레임은 사용자가 로그인할 때 받는 보안 토큰에 저장되며 이 클레임은 PostgreSQL 데이터베이스가 즉시 접근할 수 있도록 구성 매개변수로 제공됩니다. 따라서 데이터베이스의 디스크 입출력 없이 즉시 이 값을 사용할 수 있습니다. 특히 RLS에서 클레임을 사용하면 데이터베이스 호출을 크게 줄일 수 있어 크게 성능을 개선할 수 있습니다. \n\n#### 단점\n\n자동으로 업데이트 않으며 로그아웃 후 다시 로그인 해야 새 클레임이 적용됩니다. 따라서 자주 변경되는 데이터를 저장하기에는 적합하지 않습니다. supabase.auth.refreshSession()를 호출하여 강제로 새로 고칠 수는 있습니다. 다만 서버나 클레임 관리자의 수동 변경 이후 이를 알릴 수 있는 방법은 없습니다. \n\n#### 주의\n\nSupabase 인증 시스템에서 `provider`와 `providers`라는 사용자 정의 클레임을 이미 사용하므로 이 이름은 사용하지 않아야 합니다. \n\n## 특정 유저에게 권한 부여하기\n\n`set_claim` 함수는 특정 유저의 권한 정보를 사용자 정의 클레임(custom claims)으로 추가하거나 업데이트하는 데 사용됩니다. 이 함수는 Supabase에서 제공하는 사용자 관리 기능과 연동하여, `raw_app_meta_data` 필드를 업데이트합니다.\n\n```sql\n-- 특정 유저에게 claims_admin 권한 부여\nSELECT auth.set_claim(\n  user_id := '<user_id>',        -- 유저의 UUID\n  claim := 'app_metadata.claims_admin',\n  value := 'true'               -- 권한 값 (true/false, 숫자 등)\n);\n```\n\n#### **매개변수 설명**\n\n1. `user_id`: 권한을 부여할 사용자의 고유 ID(UUID). Supabase의 `auth.users` 테이블에서 확인할 수 있습니다.\n\n1. `claim`: 부여할 클레임의 이름입니다. 예를 들어, `app_metadata.claims_admin`은 관리자로 설정할 때 사용됩니다.\n\n1. `value`: 클레임의 값으로, `true`, `false`, 숫자, 문자열 등 다양한 데이터 타입을 사용할 수 있습니다.\n\n## 특정 유저의 권한 확인하기\n\n`is_claims_admin()` 함수는 현재 사용자 세션에서 `claims_admin` 권한이 있는지를 확인하는 데 사용됩니다. 이를 통해 관리 권한이 있는 사용자만 특정 기능을 사용할 수 있도록 제한할 수 있습니다.\n\n```sql\nCREATE OR REPLACE FUNCTION is_claims_admin()\nRETURNS BOOLEAN\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  -- 세션의 JWT 클레임에서 claims_admin 값 확인\n  RETURN coalesce(\n    (current_setting('request.jwt.claims', true)::jsonb)->'app_metadata'->>'claims_admin',\n    'false'\n  )::boolean;\nEND;\n$$;\n```\n\n\n\n## DB Function에서의 활용\n\n```sql\n    IF NOT is_claims_admin() THEN\n        RAISE EXCEPTION 'AccessDenied';\n    END IF;\n```\n\n## RLS에서의 활용\n\n이 두 기능은 Supabase의 RLS 정책과 함께 사용되며, 아래와 같이 `is_claims_admin()`를 활용해 RLS 정책을 정의할 수 있습니다.\n\n```sql\nCREATE POLICY \"Allow claims_admin\"\nON sensitive_table\nFOR ALL\nTO authenticated\nUSING (\n  is_claims_admin() = true\n);\n```\n\n## 기타 : Supabase 대시보드에서의 is\\_claims\\_admin()\n\n```sql\nCREATE OR REPLACE FUNCTION is_claims_admin()\n    RETURNS \"bool\"\n    LANGUAGE \"plpgsql\"\n    AS $$\n  BEGIN\n    IF session_user = 'authenticator' THEN\n      --------------------------------------------\n      -- To disallow any authenticated app users\n      -- from editing claims, delete the following\n      -- block of code and replace it with:\n      -- RETURN FALSE;\n      --------------------------------------------\n      IF extract(epoch from now()) > coalesce((current_setting('request.jwt.claims', true)::jsonb)->>'exp', '0')::numeric THEN\n        return false; -- jwt expired\n      END IF;\n      If current_setting('request.jwt.claims', true)::jsonb->>'role' = 'service_role' THEN\n        RETURN true; -- service role users have admin rights\n      END IF;\n      IF coalesce((current_setting('request.jwt.claims', true)::jsonb)->'app_metadata'->'claims_admin', 'false')::bool THEN\n        return true; -- user has claims_admin set to true\n      ELSE\n        return false; -- user does NOT have claims_admin set to true\n      END IF;\n      --------------------------------------------\n      -- End of block \n      --------------------------------------------\n    ELSE -- not a user session, probably being called from a trigger or something\n      return true;\n    END IF;\n  END;\n$$;\n```\n\nSupabase 대시보드에서 is\\_claims\\_admin()을 실행하면 반드시 true로 떨어집니다! Role을 바꿔서 시뮬레이션 하더라도 그렇습니다. 왜냐하면 대시보드에서 실행한 경우, `session_user` 가 `postgres` 이기 때문입니다. 실제 작동은 원하는대로 작동하지만, 대시보드에서의 테스트가 실제 실행 결과값과 달라져 테스트가 어려워지게 됩니다. Role을 바꾸는게 어떻게 작동하길래 이런 결과가 나올까요?\n\n### Supabase의 Role 변경 기능\n\nSupabase SQL Editor에서는 데이터베이스의 다양한 Role을 사용하여 쿼리를 실행할 수 있는 기능이 제공됩니다. 이는 PostgreSQL의 Role-Based Access Control (RBAC) 기능을 활용한 것으로, 서로 다른 Role을 적용하여 데이터베이스 접근을 시뮬레이션하고, 특정 Role이 어떤 데이터에 접근할 수 있는지 테스트할 수 있습니다.\n\n이 Role 변경 기능은 주로 보안 설정을 테스트하거나 다양한 사용자 권한을 시뮬레이션하는 데 사용됩니다. 예를 들어, 어떤 Role이 특정 테이블에 대한 접근 권한을 가지고 있는지, 또는 특정 작업을 수행할 수 있는지 등을 확인할 수 있습니다.\n\n### Role 변경과 session\\_user\n\n하지만 이때 Role을 변경하더라도 `session_user`는 변경되지 않습니다. `session_user`는 데이터베이스에 연결된 사용자의 로그인 이름을 반영하며, 세션의 생애 주기 동안 고정됩니다. Role을 변경한다고 해서 세션을 시작한 사용자가 변경되는 것은 아닙니다. Role은 현재 세션에서 실행 권한을 임시로 변경하는 것에 불과하므로, 이는 `session_user`의 값을 바꾸지 않습니다.\n\n이 기능은 데이터베이스의 보안 설정을 테스트하거나 다른 사용자의 권한으로 쿼리를 실행해 보려는 개발자에게 유용합니다. 하지만 `session_user`는 데이터베이스 세션의 보안 컨텍스트를 유지하는 중요한 요소로, 이를 통해 누가 실제로 데이터베이스에 접근하고 있는지 확인할 수 있습니다.\n\n### `session_user` 와 `current_user`\n\nEdge funcion이나 DB Function을 사용한 경우, 관리자 계정 또는 사용자 계정 둘 모두 `session_user`는 `authenticator` 이며, `current_user`는 `authenticated` 입니다. \n\n#### session\\_user vs current\\_user\n\n- `session_user`: 데이터베이스 세션을 시작할 때 설정된 사용자 신원을 나타냅니다. 이 값은 세션 동안 변경되지 않습니다.\n\n- `current_user`: 현재 실행 중인 명령이나 쿼리의 맥락에서 적용되는 사용자를 나타냅니다. `SET ROLE` 또는 `SET SESSION AUTHORIZATION`과 같은 명령을 사용하여 세션 중에 변경될 수 있습니다.\n\n결국 큰 차이는 변경할 수 있냐, 없냐에 있다고 할 수 있습니다. \n\n하지만 current\\_user를 변경하는 것은 클라이언트에서 거의 불가능한 작업입니다. `SET ROLE`이나 `SET SESSION AUTHORIZATION` 같은 명령을 통해 변경을 시도한다고 해도, 본인이 가지지 않은 권한으로의 변경은 사실상 불가능합니다. \n\n 특히 Supabase는 인증 토큰과 서버 사이드의 보안 설정을 통해 사용자 신원을 관리하며, 클라이언트에서 `current_user`를 임의로 변경할 수 없도록 합니다. 따라서, Supabase를 사용하는 경우 `current_user`를 기반으로 하는 보안 체크도 충분히 안전합니다. \n\n\n\n"},{"excerpt":"Supabase 특성 상, RLS와 뷰를 적극 활용해야 합니다. 그런데 create or replace 명령어를 사용해서 뷰를 생성하면 뷰의 생성에 사용된 테이블들의 RLS가 적용되지 않는 문제가 있습니다. \n예를 들어,  purchases등의 테이블에 RLS를 설정해서 본인의 것만 조회를 할 수 있도록 설정해두고 뷰를 생성한다면,  를 사용해서 조회한다…","fields":{"slug":"/Supabase에서-뷰에도-RLS-적용하기/"},"frontmatter":{"date":"April 12, 2024","title":"Supabase에서 뷰에도 RLS 적용하기","tags":["Supabase","Postgresql","DataBase"]},"rawMarkdownBody":"Supabase 특성 상, RLS와 뷰를 적극 활용해야 합니다. 그런데 create or replace 명령어를 사용해서 뷰를 생성하면 뷰의 생성에 사용된 테이블들의 RLS가 적용되지 않는 문제가 있습니다. \n예를 들어, \n\n```sql\nCREATE OR REPLACE VIEW v_user_data AS\nSELECT\n  id,\n  user_id,\n  purchase_date,\n  amount\nFROM\n  purchases\nWHERE\n  valid_until > now() \nORDER BY\n  purchase_date;\n\n```\n\npurchases등의 테이블에 RLS를 설정해서 본인의 것만 조회를 할 수 있도록 설정해두고 뷰를 생성한다면, \n\n```sql\nselect * from v_user_data \n```\n\n를 사용해서 조회한다면 RLS가 적용되지 않고 모든 ROW가 조회되는 것을 볼 수 있습니다. \n\n```sql\nselect\n  *\nfrom\n  (\n    SELECT\n      id,\n      user_id,\n      purchase_date,\n      amount\n    FROM\n      purchases\n    WHERE\n      valid_until > now()\n    ORDER BY\n      purchase_date\n  ) T\n```\n\n이렇게 조회를 한다면 정상적으로 RLS가 적용되어 본인의 것만 조회가 됨을 확인할 수 있습니다. \n\n이는 제가 의도한 작동이 아니며, 뷰에도 RLS가 적용되게 할 필요가 있습니다. \n\n## security invoker\n\n이를 위해 security invoker 옵션이 PostgreSQL v15부터 도입되었습니다. 이를 통해 뷰를 통해 데이터에 접근할 때 원본 테이블의 행 레벨 보안(Row-Level Security, RLS) 정책이 호출자(invoker) 기준으로 적용됩니다. 사용법은 다음과 같습니다. \n\n뷰를 생성한 후에, \n\n```sql\nalter view v_user_data set (security_invoker=on);\n```\n\n이렇게 `security_invoker` 옵션을 on으로 설정해주면 원본 테이블들의 RLS가 뷰에도 적용된 것을 확인할 수 있습니다. \n\n\n\n"},{"excerpt":"정답 URL 객체를 활용하면 됩니다.  자세히! Deno는 웹 표준 API를 지원하기 때문에,   객체와  객체를 제공합니다. 이를 활용하면 쿼리 스트링을 쉽게 다룰 수 있습니다. 먼저  객체에서 URL을 가져온 후,  객체를 생성하고 이 객체의  프로퍼티를 통해 쿼리 파라미터에 접근할 수 있습니다.아래는 GET 방식으로 넘어온 쿼리 스트링 변수를 어떻게…","fields":{"slug":"/Deno에서-URL의-쿼리-스트링Query-String-다루기/"},"frontmatter":{"date":"April 02, 2024","title":" Deno에서 URL의 쿼리 스트링(Query String) 다루기","tags":["Deno","Typescript"]},"rawMarkdownBody":"## 정답\n\nURL 객체를 활용하면 됩니다. \n\n## 자세히!\n\nDeno는 웹 표준 API를 지원하기 때문에,  `URL` 객체와 `URLSearchParams` 객체를 제공합니다. 이를 활용하면 쿼리 스트링을 쉽게 다룰 수 있습니다. 먼저 `Request` 객체에서 URL을 가져온 후, `URL` 객체를 생성하고 이 객체의 `searchParams` 프로퍼티를 통해 쿼리 파라미터에 접근할 수 있습니다.아래는 GET 방식으로 넘어온 쿼리 스트링 변수를 어떻게 처리하는지에 대한 예시입니다.\n\n```typescript\n// Deno.serve() 안에 있는 async 함수 내부에 추가합니다.\n// req 객체에서 URL을 가져온 후 URLSearchParams 객체를 사용하여 쿼리 파라미터에 접근합니다.\n\nconst url = new URL(req.url, `http://${req.headers.get(\"host\")}`);\nconst queryParams = url.searchParams;\n\n// 예를 들어, \"user_id\"라는 쿼리 파라미터 값을 얻고 싶다면\nconst userId = queryParams.get(\"user_id\"); // \"user_id\" 쿼리 스트링의 값을 가져옵니다.\n\n// 이제 userId 변수를 사용하여 필요한 로직을 구현할 수 있습니다.\n```\n\n## URLSearchParams?\n\n`URLSearchParams` 객체는 URL의 쿼리 스트링을 편리하게 다룰 수 있는 다양한 메서드를 제공합니다.\n\n- `get(name)`: 지정된 이름의 첫 번째 값을 반환합니다.\n\n- `getAll(name)`: 지정된 이름의 모든 값을 배열로 반환합니다.\n\n- `has(name)`: 지정된 이름의 쿼리 파라미터가 있는지 확인합니다.\n\n- `set(name, value)`: 지정된 이름과 값으로 새 쿼리 파라미터를 설정합니다.\n\n- `append(name, value)`: 지정된 이름과 값으로 새 쿼리 파라미터를 추가합니다.\n\n- `delete(name)`: 지정된 이름의 쿼리 파라미터를 제거합니다.\n\n예를 들어, `URLSearchParams` 객체를 활용하여 여러 개의 값을 가진 쿼리 파라미터를 처리할 수 있습니다.\n\n```typescript\nconst url = new URL(\"https://example.com/?tags=javascript&tags=deno&tags=nodejs\");\nconst queryParams = url.searchParams;\nconst tags = queryParams.getAll(\"tags\");// [\"javascript\", \"deno\", \"nodejs\"]\n```\n\n"},{"excerpt":"1. 조인 postgresql RLS에서 A테이블은 모두가 읽을 수 있고, B테이블은 나만 읽을 수 있다고 합시다. \n그럼 A와 B를 조인해서 만들어진 뷰는 어떻게 될까요? 내가 아닌 유저가 이 뷰를 읽으려고 시도하면 어떻게 될까요?  정답 이 사용자는 A 테이블에는 접근할 수 있지만, B 테이블에 대한 접근 권한이 없습니다. 따라서, RLS 정책에 의해…","fields":{"slug":"/RLS에-대한-궁금증-모음/"},"frontmatter":{"date":"March 21, 2024","title":"RLS에 대한 궁금증 모음","tags":["RLS","Postgresql"]},"rawMarkdownBody":"## 1. 조인\n\npostgresql RLS에서 A테이블은 모두가 읽을 수 있고, B테이블은 나만 읽을 수 있다고 합시다. \n그럼 A와 B를 조인해서 만들어진 뷰는 어떻게 될까요? 내가 아닌 유저가 이 뷰를 읽으려고 시도하면 어떻게 될까요? \n\n### 정답\n\n이 사용자는 A 테이블에는 접근할 수 있지만, B 테이블에 대한 접근 권한이 없습니다. 따라서, RLS 정책에 의해 B 테이블에서의 데이터 접근이 제한됩니다. 결과적으로, 조인된 뷰를 조회하려고 할 때 두 가지 가능한 결과가 있습니다:\n\n- 접근 거부: B 테이블에 접근할 수 없기 때문에, RLS 정책이 매우 엄격하게 설정되어 있어 전체 뷰에 대한 접근이 거부될 수 있습니다.\n\n- 부분적 결과: RLS 정책이나 쿼리 구조에 따라, B 테이블과 관련 없는 데이터(즉, A 테이블에서만 오는 데이터)만 조회할 수 있게 될 수도 있습니다. 이는 RLS 정책 구현 방법과 쿼리의 구체적인 세부 사항에 따라 달라집니다.\n\n### 해설\n\n#### RLS와 뷰의 작동 원리\n\nPostgreSQL에서 RLS는 테이블 레벨에서 설정되며, 해당 테이블에 대한 쿼리가 실행될 때마다 RLS 정책이 적용됩니다. 이 정책은 `SELECT`, `INSERT`, `UPDATE`, `DELETE` 명령에 대해 설정할 수 있으며, 사용자가 명령을 실행할 때마다 정책에 정의된 조건에 따라 접근이 허용되거나 거부됩니다.\n\n뷰는 기본적으로 하나 이상의 테이블에서 데이터를 검색하기 위한 쿼리를 저장한 객체입니다. 뷰 자체에는 데이터가 저장되어 있지 않으며, 뷰를 쿼리할 때마다 저장된 쿼리가 실행되어 결과가 반환됩니다.\n\n#### 조인된 뷰의 접근 제어\n\nA와 B를 조인해서 만들어진 뷰에 대한 접근 권한은, 뷰를 구성하는 기본 테이블의 RLS 정책과 해당 사용자의 권한에 따라 달라집니다. 즉, 뷰를 통해 데이터에 접근하려 할 때, 뷰에 포함된 각 테이블에 대한 접근 권한이 평가되고, 모든 관련 RLS 정책이 적용됩니다.\n\n## 2. 아무 권한 설정도 없다면? \n\nPostgreSQL의 행 레벨 보안(Row-Level Security, RLS)가 enable 되어있는데, 예를 들어 INSERT에 대해서 아무 policy가 정의되어 있지 않은데 내가 이 테이블에 Insert를 시도한다면 어떻게 될까요?\n\n### 정답\n\nPostgreSQL에서 행 레벨 보안(Row-Level Security, RLS)이 활성화되어 있고, 특정 작업(예를 들어 INSERT)에 대한 정책이 정의되어 있지 않다면, 그 작업에 대한 기본 정책은 \"DENY\"가 됩니다. 즉, 아무런 INSERT 정책이 설정되어 있지 않으면, 해당 테이블에 대한 INSERT 시도는 기본적으로 거부됩니다.\n\n## 3. 지금까지 설정한 RLS를 조회하는 방법은? \n\n다음의 쿼리문으로 여태 설정한 RLS들을 확인할 수 있습니다. \n\n```sql\nSELECT\n  pol.polname AS policy_name,\n  nsp.nspname AS schema_name,\n  tab.relname AS table_name,\n  pol.polcmd AS command,\n  pg_get_expr(pol.polqual, pol.polrelid) AS using,\n  pg_get_expr(pol.polwithcheck, pol.polrelid) AS with_check\nFROM\n  pg_policy pol\n  JOIN pg_class tab ON pol.polrelid = tab.oid\n  JOIN pg_namespace nsp ON tab.relnamespace = nsp.oid\nWHERE\n  tab.relname = '테이블 이름';\n```\n\n\n\n"},{"excerpt":"서론 PostgreSQL에서는 관계형 데이터를 JSON 형식으로 변환하거나 JSON 데이터를 관계형 테이블로 저장할 수 있는 다양한 기능을 제공합니다. 이번 글에서는 JSON 데이터를 생성하고 다루는 세 가지 유용한 함수인 , , 에 대해 알아보겠습니다. row_to_json  함수는 관계형 데이터의 단일 행(row)을 JSON 객체로 변환합니다. 이 함…","fields":{"slug":"/PostgreSQL에서-테이블-로우를-JSON으로-변환하기/"},"frontmatter":{"date":"March 20, 2024","title":"PostgreSQL에서 테이블 로우를 JSON으로 변환하기","tags":["Postgresql"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nPostgreSQL에서는 관계형 데이터를 JSON 형식으로 변환하거나 JSON 데이터를 관계형 테이블로 저장할 수 있는 다양한 기능을 제공합니다. 이번 글에서는 JSON 데이터를 생성하고 다루는 세 가지 유용한 함수인 `row_to_json`, `json_agg`, `json_build_object`에 대해 알아보겠습니다.\n\n## row\\_to\\_json\n\n`row_to_json` 함수는 관계형 데이터의 단일 행(row)을 JSON 객체로 변환합니다. 이 함수는 테이블의 모든 컬럼을 자동으로 JSON 키-값 쌍으로 매핑하므로, 컬럼을 하나씩 지정할 필요가 없어 편리합니다.\n\n```sql\nSELECT row_to_json(row) FROM (\n  SELECT id, name, email FROM users WHERE id = 1\n) row;\n```\n\n위 쿼리는 `users` 테이블에서 `id`, `name`, `email` 컬럼을 선택하고, 이를 JSON 객체로 변환합니다.\n\n## json\\_agg\n\n`json_agg` 함수는 여러 행을 JSON 배열로 집계합니다. 이 함수는 하위 쿼리에서 생성된 행 집합을 JSON 배열로 반환합니다.\n\n```sql\nSELECT json_agg(row_to_json(row)) FROM (\n  SELECT id, name, email FROM users\n) row;\n```\n\n위 쿼리는 `users` 테이블의 모든 행을 JSON 객체로 변환한 다음, 이러한 JSON 객체들을 JSON 배열로 집계합니다.\n\n## json\\_build\\_object\n\n`json_build_object` 함수는 키-값 쌍을 입력받아 JSON 객체를 생성합니다. 이 함수는 동적으로 JSON 객체를 구성할 때 유용합니다.\n\n```sql\nSELECT json_build_object(\n  'id', id,\n  'name', name,\n  'email', email\n) FROM users WHERE id = 1;\n```\n\n위 쿼리는 `id`, `name`, `email` 컬럼 값을 사용하여 JSON 객체를 동적으로 생성합니다.\n\n이들 함수를 조합하면 더욱 복잡한 JSON 데이터 구조를 생성할 수 있습니다. 예를 들어, 다음 쿼리는 사용자별 주문 내역을 JSON 형식으로 반환합니다.\n\n```sql\nSELECT json_build_object(\n  'user', row_to_json(u),\n  'orders', json_agg(row_to_json(o))\n) FROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id;\n```\n\n이 쿼리는 `users` 테이블과 `orders` 테이블을 조인하여, 각 사용자의 정보와 해당 사용자의 주문 내역을 JSON 객체로 반환합니다. `row_to_json`은 사용자 정보와 각 주문 정보를 JSON 객체로 변환하고, `json_agg`는 주문 내역을 JSON 배열로 집계합니다. 마지막으로 `json_build_object`는 사용자 정보와 주문 내역 JSON 배열을 포함하는 최종 JSON 객체를 생성합니다.\n\n## 다른 함수들\n\n1. json\\_object\\_agg(key, value)\n\n    - `json_agg` 함수와 유사하지만, 키-값 쌍으로 구성된 JSON 객체를 생성합니다.\n\n    - 예시: `SELECT json_object_agg(id, name) FROM users;`\n\n1. json\\_populate\\_record(base\\_record, json)\n\n    - JSON 데이터를 PostgreSQL 레코드 타입으로 변환합니다.\n\n    - 예시: `SELECT json_populate_record(null::users, '{\"id\":1, \"name\":\"John\"}'::json);`\n\n1. json\\_to\\_record(json)\n\n    - JSON 데이터를 레코드 타입으로 변환하고, 필드 이름을 키로 사용합니다.\n\n    - 예시: `SELECT * FROM json_to_record('{\"id\":1, \"name\":\"John\"}') AS x(id int, name text);`\n\n1. json\\_array\\_elements(json)\n\n    - JSON 배열의 각 요소를 별도의 행으로 반환합니다.\n\n    - 예시: `SELECT json_array_elements('[1,2,3]');`\n\n1. json\\_array\\_elements\\_text(json)\n\n    - JSON 배열의 각 요소를 텍스트로 반환합니다.\n\n    - 예시: `SELECT json_array_elements_text('[\"a\", \"b\", \"c\"]');`\n\n1. json\\_extract\\_path(json, path)\n\n    - JSON 데이터에서 지정된 경로의 값을 추출합니다.\n\n    - 예시: `SELECT json_extract_path('{\"a\": [1,2,3], \"b\": 4}', '$.a[0]');`\n\n1. json\\_extract\\_path\\_text(json, path)\n\n    - JSON 데이터에서 지정된 경로의 값을 텍스트로 추출합니다.\n\n    - 예시: `SELECT json_extract_path_text('{\"a\": [1,2,3], \"b\": 4}', '$.b');`  \n\n## 관계형 데이터 조인 vs JSON 데이터 변환\n\n관계형 데이터를 클라이언트에게 제공할 때, 데이터를 그대로 조인하여 내려주는 방식과 JSON 형식으로 변환하여 내려주는 방식 중 선택할 수 있습니다. 각 방식의 장단점은 다음과 같습니다.\n\n### 관계형 데이터 조인\n\n- 장점:\n\n    - 별도의 데이터 변환 과정이 필요 없어 처리 속도가 빠름\n\n    - 관계형 데이터베이스의 성능 최적화 기능을 그대로 활용할 수 있음\n\n    - 클라이언트에서 데이터 구조 변경이 필요 없음\n\n- 단점:\n\n    - 데이터베이스 스키마 구조에 따라 데이터 형태가 고정됨\n\n    - 계층적 데이터 구조를 표현하기 어려움\n\n    - 클라이언트 측에서 추가 데이터 가공이 필요할 수 있음\n\n### JSON 데이터 변환\n\n- 장점:\n\n    - 데이터 구조를 유연하게 표현할 수 있음\n\n    - 계층적 데이터 구조를 쉽게 표현 가능\n\n    - 클라이언트 측에서 추가 데이터 가공이 필요 없음\n\n    - 다양한 클라이언트(웹, 모바일 앱 등)에 호환 가능\n\n- 단점:\n\n    - 데이터 변환 과정에서 추가 처리 비용 발생\n\n    - 대량 데이터 처리 시 성능 이슈 발생 가능\n\n    - 데이터베이스 최적화 기능을 활용하기 어려움\n\n일반적으로 관계형 데이터를 직접 내려주는 방식이 더 간편하고 성능이 좋습니다. 하지만 데이터 구조가 복잡하거나 다양한 클라이언트에 대응해야 하는 경우, JSON으로 변환하여 내려주는 방식이 유리할 수 있습니다.\n\n따라서 데이터의 양과 복잡성, 클라이언트 유형, 성능 요구 사항 등을 종합적으로 고려하여 적절한 방식을 선택해야 합니다. 또한 두 가지 방식을 혼합하여 사용할 수도 있습니다. 예를 들어, 대량의 단순 데이터는 관계형 데이터로 내려주고, 복잡한 구조의 데이터는 JSON으로 변환하여 내려주는 식입니다.\n\n## 결론\n\n이번 글에서는 PostgreSQL에서 관계형 데이터를 JSON으로 변환하는 방법과 관련 함수들에 대해 알아보았습니다. `row_to_json`, `json_agg`, `json_build_object` 등의 함수를 활용하면 간단한 단일 행부터 복잡한 계층적 구조까지 다양한 형태의 JSON 데이터를 생성할 수 있습니다.\n\n하지만 실제 프로젝트에서는 단순히 JSON 데이터를 생성하는 것 이상의 고려 사항이 있습니다. 먼저 JSON 변환 작업에 따른 성능 오버헤드를 염두에 두어야 합니다. 대량의 데이터를 JSON으로 변환할 경우 성능 이슈가 발생할 수 있으므로, 필요에 따라 적절한 인덱싱이나 파티셔닝 등의 최적화 작업이 수반되어야 합니다.\n\n또한 JSON 데이터를 활용하는 클라이언트 유형과 요구 사항을 파악하는 것도 중요합니다. 웹 API와 모바일 앱의 요구 사항이 다를 수 있으므로, 클라이언트 측에서 추가 작업이 필요한지 여부를 고려해야 합니다. 이를 위해 프론트엔드 개발자와의 긴밀한 협업이 필수적입니다.\n\n\n\n"},{"excerpt":"서론 PostgreSQL의 ENUM 타입은 유연성이 제한적이라는 단점이 있습니다. 일단 생성되면 ENUM 타입에 값을 직접 추가하거나 삭제할 수 없기 때문입니다. 하지만 간단한 우회 방법을 통해 ENUM 타입 값 삭제가 가능합니다. 이 포스트에서는 기존에 정의된 ENUM 타입에서 하나의 값만 삭제하는 방법에 대해 알아보겠습니다. 시나리오 기존에 정의된  …","fields":{"slug":"/Postgresql에서-ENUM-타입에서-값-삭제하기/"},"frontmatter":{"date":"March 19, 2024","title":"Postgresql에서 ENUM 타입에서 값 삭제하기","tags":["Postgresql"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nPostgreSQL의 ENUM 타입은 유연성이 제한적이라는 단점이 있습니다. 일단 생성되면 ENUM 타입에 값을 직접 추가하거나 삭제할 수 없기 때문입니다. 하지만 간단한 우회 방법을 통해 ENUM 타입 값 삭제가 가능합니다.\n\n이 포스트에서는 기존에 정의된 ENUM 타입에서 하나의 값만 삭제하는 방법에 대해 알아보겠습니다.\n\n### 시나리오\n\n기존에 정의된 `mood` ENUM 타입이 아래와 같이 있다고 가정해봅시다.\n\n```sql\nsqlCopy codeCREATE TYPE mood AS ENUM ('sad', 'ok', 'happy');\n\n```\n\n여기서 'ok' 값을 삭제하고자 한다면 어떻게 해야할까요?\n\n## 값 삭제 절차\n\n1. **새 ENUM 타입 생성**: 삭제하려는 값을 제외한 새 ENUM 타입을 생성합니다.\n\n    ```sql\n    CREATE TYPE mood_new AS ENUM ('sad', 'happy');\n    ```\n\n1. **컬럼 타입 변경**: 기존 컬럼의 타입을 새로 생성한 ENUM 타입으로 변경합니다.\n\n    ```sql\n    ALTER TABLE your_table\n    ALTER COLUMN your_column TYPE mood_new\n    USING your_column::text::mood_new;\n    ```\n\n1. **기존 ENUM 타입 삭제**: 기존 ENUM 타입을 DROP 합니다.\n\n    ```sql\n    DROP TYPE mood;\n    ```\n\n1. **새 ENUM 이름 변경(옵션)**: 필요하다면 새 ENUM 타입의 이름을 기존 이름으로 변경할 수 있습니다.\n\n    ```sql\n    ALTER TYPE mood_new RENAME TO mood;\n    ```\n\n## 주의사항\n\n- 삭제하려는 ENUM 값이 기존 데이터에 사용되고 있다면, 이 절차를 진행하기 전에 해당 데이터를 먼저 변경하거나 삭제해야 합니다.\n\n- 이 작업은 ENUM 타입을 사용하는 모든 테이블과 뷰에 영향을 미칠 수 있으므로, 실행 전 데이터베이스 백업을 진행하는 것이 안전합니다.\n\n"},{"excerpt":"JSON 데이터와 스키마 유효성 검사의 필요성 데이터베이스에서 데이터 무결성을 보장하기 위해서는 데이터에 대한 제약조건을 정의하고 검사하는 것이 필수적입니다. 관계형 데이터베이스에서는 테이블 스키마를 정의하여 열 데이터 타입, NOT NULL 제약조건 등을 설정할 수 있습니다. 하지만 JSON 데이터 타입의 경우, 스키마가 존재하지 않기 때문에 유연성은 …","fields":{"slug":"/postgresql의-pg_jsonschema/"},"frontmatter":{"date":"March 18, 2024","title":"postgresql의 pg_jsonschema","tags":["Postgresql","Supabase"]},"rawMarkdownBody":"![](image1.png)\n## JSON 데이터와 스키마 유효성 검사의 필요성\n\n데이터베이스에서 데이터 무결성을 보장하기 위해서는 데이터에 대한 제약조건을 정의하고 검사하는 것이 필수적입니다. 관계형 데이터베이스에서는 테이블 스키마를 정의하여 열 데이터 타입, NOT NULL 제약조건 등을 설정할 수 있습니다. 하지만 JSON 데이터 타입의 경우, 스키마가 존재하지 않기 때문에 유연성은 높지만 데이터 무결성 보장이 어렵습니다.\n\n따라서 JSON 데이터에 대해서도 스키마를 정의하고 유효성 검사를 수행할 수 있다면, 데이터 무결성을 높이고 애플리케이션의 안정성을 향상시킬 수 있습니다. 여기에서 JSON 스키마와 pg\\_jsonschema 확장 기능이 등장합니다.\n\n## JSON 스키마란?\n\nJSON 스키마는 JSON 데이터의 구조와 형식을 정의하는 스키마입니다. 키-값 쌍의 데이터 타입, 필수 필드, 허용되는 값의 범위 등을 지정할 수 있습니다. JSON 스키마는 JSON 데이터 포맷으로 작성되며, 다양한 규칙과 제약조건을 정의할 수 있습니다.\n\n예를 들어, 다음과 같은 JSON 스키마를 정의할 수 있습니다:\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": { \"type\": \"string\" },\n    \"age\": { \"type\": \"number\", \"minimum\": 0 }\n  },\n  \"required\": [\"name\"]\n}\n\n```\n\n이 스키마는 JSON 객체여야 하며, name 필드는 문자열이어야 하고 필수 필드이며, age 필드는 0 이상의 숫자여야 함을 정의합니다.\n\n## pg\\_jsonschema 소개\n\npg\\_jsonschema는 PostgreSQL에서 JSON 데이터에 대한 스키마 유효성 검사를 지원하는 확장 기능입니다. 이 확장 기능을 사용하면 JSON 스키마를 정의하고, 해당 스키마에 따라 JSON 데이터의 유효성을 검사할 수 있습니다. pg\\_jsonschema는 오픈 소스 프로젝트로, Supabase에서 개발 및 유지보수하고 있습니다.\n\n## pg\\_jsonschema의 설치\n\n### Supabase의 경우\n\n만약 supabase를 사용하고 있다면, 다음의 명령어만으로 간단하게 확장을 활성화 할 수 있습니다.\n\n```sql\nCREATE EXTENSION pg_jsonschema;\n```\n\n만약 supabase가 아니라면 설치가 필요합니다. \n\n### Supabase가 아닌 경우\n\n1. 사전 요구사항\n\n    - PostgreSQL 12 이상 버전\n\n    - PostgreSQL 데이터베이스 관리자 권한\n\n1. 소스 코드를 다운로드 합니다. pg\\_jsonschema 소스 코드는 GitHub 저장소에서 다운로드할 수 있습니다.\n\n    ```shell\n    git clone https://github.com/supabase/pg_jsonschema.git\n    ```\n\n1. 빌드 및 설치 소스 코드 디렉터리로 이동한 후, 다음 명령어를 실행하여 확장 기능을 빌드하고 설치합니다.이 명령은 pg\\_jsonschema 확장 기능을 PostgreSQL 데이터베이스에 설치합니다.\n\n    ```shell\n    cd pg_jsonschema\n    make install\n    ```\n\n1. 확장 기능 활성화 PostgreSQL 데이터베이스에 접속한 후, 다음 SQL 문을 실행하여 pg\\_jsonschema 확장 기능을 활성화합니다.\n\n    ```sql\n    CREATE EXTENSION pg_jsonschema;\n    ```\n\n    성공적으로 활성화되면 다음과 같은 메시지가 표시됩니다.\n\n    ```plain text\n    CREATE EXTENSION\n    ```\n\n이제 pg\\_jsonschema 확장 기능을 사용할 수 있게 되었습니다.\n\n## pg\\_jsonschema 기능 및 활용\n\npg\\_jsonschema는 다음과 같은 두 가지 주요 함수를 제공합니다:\n\n- `json_matches_schema(schema json, instance json)`: JSON 데이터와 JSON 스키마를 입력받아, 해당 데이터가 스키마를 만족하는지 여부를 반환합니다.\n\n- `jsonb_matches_schema(schema json, instance jsonb)`: JSONB 데이터와 JSON 스키마를 입력받아, 해당 데이터가 스키마를 만족하는지 여부를 반환합니다.\n\n이 함수들을 사용하여 PostgreSQL 테이블의 CHECK 제약조건을 정의할 수 있습니다. 예를 들어, 다음과 같이 제약조건을 추가할 수 있습니다:\n\n```sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    data JSONB NOT NULL,\n    CHECK (jsonb_matches_schema(\n        schema => '{\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": { \"type\": \"string\" },\n                \"age\": { \"type\": \"number\", \"minimum\": 0 }\n            },\n            \"required\": [\"name\"]\n        }'::json,\n        instance => data\n    ))\n);\n\n```\n\n이렇게 하면 users 테이블의 data 열에 저장되는 JSONB 데이터가 반드시 지정된 스키마를 만족해야 합니다. 스키마를 위반하는 데이터를 INSERT 또는 UPDATE하려고 하면 오류가 발생합니다.\n\n## pg\\_jsonschema를 활용한 실제 예제\n\n실제 예제를 통해 pg\\_jsonschema를 어떻게 활용할 수 있는지 살펴보겠습니다. 여기서는 게시물 데이터를 저장하는 posts 테이블을 예로 들겠습니다.\n\n```sql\nCREATE TABLE posts (\n    id SERIAL PRIMARY KEY,\n    author_id UUID NOT NULL,\n    content JSONB NOT NULL,\n    CHECK (jsonb_matches_schema(\n        schema => '{\n            \"type\": \"object\",\n            \"properties\": {\n                \"title\": { \"type\": \"string\" },\n                \"body\": { \"type\": \"string\" },\n                \"tags\": {\n                    \"type\": \"array\",\n                    \"items\": { \"type\": \"string\" },\n                    \"minItems\": 1\n                }\n            },\n            \"required\": [\"title\", \"body\", \"tags\"]\n        }'::json,\n        instance => content\n    ))\n);\n\n```\n\n이 테이블에서는 content 열에 JSONB 데이터가 저장되며, 이 데이터는 반드시 title(문자열), body(문자열), tags(문자열 배열, 최소 1개 이상의 항목) 필드를 포함해야 합니다.\n\n다음과 같이 유효한 데이터를 INSERT할 수 있습니다:\n\n```sql\nINSERT INTO posts (author_id, content)\nVALUES (\n    '8c189b74-9865-4e0b-9bfd-a1a6a3e7fcc0',\n    '{ \"title\": \"Hello, World!\", \"body\": \"This is my first post.\", \"tags\": [\"hello\", \"world\"] }'::jsonb\n);\n\n```\n\n하지만 다음과 같이 스키마를 위반하는 데이터를 INSERT하려고 하면 오류가 발생합니다:\n\n```sql\nINSERT INTO posts (author_id, content)\nVALUES (\n    '8c189b74-9865-4e0b-9bfd-a1a6a3e7fcc0',\n    '{ \"title\": 123, \"body\": \"Invalid data\" }'::jsonb\n);\n\n-- ERROR:  23514: check constraint \"posts_content_check\" of relation \"posts\" is violated by some row\n\n```\n\n이처럼 pg\\_jsonschema를 활용하면 PostgreSQL 데이터베이스 내에서 JSON 데이터에 대한 스키마 유효성 검사를 수행할 수 있습니다. 이를 통해 데이터 무결성을 높이고, 잘못된 데이터로 인한 애플리케이션 오류를 방지할 수 있습니다.\n\n## 마무리\n\n이 글에서는 PostgreSQL 확장 기능인 pg\\_jsonschema에 대해 알아보았습니다. JSON 데이터의 스키마 유효성 검사 필요성과 JSON 스키마의 개념, pg\\_jsonschema의 기능과 활용 방법, 실제 예제를 통해 실습해보는 과정을 다루었습니다.\n\npg\\_jsonschema를 사용하면 유연한 JSON 데이터의 장점을 그대로 활용하면서도, 데이터 무결성을 보장할 수 있습니다. 이를 통해 데이터베이스 애플리케이션의 안정성과 신뢰성을 높일 수 있습니다.\n\n"},{"excerpt":"서론 어떤 함수를 만들었었는지, 어떤 트리거를 만들었었는지가 관리하기가 어렵다고 느낍니다. 현재 개발 상황 상 따로 백엔드 서버도 없는 상황이어서 쿼리문을 관리하려고 합니다.  Supabase Dashboard에서도 확인이 가능하긴 하지만, 제한적인 부분이 있어 깔끔하게 쿼리문으로 확인하는 방법을 기록해두려고 합니다.  Function 생성 쿼리문 확인하…","fields":{"slug":"/Postgresql-Function-Trigger-생성-쿼리문-확인하기/"},"frontmatter":{"date":"March 15, 2024","title":"Postgresql Function, Trigger 생성 쿼리문 확인하기","tags":["Postgresql"]},"rawMarkdownBody":"## 서론\n\n어떤 함수를 만들었었는지, 어떤 트리거를 만들었었는지가 관리하기가 어렵다고 느낍니다. 현재 개발 상황 상 따로 백엔드 서버도 없는 상황이어서 쿼리문을 관리하려고 합니다. \n\nSupabase Dashboard에서도 확인이 가능하긴 하지만, 제한적인 부분이 있어 깔끔하게 쿼리문으로 확인하는 방법을 기록해두려고 합니다. \n\n## Function 생성 쿼리문 확인하기\n\n```sql\nSELECT pg_get_functiondef(f.oid)\nFROM pg_catalog.pg_proc f\nJOIN pg_catalog.pg_namespace n ON (f.pronamespace = n.oid)\nWHERE n.nspname = '스키마명'  -- 함수가 위치한 스키마명\nAND f.proname = '함수명';    -- 확인하고 싶은 함수명\n```\n\n## Trigger 생성 쿼리문 확인하기\n\n```sql\nSELECT \n    t.tgname AS trigger_name,\n    pg_catalog.pg_get_triggerdef(t.oid) AS trigger_definition,\n    p.proname AS trigger_function,\n    c.relname AS table_name,\n    n.nspname AS schema_name\nFROM \n    pg_catalog.pg_trigger t\n    JOIN pg_catalog.pg_proc p ON t.tgfoid = p.oid\n    JOIN pg_catalog.pg_class c ON t.tgrelid = c.oid\n    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid\nWHERE \n    c.relname = '테이블명'  -- 트리거가 설정된 테이블명\n    AND n.nspname = '스키마명';  -- 테이블이 위치한 스키마명\n```\n\n## 뷰 생성 쿼리문 확인하기\n\n사실 뷰 생성 쿼리문은 전문을 supabase에서 확인할 수 있습니다. 근데 그냥 서비스로 같이 기록합니다. \n\n```sql\nSELECT pg_get_viewdef('\"스키마명\".\"뷰명\"', true);\n```\n\n"},{"excerpt":"서론 supabase에는 DB 마이그레이션 기능이 있어, 연결된 프로젝트의 스키마를 그대로 로컬 supabase DB에 적용할 수 있습니다. 버전관리도 가능합니다.  근데 이것도 완벽한 기능은 아니어서, 꼬일때가 있고 그래서 다시 마이그레이션을 실행해야 할 수도 있습니다. 그 방법을 기록합니다.  마이그레이션 기록 초기화하기 로컬 마이그레이션 파일 삭제 …","fields":{"slug":"/Supabase-CLI-db-migration-이력-초기화하기/"},"frontmatter":{"date":"March 14, 2024","title":"Supabase CLI db migration 이력 초기화하기","tags":["Supabase"]},"rawMarkdownBody":"## 서론\n\nsupabase에는 DB 마이그레이션 기능이 있어, 연결된 프로젝트의 스키마를 그대로 로컬 supabase DB에 적용할 수 있습니다. 버전관리도 가능합니다. \n\n근데 이것도 완벽한 기능은 아니어서, 꼬일때가 있고 그래서 다시 마이그레이션을 실행해야 할 수도 있습니다. 그 방법을 기록합니다. \n\n## 마이그레이션 기록 초기화하기\n\n### 로컬 마이그레이션 파일 삭제\n\n다음의 명령어를 입력해 로컬 마이그레이션 파일들을 삭제합니다. \n\n```bash\nrm supabase/migrations/*\n```\n\n### 마이그레이션 기록 삭제\n\n기록을 하나하나 reverted 해줘야 합니다. `supabase migration list` 명령어로 초기화 할 마이그레이션 리스트와 REMOTE 키값을 확인합니다. \n\n다음의 명령어로 한땀 한땀 삭제합니다. \n\n```bash\nsupabase migration repair --status reverted [키값]\n```\n\n이렇게 모든 기록을 삭제 후, `supabase db pull` 을 실행해 다시 DB를 덤프하면 됩니다. \n\n\n\n"},{"excerpt":"개요 Edge function을 열심히 개발하고 있었는데, 코드 자동완성이나 줄바꿈이나 이런것들이 적용되지 않았습니다.  Edge function은 Deno를 사용하는데, 이것때문인 것 같긴 했는데 짬이 안나 살펴보지 못하다가 이제서야 살펴보니, 뭔가를 하지 않아서였습니다.  Deno 설치 정말 당연하게도 Deno를 먼저 설치해야 합니다. 이런것도 하지 …","fields":{"slug":"/Supabase-EdgeFunction---Deno-개발환경-꾸미기/"},"frontmatter":{"date":"March 14, 2024","title":"Supabase EdgeFunction  - Deno 개발환경 꾸미기","tags":["Supabase","Deno"]},"rawMarkdownBody":"![](image1.png)\n## 개요\n\nEdge function을 열심히 개발하고 있었는데, 코드 자동완성이나 줄바꿈이나 이런것들이 적용되지 않았습니다. \n\nEdge function은 Deno를 사용하는데, 이것때문인 것 같긴 했는데 짬이 안나 살펴보지 못하다가 이제서야 살펴보니, 뭔가를 하지 않아서였습니다. \n\n## Deno 설치\n\n정말 당연하게도 Deno를 먼저 설치해야 합니다. 이런것도 하지 않고 개발을 하고 있었습니다.. 맥 기준으로 설치 방법을 정리하겠습니다. \n\n### Deno 설치\n\n다음의 명령어를 통해 설치 여부를 먼저 확인해줍니다. \n\n```bash\ndeno --version\n```\n\n설치가 되어있지 않다면 설치를 진행합니다. [공식사이트](https://docs.deno.com/runtime/manual)에서 자세한 설치법을 확인할 수 있습니다. \n\n```bash\ncurl -fsSL https://deno.land/install.sh | sh\n```\n\n를 이용해 설치를 해줍니다. \n\n그 다음, ~/.zshrc 를 수정해 환경변수에 deno 실행 명령어를 등록합니다. \n\n### Deno VS Code 확장 설치\n\nvscode를 사용하고 있으므로 deno 확장을 설치해줍니다. \n\n### Deno 확장 활성화 및 설정\n\n확장 설치 후, Deno를 사용할 프로젝트 폴더에 대해 Deno 확장을 활성화해야 합니다. 프로젝트의 루트 디렉토리에 `.vscode` 폴더를 생성하고, 그 안에 `settings.json` 파일을 만들어 다음 설정을 추가합니다.\n\n```json\n{\n    \"deno.enable\": true,\n    \"deno.lint\": true,\n    \"deno.unstable\": true\n}\n```\n\n\n\n여기까지 해주고 vs code를 재실행하면 이제 개발환경이 잘 꾸며진 것을 확인할 수 있습니다. \n\n## 오류\n\n`Uncached or missing remote URL` 오류가 발생할때가 있습니다. 이 오류는 지정된 URL에서 모듈을 캐시할 수 없거나 URL이 잘못되었을 때 발생합니다. 이 오류는 Deno가 모듈을 다운로드하고 캐싱하는 방식과 관련이 있습니다. 오류 메시지는 `https://esm.sh/@supabase/supabase-js@2`로 지정된 모듈을 Deno가 찾지 못했거나 캐시하지 못했다는 것을 나타냅니다. \n\n### 해결법\n\nDeno는 모듈을 한 번 다운로드하면 로컬에 캐시합니다. 캐시된 모듈이 문제를 일으킬 수 있으므로, 모듈 캐시를 강제로 업데이트해 볼 수 있습니다. 다음 명령어를 사용하여 모듈을 강제로 캐시에서 업데이트하고 다시 로드할 수 있습니다.\n\n```bash\ndeno cache --reload https://esm.sh/@supabase/supabase-js@2\n```\n\n### 캐시 초기화\n\n외부 모듈이 자동으로 캐시되지 않거나 오래된 버전의 캐시가 문제를 일으킬 수 있습니다. Deno 캐시를 갱신하려면, 터미널에서 `deno cache --reload` 명령어를 사용할 수 있습니다.\n\n\n\n"},{"excerpt":"개요 지난 시간에 Egde Function을 만들어서 배포하는 것까지 진행해봤습니다. 하지만 저기서 끝낸다면 로컬에 DB가 없으므로 DB에 접근을 필요로 하는 Edge Function은 로컬에서의 정상적인 테스트가 불가능합니다.  따라서 이번에는 테스트에 필요한 환경을 구성하는 방법을 정리합니다.  사전 작업 테스트를 한다는 것은 실제 개발 또는 Prod…","fields":{"slug":"/Supabase-Local-개발-환경-꾸미기---DB-세팅/"},"frontmatter":{"date":"March 13, 2024","title":"Supabase Local 개발 환경 꾸미기 - DB 세팅","tags":["Supabase"]},"rawMarkdownBody":"![](image1.png)\n## 개요\n\n[지난 시간](https://sharknia.github.io/Supabase-Local-Dev-환경-꾸미기)에 Egde Function을 만들어서 배포하는 것까지 진행해봤습니다. 하지만 저기서 끝낸다면 로컬에 DB가 없으므로 DB에 접근을 필요로 하는 Edge Function은 로컬에서의 정상적인 테스트가 불가능합니다. \n\n따라서 이번에는 테스트에 필요한 환경을 구성하는 방법을 정리합니다. \n\n## 사전 작업\n\n테스트를 한다는 것은 실제 개발 또는 Production 서버와 같은 환경을 꾸며야 한다는 것이고, 이는 데이터나 스키마가 동일해야 한다는 것으로, 현재로서는 개발 환경 프로젝트와 동일한 DB 환경을 꾸미는 것을 목표로 합니다. 따라서, `supabase link ..` 작업까지는 완료가 되어있어야 합니다. \n\n## DB 스키마 맞추기\n\n### supabase db pull\n\n다음의 명령어를 통해 연결된 프로젝트의 스키마를 모두 생성하는 쿼리문을 생성할 수 있습니다. \n\n```bash\nsupabase db pull --linked\n```\n\n이렇게 하면 supbase/migrations 디렉토리 안에 `<timestamp>_remote_schema.sql` 꼴의 쿼리문 파일이 생성됩니다. 해당 쿼리문은 자동으로 실행되지는 않으며, 직접 쿼리문을 실행해줘야 합니다. \n\n초기 설정 이후에는 `supabase db pull --linked`를 실행하면 변경된 사항만 자동으로 쿼리문을 생성합니다. 스키마를 지정해 특정 스키마만 pull 할 수도 있습니다. \n\n로컬에서 DB가 돌게 되므로, 대시보드는 없지만 pgAdmin등을 통해 로컬 DB에도 연결할 수 있습니다. 연결 정보가 생각나지 않는다면 `supabase status` 명령어를 통해 다시 확인할 수 있습니다. \n\n### supabase migration\n\n버전 관리 기능도 갖추고 있습니다. \n\n- supabase migration list : 마이그레이션 상태를 확인합니다. \n\n- supabase migration up : 보류 중인 마이그레이션을 로컬 데이터베이스에 적용합니다. \n\n### supabase db reset\n\n여기까지 해두면, supabase db reset을 할 때마다 데이터베이스를 지웠다가 스키마를 다시 생성합니다. 언제든지 초기 상태로 돌아올 수 있습니다. \n\n## 테스트 데이터 맞추기\n\n원활한 테스트를 위해서는 스키마는 물론이고 테스트 데이터도 들어있어야 합니다. db reset를 할 때마다 테스트 데이터를 넣기가 여간 번거로운 일이 아닐텐데, seed.sql 파일에 insert 구문을 미리 작성해두면 db reset에서 데이터베이스 삭제 후, migration 쿼리문을 통해 스키마를 맞춘 후 seed.sql의 insert 구문을 실행해 테스트 데이터까지 삽입한 깔끔한 테스팅 상태로 맞춰줍니다. \n\n## 참고\n\n[https://supabase.com/docs/guides/cli/local-development](https://supabase.com/docs/guides/cli/local-development)\n\n[https://supabase.com/docs/reference/cli/supabase-db-pull](https://supabase.com/docs/reference/cli/supabase-db-pull)\n\n[https://supabase.com/docs/guides/cli/seeding-your-database](https://supabase.com/docs/guides/cli/seeding-your-database)\n\n"},{"excerpt":"Row-Level Security (RLS) 란? Row-Level Security (RLS)는 데이터베이스 시스템에서 제공하는 보안 기능 중 하나로, 데이터베이스의 각 행(row)에 대한 접근을 제어하는 세밀한 방법을 제공합니다. RLS를 사용하면 데이터베이스 사용자나 응용 프로그램이 특정 조건을 만족하는 행에만 접근하거나 그러한 행을 변경할 수 있도록…","fields":{"slug":"/Supabase와-Row-Level-Security-RLS/"},"frontmatter":{"date":"March 12, 2024","title":"Supabase와 Row-Level Security (RLS) ","tags":["Supabase","Postgresql","RLS"]},"rawMarkdownBody":"![](image1.png)\n## Row-Level Security (RLS) 란?\n\nRow-Level Security (RLS)는 데이터베이스 시스템에서 제공하는 보안 기능 중 하나로, 데이터베이스의 각 행(row)에 대한 접근을 제어하는 세밀한 방법을 제공합니다. RLS를 사용하면 데이터베이스 사용자나 응용 프로그램이 특정 조건을 만족하는 행에만 접근하거나 그러한 행을 변경할 수 있도록 정책을 설정할 수 있습니다. 이는 사용자별 데이터 접근 제한, 다중 사용자 환경에서의 데이터 격리, 그리고 민감한 정보의 보호와 같은 목적을 위해 중요합니다.\n\n### 특징\n\n#### 세밀한 접근 제어\n\nRLS는 사용자 또는 사용자 그룹별로 데이터베이스 행에 대한 접근 권한을 설정할 수 있게 해줍니다. 이를 통해 특정 데이터에 대한 접근을 정밀하게 제한할 수 있습니다.\n\n#### 정책 기반 보안\n\nRLS는 데이터베이스 레벨에서 정책(policy)을 정의하여 작동합니다. 이 정책들은 데이터에 대한 `SELECT`, `INSERT`, `UPDATE`, `DELETE`와 같은 작업 수행 시 적용되어, 해당 작업이 정책에 정의된 규칙을 충족시킬 때만 수행될 수 있습니다.\n\n#### 투명성\n\nRLS는 데이터베이스 시스템 내에서 자동으로 적용되므로, 응용 프로그램 코드를 변경하지 않고도 데이터 접근 제어를 강화할 수 있습니다. 사용자는 자신이 접근 권한을 가진 데이터만 볼 수 있으며, RLS 정책에 의해 접근이 제한된 데이터는 *존재하지 않는 것처럼* 처리됩니다.\n\n## 예제 파헤쳐보기\n\n먼저 RLS를 하나 설정해보고, 이 구문이 뭘 의미하는지 한 번 살펴보겠습니다. \n\n우선, 특정 테이블에 대해 RLS를 활성화 해줍니다. \n\n```sql\nALTER TABLE my_table ENABLE ROW LEVEL SECURITY;\n```\n\n특정 조건에 따라 데이터 접근을 제어하는 정책을 추가합니다. 예를 들어, 사용자가 자신의 데이터에만 접근할 수 있도록 하는 정책을 추가할 수 있습니다.\n\n```sql\nCREATE POLICY my_policy ON my_table FOR SELECT\nUSING (user_id = current_user_id());\n```\n\n이러한 방식으로 RLS를 구성함으로써, 데이터베이스 관리자는 데이터의 보안과 무결성을 유지하면서도 필요한 사용자에게 필요한 데이터에 대한 접근을 허용할 수 있습니다.\n\n이는 모든 쿼리에 WHERE 절을 추가하는 것으로 생각하면 좀 더 쉽습니다. 예를 들어 위 정책의 경우, \n\n```sql\nselect * from my_table\nwhere current_user_id = my_table.user_id;\n-- Policy is implicitly added.\n```\n\n이렇게 정책이 where 절에 암묵적으로 추가된다고 생각하면 이해하기 쉽습니다. \n\n### Supabase에서의 RLS\n\n좀 더 복잡하고, Supabase에 특화된 예제를 보면 다음과 같습니다. \n\n```python\ncreate policy \"Allow authenticated uploads\"\non storage.objects\nfor insert\nto authenticated\nwith check (\n  (storage.foldername(name))[1] = auth.uid()::text\n);\n```\n\n여기서 설정한 정책은 \"Allow authenticated uploads\"라는 이름으로, `storage.objects` 테이블에 대한 `insert` 작업에 적용됩니다. 이 정책은 다음과 같은 조건을 충족시키는 경우에만 삽입 작업을 허용합니다:\n\n\n\n객체의 이름에서 파생된 폴더 이름 (`storage.foldername(name)` 함수를 통해 추출)의 첫 번째 요소가 현재 인증된 사용자의 ID (`auth.uid()`)와 일치해야 합니다. 이는 사용자가 자신의 고유 폴더에만 파일을 업로드할 수 있도록 제한하는 조건입니다.\n\n\n\n요약하자면, 이 RLS 정책은 인증된 사용자가 'profile\\_images' 버킷 내에 자신의 사용자 ID에 해당하는 폴더에만 파일을 업로드할 수 있게 합니다.\n\n## Using과 Check\n\n위의 두 예제를 살펴보면 각각 Using과 Check를 사용한 것을 볼 수 있습니다. \n\nPostgreSQL의 행 수준 보안(Row-Level Security, RLS) 정책을 사용할 때, `WITH CHECK`와 `USING` 절은 서로 다른 목적으로 사용됩니다. 이들의 주요 차이점은 정책이 적용되는 시점과 데이터 조작 작업(DML) 유형에 따라 달라집니다.\n\n### `WITH CHECK`\n\n- `WITH CHECK` 절은 `INSERT`와 `UPDATE` 작업에 대해 사용됩니다.\n\n- 이 절은 새로운 데이터가 삽입되거나 기존 데이터가 업데이트될 때 적용되어야 하는 조건을 정의합니다.\n\n- `WITH CHECK`은 해당 작업이 실행된 후 최종적으로 데이터가 정책에 정의된 조건을 만족하는지 검증합니다. 만약 조건을 만족하지 않는다면, 작업은 거부됩니다.\n\n- 예를 들어, 사용자가 자신의 폴더에만 파일을 삽입하거나 업데이트할 수 있도록 제한하는 정책을 설정할 때 사용됩니다.\n\n### `USING`\n\n- `USING` 절은 `SELECT`와 `DELETE` 작업에 대해 사용됩니다.\n\n- 이 절은 해당 작업을 수행할 때 접근이 허용되는 행을 결정하는 조건을 정의합니다.\n\n- `SELECT` 작업에 대해, `USING` 조건을 만족하는 행만 조회될 수 있습니다.\n\n- `DELETE` 작업에 대해, `USING` 조건을 만족하는 행만 삭제될 수 있습니다.\n\n- 예를 들어, 사용자가 자신의 폴더에 있는 파일만 조회하거나 삭제할 수 있도록 제한하는 정책을 설정할 때 사용됩니다.\n\n### 예시\n\n결론적으로, `WITH CHECK`와 `USING`의 차이는 적용되는 데이터 조작 작업의 유형과 해당 조건이 검증되는 시점에 있습니다.\n\n## Roles\n\n### to authenticated\n\n`to authenticated` 구문은 해당 정책이 인증된 사용자, 즉 로그인한 사용자에게만 적용됨을 의미합니다. Supabase 및 PostgreSQL의 Row-Level Security (RLS) 정책에서는, 특정 데이터베이스 작업(예: 삽입, 조회, 수정, 삭제)에 대한 접근 권한을 세밀하게 제어할 수 있습니다. 여기서 `to authenticated`는 인증 과정을 성공적으로 마친 사용자, 즉 시스템이 식별하고 인증한 사용자에게만 해당 작업을 허용한다는 것을 명시합니다.\n\n### to anon\n\n이와 반대되는 개념으로는 `to anon`이 있습니다. Supabase Auth는 모든 요청을 `authenticated` 또는 `anon` 에 매핑합니다. `authenticated` 이 로그인한 유저라면, anon은 로그인 하지 않은 유저를 의미합니다. \n\n### 예제\n\n```sql\ncreate policy \"Profiles are viewable by everyone\"\non profiles for select\nto authenticated, anon\nusing ( true );\n\n-- OR\n\ncreate policy \"Public profiles are viewable only by authenticated users\"\non profiles for select\nto authenticated\nusing ( true );\n\n```\n\n“Profiles are viewable by everyone” 정책은 인증된 유저와 인증되지 않은 유저 모두에게 읽기를 허용하며, \"Public profiles are viewable only by authenticated users\" 정책은 오직 인증된 유저에게만 읽기를 허용합니다. \n\n### service\\_role\n\nSupabase는 `service_role` 같은 특별한 \"서비스\" 키를 제공하는데, 이는 RLS를 우회할 수 있어 관리 작업에 유용하나, 클라이언트 사이드나 고객에게 노출되어서는 안 됩니다. 이는 반드시 백엔드 단에서만 사용해야 합니다. \n\n\n\n**Supbase는 클라이언트 라이브러리가 Service Key로 초기화된 경우에도 서명한 사용자의 RLS 정책을 준수합니다.**\n\n## auth 모듈\n\n### auth.uid()\n\n`auth.uid()`는 현재 인증된 사용자의 고유 식별자(ID)를 반환하는 함수입니다. Supabase는 내부적으로 사용자 인증 시스템을 갖추고 있으며, 사용자가 로그인할 때 각 사용자에게 고유한 ID를 할당합니다. 이 ID는 사용자의 세션 및 인증 정보와 연결되어 있으며, `auth.uid()`를 통해 현재 세션의 사용자 ID를 쉽게 조회할 수 있습니다. 이를 통해 데이터베이스 내에서 사용자별 데이터 접근을 제어하거나 사용자 식별 정보를 기반으로 한 작업을 수행할 수 있습니다.\n\n예를 들어, 사용자가 파일을 업로드할 때, `auth.uid()`를 사용하여 해당 파일이 특정 사용자에게 속함을 식별하거나, 사용자별로 데이터를 격리하여 보안을 강화하는 데 활용할 수 있습니다. 사용자가 로그인 상태가 아니라면, `auth.uid()`는 `null`이나 유효하지 않은 값을 반환할 수 있으므로, `to authenticated` 구문과 함께 사용하여 인증된 사용자에게만 특정 작업을 허용하는 방식으로 보안을 관리합니다.\n\n### auth.jwt()\n\n현재 요청을 하는 사용자의 JWT(Json Web Token)를 반환합니다. JWT 내에는 사용자의 `app_metadata` 및 `user_metadata` 컬럼에 저장된 정보가 포함될 수 있습니다.\n\n- `user_metadata`: 사용자가 `supabase.auth.update()` 함수를 통해 업데이트할 수 있는 메타데이터입니다. 사용자 관련 정보(예: 선호 설정)를 저장하는 데 적합하지만, 인증 데이터를 저장하기에는 적합하지 않습니다.\n\n- `app_metadata`: 사용자가 업데이트할 수 없기 때문에 인증 데이터를 저장하는 데 적합한 메타데이터입니다. 예를 들어, 사용자가 특정 팀에 속해 있는지 여부를 판단하는 데 사용할 수 있는 팀 데이터를 여기에 저장할 수 있습니다.\n\n```sql\ncreate policy \"User is in team\"\non my_table\nto authenticated\nusing ( team_id in (select auth.jwt() -> 'app_metadata' -> 'teams'));\n```\n\n예를 들어, 사용자가 어떤 팀에 속해 있는지 확인하는 데 `app_metadata` 내에 저장된 팀 데이터(팀 ID 배열 등)를 사용할 수 있습니다. \n\n## 결론\n\nSupabase에서는 항상 RLS를 사용할 것을 권장하고 있습니다. 만약 모두의 접근이 필요한 경우에도, RLS를 활성화하고 인증된 유저와 인증되지 않은 유저에게 권한을 부여하는 방식을 사용할 것을 권장합니다. \n\n## 참고\n\n[https://supabase.com/docs/guides/database/postgres/row-level-security](https://supabase.com/docs/guides/database/postgres/row-level-security)\n\n"},{"excerpt":"Supabase CLI 설치 우선 supabase CLI를 설치해야 합니다.  제대로 설치가 되면 다음의 명령어를 통해 설치가 된 것을 확인합니다.  login을 진행합니다. 터미널의 다음의 명령어를 입력한 후, 로그인은 웹에서 이뤄집니다.  Project를 연결해야 합니다. 먼저 다음의 명령어를 통해 프로젝트 리스트를 확인합니다.  다음과 같은 결과가 …","fields":{"slug":"/Supabase-Local-Dev-환경-꾸미기/"},"frontmatter":{"date":"March 11, 2024","title":"Supabase Local Dev 환경 꾸미기","tags":["Supabase","Postgresql","Edge-Function"]},"rawMarkdownBody":"![](image1.png)\n## Supabase CLI 설치\n\n우선 supabase CLI를 설치해야 합니다. \n\n```shell\nbrew install supabase/tap/supabase\n```\n\n제대로 설치가 되면 다음의 명령어를 통해 설치가 된 것을 확인합니다. \n\n```shell\nsupabase help\n```\n\nlogin을 진행합니다. 터미널의 다음의 명령어를 입력한 후, 로그인은 웹에서 이뤄집니다. \n\n```shell\nsupabase login\n```\n\nProject를 연결해야 합니다. 먼저 다음의 명령어를 통해 프로젝트 리스트를 확인합니다. \n\n```shell\nsupabase projects list\n```\n\n다음과 같은 결과가 출력됩니다. \n\n```shell\n    LINKED │        ORG ID        │     REFERENCE ID     │   NAME   │         REGION         │  CREATED AT (UTC)\n  ─────────┼──────────────────────┼──────────────────────┼──────────┼────────────────────────┼──────────────────────\n           │ [ORG ID        ]     │ [REFERENCE ID     ]  │ [NAME]   │ Northeast Asia (Seoul) │ 2024-02-17 04:10:12\n```\n\n이 중에서 REFERENCE ID를 사용해 project에 연결할 수 있습니다. \n\n```shell\nsupabase link --project-ref [REFERENCE ID]\n```\n\n#### 참고\n\n## Edge Function 로컬에서 개발하기\n\n로컬에 supabase project를 생성합니다. \n\n```shell\nsupabase init\n```\n\n다음의 명령어를 사용하면 예제 Edge function을 만들 수 있습니다. 이 명령어는 함수를 로컬에 생성합니다. \n\n```shell\nsupabase functions new hello-world\n```\n\n다음과 같은 구조로 Edge Function이 생성됩니다. \n\n```shell\n└── supabase\n    ├── functions\n    │   └── hello-world\n    │   │   └── index.ts ## Your function code\n    └── config.toml\n```\n\n### 예제 코드\n\n```typescript\nDeno.serve(async (req) => {\n  const { name } = await req.json()\n  const data = {\n    message: `Hello ${name}!`,\n  }\n\n  return new Response(JSON.stringify(data), { headers: { 'Content-Type': 'application/json' } })\n})\n```\n\nEdge Function은 native Deno.serve를 사용합니다. \n\n## Edge Function 로컬에서 실행하기\n\nstart 명령어를 사용하면, 도커를 이용해서 로컬에서 supabase service를 실행할 수 있습니다. \n\n```shell\nsupabase start\n```\n\n성공적으로 실행되면 다음과 같은 내용이 출력됩니다. 해당 내용은 `supabase status` 명령어를 사용해 다시 볼 수 있습니다.\n\n```shell\nStarted supabase local development setup.\n\n         API URL: http://localhost:54321\n          DB URL: postgresql://postgres:postgres@localhost:54322/postgres\n      Studio URL: http://localhost:54323\n    Inbucket URL: http://localhost:54324\n        anon key: eyJh......\nservice_role key: eyJh......\n```\n\n`supabase stop` 명령을 사용하여 로컬 데이터베이스를 재설정하지 않고 언제든지 모든 서비스를 중지할 수 있습니다. 또한 `supabase stop --no-backup`을 사용하여 모든 서비스를 중지하고 로컬 데이터베이스를 재설정할 수 있습니다.\n\n\n\n다음의 명령어를 사용해 Function Watcher를 실행합니다. \n\n```shell\nsupabase functions serve\n```\n\n이 명령어는 hot-reloading 기능을 가지고 있어 코드에 수정이 생기면 Deno server를 자동으로 재시작합니다. \n\n\n\n여기까지 마친 후, 다음의 명령어를 통해 방금 만든 Edge Function을 실행해볼 수 있습니다. \n\n```shell\ncurl --request POST 'http://localhost:54321/functions/v1/hello-world' \\\n  --header 'Authorization: Bearer SUPABASE_ANON_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{ \"name\":\"Functions\" }'\n```\n\nSUPABASE\\_ANON\\_KEY는 start 이후 생성된 anon key를 사용하면 됩니다. 성공적으로 실행되면 다음과 같은 문구를 확인할 수 있습니다. \n\n```shell\n{\"message\":\"Hello Functions!\"}\n```\n\n## Edge Function 배포하기\n\n기본적인 배포는 다음의 명령어를 사용합니다. 모든 Edge Function을 배포합니다. \n\n```shell\nsupabase functions deploy\n```\n\n특정 Edge function만 배포할 수도 있습니다. \n\n```shell\nsupabase functions deploy hello-world\n```\n\n기본적으로 Edge Function은 JWT 토큰이 필수입니다. 토큰이 필요없는 Edge function을 만들기 위해서는 `--no-verify-jwt` 옵션을 사용합니다. \n\n```shell\nsupabase functions deploy hello-world --no-verify-jwt\n```\n\n배포가 성공적으로 이뤄지면 supabase 대쉬보드에서도 이를 확인할 수 있습니다. \n\n\n\n다음의 명령어를 사용해 방금 배포한 Edge Function을 테스트 해볼 수 있습니다. \n\n```shell\ncurl --request POST 'https://[REFERENCE ID].supabase.co/functions/v1/hello-world' \\\n  --header 'Authorization: Bearer ANON_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{ \"name\":\"Functions\" }'\n```\n\n`--no-verify-jwt` 옵션을 사용한 경우에는 ANON\\_KEY가 필요 없으며, ANON\\_KEY는 Supabase 대시보드에서 확인할 수 있습니다. \n\n## Edge Function 삭제\n\n다음의 명령어를 통해 Edge Function을 삭제할 수 있습니다.\n\n```shell\nsupabase functions delete hello-world --project-ref dcrdrguovpqchlptvlqy\n```\n\n단 이 명령어는 로컬에서는 작동하지 않습니다. 로컬의 함수는 수동으로 삭제하면 됩니다. \n\n## 기타 Edge Function 관련 CLI 명령어\n\nfunction의 리스트를 확인합니다. 이 코드는 연결된 프로젝트의 함수 리스트를 가져옵니다. \n\n```shell\nsupabase functions list\n```\n\n특정 function을 다운로드 합니다. \n\n```shell\nsupabase functions download <Function name>\n```\n\n공통적으로 `--project-ref` **옵션을 사용해 특정 project를 명시할 수 있습니다.** \n\n\n\n## Deno 환경에서 supabase client 사용하기\n\nDeno 환경에서는 npm을 사용하지 않으므로, supabase client가 필요할 경우 다음의 방법을 사용해 import 합니다. \n\n```typescript\n  import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n```\n\n또는 import map을 사용할 수 있습니다. \n\nsupabase/functions/import\\_map.json 파일을 생성 후, 다음의 내용을 입력해줍니다. \n\n```json\n{\n    \"imports\": {\n        \"supabase\": \"https://esm.sh/@supabase/supabase-js@2\"\n    }\n}\n```\n\nimport map을 사용하면 다음과 같이 쉽게 import 할 수 있습니다. \n\n```typescript\nimport { createClient } from 'supabase';\n```\n\n## Deno 환경에서 환경변수 사용하기\n\n다음의 코드를 사용해 환경변수를 사용할 수 있습니다. \n\n```typescript\nDeno.env.get(MY_SECRET_NAME)\n```\n\n현재 설정되어있는 환경 변수들은 다음의 명령어를 통해 확인할 수 있습니다. \n\n```shell\nsupabase secrets list\n```\n\n기본적으로 supabase는 4개의 환경 변수를 설정하지 않아도 지원합니다. \n\n- SUPABASE\\_URL\n\n- SUPABASE\\_ANON\\_KEY\n\n- SUPABASE\\_SERVICE\\_ROLE\\_KEY\n\n- SUPABASE\\_DB\\_URL\n\n### 환경변수 추가하기 - 로컬\n\n추가적으로 환경변수를 설정해야 할 때가 있습니다. 이 때 다음의 방법을 사용합니다. \n\n먼저 환경 변수를 생성합니다. \n\n```shell\necho \"MY_NAME=Yoda\" >> ./supabase/.env.local\n```\n\n그리고 로컬에서 serve 시에 다음의 옵션값을 함께 줍니다. \n\n```shell\nsupabase functions serve --env-file ./supabase/.env.local\n```\n\n이렇게 하면 로컬에서 MY\\_NAME 환경 변수를 사용할 수 있습니다. \n\n### 환경변수 추가하기 - 실서버\n\n우선 실서버에 적용할 환경변수 파일을 생성합니다. \n\n```shell\ncp ./supabase/.env.local ./supabase/.env\n```\n\n그리고 다음의 명령어를 사용해 환경변수를 등록합니다. \n\n```shell\nsupabase secrets set --env-file ./supabase/.env\n\n# You can also set secrets individually using:\nsupabase secrets set MY_NAME=Chewbacca\n```\n\n#### 참고\n\n[https://supabase.com/docs/guides/functions/secrets](https://supabase.com/docs/guides/functions/secrets)\n\n"},{"excerpt":"unique_constraints란? 테이블의 특정 컬럼 또는 컬럼의 조합에 대해 유니크 제약조건을 설정하는 데 사용됩니다. 이는 데이터베이스에 동일한 값을 가진 중복 레코드가 없도록 보장하는 데 유용합니다. unique_constraints 설정하기 클래스 내부에  속성을 정의하고, unique_constraints 튜플을 이 속성에 할당합니다. 이 튜…","fields":{"slug":"/SQLModel에서-unique_constraints-설정하기/"},"frontmatter":{"date":"March 05, 2024","title":"SQLModel에서 unique_constraints 설정하기","tags":["SqlAlchemy","SQLModel"]},"rawMarkdownBody":"## unique\\_constraints란? \n\n테이블의 특정 컬럼 또는 컬럼의 조합에 대해 유니크 제약조건을 설정하는 데 사용됩니다. 이는 데이터베이스에 동일한 값을 가진 중복 레코드가 없도록 보장하는 데 유용합니다.\n\n## unique\\_constraints 설정하기\n\n클래스 내부에 `__table_args__` 속성을 정의하고, unique\\_constraints 튜플을 이 속성에 할당합니다. 이 튜플 내에서, 각 유니크 제약조건은 컬럼 이름을 담은 튜플로 표현됩니다.\n\n### 예시 코드\n\n다음은 User 모델에 대해 `email`과 `username` 컬럼 조합에 유니크 제약조건을 설정하는 예시입니다.\n\n```python\nfrom sqlmodel import Field, SQLModel\n\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    username: str\n    email: str\n    \n    __table_args__ = (\n        UniqueConstraint('username', 'email'),\n    )\n\n```\n\n위 코드는 같은 `username`과 `email` 값을 가진 두 개의 `User` 레코드가 데이터베이스에 존재할 수 없음을 의미합니다.\n\n### 주의 사항\n\n`__table_args__` 는 SQLAlchemy의 기능을 직접 사용합니다. 따라서, SQLModel을 사용할 때 SQLAlchemy에 대한 기본적인 이해가 필요할 수 있습니다.\n\n\n\n"},{"excerpt":"서론 JSON은 이제는 아주 익숙한 데이터 저장 형식입니다. Postgresql에서는 JSON 타입을 지원해 이 데이터를 효과적으로 다룰 수 있도록 하는데, JSONB 타입을 또 지원합니다.  이 두 타입은 저장 방식과 처리 속도, 기능성 측면에서 차이가 있습니다. 각각의 타입은 특정 사용 사례에 더 적합할 수 있으므로, 차이점을 이해하는 것이 중요합니다…","fields":{"slug":"/Postgresql의-JSON과-JSONB/"},"frontmatter":{"date":"March 04, 2024","title":"Postgresql의 JSON과 JSONB","tags":["Postgresql"]},"rawMarkdownBody":"## 서론\n\nJSON은 이제는 아주 익숙한 데이터 저장 형식입니다. Postgresql에서는 JSON 타입을 지원해 이 데이터를 효과적으로 다룰 수 있도록 하는데, JSONB 타입을 또 지원합니다. \n\n이 두 타입은 저장 방식과 처리 속도, 기능성 측면에서 차이가 있습니다. 각각의 타입은 특정 사용 사례에 더 적합할 수 있으므로, 차이점을 이해하는 것이 중요합니다.\n\n## **JSON**\n\n- `JSON` 데이터 타입은 입력된 JSON 데이터를 텍스트 기반의 형식으로 저장합니다.\n\n- 데이터는 그대로의 형태로 저장되므로, 입력 시점과 동일한 형태로 데이터를 검색할 수 있습니다.\n\n- `JSON` 타입은 JSON 데이터를 분석하거나 조작할 때 마다 원본 텍스트를 해석해야 하므로, `JSONB`에 비해 연산이 느릴 수 있습니다.\n\n- `JSON` 형식은 원본 데이터의 형식을 보존하는 것이 중요할 때 유용합니다.\n\n## **JSONB**\n\n- `JSONB` 데이터 타입은 JSON 데이터를 바이너리 형식으로 저장합니다. 이는 데이터를 저장하기 전에 처리(분석 및 재구성)하여 저장 공간을 최적화하고, 검색 및 조작 속도를 향상시킵니다.\n\n- `JSONB`는 내부적으로 인덱싱이 가능하므로, 큰 데이터 세트에서 검색과 조작이 훨씬 빠릅니다.\n\n- 바이너리 형식으로 인해, 데이터를 저장할 때 순서가 변경되거나 중복 키가 제거될 수 있습니다. 따라서, 원본 데이터의 정확한 형식과 순서를 보존하지 않습니다.\n\n- `JSONB`는 연산자와 함수를 지원하여, 데이터에 대한 복잡한 쿼리를 수행할 수 있게 해줍니다.\n\n## 주요 차이점 요약 및 결론\n\n- **저장 방식:**  `JSON`은 텍스트 기반으로, `JSONB`는 바이너리 형식으로 데이터를 저장합니다.\n\n- **처리 속도:**  `JSONB`는 데이터 처리와 쿼리 수행이 더 빠릅니다, 특히 인덱스를 사용할 때.\n\n- **공간 효율:**  `JSONB`는 데이터를 최적화하여 저장하기 때문에, 같은 데이터를 `JSON`보다 적은 공간을 사용해 저장할 수 있습니다.\n\n- **기능성:**  `JSONB`는 `JSON`보다 더 많은 연산자와 함수를 지원합니다.\n\n사용 사례에 따라, 원본 JSON 데이터의 형식과 순서를 정확히 유지할 필요가 있다면 JSON을, 데이터에 대한 빠른 검색과 조작을 필요로 한다면 JSONB를 선택하는 것이 좋습니다.\n\n\n\n"},{"excerpt":"Geocoder와 Reverse Geocoder란? 간단하게 이야기해 주소로 위도와 경도를 얻거나(Geocoder) 위도와 경도로 주소를 얻는(Reverse Geocoder) 것을 의미합니다.  왜 필요할까? 주소로는 서로간의 거리를 계산하기 힘들기 떄문에 변환이 필요하고, 위도와 경도는 사람이 읽기 힘들기 때문에 다시 주소로 변환이 필요합니다. 이 때 …","fields":{"slug":"/Geocoder와-Reverse-Geocoder/"},"frontmatter":{"date":"February 27, 2024","title":"Geocoder와 Reverse Geocoder","tags":["ETC","Python"]},"rawMarkdownBody":"![](image1.png)\n## Geocoder와 Reverse Geocoder란? \n\n간단하게 이야기해 주소로 위도와 경도를 얻거나(Geocoder) 위도와 경도로 주소를 얻는(Reverse Geocoder) 것을 의미합니다. \n\n### 왜 필요할까? \n\n주소로는 서로간의 거리를 계산하기 힘들기 떄문에 변환이 필요하고, 위도와 경도는 사람이 읽기 힘들기 때문에 다시 주소로 변환이 필요합니다. 이 때 사용하는 기술입니다. \n\n## geopy\n\n파이썬에서 geopy를 사용하면 간단하게 Reverse Geocoder와 Geocoder를 구현해볼 수 있습니다. \n\n테스트에 앞서 우선 geopy를 설치해줍니다.\n\n```bash\npip install geopy\n```\n\n### Geocoder 예제\n\n```bash\nfrom geopy.geocoders import Nominatim\n\n# Nominatim 서비스 초기화 (user_agent 설정 필요)\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n\n# 변환하고자 하는 주소\naddress = \"서울특별시 중구 세종대로 110\"\n\n# 주소를 위도와 경도로 변환\nlocation = geolocator.geocode(address)\n\nif location:\n    print((location.latitude, location.longitude))\nelse:\n    print(\"좌표를 찾을 수 없습니다.\")\n\n```\n\n위의 코드를 실행하면 해당 주소에 해당하는 좌표(위도와 경도) 값을 얻을 수 있습니다. \n\n### Reverse Geocoder 예제\n\n```python\nfrom geopy.geocoders import Nominatim\n\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n# 예시 좌표, language 인자를 reverse 메소드 호출 시 사용\nlocation = geolocator.reverse(\"37.541578, 126.840436\", exactly_one=True, language=\"ko\")\n\nif location:\n    print(location.address)\nelse:\n    print(\"주소를 찾을 수 없습니다. \")\n\n```\n\n변환된 주소를 얻을 수 있습니다. \n\n```bash\n강서로, 화곡1동, 강서구, 서울특별시, 07726, 대한민국\n```\n\nreerse method에 language를 함께 넣어주면 한글 주소를 얻을 수 있습니다. \n\n### 이외에는?\n\n`geopy`는 지오코딩과 역 지오코딩 외에도 다양한 기능을 제공합니다:\n\n1. 거리 계산: `geopy.distance`를 사용하면 두 지점(위도와 경도 좌표) 사이의 거리를 계산할 수 있습니다. 여러 가지 거리 계산 방법(예: Vincenty, Great Circle)을 제공합니다.\n\n1. 다양한 지오코더 지원: `Nominatim` 외에도 `GoogleV3`, `Bing`, `OpenCage`, `ArcGIS` 등 다양한 지오코딩 서비스를 지원합니다. 각 서비스마다 특징과 사용 방법이 다를 수 있습니다.\n\n1. 유연한 API 사용: `geopy`는 사용자가 다양한 외부 지오코딩 서비스에 쉽게 접근할 수 있도록 돕습니다. 이를 통해 개발자는 특정 서비스의 API 제한이나 사용 제한에 구애받지 않고 서비스를 선택할 수 있습니다.\n\n### Nominatim?\n\n위의 예시에서는 `Nominatim` 을 사용했습니다. `Nominatim`은 OpenStreetMap 데이터를 기반으로 하는 지리적 위치 조회 서비스로, 공개된 무료 API 이지만 공정한 사용 정책이 걸려있어 실제 서비스에서 사용하기에는 무리가 있습니다. \n\n## 그 밖에 다른 서비스는? \n\n그렇다면 상용화를 위해서는 어떤 서비스를 이용해야 할까요? 간략하게 찾아서 정리해봤습니다. 국내에서는 크게 네이버/카카오/구글을 사용할 수 있습니다. \n\n### Naver Map API\n\n- 월 300만건 무료, REST API 지원, 300만개 초과시 건당 0.5원\n\n- [https://www.ncloud.com/product/applicationService/maps](https://www.ncloud.com/product/applicationService/maps)\n\n- [https://guide.ncloud-docs.com/docs/maps-reversegeocoding-api](https://guide.ncloud-docs.com/docs/maps-reversegeocoding-api)\n\n### Kakao Map API\n\n- 월 300만건, 하루 30만건, 초과 시 협의 필요\n\n- [https://developers.kakao.com/docs/latest/ko/getting-started/quota](https://developers.kakao.com/docs/latest/ko/getting-started/quota)\n\n- [https://apis.map.kakao.com/web/documentation/#services\\_Geocoder](https://apis.map.kakao.com/web/documentation/#services_Geocoder)\n\n### Google Maps Platform\n\n- 10만회 이하 : 1000회당 5달러\n\n- 10만회 ~ 50만회 : 1000회당 4달러\n\n- 50만회 이상 : 협의\n\n- \n\n- [월간 200달러의 무료 크레딧을 제공합니다.](https://mapsplatform.google.com/pricing/?hl=ko) \n\n[https://mapsplatform.google.com/pricing/?hl=ko](https://mapsplatform.google.com/pricing/?hl=ko)\n\n[https://developers.google.com/maps/documentation/geocoding/usage-and-billing?hl=ko](https://developers.google.com/maps/documentation/geocoding/usage-and-billing?hl=ko)\n\n\n\n"},{"excerpt":"서론 새로운 프로젝트의 백엔드 언어가 파이썬으로 확정되었습니다. 파이썬 버전은 추후 머신러닝 사용을 감안하여 현재 라이브러리와 가장 호환성이 좋을 것으로 생각되는 3.9버전을 사용하려고 합니다. 다만, 파이썬 3.9는 그다지 최신 버전은 아닙니다. 최근에 타입스크립트를 만지면서 nvm을 좋게 사용한 경험도 있고, 이번에 내친김에 파이썬에서도 버전관리를 해…","fields":{"slug":"/Apple-Silicon과-pyenv/"},"frontmatter":{"date":"February 26, 2024","title":"Apple Silicon과 pyenv","tags":["Python","Homebrew","Pyenv"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n새로운 프로젝트의 백엔드 언어가 파이썬으로 확정되었습니다. 파이썬 버전은 추후 머신러닝 사용을 감안하여 현재 라이브러리와 가장 호환성이 좋을 것으로 생각되는 3.9버전을 사용하려고 합니다. 다만, 파이썬 3.9는 그다지 최신 버전은 아닙니다. 최근에 타입스크립트를 만지면서 nvm을 좋게 사용한 경험도 있고, 이번에 내친김에 파이썬에서도 버전관리를 해주는 툴인 pyenv를 사용해서 환경을 꾸며보기로 했습니다. \n\n## 설치\n\n설치는 아주 간단합니다. homebrew는 미리 설치되어 있어야 합니다. \n\n```bash\nbrew install pyenv\n```\n\npyenv를 설치해주고,\n\n```bash\npyenv install --list | grep 3.9\n```\n\n이 명령어를 사용하면 현재 설치 가능한 파이썬 3.9버전의 리스트를 확인할 수 있습니다. 현재 3.9의 최신 버전은 3.9.18로 보입니다. 해당 버전을 설치해줍니다. \n\n```bash\npyenv install 3.9.18\n```\n\n간단하게 파이썬이 설치.. 되지 않았습니다! 오류가 납니다. \n\n![](image2.png)\n## 해결\n\n챗지피티한테 물어보거나 검색을 해보는 등 여러가지 방법을 써보면 많은 방법이 나옵니다. zshrc에 여러 명령어를 export 해주거나 특정 라이브러리를 설치해주거나.. 또는 파이썬 버전을 바꾸던가 \n\n여러가지 방법을 시도했지만 잘 되지 않았습니다. 여전히 같은 오류가 발생합니다. \n\n문제는 홈브루에 있었습니다. \n\n### Homebrew 버전 확인 및 최신 버전 설치\n\n#### Homebrew 버전 확인\n\n아래 코드로 내 homebrew를 확인할 수 있습니다. \n\n```bash\nwhich brew\n```\n\n이 코드를 실행했을 때에,\n\n```bash\n/usr/local/bin/brew\n```\n\n가 출력된다면 구버전의 Homebrew가 설치되어 있는 것입니다. \n\n#### Homebrew 구버전 삭제\n\n다음의 명령어로 Homebrew를 삭제할 수 있습니다. \n\n```bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/uninstall.sh)\"\n```\n\n해당 코드를 실행한 뒤에도 `which brew` 를 실행해보면 여전히 위치가 나옵니다. 이 경우에는 삭제 코드를 다시 실행해주면 됩니다. 정상적으로 삭제 된다면 `command not found` 가 발생합니다. \n\n#### Homebrew 최신 버전 설치\n\n아래의 명령어로 최신 버전을 설치할 수 있습니다. \n\n```bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n```\n\n설치가 완료되면 다음과 같은 문구가 나타납니다. 버전에 따라 다를 수 있습니다. \n\n```bash\n==> Installation successful!\n\n==> Homebrew has enabled anonymous aggregate formulae and cask analytics.\nRead the analytics documentation (and how to opt-out) here:\n  https://docs.brew.sh/Analytics\nNo analytics data has been sent yet (nor will any be during this install run).\n\n==> Homebrew is run entirely by unpaid volunteers. Please consider donating:\n  https://github.com/Homebrew/brew#donations\n\n==> Next steps:\n- Run these two commands in your terminal to add Homebrew to your PATH:\n    (echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') >> /Users/UserName/.zprofile\n    eval \"$(/opt/homebrew/bin/brew shellenv)\"\n- Run brew help to get started\n- Further documentation:\n    https://docs.brew.sh\n\n```\n\nNext steps를 보니, 다음의 명령어를 실행하라고 합니다. 위는 제 예시로, 터미널의 명령어를 복사하면 됩니다. 줄의 명령어를 실행하고 `which brew` 를 다시 실행해보면\n\n```bash\n/opt/homebrew/bin/brew\n```\n\n이렇게 표시되면 최신 버전의 brew가 설치된 것입니다. \n\n### 다시 pyenv, python 설치\n\n다시 위로 돌아가서, pyenv과 python을 설치해주면 이번에는 오류가 발생하지 않습니다. 해~결~\n\n## pyenv 설치 후\n\n설치된 파이썬 버전 리스트를 확인할 수 있습니다. \n\n```bash\npyenv versions\n```\n\n현재 사용중인 파이썬 버전을 확인할 수 있습니다. \n\n```bash\npyenv version\n```\n\n사용중인 파이썬을 설치한 버전으로 바꿔줍니다. \n\n```bash\npyenv global 3.9.18\n```\n\n현재 디렉토리에서만 사용할 파이썬 버전을 지정할 수 있습니다.\n\n```bash\npyenv local 3.9.18\n```\n\n쉘에만 적용할수도 있습니다. \n\n```bash\npyenv shell 3.9.9\n```\n\n## 참고\n\n[https://www.lainyzine.com/ko/article/how-to-install-homebrew-for-m1-apple-silicon/](https://www.lainyzine.com/ko/article/how-to-install-homebrew-for-m1-apple-silicon/)\n\n"},{"excerpt":"서론 FastAPI-Python을 새로운 프로젝트의 백엔드 언어로 선정하였습니다. 프로젝트 초기화 과정을 기록합니다.  poetry 환경 구성은 이미 끝났다고 가정하고, 해당 라이브러리를 활용하여 환경을 꾸미겠습니다.  필수라고 생각되는 라이브러리 설치 FastAPI FastAPI를 웹 프레임워크로 선정하였으니, 이를 먼저 설치해줍니다.  Uvicorn …","fields":{"slug":"/FastAPI-프로젝트의-시작/"},"frontmatter":{"date":"February 26, 2024","title":"FastAPI 프로젝트의 시작","tags":["FastAPI","Python"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nFastAPI-Python을 새로운 프로젝트의 백엔드 언어로 선정하였습니다. 프로젝트 초기화 과정을 기록합니다. \n\n[poetry](https://sharknia.github.io/Poetry) 환경 구성은 이미 끝났다고 가정하고, 해당 라이브러리를 활용하여 환경을 꾸미겠습니다. \n\n## 필수라고 생각되는 라이브러리 설치\n\n### FastAPI\n\nFastAPI를 웹 프레임워크로 선정하였으니, 이를 먼저 설치해줍니다. \n\n```bash\npoetry add fastapi\n```\n\n### Uvicorn\n\nASGI 서버로 FastAPI 애플리케이션을 호스팅하는 데 사용됩니다. FastAPI 공식 문서에서도 Uvicorn을 사용하는 것을 권장하고 있습니다.\n\n```bash\npoetry add uvicorn\n```\n\n### SQLAlchemy 2.0\n\n이미 직전 회사에서 많은 일을 함께 겪었던 ~~(^^)~~ 든든한 동지네요. 이번에도 2.0을 설치는 하지만, 비동기 엔진은 절대 사용하지 않을 예정입니다. 아무튼 설치해줍니다. \n\n```bash\npoetry add sqlalchemy@^2.0\n```\n\n### Alembic\n\nSQLAlchemy를 위한 데이터베이스 마이그레이션 도구입니다. 데이터베이스 스키마 변경을 버전 관리할 수 있게 해줍니다. 지난 회사에서는 이 부분을 적용하지 못했었는데, 이번에는 잘 해보려고 합니다. \n\n```bash\npoetry add alembic\n```\n\n### HTTPX\n\n비동기 HTTP 클라이언트 라이브러리입니다. FastAPI 애플리케이션 내부에서 비동기적으로 외부 API 호출을 수행할 때 사용할 수 있습니다.\n\n```bash\npoetry add httpx\n```\n\n### Passlib & python-jose\n\n보안 및 인증을 위한 라이브러리입니다. Passlib는 비밀번호 해싱을, python-jose는 JWT(JSON Web Tokens) 생성 및 검증을 위해 사용됩니다. 기본적으로 JWT 토큰을 사용하게 될 것 같으므로 설치해줍니다. Zsh가 괄호 `[]`를 특별한 패턴으로 해석하기 때문에 위와는 설치 명령어가 조금 다릅니다. \n\n```bash\npoetry add 'passlib[bcrypt]' 'python-jose[cryptography]'\n```\n\n## 테스트 코드를 위한 라이브러리 설치\n\n이번에도 프로젝트 작업시간이 굉장히 빡빡한데, 가능하면 TDD를 지켜보고 싶습니다. 가능할지 모르겠지만, 일단 라이브러리 설치는 해두려고 합니다. 해당 라이브러리들은 실서버 환경에서는 필요하지 않으므로 `--dev` 옵션을 붙여주겠습니다. \n\n### pytest\n\nPython에서 가장 널리 사용되는 테스트 프레임워크 중 하나로, 강력한 기능과 플러그인 시스템을 제공합니다. \n\n```bash\npoetry add --dev pytest\n```\n\n### pytest-asyncio\n\n비동기 코드를 위한 pytest 플러그인으로, asyncio를 사용하는 비동기 함수를 테스트할 수 있게 해줍니다. FastAPI는 비동기 프레임워크이므로, 이 플러그인은 비동기 요청 및 비동기 데이터베이스 작업을 테스트하는 데 유용합니다.\n\n```bash\npoetry add --dev pytest-asyncio\n```\n\n### pytest-cov\n\n코드 커버리지를 측정하기 위한 pytest 플러그인입니다. 테스트가 얼마나 많은 코드를 실행하는지를 분석하고, 테스트가 누락된 영역을 식별하는 데 도움을 줍니다.\n\n```bash\npoetry add --dev pytest-cov\n```\n\n\n\n"},{"excerpt":"서론 작업을 하다보면 항상 뭔가 찾게 됩니다. 좀 더 편한 것, 좀 덜 귀찮은 것, 좀 더 새로운 것 등등..  항상 새로운 것에서 자극을 많이 받는 것 같습니다. 매일 하던 것만 하면 재미 없잖아요.  접한지는 조금 되었지만 사용하지 않고 있었는데, 이제 새롭게 사용해보려고 하는 건 arc 브라우저 입니다.  업무용으로는 크롬을 사용하고, 개인용 윈도우…","fields":{"slug":"/arc-browser/"},"frontmatter":{"date":"February 22, 2024","title":"arc browser","tags":["ETC"]},"rawMarkdownBody":"## 서론\n\n작업을 하다보면 항상 뭔가 찾게 됩니다. 좀 더 편한 것, 좀 덜 귀찮은 것, 좀 더 새로운 것 등등.. \n\n항상 새로운 것에서 자극을 많이 받는 것 같습니다. 매일 하던 것만 하면 재미 없잖아요. \n\n접한지는 조금 되었지만 사용하지 않고 있었는데, 이제 새롭게 사용해보려고 하는 건 arc 브라우저 입니다. \n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n저는 지금까지는 개인적인 용도로는 사파리를 주로 사용하고 있었습니다. 아이폰-아이패드-맥을 모두 사용하고 있기 때문이기도 하고 딱~히 불편함을 못느끼고 통합성이 너무 좋아서 불편 없이 사용하고 있었습니다. \n\n업무용으로는 크롬을 사용하고, 개인용 윈도우 데스크탑에서는 엣지를 사용하고 있었는데 왜 그렇냐? 하면 사실 딱히 이유는 없습니다.. 다만 업무용 환경과 개인용 환경이 합쳐지는 걸 좋아하지 않아 구분해서 사용하고 있었다, 정도겠네요. \n\n## 그런데 왜 arc 브라우저를 갑자기? \n\n앞에서 언급한대로 새로운게 좋기 때문입니다. 이번에 또 이직을 하게 되면서 환경이 확 바뀌게 되었는데, 뭔가 같이 변화를 가져가면 좋잖아요. \n\n주변 개발자들이 많이 사용하고 있었기도 합니다. 추천도 받았구요. 자세한 장점은 좀 더 써보면 정리할 수 있지 않을까 합니다. \n\n### 첫인상?\n\n일단 디자인이 유려합니다. 주변에서 추천해주신 분들 중에서는 arc 브라우저를 사용하다가 크롬이나 엣지를 보면 촌스러워 보인다고 말씀해주신 분도 있습니다. ~~(사실 엣지는 요즘.. 코파일럿이 지저분하게 붙었다는 느낌이어서 번잡하다는 느낌이 들긴 합니다.)~~ \n\n## 어떤 기능들을 사용하나요? \n\n### Little Arc\n\n외부 어플리케이션에서 링크를 클릭하여 브라우저를 여는 경우가 있습니다. 독특하게도 이 때 아크 브라우저에서는 해당 링크가 Little Arc 창에서 열립니다. \n\n다른 열려있는 탭들에 영향을 주지 않고 모달처럼 더 작은 브라우저 창에서 새로운 링크가 열리게 됩니다. 따라서 산만하지 않게 원래의 작업을 유지할 수 있습니다. 물론 메인 탭에 리틀 아크에서 열린 탭을 포함시킬 수도 있습니다. \n\n외부 링크가 아니고 직접 실행할 수도 있는데, 브라우저 상에서 뿐만 아니라 시스템 전역 어디에서든 `Cmd` + `Option` + `N` **** 단축키를 통해 리틀 아크를 실행할 수 있습니다. \n\n### Space\n\nArc 브라우저에서는 Space라는 분리된 공간을 제공합니다. 이는 사파리의 신규 기능인 프로필 기능과 비슷하지만, 더 유려한 UI와 편의성을 제공합니다. \n\n사용자는 Space를 통해 작업 탭과 개인 또는 프로젝트 별로 관련된 탭을 그룹화 하여 분리할 수 있습니다. 각 Space는 자체적인 탭, 테마, 아이콘 등을 가지며 그러면서도 즐겨찾기는 모든 Space에서 공유되어 편리하게 이용할 수 있습니다. \n\n### 이외에도..\n\n이외에도 많은 독특한 기능을 제공합니다. 개인적으로는 Split 기능도 열심히 사용을 해보려고 합니다. \n\n개인적으로는 한글이 지원이 아직 안된다는 점, 윈도우에서 지원이 아직 제공되지 않는다는 점이 아쉽습니다. \n\n자세한 내용은 더욱 써보고 정리해보려고 합니다. 해당 페이지에서는 개인 확인 용도로 기록을 늘려가려고 합니다. \n\n## 같이 보기\n\n[https://www.hongkiat.com/blog/arc-browser-keyboard-shortcuts/](https://www.hongkiat.com/blog/arc-browser-keyboard-shortcuts/)\n\n"},{"excerpt":"서론 새로 개발할 서비스의 DB 설계 작업이 시작되었습니다. DB를 설계하는 것은 많이 해본 일이어서 저도 모르게 무결성이나 관계성 등 중요한 내용을 챙기고 있지만, 이런것들은 말로 설명하라고 하면 어려울 때도 많습니다.  그래서 이번 기회에 해당 개념들에 대해서 기회가 날때마다 정리하는 습관을 들이려고 합니다.  DB가 문서도 없고, 관계성도 정의되어 …","fields":{"slug":"/식별-관계와-비식별-관계-그리고-CASCADE-옵션의-이해/"},"frontmatter":{"date":"February 18, 2024","title":"식별 관계와 비식별 관계, 그리고 CASCADE 옵션의 이해","tags":["DataBase","DataBase Design"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n새로 개발할 서비스의 DB 설계 작업이 시작되었습니다. DB를 설계하는 것은 많이 해본 일이어서 저도 모르게 무결성이나 관계성 등 중요한 내용을 챙기고 있지만, 이런것들은 말로 설명하라고 하면 어려울 때도 많습니다. \n\n그래서 이번 기회에 해당 개념들에 대해서 기회가 날때마다 정리하는 습관을 들이려고 합니다. \n\n### DB가 문서도 없고, 관계성도 정의되어 있지 않다면?\n\n전에 있던 회사에서는 정규화는 고사하고 아예  테이블들의 관계를 설정하지 않았었고, 관계성의 정의에 대한 문서화도 되어있지않아 어느 테이블의 어느 컬럼이 다른 어느 테이블과의 관계를 가지는지를 확인하기 위해서는 “감”을 사용하거나 코드를 읽고 파악해야 해 그야말로 전쟁터를 방불케했습니다. 무언가 하나를 수정할 경우 데이터 무결성이 깨지고 오류가 발생하는 것을 막는 일은 전쟁터에서 많아도, 신경을 곤두세워도 피하기 쉽지 않았습니다. \n\n어떻게 생각하면 이렇게 문서도 없고 관계성도 정의되지 않은데다가 하나의 프로젝트를 여러 개발자가 손 댔을 경우 얼마나 유지보수가 어려워지는지, 개발 생산성이 얼마나 떨어지는지 깨닫게 되는 좋은 경험을 했구나, 하는 생각도 합니다. 그런 아비규환을 겪기 전에는 무결성이나 관계성 정의가 중요하다고 공부를 해도 와닿지 않았었거든요. ~~(역시 사람은 맞아봐야 깨닫습니다)~~  \n\n특히 이번 프로젝트에서는 개인적으로 많은 목적을 담고 있기 때문에 저도 함께 등한시 했던 작업들에 더 주의를 기울이려고 합니다. 오늘은 체계적인 문서화를 위해 DB 설계를 [ERD-Cloud](https://www.erdcloud.com/)로 진행했고, 그 과정에서 새롭게 정리된 개념들에 대해 정리하고 넘어가겠습니다. \n\n## 관계 유형\n\n먼저 관계 유형에 대해 간단하게 정리하고 넘어가겠습니다. 관계 유형은 테이블 간의 데이터가 어떻게 연결되는지를 정의합니다. 기본적으로 세 가지 관계 유형이 있습니다. \n\n### 1:1 관계\n\n이 관계는 한 테이블의 레코드가 다른 테이블의 단 하나의 레코드와만 관련될 때 발생합니다. 예를 들어 사용자 테이블과 사용자 상세 정보 테이블 간의 관계를 생각하면 쉽습니다. 상세 정보는 한 유저 당 하나만 존재합니다. \n\n종종 데이터를 논리적으로 분리하거나, 특정 정보에 대한 보안 요구 사항이 다를 때 사용됩니다. \n\n### 1:N 관계\n\n가장 흔한 관계유형입니다. 외래키를 사용하여 부모 테이블과 자식 테이블 간의 연결을 구현합니다. 한 테이블의 레코드가 다른 테이블의 여러 레코드와 관련될 수 있습니다. 예를 들면 사용자 테이블과 주문 리스트 테이블을 생각하면 쉽습니다. 한 사용자가 주문을 여러번 할 수 있기 때문에, 유저 테이블에는 레코드가 하나지만 주문 테이블에는 여러 레코드가 존재할 수 있습니다. \n\n### N:N 관계\n\n두 테이블 간에 서로 다수의 레코드가 관련될 수 있습니다. 이 관계는 직접적으로 데이터베이스에 표현될 수 없으며, 일반적으로 두 테이블간의 다대다 관계를 1:N으로 분해하는 연결 테이블을 사용하여 구현됩니다. 예를 들어, 학생 테이블과 수업 간의 관계에서 한 학생이 여러 수업을 듣고 하나의 수업에 여러 학생이 등록될 수 있습니다. \n\n## 식별 관계와 비식별 관계\n\n식별 관계와 비식별 관계는 데이터베이스 설계에서 테이블 간의 연결 방식을 정의하는 두 가지 방법입니다. \n\n### 식별 관계(Identifying Relationship)\n\n이 관계에서는 자식 테이블의 기본키에 부모 테이블의 기본키가 포함됩니다. 이는 자식 테이블의 레코드가 부모 테이블의 레코드 없이는 존재할 수 없음을 의미합니다. \n\n이 관계는 강한 의존성이 있는 테이블 간에 사용됩니다. 예를 들어 자식 테이블이 부모 테이블의 생명주기에 밀접하게 결합되어 있을 때 적합합니다. \n\n### 비식별 관계 (Non-Identifying Relationship)\n\n비식별 관계에서는 자식 테이블이 부모 테이블을 참조하는 외래키를 가지지만 이 외래키가 자식 테이블의 기본 키에는 속하지 않습니다. 자식 테이블의 레코드는 부모 테이블의 레코드와 독립적으로 존재할 수 있습니다. 예를 들어, 직원 테이블에서 부서 테이블을 참조하는 경우 직원은 부서에 속하지 않더라도 독립적으로 존재할 수 있습니다. \n\n즉 테이블간 서로 관련은 있지만, 서로의 존재에 필수적이지 않을 때 적합합니다. \n\n### 무조건 식별관계로 묶어두는게 정답일까?\n\n식별 관계는 테이블간의 강한 연결과 무결성을 보장하는 데 유용합니다. 반면 비식별 관계는 데이터 모델의 확장성과 유연성을 높이는 데 도움이 됩니다. \n\n그냥 단순하게 생각하면 반드시 가능할때마다 식별관계로 묶어두는게 좋지 않을까? 라고 쉽게 생각이 들기도 합니다. 하지만 식별 관계를 과도하게 사용하면 여러가지 문제가 발생할 수 있습니다. \n\n#### 1. 유연성 감소\n\n식별 관계는 자식 테이블이 부모 테이블의 존재에 종속되는 것이나 똑같습니다. 이렇게 만들어진 강한 결합도는 요구 사항이 변경될 때 테이블간의 관계를 수정하기 어렵게 만듭니다. \n\n만약 부모 테이블의 기본 키 구조가 변경되면 이를 참조하는 모든 자식 테이블의 구조가 함께 변경되어야 합니다. 이는 유지보수에 문제를 초래할 수 있습니다. \n\n#### 2. 확장성 문제\n\n식별 관계는 데이터 모델을 특정 구조에 굳건히 묶어 놓음으로써 시스템의 확장성을 제한할 수 있습니다. 특히 대규모 시스템에서는 요구 사항이 시간에 따라 변할 수 있으므로 유연하게 대응할 수 있는 비식별 관계가 더 유리할 수 있습니다. \n\n#### 3. 데이터 모델의 복잡성 증가\n\n과도하게 증가한 복잡성은 데이터 모델을 이해하고 관리하는 데 더 많은 노력이 필요함을 의미합니다. \n\n### 그렇다면 싹 다 비식별 관계로 묶으면 되지 않을까?\n\n식별 관계를 사용했을 때 발생하는 문제점들은 유연함과 단순함이 너무 없어 어지럽기도 합니다. 그렇다면 비식별 관계만으로 정의를 하면 이 모든 문제들이 해결될까요? 물론 비식별 관계만을 고집하면 발생할 수 있는 문제점들도 있습니다. \n\n#### 1. 데이터 무결성의 약화, 데이터 관리의 어려움\n\n비식별 관계는 자식 테이블이 부모 테이블에 느슨하게 연결되어 있으므로 데이터의 무결성을 유지하기 위한 추가적인 제약 조건이나 규칙을 구현해야 할 수 있습니다. 즉, 데이터의 일관성과 정확성을 보장하는 데 있어 추가적인 노력이 필요할 수도 있음을 의미합니다. \n\n이는 데이터 관리의 어려움으로 이어집니다. 예를 들어 부모 테이블이 삭제될 때 자식 테이블에 대한 처리를 비식별관계에서는 수동으로 관리해야 할 수 있습니다. \n\n#### 2. 관계의 명확성 감소\n\n테이블간의 관계가 덜 명확해지면, 복잡한 데이터 모델에서 테이블간의 관계를 이해하고 추적하기 어렵게 만들 수 있습니다. \n\n#### 3. 과도한 유연성의 함정\n\n과도한 유연성은 결국 모델의 명확성을 흐리게 하고 미래의 확장성에 대한 고려 없이 단기적인 해결에 치중하게 만들 수 있습니다. 대충 지어진 모래성이 무너지는 느낌이 이런게 아닐까요? 큰 집일수록 기초가 튼튼해야 함을 전 회사에서 느꼈습니다. \n\n### 결론\n\n데이터베이스 설계에 있어서 식별 관계와 비식별 관계를 적절히 혼합하여 사용하는 것이 중요합니다. 각각의 관계 유형이 가진 장단점을 이해하고 프로젝트의 요구 사항과 목표에 따라 최적의 접근 방식을 선택해야 합니다. 따라서, 단일한 접근 방식에만 의존하기보다는 유연하게 접근하여, 각 상황에 가장 적합한 데이터 모델링 전략을 개발하는 것이 필요합니다.\n\n## CASCADE\n\nCASCADE는 데이터베이스 관리 시스템에서 외래 키 제약 조건의 일환으로 사용할 수 있는 옵션입니다. 이 옵션은 부모 테이블의 레코드가 업데이트 되거나 삭제될 때 관련된 자식 테이블의 레코드에 대해 자동으로 같은 작업을 수행하도록 설정합니다. \n\n### CASCADE의 주요 기능\n\n#### CASCADE DELETE\n\n부모 테이블의 레코드가 삭제될 경우 해당 레코드와 연결된 자식 테이블의 레코드도 자동으로 삭제됩니다. 이는 참조 무결성을 유지합니다. \n\n#### CASCADE UPDATE\n\n부모 테이블의 기본 키 값이 변경될 경우 해당 변경이 자식 테이블의 관련 외래키 값에도 자동으로 반영됩니다. \n\n### 비식별 관계에서의 CASCADE 사용\n\nCASCADE 설정은 식별/비식별 관련없이 모두 적용할 수 있습니다. 따라서 이 설정을 활용해 위에서 살펴봤던 비식별 관계의 약점을 보완할 수 있습니다. \n\n#### 비식별 관계의 약점 보완\n\n비식별 관계에서 CASCADE를 사용하면 참조하는 부모 테이블이 변경될 때 자식 테이블의 데이터를 자동으로 갱신하여 데이터를 수동으로 관리하는 번거로움을 줄일 수 있으며, 일관성을 유지할 수 있습니다. \n\n#### 반드시 CASCADE + 비식별이 식별보다 유리하지 않을까? \n\n이 방법을 사용하면 유연성, 무결성 유지, 고아 레코드 방지 등에 있어서 비식별 관계의 약점을 모두 보완하는 것처럼 보이기도 합니다. 따라서 단순히 생각하면 식별 관계를 사용하는 것보다 우월하다고 생각될 수 있습니다. 하지만 데이터 모델의 명확성과 이해도를 유지하는 것 역시 중요한 고려 사항입니다. 따라서, 식별 관계와 비식별 관계의 선택은 각각의 상황에 따라 그 장단점을 잘 이해하고, 프로젝트의 목표와 요구 사항에 가장 잘 맞는 방식을 선택하는 것이 필요합니다.\n\n### CASCADE 사용시 고려 사항\n\nCASCADE도 만능은 아닙니다. 사용시 다음과 같은 점들을 고려해야 합니다. \n\n#### 데이터 수정/삭제에서 오는 부작용\n\nCASCADE 옵션은 데이터베이스의 무결성을 유지하는 데 유용하지만 데이터를 삭제하거나 업데이트 할 때 예상치 못한 부작용이 발생할 수 있으므로 주의가 필요합니다. \n\n항상 데이터모델과 비즈니스 로직을 신중하게 고려해야 합니다. 무분별한 사용은 데이터의 실수를 일으킬 수 있습니다. \n\n#### 성능 영향\n\n대량의 데이터가 관련되어 있는 경우 CASCADE 동작은 데이터베이스의 성능에 영향을 줄 수 있습니다.\n\n## 개인적인 오늘의 결론\n\n첫 번째 회사에 오래 있었고, 두 번째 회사에서 일한지는 얼마 되지 않았지만 첫번째 회사에서 “엉망으로 일할 때 발생하는 문제” 들을 몸으로 겪었다면, 두번쨰 회사에서는 그것들을 해결하는 방법들에 대해 고민하는 시간을 많이 가졌던 것 같습니다. \n\n오늘 살펴본 것도 몸으로는 아는 내용들이었고 자연스럽게 구현하고 중요하게 여기던 내용들입니다. 이런것들을 몰라도 된다고 무심결에 무시하던 때도 있었는데, 이런 내용들을 정리된 이론으로 머릿속에 다시 집어넣는게 중요하다고 요즘에 크게 느낍니다. \n\n남한테 설명할 때에도 그렇고 결국 내가 꺠달은 것들을 미리 깨달은 사람들이 잘 정리해 둔 것들을 굳이 피해가는 것도 멍청한 일이라는 생각이 듭니다.\n\n다만, 아예 백지 상태에서 배우는 것보다는 (너무 많은 시간을 고생했지만) 잘못된 반례들을 겪고 공부하니 훨씬 더 잘 다가오는 감이 있습니다. \n\n앞으로도 여태까지와는 다른 방향으로 더 성장하는 개발자가 되려고 합니다. \n\n"},{"excerpt":"서론 현재 개발을 준비중인 서비스에는 인앱결제가 포함되어있습니다. 여태까지 운이 좋았던 것인지, 운이 나빴던 것인지 아무튼 인앱결제를 구현한 적이 없어서 구현 전에 역시 미리 서치해두려고 합니다.  개발 예정 서비스에서는 내부 재화를 인앱결제로 구매하고 기타 아이템들은 내부 재화로 구매할 수 있게 만들 예정입니다. 내부 재화 구매 프로세스 구매는 결국 어…","fields":{"slug":"/인앱결제에서-백엔드는-무엇을-준비해야-할까/"},"frontmatter":{"date":"February 17, 2024","title":"인앱결제에서 백엔드는 무엇을 준비해야 할까?","tags":["ETC"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n현재 개발을 준비중인 서비스에는 인앱결제가 포함되어있습니다. 여태까지 운이 좋았던 것인지, 운이 나빴던 것인지 아무튼 인앱결제를 구현한 적이 없어서 구현 전에 역시 미리 서치해두려고 합니다. \n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n이 과정은 크게 각 플랫폼별 인앱 결제 API 연동, 결제 요청 및 검증, 결제 성공 시 내부 재화 충전, 그리고 결제 실패 및 취소 처리로 나눌 수 있습니다.\n\n개발 예정 서비스에서는 내부 재화를 인앱결제로 구매하고 기타 아이템들은 내부 재화로 구매할 수 있게 만들 예정입니다.\n\n## 내부 재화 구매 프로세스\n\n구매는 결국 어떻게 이뤄질까? 앱 사용자가 상품 구매를 한다고 하고 어떤 흐름으로 결제가 이뤄져 어떻게 사용자가 내부 재화를 가지게 되는지를 먼저 그려보겠습니다. \n\n### 1. 사용자가 앱에서 내부 재화 구매를 선택\n\n#### FE\n\n사용자가 앱을 열고, 상점 페이지로 이동해 내부 재화를 구매하고자 하는 상품을 선택합니다. \n\n앱은 사용자가 선택한 상품의 정보를 보여주고 구매 버튼을 노출합니다. \n\n#### BE\n\n백엔드는 상품 정보를 제공합니다. \n\n### 2. 사용자가 구매 버튼을 누름\n\n#### FE\n\n구매 버튼이 눌러지면 앱은 해당 플랫폼의 인앱 결제 프로세스를 시작합니다. (iOS : StoreKit, Android : Google Play Billing Libarary)\n\n사용자는 구매 확인, 결제 방법 선택(등록된 결제 수단 사용 등) 등의 단계를 수행합니다. \n\n### 3. 결제 프로세스 진행\n\n#### FE\n\n사용자는 결제 과정을 진행하며 이 과정은 앱스토어세서 직접 관리됩니다. \n\n과정이 종료된 후, 결제 성공/실패/취소 등의 결과가 어플리케이션으로 전달됩니다.\n\n#### BE\n\n결제가 성공한 경우, 어플리케이션에서 결제 성공 정보와 결제 영수증을 백엔드로 전송합니다. \n\n백엔드는 영수증 정보를 플랫폼의 서버(애플 혹은 구글)에 검증 요청을 합니다. \n\n검증이 통과하면 백엔드는 사용자의 계정에 해당하는 내부 재화를 충전하고 충전 결과를 앱에 알립니다. \n\n### 4 구매 완료 및 내부 재화 충전\n\n#### FE\n\n백엔드로부터 내부 재화 충전 성공 응답을 받으면 앱은 사용자 인터페이스를 업데이트하여 새로운 재화 잔액을 표시합니다. \n\n사용자에게 구매 성공 알림을 제공할수도 있습니다. \n\n#### BE\n\n내부 재화 충전과 관련된 모든 정보를 데이터베이스에 기록합니다. \n\n구매 내역을 로깅하여 추후 문제 해결이나 사용자 문의에 대응하도록 할 수도 있습니다. \n\n## 어플리케이션 안에서의 환불 요청 프로세스\n\n환불에도 대응을 해야 할 것이므로, 환불에 관해서도 프로세스를 미리 서치했습니다. \n\n### 1. 사용자가 환불을 요청\n\n#### FE\n\n사용자는 앱 내에서 환불을 요청할 수 있는 UI를 통해 환불을 신청합니다. 앱은 사용자의 환불 요청 정보를 백엔드로 전송합니다. \n\n### 2. 환불 요청 검증\n\n#### BE\n\n사용자의 구매 내역과 환불 요청을 검증합니다. 구매 영수증, 환불 정책 준수 등을 검증합니다. \n\n환불 가능 여부를 결정합니다. \n\n### 3.  환불 처리\n\n#### BE\n\n환불이 가능할 경우, 백엔드는 환불 처리를 진행합니다. 내부 재화를 차감합니다. \n\n이후 실제 결제가 이루어진 플랫폼(애플, 구글 등)을 통해 환불을 진행해야 합니다. 결제 플랫폼에 환불 요청을 전송합니다. \n\n### 4. 환불 결과 통지\n\n#### FE\n\n백엔드로부터 환불 처리 결과를 전송받습니다. 사용자에게 이를 통지합니다. \n\n## 앱스토어에서 직접 환불 처리 시 프로세스\n\n환불에는 앱 내에서 진행하는 경우와 플랫폼을 통해 환불이 진행되는 경우 두 가지가 있습니다. 위에서 알아본 방법은 첫번째 방법이며, 여기서 알아볼 방법은 두 번째 방법입니다. 두 번째 방법의 경우에는 앱 개발자의 개입이 불가능하게 진행이 됩니다.\n\n### 1. 앱스토어에서 환불 처리\n\n사용자가 앱스토어의 고객 지원을 통해 환불을 요청합니다. 앱스토어는 이를 진행합니다. \n\n### 2. 환불 통지 수신\n\n#### BE\n\n앱스토어는 환불 처리 결과를 앱의 백엔드에 통지합니다. 이는 대부분 웹훅 형태로 제공됩니다. \n\n백엔드는 앱스토어로부터 환불 통지를 받고 해당 정보를 검증합니다. 검증 과정에는 통지의 유효성 확인, 환불된 결제와 연관된 사용자 및 내부 재화 찾기 등이 포함됩니다. \n\n### 3. 내부 재화 상태 업데이트\n\n#### BE\n\n환불 통지가 검증된다면, 사용자의 내부 재화 상태를 업데이트합니다. \n\n## 결론\n\n길게 적게 되었지만 사실 제가 궁금했던 부분을 “실제 결제 과정을 어디에서 담당하냐” 였습니다. 결제 과정은 앱스토어에서 직접 담당하므로 결국 인앱결제에 있어서 백엔드가 해야 할 가장 핵심 프로세스는 영수증 검증인것으로 보입니다.\n\n물론, 금전을 직접 다루는 중요한 과정이므로 보안 등의 사항에 특별히 유념해서 더 신경써야 하겟습니다. \n\n오늘은 이론적인 부분만 미리 살펴봤습니다. 언제나 그렇지만 실재 구현에 들어가면 항상 예외적인 상황이 발생합니다. 나중에 구현을 할 경우에 그 부분에 대해서도 자새히 기록을 해보려고 합니다. \n\n"},{"excerpt":"서론 새로운 서비스의 개발을 준비하고 있습니다. 그 서비스를 위해서 위치 기반 검색이 가능해야 할 것으로 생각하고 있습니다.  아직 해당 기능에 대해서 구현 해본 적도 없고, 어떻게 구현 해야 할지 감도 잡히지 않아 아직 개발단계에는 이르지 않았지만 이에 대해 미리 서치한 내용을 기록하려고 합니다.  Postgresql을 사용하게 될 것이므로, Postg…","fields":{"slug":"/위치-기반-검색-기능의-구현/"},"frontmatter":{"date":"February 13, 2024","title":"위치 기반 검색 기능의 구현","tags":["Postgresql","DataBase","ETC"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n새로운 서비스의 개발을 준비하고 있습니다. 그 서비스를 위해서 위치 기반 검색이 가능해야 할 것으로 생각하고 있습니다. \n\n아직 해당 기능에 대해서 구현 해본 적도 없고, 어떻게 구현 해야 할지 감도 잡히지 않아 아직 개발단계에는 이르지 않았지만 이에 대해 미리 서치한 내용을 기록하려고 합니다. \n\nPostgresql을 사용하게 될 것이므로, Postgreslql에서의 구현 방법도 미리 기록합니다. \n\n위치 기반 검색 기능은 사용자의 위치 정보(위도와 경도)를 데이터베이스에 저장하고, 이를 기반으로 특정 거리 내에 있는 다른 사용자나 장소를 찾는 것입니다. \n\n## 위치 기반 검색 기능의 구현\n\n### 위치 정보의 저장\n\n사용자의 위치 정보(위도, 경도)를 저장합니다. 위치 정보는 주소 변환(Geocoding)이나 모바일 기기의 GPS를 통해 직접 얻을 수 있습니다.  또 외부 API를 사용해 위도,경도 정보를 다시 주소로 변환할 수도 있습니다. \n\n### 거리 계산\n\n데이터베이스에 저장된 위치 정보를 사용하여 사용자 간의 거리를 계산합니다. 이 때 대표적으로 사용되는 알고리즘은 하버사인 공식 입니다. 하버사인 공식은 두 지점 사이의 거리를 계산할 때 지구의 구형을 고려하여 보다 정확한 결과를 제공합니다. \n\n### 거리 기반 검색\n\n사용자의 위치에서 특정 거리 내에 있는 사용자나 장소를 찾기 위해 SQL 쿼리 내에서 거리 계산 로직을 사용합니다. 예를 들어 SQL의 HAVING 절을 사용하여 계산된 거리가 특정 버위 내에 있는 레코드만 선택할 수 있습니다. \n\n### 인덱싱과 최적화 \n\n위치 기반 검색의 효율을 높이기 위해 공간 데이터를 효율적으로 쿼리할 수 있는 공간 인덱스 (Spatial Index)를 사용할 수 있습니다. \n\n## 공간 인덱스(Spatial Index)\n\n### 공간 인덱스란? \n\n공간 인덱스(Spatial Index)는 공간 데이터(위치 정보를 포함한 데이터)를 효율적으로 쿼리하기 위한 데이터베이스 인덱스입니다. 공간 데이터는 위도와 경도로 표현되는 지점(point), 선(line), 면(area) 등 다양한 형태가 있을 수 있습니다. 이러한 데이터를 빠르게 검색하고, 공간적 관계(예: 교차, 인접, 포함 등)를 효율적으로 분석하기 위해 공간 인덱스를 사용합니다.\n\n### 공간 인덱스의 종류\n\n- R-트리(R-tree): 계층적 구조를 가진 트리 기반 인덱스로, 사각형 영역을 노드로 사용하여 공간 데이터를 구분합니다. 각 노드는 자식 노드를 포함하는 최소한의 경계 사각형을 가집니다. R-트리는 공간 검색과 범위 쿼리에 효율적입니다.\n\n- 쿼드트리(Quadtree): 공간을 사각형 영역으로 재귀적으로 분할하는 트리 기반 인덱스입니다. 각 노드는 최대 네 개의 자식(북동, 북서, 남동, 남서)을 가질 수 있으며, 공간을 균등하게 분할합니다.\n\n- GiST(Generalized Search Tree): 다양한 검색 트리를 일반화한 구조로, R-트리, B-트리 등 여러 종류의 인덱스를 하나의 프레임워크 내에서 지원합니다. PostgreSQL에서 사용됩니다.\n\n- SP-GiST(Space-Partitioned Generalized Search Tree): 공간을 분할하는 기법에 기반한 인덱스로, GiST의 확장형입니다. 공간을 비균등하게 분할하여 더 효율적인 검색을 가능하게 합니다.\n\n### 공간 인덱스의 사용\n\n공간 인덱스는 공간 쿼리를 실행할 때 데이터베이스가 전체 데이터를 순차적으로 검색하는 대신, 인덱스를 활용하여 필요한 데이터를 빠르게 찾아낼 수 있도록 합니다. 예를 들어, 특정 지역 내의 모든 관심 지점을 찾거나, 두 지점 사이의 거리를 계산할 때 공간 인덱스를 활용할 수 있습니다.\n\n### 공간 인덱스의 장점\n\n- 쿼리 성능 향상: 공간 데이터에 대한 쿼리를 실행할 때 검색 시간을 크게 줄일 수 있습니다.\n\n- 공간 관계 분석 용이: 데이터 간의 공간적 관계를 빠르게 분석할 수 있어, 인접한 데이터 찾기, 영역 내 데이터 검색 등이 용이합니다.\n\n### 공간 인덱스를 지원하는 데이터베이스 예\n\n- PostgreSQL의 PostGIS 확장: 공간 데이터를 위한 강력한 지원과 함께 R-트리 기반의 GiST 인덱스를 제공합니다.\n\n- MySQL: R-트리 기반의 공간 인덱스를 지원합니다.\n\n- SQLite의 SpatiaLite 확장: 공간 데이터를 위한 기능과 함께 R-트리 인덱스를 지원합니다.\n\n- MongoDB: 2D 및 2DSphere 인덱스를 통해 공간 데이터를 위한 인덱싱을 제공합니다.\n\n## Postgresql에서의 공간 인덱스\n\nPostgresql에는 지리적 객체를 저장, 쿼리 및 조작할 수 있는 기능을 추가하는 PostGIS 확장이 존재합니다. \n\n### PostGIS 확장 설치\n\n다음의 명령어를 통해 PostGIS 확장을 설치할 수 있습니다. \n\n```sql\nCREATE EXTENSION IF NOT EXISTS postgis;\n```\n\n### 공간 데이터 저장을 위한 테이블\n\n위치 데이터를 저장할 테이블을 생성하고 공간 정보를 저장할 수 있는 `GEOMETRY` 또는 `GEOGRAPHY` 타입의 컬럼을 포함시킵니다.\n\n```sql\nCREATE TABLE places (\n  id SERIAL PRIMARY KEY,\n  name VARCHAR(255),\n  location GEOGRAPHY(POINT, 4326) -- 위도와 경도를 사용하는 점 타입\n);\n```\n\n#### `GEOMETRY` 와 `GEOGRAPHY`  \n\n`GEOGRAPHY`와 `GEOMETRY`는 PostgreSQL의 PostGIS 확장에서 제공하는 두 가지 공간 데이터 타입입니다. \n\n- `GEOGRAPHY`\n\n    - 목적: 지구의 곡률을 고려하여 실제 지리적 위치를 더 정확하게 모델링합니다.\n\n    - 사용 케이스: 큰 거리를 다루거나, 극지방 같이 지구의 곡률이 중요한 계산에 영향을 미치는 지역에서 주로 사용됩니다.\n\n    - 계산 방식: 지구를 타원체로 가정하고, 위도와 경도를 사용하여 거리와 면적을 계산합니다. 이로 인해 `GEOMETRY`에 비해 계산이 더 복잡하고 느릴 수 있습니다.\n\n    - 좌표 체계: 기본적으로 WGS 84 (EPSG:4326) 좌표 체계를 사용합니다.\n\n- `GEOMETRY`\n\n    - 목적: 평면 상의 점들을 사용하여 공간 데이터를 모델링합니다. 지구의 곡률을 고려하지 않고, 모든 계산을 유클리드(평면) 기하학을 사용하여 수행합니다.\n\n    - 사용 케이스: 소규모 지역 또는 곡률이 중요하지 않은 계산에 주로 사용됩니다. 예를 들어, 도시 또는 건물 내부와 같은 상대적으로 작은 지역에서 사용됩니다.\n\n    - 계산 방식: 유클리드 기하학을 바탕으로 거리, 면적 등을 계산합니다. 이는 `GEOGRAPHY`에 비해 계산이 더 간단하고 빠릅니다.\n\n    - 좌표 체계: 다양한 좌표 체계를 지원하며, 사용자가 필요에 따라 선택할 수 있습니다.\n\n- 주요 차이점\n\n    - 계산 정확도와 복잡성: `GEOGRAPHY`는 지구의 곡률을 고려하기 때문에 더 복잡하고, 대규모 거리 계산에 적합합니다. 반면, `GEOMETRY`는 간단한 평면 계산에 적합합니다.\n\n    - 성능: `GEOMETRY`는 일반적으로 `GEOGRAPHY`보다 계산이 더 빠르지만, 이는 계산의 복잡성과 정확도에 따라 달라질 수 있습니다.\n\n    - 적용 범위: `GEOGRAPHY`는 전 세계적인 지리적 데이터 처리에 적합하고, `GEOMETRY`는 지역적 또는 평면적 공간 데이터 처리에 더 적합합니다.\n\n    - 대규모, 정확한 지리적 계산이 필요한 경우 `GEOGRAPHY`를, 소규모 또는 빠른 계산이 필요한 경우 `GEOMETRY`를 사용하는 것이 좋습니다.\n\n#### 결론\n\n제가 개발할 서비스의 경우, 대부분의 상호 작용이 도시 수준에서 일어나고, 성능과 응답 시간이 중요한 요소라고 생각되기 때문에 `GEOMETRY`가 더 유리해 보입니다. \n\n### 공간 인덱스 생성\n\n공간 인덱스도 생성할 수 있습니다. \n\n```sql\nCREATE INDEX idx_places_location ON places USING GIST (location);\n```\n\n### 위치 기반 검색 쿼리의 작성법\n\n사용자의 위치를 기반으로 가까운 장소를 찾아주는 `ST_DWithin` 같은 공간 함수를 사용하는 쿼리를 작성합니다.\n\n```sql\n-- 사용자 위치 (위도 37, 경도 127) 주변 10km 이내의 장소 검색\nSELECT name FROM places\nWHERE ST_DWithin(\n  location,\n  ST_MakePoint(127, 37)::geography,\n  10000 -- 거리(미터 단위)\n);\n```\n\n#### 공간함수(Spatial functions)란? \n\n공간 함수는 PostGIS, PostgreSQL의 확장 기능 중 하나입니다. 공간 데이터를 처리하기 위한 함수로, 지리적 위치의 관계, 형태, 거리 등을 계산하고 분석하는 데 사용됩니다.\n\n#### PostGIS에서 제공하는 주요 공간 함수\n\n1. 기하학적 관계 판단 함수:\n\n    - `ST_Contains(A, B)`: A가 B를 포함하는지 여부를 반환합니다.\n\n    - `ST_Intersects(A, B)`: A와 B가 교차하는지 여부를 반환합니다.\n\n    - `ST_Within(A, B)`: A가 B 내부에 있는지 여부를 반환합니다.\n\n    - `ST_Touches(A, B)`: A와 B가 접하는지 여부를 반환합니다.\n\n    - `ST_DWithin(A, B, distance)`: A와 B가 지정된 거리 `distance` 이내에 있는지 여부를 반환합니다.\n\n1. 기하학적 변환 및 생성 함수:\n\n    - `ST_Buffer(geometry, radius)`: 주어진 지오메트리 주변에 지정된 반경의 버퍼를 생성합니다.\n\n    - `ST_Centroid(geometry)`: 지오메트리의 중심점(centroid)을 계산합니다.\n\n    - `ST_MakePoint(x, y)`: x, y 좌표로 점을 생성합니다.\n\n1. 거리 및 길이 측정 함수:\n\n    - `ST_Distance(A, B)`: A와 B 사이의 최소 거리를 계산합니다.\n\n    - `ST_Length(geometry)`: 선형 지오메트리의 길이를 계산합니다.\n\n1. 공간 분석 함수:\n\n    - `ST_Area(geometry)`: 면적을 계산합니다.\n\n    - `ST_Union(A, B)`: 두 지오메트리의 합집합을 생성합니다.\n\n    - `ST_Intersection(A, B)`: 두 지오메트리의 교집합 영역을 계산합니다.\n\n1. 공간 색인 및 최적화를 위한 함수:\n\n    - `ST_GeomFromText('POINT(0 0)')`, `ST_GeomFromEWKT(...)`: Well-Known Text(EWKT)로부터 지오메트리를 생성합니다.\n\n    - `ST_SetSRID(geometry, srid)`: 지오메트리에 공간 참조 시스템(Spatial Reference System Identifier, SRID)을 설정합니다.\n\n## 같이 보기\n\n[PostGIS 3.0.0 사용자 지침서](https://postgis.net/docs/manual-3.0/postgis-ko_KR.html)\n\n"},{"excerpt":"서론 DB 설계를 하다보면 index를 정의해야 하는 경우가 많습니다. 만약 Sqlalchemy의 create_all() 메소드를 사용해 테이블을 생성하고 있다면 Sqlalchemy에서 동시에 index를 정의할 수 있습니다.  Sqlalchemy에서의 인덱스 정의 단일 컬럼 인덱스 Sqlalchemy의  객체를 생성할 때에   플래그를 설정하면 간단하게…","fields":{"slug":"/Sqlalchemy의-index/"},"frontmatter":{"date":"February 13, 2024","title":"Sqlalchemy의 index","tags":["SqlAlchemy"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\nDB 설계를 하다보면 index를 정의해야 하는 경우가 많습니다.\n\n만약 Sqlalchemy의 [create\\_all()](https://sharknia.github.io/SQLAlchemy-create_all-메소드로-데이터베이스-테이블-자동-생성하기) 메소드를 사용해 테이블을 생성하고 있다면 Sqlalchemy에서 동시에 index를 정의할 수 있습니다. \n\n## Sqlalchemy에서의 인덱스 정의\n\n### 단일 컬럼 인덱스\n\nSqlalchemy의 `Column` 객체를 생성할 때에 `index=True`  플래그를 설정하면 간단하게 단일 컬럼에 대한 인덱스를 생성할 수 있습니다. \n\n```python\nColumn('user_id', Integer, index=True)\n```\n\n### 복합 인덱스\n\n두 개 이상의 컬럼을 포함하는 복합 인덱스는 `Index` 객체를 사용하여 생성할 수 있습니다. \n\n```python\nfrom sqlalchemy import Index\nIndex('my_index', MyModel.column1, MyModel.column2.desc())\n```\n\n여기서 `desc()` 메소드는 해당 컬럼을 내림차순으로 정렬하도록 지시합니다. 오름차순은 기본값이므로 `asc()`는 일반적으로 생략됩니다.\n\n### Unique 인덱스\n\nUnique 인덱스는 값의 중복을 방지하고 해당 컬럼의 각 값이 유일하다는 것을 보장합니다. \n\n```python\nColumn('email', String, unique=True)\n```\n\n### 복합 Unique 인덱스\n\nSqlAlchemy에서는 복합 인덱스에 대해서도 유니크 제약 조건을 적용할 수 있습니다. 이를 통해 테이블 내에서 특정 컬럼 조합의 유니크성을 강제할 수 있습니다. \n\nSqlAlchemy에서 복합 유니크 인덱스를 정의하는 방법은 두 가지입니다. \n\n1. `UniqueConstraint` 사용\n\n    `UniqueConstraint`는 테이블의 컬럼에 대한 유니크 제약 조건을 정의하는 데 사용됩니다. 이는 `__table_args__` 속성 내부에서 사용될 수 있습니다.\n\n    ```python\n    from sqlalchemy import Column, Integer, String, create_engine\n    from sqlalchemy.ext.declarative import declarative_base\n    from sqlalchemy.schema import UniqueConstraint\n    \n    Base = declarative_base()\n    \n    class MyModel(Base):\n        __tablename__ = 'my_model'\n        id = Column(Integer, primary_key=True)\n        column1 = Column(Integer)\n        column2 = Column(String)\n        __table_args__ = (UniqueConstraint('column1', 'column2', name='my_unique_constraint'),)\n    ```\n\n1. `Index` 사용 시 `unique=True` 설정\n\n    `Index` 객체를 생성할 때 `unique=True` 플래그를 설정하여 복합 유니크 인덱스를 생성할 수 있습니다. 이 방법은 인덱스 생성과 동시에 컬럼 조합의 유니크성도 보장합니다.\n\n    ```python\n    from sqlalchemy import Column, Integer, String, MetaData, Table\n    from sqlalchemy.schema import Index\n    \n    metadata = MetaData()\n    my_table = Table('my_table', metadata,\n        Column('id', Integer, primary_key=True),\n        Column('column1', Integer),\n        Column('column2', String),\n    )\n    \n    # 복합 유니크 인덱스 생성\n    my_unique_index = Index('my_unique_index', my_table.c.column1, my_table.c.column2, unique=True)\n    ```\n\n두 방법 모두 데이터베이스에서 해당 컬럼 조합의 유니크성을 강제합니다. `UniqueConstraint`는 주로 제약 조건을 명시적으로 표현할 때 사용되며, `Index`의 `unique=True` 설정은 검색 최적화와 유니크 제약 조건을 동시에 적용하고자 할 때 유용합니다.\n\n### **_\\_table\\_args\\__**\n\n  `__table_args__` 클래스 속성은 튜플이나 사전 형태로 제공될 수 있으며, 테이블 생성 시 추가적인 SQL 제약 조건이나 인덱스를 정의하는데 사용됩니다.  \n\n예를 들어 복합 인덱스를 `__table_args__` 클래스 속성을 사용해 모델 내부에서 정의한 예제 코드는 다음과 같습니다. \n\n```python\nclass Event(Base):\n    __tablename__ = 'events'\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String)\n    timestamp = Column(DateTime)\n    user_id = Column(Integer)\n    is_active = Column(Boolean)\n\n    # 복합 인덱스 정의\n    __table_args__ = (\n        Index('ix_events_user_id_timestamp', 'user_id', timestamp.desc()),\n    )\n```\n\n이 방식으로 정의된 인덱스는 데이터베이스에 해당 모델을 반영할 때 자동으로 생성됩니다. \n\n\n\n"},{"excerpt":"서론 현재 회사에서는 Alembic 같은 데이터마이그레이션 도구를 사용하고 있지 않습니다. 따라서 스키마 버전 관리등은 별도로 하고 있지 않으며, 다만 SqlAlchemy의 create_all() 메소드를 이용해 프로덕션 환경이나 dev 환경에서의 테이블의 누락은 안생기게끔만 간단하게 관리하고 있습니다.  이 방법은 테이블의 수정 또는 삭제를 반영할 수는…","fields":{"slug":"/SQLAlchemy-create_all-메소드로-데이터베이스-테이블-자동-생성하기/"},"frontmatter":{"date":"February 12, 2024","title":"SQLAlchemy create_all() 메소드로 데이터베이스 테이블 자동 생성하기","tags":["SqlAlchemy","Work","Python","DataBase","FastAPI"]},"rawMarkdownBody":"![](image1.png)\n## 서론\n\n현재 회사에서는 Alembic 같은 데이터마이그레이션 도구를 사용하고 있지 않습니다. 따라서 스키마 버전 관리등은 별도로 하고 있지 않으며, 다만 SqlAlchemy의 create\\_all() 메소드를 이용해 프로덕션 환경이나 dev 환경에서의 테이블의 누락은 안생기게끔만 간단하게 관리하고 있습니다. \n\n이 방법은 테이블의 수정 또는 삭제를 반영할 수는 없는 방법이지만, 아직 RDB 기반으로 옮긴지 얼마 되지 않아 상대적으로 DB의 크기가 작아 현재는 이 방법으로 충분한 상황입니다. \n\ncreate\\_all() 메소드를 사용하면서 알게 된 점을 기록해두려고 합니다. \n\n## create\\_all() 메소드란? \n\nSqlAlchemy의 create\\_all() 메소드는 `Metadata` 객체에 등록된 모든 테이블 정의를 바탕으로 해당 데이터베이스 엔진에 테이블을 생성하는데 사용됩니다. 이 메소드는 이미 생성되어있는 테이블은 생성하지 않으며, 새로 정의된(DB에 없는) 테이블만 새롭게 데이터베이스에 생성합니다. \n\n따라서, 스키마가 달라도 뭔가 수정이 되었어도 DB에 존재만 한다면 생성하거나 덮어쓰지 않으므로 만약 수정 사항을 반영하는걸 해당 메소드로 진행하려면 테이블을 drop한 뒤 생성해야 합니다. 물론 데이터가 많지 않거나 데이터를 날려도 상관없는 dev 환경에서만 유효한 방법입니다. \n\n### 기본적인 사용 방법\n\n```python\nfrom sqlalchemy import create_engine, MetaData\n\n# 데이터베이스 엔진 생성\nengine = create_engine('sqlite:///example.db', echo=True)\n\n# MetaData 인스턴스 생성\nmetadata = MetaData()\n\n# 테이블 정의들...\n# 예: users_table = Table('users', metadata, Column('id', Integer, primary_key=True), ...)\n\n# create_all 호출하여 모든 테이블 생성\nmetadata.create_all(engine)\n```\n\n`create_engine` 함수는 데이터베이스와의 연결을 설정합니다. `echo=True`는 SQLAlchemy가 실행하는 SQL 명령을 콘솔에 출력하도록 설정하는 옵션입니다.\n\n`create_all()` 메소드는 첫 번째 인자로 `Engine` 객체를 받습니다. 이 `Engine` 객체는 SQLAlchemy가 데이터베이스와 통신하기 위해 사용하는 기본 구성 요소입니다.\n\n### 비슷한 메소드들? \n\n#### drop\\_all()\n\n`drop_all()` 메소드는 `create_all()`과 반대로 작동합니다. `MetaData` 객체에 등록된 모든 테이블을 데이터베이스에서 삭제합니다. 테이블에 데이터가 있을 경우, 데이터도 함께 삭제되므로 사용할 때 주의가 필요합니다.\n\n```python\nmetadata.drop_all(engine)\n```\n\n#### Table.create() 및 Table.drop()\n\n`create_all()` 또는 `drop_all()` 메소드 대신, 개별 `Table` 객체에 대해 `create()` 또는 `drop()` 메소드를 호출할 수 있습니다. 이 방법을 사용하면 특정 테이블에 대한 작업을 더 세밀하게 제어할 수 있습니다.\n\n```python\n# 개별 테이블 생성\nusers_table.create(engine)\n\n# 개별 테이블 삭제\nusers_table.drop(engine)\n```\n\n#### reflect()\n\n`reflect()` 메소드는 데이터베이스의 기존 스키마 정보를 `MetaData` 객체로 로드합니다. 이 메소드는 기존 데이터베이스 구조를 SQLAlchemy 모델로 반영할 때 유용합니다.\n\n```python\nmetadata.reflect(engine)\n```\n\n## 나는 어떻게 사용하고 있나? \n\ndev 또는 로컬 환경에서 테스트를 할 경우 적극적으로 사용하고 있습니다. 주로 처음 테이블을 생성할 때에 굳이 create 쿼리문을 별도로 쓰지 않고 모델만 생성 후 해당 메소드를 이용해 테이블을 생성하거나, 테이블에 수정 사항이 생긴 경우 drop 후 create\\_all() 메소드를 생성해 다시 DB 테이블을 생성합니다. \n\n물론, 그때 그때 create 함수를 쓰기는 너무나 귀찮은 일이기 때문에, FastAPI의 lifespan을 활용해서 FastAPI 어플리케이션이 시작될 때에 자동으로 create\\_all() 메소드를 호출하도록 세팅을 해두었습니다. \n\n```python\nfrom sqlalchemy.orm import declarative_base\n\nBase = declarative_base()\n_db_connection: Engine = Optional[Engine]\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    await supabase_on_startup()\n    settings = Settings()\n    logger.info(f\"[SERVER_ENVIRONMENT] {settings.desc.SERVER_ENVIRONMENT}\")\n    yield\n    await supabase_on_shutdown()\n\n\nasync def on_startup():\n    global _db_connection\n    DATABASE_URL = f\"postgresql+asyncpg://postgres.{settings.postgresql_setting.projectid}:{settings.postgresql_setting.password}@aws-0-ap-northeast-2.pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n\n    # pool_size와 max_overflow의 초기값 설정\n    pool_size = 5  # 기본값\n    max_overflow = 10  # 기본값\n    echo = False\n\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        pool_size=pool_size,\n        max_overflow=max_overflow,\n        echo=echo,\n    )\n    # 비동기 엔진을 사용하여 테이블 생성\n    async with _db_connection.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n```\n\n이런식으로 구현해두었습니다. 이렇게 하면 어플리케이션이 수정될 때마다 존재하지 않는 테이블을 생성하게 됩니다. 다만, 이렇게 구현했을 때에 주의해야 할 점이 있습니다. \n\n### 주의\n\n위에서 언급했듯이, create\\_all() 메소드는 `Metadata`에 등록된 테이블 정보를 가지고 테이블을 생성합니다. 뒤집어 말하면 `Metadata`에 테이블이 등록되지 않는다면 생성되지 않는 다는 말인데, 따라서  `Metadata`에 테이블 정보가 등록되는 시점을 정확히 아는게 중요합니다. \n\n### Metadata에 테이블 정보가 등록되는 시점\n\n`MetaData` 객체에 테이블 정보가 등록되는 시점은 해당 `Table` 객체가 생성될 때입니다. SqlAlchemy에서는 `Table` 객체를 `MetaData` 객체에 직접 연결하여 테이블을 정의합니다. 클래스 정의 내에서 `Column`과 같은 필드를 사용하여 `Base` 클래스를 상속받는 모델을 정의하면, 이러한 모델 클래스가 생성될 때 내부적으로 `Table` 객체가 생성되고 `MetaData` 객체에 자동으로 등록됩니다. `declarative_base()`를 호출하여 생성된 `Base` 클래스는 내부적으로 `MetaData` 인스턴스를 포함하고 있으며, 이 `MetaData` 인스턴스는 모델 클래스를 통해 정의된 모든 테이블의 메타데이터를 관리합니다.\n\n따라서, 모델 클래스를 정의하는 순간 해당 클래스에 연결된 `Table` 객체가 생성되고, 이 객체는 자동으로 `Base` 클래스에 내장된 `MetaData` 객체에 등록됩니다. 이후 `Base.metadata.create_all(engine)`와 같은 방식으로 호출하면, `MetaData`에 등록된 모든 테이블에 대한 생성 명령이 데이터베이스 엔진으로 전송됩니다.\n\n#### FastAPI에서 모델 클래스가 정의되는 순간은? \n\nFastAPI 어플리케이션에서 “모델 클래스가 정의되는 순간”은 Python 스크립트가 실행되어 해당 모델 클래스가 메모리에 로드되는 시점을 의미합니다. \n\nFastAPI 어플리케이션을 실행하면(예를 들어 `uvicorn main:app` 등의 명령어를 사용하여) Python 인터프리터는 main.py를 로드하고 실행합니다. 이 과정에서 main.py에서 직접적으로, 또는 임포트된 모듈 내에서 다시 임포트된 SqlAlchemy의ㅏ 모델 클래스들도 함께 로드되고 메모리에 생성됩니다. 바로 이 시점에 각 모델 클래스에 해당하는 `Table` 객체가 `Metadata` 객체에 등록됩니다. \n\n다시 쉽게 이야기하자면, app.py 또는 main.py 같은 파일과 이 파일에 임포트 된 파일들, 다시 임포트 된 파일들 중에 모델 객체가 있어야만 합니다. 외딴섬처럼 덩그러니 model 파일을 생성해두고 아무곳에도 import 하지 않는다면(메인 py 파일과 연결되어 있지 않다면) 테이블은 생성되지 않습니다. 어떻게 생각하면 당연한 일인데, 이 당연한 부분을 놓쳐 길게 헤맸던 기억이 있습니다. 이 기억 때문에 해당 문서를 작성한 셈입니다. \n\n"},{"excerpt":"서론 개인적으로 이것저것 만들려고 시도한 적은 많았는데, 완성한 적은 이번이 처음입니다. (nolog 프로젝트) 라이센스도 등록해보고 릴리즈도 처음 해봤는데, 릴리즈를 하려다 보니 다음과 같은 문구가 있었습니다.  “버전 관리” 에 대해서는 추상적으로 알고 있었지만, 시맨틱 버전 관리라는 개념의 이름에 대해서는 처음 알았습니다. 부끄러운 일이기도 하고, …","fields":{"slug":"/시맨틱-버전-관리/"},"frontmatter":{"date":"February 08, 2024","title":"시맨틱 버전 관리","tags":["ETC"]},"rawMarkdownBody":"## 서론\n\n개인적으로 이것저것 만들려고 시도한 적은 많았는데, 완성한 적은 이번이 처음입니다. ([nolog 프로젝트](https://sharknia.github.io/series/GitHub-Pages와-Notion-API-연동/)) 라이센스도 등록해보고 릴리즈도 처음 해봤는데, 릴리즈를 하려다 보니 다음과 같은 문구가 있었습니다. \n\n\n        If you’re new to releasing software, we highly recommend to learn more about semantic versioning.\n\n“버전 관리” 에 대해서는 추상적으로 알고 있었지만, 시맨틱 버전 관리라는 개념의 이름에 대해서는 처음 알았습니다. 부끄러운 일이기도 하고, 또 이번 기회에 잘 알아둬야겠다고 생각이 들어서 한 번 정리해두려고 합니다. \n\n## 시맨틱 버전 관리란? \n\n시맨틱 버전 관리는 소프트웨어 버전 번호를 할당하고 증가시키는 규칙 시스템입니다. 이 시스템은 소프트웨어의 버전을 명확하게 해줘 개발자가 소프트웨어의 변경 사항을 쉽게 이해할 수 있게 돕습니다. 시맨틱 버전 관리는 주로 세 부분으로 구성된 버전 번호를 사용합니다. \n\n### Major Version\n\n이전 버전과 호환되지 않는 새로운 변경사항을 도입했을 때 증가합니다.\n\n### Minor Version\n\n이전 버전과 호환되면서 새로운 기능이 추가되었을 때 증가합니다. \n\n### Patch Version\n\n이전 버전과 호환되는 버그 수정이 이루어졌을 때 증가합니다. \n\n버전 번호는 `Major.Minor.Patch` 형식으로 표현됩니다. 예를 들어 `2.3.1` 과 같은 형태로 나타날 수 있습니다. \n\n## 목적\n\n소프트웨어 어떤 버전이 다른 버전과 어떻게 다른지를 명확하게 명시하여 소프트웨어를 사용하는 개발자들이 해당 변경사항을 쉽게 파악하고 대응할 수 있도록 합니다. \n\n예를 들어 어떤 라이브러리의 Major 버전이 변경되었다면 이는 기존 코드와 호환되지 않을 수 있는 중대한 변경이 있었음을 의미합니다. 따라서 개발자는 코드를 업데이트 하기 전에 해당 변경 사항을 주의 깊게 검토할 필요가 있습니다. \n\n"},{"excerpt":"nolog 소개 많은 개발자들이 한 번쯤은 \"나만의 블로그를 운영해야지\"라고 생각해본 적이 있을 겁니다. 하지만 실제로 블로그를 시작하고 유지하는 일은 상당한 도전과제입니다. 공부하고 개발하는 것만으로도 바쁜데, 여기에 글을 쓰고 관리하는 일은 때때로, 아니 항상 부담스러운 일입니다. 노션으로 공부를 하는 개발자가 있습니다. 일을 하면서일수도 있고, 따로…","fields":{"slug":"/Readme/"},"frontmatter":{"date":"February 08, 2024","title":"Readme","tags":["Notion-API","Typescript","Blogging","Hobby"]},"rawMarkdownBody":"## nolog\n\n### 소개\n\n많은 개발자들이 한 번쯤은 \"나만의 블로그를 운영해야지\"라고 생각해본 적이 있을 겁니다. 하지만 실제로 블로그를 시작하고 유지하는 일은 상당한 도전과제입니다. 공부하고 개발하는 것만으로도 바쁜데, 여기에 글을 쓰고 관리하는 일은 때때로, 아니 항상 부담스러운 일입니다.\n\n노션으로 공부를 하는 개발자가 있습니다. 일을 하면서일수도 있고, 따로 짬을 내어 공부를 하면서일수도 있습니다. 그러다가 한 번 쯤은 생각하게 됩니다. 노션으로 한 공부나 작업 기록이 블로그에 자동으로 업로드된다면 좋을텐데!\n\n저 역시 마찬가지입니다. 저는 귀찮은 일이 너무 귀찮습니다. 공부나 블로그나 어차피 한 번에 할 수 있는거 아닌가? 공부만 하면 포스팅이 되면 안되나? 왜 똑같은 일을 두 번 세 번 해야 하는걸까?\n\n#### 그래서 만들었습니다.\n\nnolog는 노션에 집중해서 작업하거나 공부해서 기록하는 것만으로도 그 내용이 자동으로 마크다운으로 변환되어 GitHub 기반의 블로그에 포스팅될 수 있도록 해주는 프로그램입니다. 취업 준비, 지식 공유, 개인 브랜딩 등 여러 목적으로 블로그를 운영하고자 하는 분들에게 nolog는 시간과 노력을 크게 절약해 줄 것입니다.\n\n### 시작하기\n\n#### Notion 준비\n\n1. 노션 계정을 준비합니다.\n\n1. [Base Template](https://www.notion.so/248d5b9bf2a644b4b25485c828d5b04f)을 내 노션으로 복제합니다.\n\n    ![](image1.png)\n1. 복제된 페이지에서 공유 - 링크 복사를 눌러 페이지의 주소를 얻습니다. 해당 주소는 `username.notion.site/NOTION_PAGE_ID` 꼴입니다. `NOTION_PAGE_ID`를 기록합니다.\n\n1. [Notion API 관리 페이지](https://www.notion.so/my-integrations)에서 `새 API 통합 만들기` 를 선택해 API를 생성해줍니다. 워크스페이스별로 권한을 관리하므로, 정확한 워크스페이스를 선택하여 진행합니다.\n\n1. `프라이빗 API 통합 시크릿키`를 기록합니다.\n\n1. 콘텐츠 기능은 읽기/업데이트/입력을 필수로 선택해야 합니다.\n\n    ![](image2.png)\n1. 노션에서 연결된 워크스페이스를 선택하고, 위에서 만든 통합 API를 연결해줍니다.\n\n    ![](image3.png)\n#### 프로젝트 준비\n\n1. 이 저장소의 별표를 누르고 여러분의 프로필로 포크합니다. 그 후 로컬로 클론합니다.\n\n1. (Optional) [github.io](http://github.io/) 기반의 블로그 저장소를 준비합니다.\n\n1. `.env.example` 파일의 이름을 `.env`로 변경하고 설정값들을 세팅합니다.\n\n    - NOTION-KEY : 프라이빗 API 통합 시크릿키를 입력합니다.\n\n    - NOTION_DATABASE_ID : NOTION_PAGE_ID를 입력합니다.\n\n    - BLOG_URL : 깃허브 블로그의 URL을 입력합니다. 노션 페이지 링크를 실제 블로그 링크로 변경하는데 사용됩니다.\n\n    - SAVE_DIR : 콘텐츠들이 저장될 디렉토리입니다. [github.io](http://github.io/) 블로그의 저장소에서 포스팅될 마크다운들이 저장될 디렉토리입니다.\n\n    - SAVE_SUB_DIR (Optional) : 포스트들이 저장될 하위 디렉토리입니다. 노션 페이지의 프로퍼티들로 정의합니다. 예를 들어 date/tag/로 입력한 경우, date와 tag가 노션 페이지의 속성에 존재해야 합니다.\n\n    - BLOG_REPO : 블로그 레포지토리의 경로입니다.\n\n    - GIT_USER_NAME : 깃허브 User Name 입니다.\n\n    - GIT_USER_EMAIL : 깃허브 User Email 입니다.\n\n1. 저장소를 내려 받은 곳에서 다음의 명령어를 사용합니다.\n\n    - nvm use\n\n    - npm install\n\n    - npm install -g ts-node\n여기까지 세팅 후 ts-node index.ts를 실행합니다. 메타데이터 파일이 초기화됩니다. 초기화된 메타데이터를 저장소에 커밋/푸쉬합니다.\n\n### 사용법\n\n#### 노션 페이지 마크다운 변환\n\n변환은 상태값을 기준으로 작동합니다. 상태값의 정의는 다음과 같습니다.\n\n- Writting : 작성중인 글입니다. 블로그에 반영되지 않습니다.\n\n- Ready : Ready 상태의 글은 프로그램이 실행되면 마크다운으로 변환됩니다. 변환 된 후, updated 상태로 바뀝니다.\n\n- Updated : 마크다운 변환이 완료된 글입니다.\n\n- ToBeDeleted : 삭제할 글입니다. 타겟 디렉토리 안의 해당하는 마크다운 파일을 삭제합니다.\n\n- Deleted : 삭제된 글입니다.\n\n노션에 원하는대로 글을 작성하고, 적당한 상태값을 설정한 뒤 `ts-node index.ts`로 프로그램을 실행하면 지정한 디렉토리에 마크다운 파일이 저장됩니다.\n\n#### 호환 노션 블록\n\n노션의 다양한 기본 블록을 모두 지원합니다.\n또한, `미디어 - 이미지`, `미디어 - 코드`, `미디어 - 북마크` 블록들을 지원합니다.\n\n#### 페이지 링크 기능\n\n같은 데이터베이스(블로그 데이터베이스)에 속한 노션 페이지라면 해당 페이지 링크를 걸면 그 페이지에 해당하는 블로그 주소로 링크로 변환되는 기능을 제공합니다.\n\n### [Optional] 블로그 자동 배포\n\n로컬에서만 사용해도 노션 글을 마크다운으로 바꿔주는 기능을 갖고 있지만, [github.io](http://github.io/) 기반의 블로그 자동 배포도 가능합니다. 이를 위해서 깃허브 액션을 사용합니다.\n\n#### Git Token 발행\n\n깃허브의  `Settings` - `Developer Settings` - `Personal Access Token` 메뉴에서 토큰을 발행해야 합니다.\n\n#### 변수 설정\n\n레포지토리에 Secrets와 Variables를 설정해줘야 합니다.\n\n![](image4.png)\n저장소의 `Settings` - `Secrets and varaibles` - `Actions`에서 깃허브 액션에 필요한 환경변수를 설정할 수 있습니다.\n\n- Secrtes\nGITTOKEN, NOTION_DATABASE_ID, NOTION_KEY를 생성 해줍니다. GITTOKEN은 방금 발행한 Git Token 값이며, 나머지는 환경변수에 입력한 것과 동일한 값입니다.\n\n- Varaiables\nBLOG_REPO, BLOG_URL, GIT_USER_EMAIL, GIT_USER_NAME, SAVE_DIR를 생성합니다. 환경변수 입력과 동일한 값입니다.\n\n#### 워크플로우 설정\n\n깃허브 기반의 블로그 배포는 사용하는 테마에 따라 방법이 완전히 다릅니다. 따라서 깃허브 액션 워크플로우를 자동 배포까지 하는 경우, 자동 배포는 별도로 구성하여 블로그 저장소에 커밋/푸쉬까지만 하면 되는 케이스별로 구분해두었습니다.\n\n포크를 한 경우에는 기본적으로 워크플로우가 비활성화 되어있습니다. 원하는 워크플로우 고른 후 활성화 해주면 됩니다.\n\n#### 블로그 저장소에 커밋/푸쉬까지만 자동화하는 워크플로우\n\n`workflow_2.yml.disabled` 워크플로우를 사용하면 됩니다. 파일 이름에서 .disabled를 제거해야 합니다.\n노션에서 글을 가져와 깃허브 블로그 레포지토리에 마크다운 파일을 집어넣고 커밋/푸쉬 하는 것까지 자동화 되어있습니다.\n\n#### 블로그 배포까지 자동화하는 워크플로우\n\n`main.yml` 파일의 `blog-deploy` Job을 참조해서 워크플로우를 각자 블로그 배포 형식에 맞게 수정하면 됩니다. 해당 파일은 제가 실제로 배포에 사용하는 파일입니다.\n\n### 개발과정\n\n[개발과정 몰아보기](https://sharknia.github.io/series/GitHub-Pages%EC%99%80-Notion-API-%EC%97%B0%EB%8F%99/)\n\n### 문의\n\n[zel@kakao.com](mailto:zel@kakao.com)\n\n"},{"excerpt":"서론 파이썬에서 잠깐 테스트코드를 맛본적이 있습니다. 문법 자체는 조금 복잡했지만(익숙하지 않았지만) 설치 자체는 그다지 어렵지 않았던 것으로 기억합니다.  그래서 타입스크립트에서도 그럴 줄 알았습니다.  설치 일단, 설치가 한 두개가 아니었습니다. 또, 설치하는 패키지를 package.json 파일의 devDependencies에 추가했습니다. 이는 개…","fields":{"slug":"/Typescript의-Testcode-맛보기/"},"frontmatter":{"date":"February 06, 2024","title":"Typescript의 Testcode 맛보기","tags":["Typescript"]},"rawMarkdownBody":"## 서론\n\n파이썬에서 잠깐 테스트코드를 맛본적이 있습니다. 문법 자체는 조금 복잡했지만(익숙하지 않았지만) 설치 자체는 그다지 어렵지 않았던 것으로 기억합니다. \n\n그래서 타입스크립트에서도 그럴 줄 알았습니다. \n\n## 설치 \n\n일단, 설치가 한 두개가 아니었습니다. 또, 설치하는 패키지를 package.json 파일의 devDependencies에 추가했습니다. 이는 개발 시에만 필요한 의존성을 나타냅니다. 처음에는 멋모르고 다음의 패키지 하나만 설치해주었습니다. \n\n### @types/jest 설치\n\n```shell\nnpm i --save-dev @types/jest\n```\n\n#### `--save-dev`\n\n설치하는 패키지를 package.json 파일의 devDependencies에 추가하는 옵션이 바로 이 옵션입니다. 프로덕션에서는 필요하지 않은 테스트 라이브러리, 빌드 도구 등을 설치할 때에 해당 옵션을 붙여 설치합니다. 이 옵션을 붙여 설치할 경우, 다음과 같이 추가됩니다. \n\n```yaml\n\"devDependencies\": {\n    \"@types/jest\": \"^26.0.0\"\n}\n```\n\n이후 빌드 시스템이나 배포 파이프라인에서 `npm install --production` 명령어를 사용하면 이 곳에 정의된 패키지들은 설치를 건너뛰게 됩니다. \n\n#### `@types/jest`\n\nJest는 자바스크립트 테스팅 프레임워크입니다. `@types/jest` **** 는 Jest의 타입 정의를 포함하는 타입스크립트 패키지입니다. 타입스크립트 프로젝트에서 Jest를 사용할 때, Jest의 함수와 객체에 대한 자동 완성, 타입 체킹 등의 이점을 제공합니다. \n\n그렇습니다. Jest는 별도로 설치해야 합니다. \n\n### Jest 설치\n\nJest 역시 설치해줍니다. 역시 개발 환경에서만 사용할 라이브러리 이므로 —save-dev 옵션을 사용합니다.\n\n```bash\nnpm install --save-dev jest\n```\n\nJest 설치 후 `package.json`  scripts 섹션에 다음의 내용을 추가합니다. \n\n```json\n\"scripts\": {\n    \"test\": \"jest\"\n}\n```\n\n이렇게 하면 `npm test` 명령어가 Jest를 실행합니다. \n\n### TypeScript를 위한 Jest 설정\n\nJest는 기본적으로 JavaScript 파일을 지원하지만 TypeScript 파일을 처리하기 위해서는 추가적인 설정이 필요합니다. \n\n#### `ts-jest` 설치\n\nJest가 TypeScript 파일을 이해할 수 있도록 ts-jest를 설치해야 합니다. \n\n```json\nnpm install --save-dev ts-jest\n```\n\n#### jest.config.js\n\n프로젝트 루트에 jest.config.js 파일을 생성 또는 수정하여 ts-jest를 사용하도록 Jest를 구성합니다. \n\n```json\n// jest.config.js\nmodule.exports = {\n  preset: 'ts-jest',\n  testEnvironment: 'node',\n};\n```\n\n### `npm test`\n\n모든 설정을 마친 후 npm test를 실행하면 `__test__` 를 자동으로 탐색하여 `.test.js` 또는 `.spec.js` 접미사를 가진 파일의 테스트를 실행합니다. \n\n## 테스트 파일의 경로\n\n테스트 파일을 어디에 둘 것인가? 는 몇가지 패턴이 있고, 가장 널리 사용되는 방식은 다음의 두가지입니다. \n\n### 동일 디렉토리 내에 테스트 파일 배치 \n\n코드 파일과 동일한 디렉토리에 그에 해당하는 테스트 파일을 둡니다. 테스트 파일과 관련된 코드 파일이 가까이에 있어서 관리하기가 쉬운것이 장점입니다. 파일 이름은 대체로 테스트되는 파일의 이름을 따르며, `.test` 또는 `.spec` 접미사를 추가하여 구분합니다. \n\n### 별도의 테스트 디렉토리 사용\n\n프로젝트 루트 또는 각 기능별 디렉토리에 `__tests__` 디렉토리를 만들고, 그 안에 관련 테스트 파일을 모으는 것입니다. 이 구조는 테스트 파일을 소스 코드에서 분리하여, 소스 코드 디렉토리를 더 깔끔하게 유지할 수 있게 해줍니다.\n\n또는 프로젝트 루트에 `tests` 디렉토리를 만들고 그 안에 모든 테스트 파일을 분류하는 방법도 있습니다.\n\n## 오늘의 마무리\n\n여기까지 하면 일단 test 코드를 실행할 수는 있습니다. 코파일럿의 도움을 받아 테스트코들 작성하고, 코드를 읽는데까지는 성공했습니다(?)\n\n예를 들어 완성한 테스트 코드 하나는 다음과 같습니다. \n\n<details>\n<summary>envConfig.test.ts</summary>\n\n```python\nimport { EnvConfig } from '../envConfig';\n\ndescribe('EnvConfig', () => {\n    let envConfig: EnvConfig;\n    beforeAll(() => {\n        // 환경 변수 설정\n        process.env.BLOG_URL = 'https://example.com/';\n        process.env.SAVE_DIR = '/path/to/save';\n        process.env.SAVE_SUB_DIR = 'subdir';\n\n        // EnvConfig 인스턴스 생성\n        envConfig = EnvConfig.create();\n    });\n\n    afterAll(() => {\n        // 환경 변수 정리\n        delete process.env.BLOG_URL;\n        delete process.env.SAVE_DIR;\n        delete process.env.SAVE_SUB_DIR;\n    });\n\n    it('should have the correct notionKey value', () => {\n        expect(envConfig.notionKey).toEqual(process.env.NOTION_KEY || '');\n    });\n\n    it('should have the correct databaseid value', () => {\n        expect(envConfig.databaseid).toEqual(\n            process.env.NOTION_DATABASE_ID || '',\n        );\n    });\n\n    it('should correctly handle trailing slashes', () => {\n        expect(envConfig.blogUrl).toEqual('https://example.com'); // 끝 슬래시 제거 확인\n        expect(envConfig.saveDir).toEqual('/path/to/save/'); // 끝에 슬래시 추가 확인\n        expect(envConfig.saveSubDir).toEqual('subdir/'); // 끝에 슬래시 추가 확인\n    });\n});\n```\n\n\n</details>\n\n테스트코드를 작성하면서 느낀점은, 역시 테스트코드는 사전에 짜는게 의미가 있는 것 같습니다. 또 역시 문법이 상당히 어색합니다. 추가적인 노력이 많이 필요하다고 느끼고, 가능하면 다음에는 프로젝트 처음부터 테스트 코드를 먼저 짜보려고 합니다. \n\n오히려 좀 더 필요성을 느끼게 되었습니다. 살짝 고생했지만 좋은 시간이었습니다. \n\n"},{"excerpt":"목적 깃허브 액션에서 Job들은 서로 다른 독립된 환경을 가집니다. 각 작업은 독립적으로 실행되며, 각기 다른 러너 환경에서 다른 버전의 도구를 사용할 수 있습니다. 이것은 아주 강점이지만, 때로 다른 Job끼리 변수를 공유해야 하는 상황이 있을 수 있습니다.  이때 사용하는 것이 Output 입니다.  선언하기 기본적인 선언 방법은  의 꼴입니다. 예전…","fields":{"slug":"/Github-Action-Output/"},"frontmatter":{"date":"February 04, 2024","title":"Github Action Output","tags":["Github Actions"]},"rawMarkdownBody":"## 목적\n\n[깃허브 액션](https://sharknia.github.io/Github-Actions)에서 Job들은 서로 다른 독립된 환경을 가집니다. 각 작업은 독립적으로 실행되며, 각기 다른 러너 환경에서 다른 버전의 도구를 사용할 수 있습니다. 이것은 아주 강점이지만, 때로 다른 Job끼리 변수를 공유해야 하는 상황이 있을 수 있습니다. \n\n이때 사용하는 것이 Output 입니다. \n\n## 선언하기\n\n```yaml\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    outputs: # 다른 job에서 사용하기 위해 선언\n      output1: ${{ steps.step1.outputs.test }}\n      output2: ${{ steps.step2.outputs.test }}\n    steps:\n      - id: step1 # 해당 output을 특정하기 위한 id 선언\n        run: echo \"test=hello\" >> \"$GITHUB_OUTPUT\"\n      - id: step2\n        run: echo \"test=world\" >> \"$GITHUB_OUTPUT\"\n```\n\n기본적인 선언 방법은 `echo \"변수=값\" >> \"$GITHUB_OUTPUT”` 의 꼴입니다. 예전에는 set_output을 사용했던 것 같은데, 이 방법도 지금 작동은 제대로 하지만 해당 코드로 실행을 해보면 보안상의 문제로 곧 depacreted 될 거라는 warning이 발생합니다. \n\n반드시 id를 지정해줘야 합니다. 해당 id를 사용해 나중에 원하는 변수를 참조할 수 있습니다. \n\noutput은 동일한 job이 아니라 job2에서 사용을 하고 싶을 경우에 선언해야 합니다. \n\n## 사용하기 - 동일한 job\n\n```yaml\n- name: use output same job\n        run: |\n          echo ${{ steps.step1.outputs.test}}\n          echo ${{ steps.step2.outputs.test}}\n```\n\n같은 job 안에서 사용할때에는 `${{ steps.”id”.outputs.”변수명”}}` 의 꼴로 사용하면 됩니다. \n\n## 사용하기 - 다른 job\n\n```yaml\n\n  job2:\n    runs-on: ubuntu-latest\n    needs: job1\n    steps:\n      - env:\n          OUTPUT1: ${{needs.job1.outputs.output1}}\n          OUTPUT2: ${{needs.job1.outputs.output2}}\n        run: echo \"$OUTPUT1 $OUTPUT2\"\n```\n\n다른 job에서 해당 변수를 사용할 경우에도 [needs](https://sharknia.github.io/Github-Actions-Job---needs) 종속성 선언을 해줘야 합니다. job1에서 선언한 output을 사용할 것이므로 needs 설정을 해주고 `${{ needs.”job-이름”.outputs.”output변수명”}}` 의 꼴로 참조할 수 있습니다. \n\n또한, 변수 명의 경우 대소문자는 구분하지 않습니다. \n\n## 참조\n\n[https://docs.github.com/ko/actions/using-jobs/defining-outputs-for-jobs#예-작업-출력-정의](https://docs.github.com/ko/actions/using-jobs/defining-outputs-for-jobs#예-작업-출력-정의)\n\n"},{"excerpt":"소개 Github Actions는 깃허브에서 직접 소프트웨어 개발 워크플로우를 자동화할 수 있는 기능입니다. 개발자는 이를 사용해서 소프트웨어 빌드, 테스트, 배포와 같은 과정을 자동화하여 특정 트리거가 발생했을 경우 자동으로 실행되는 워크플로우를 작성할 수 있습니다.  강점 및 특징 깃허브와의 통합 깃허브와 깊게 통합되어 있어 깃허브 리포지토리 내에서 …","fields":{"slug":"/Github-Actions/"},"frontmatter":{"date":"February 04, 2024","title":"Github Actions","tags":["Github Actions"]},"rawMarkdownBody":"## 소개\n\nGithub Actions는 깃허브에서 직접 소프트웨어 개발 워크플로우를 자동화할 수 있는 기능입니다. 개발자는 이를 사용해서 소프트웨어 빌드, 테스트, 배포와 같은 과정을 자동화하여 특정 트리거가 발생했을 경우 자동으로 실행되는 워크플로우를 작성할 수 있습니다. \n\n## 강점 및 특징\n\n### 깃허브와의 통합\n\n깃허브와 깊게 통합되어 있어 깃허브 리포지토리 내에서 직접 워크플로우를 관리하고 실행할 수 있습니다. 따라서 코드 변경, PR, 이슈 생성 등 깃허브 이벤트에 기반한 자동화를 간편하게 설정할 수 있습니다. \n\n### 언어와 프레임워크에 대한 광범위한 지원\n\n다양한 프레임워크와 프로그래밍 언어를 지원합니다.이로 인해 거의 모든 소프트웨어 프로젝트에 적용할 수 있는 유연성을 제공합니다. \n\n### 재사용 가능한 컴포넌트\n\n마켓플레이스에서 다른 개발자가 만든 액션을 재사용하거나 자신의 커스텀 액션을 만들어 공유할 수 있습니다. \n\n### 유연한 트리거 옵션\n\n다양한 깃허브 이벤트에 대한 트리거 옵션을 제공합니다. 또한 cron 기반의 스케쥴을 통해서도 실행이 가능합니다. 이를 이용해 자유롭게 필요에 따라 자동화 작업을 실행할 수 있습니다. \n\n## Github Actions의 이해\n\n### Workflow\n\n워크플로우는 하나 이상의 작업을 실행하는 자동화된 프로세스입니다. 워크플로우는 리포지토리의 `.github/workflows` 디렉토리 아래 YAML 파일에 의해 정의되며 리포지토리의 이벤트 또는 스케쥴러에 따라 실행됩니다. \n\n### Jobs\n\n크게 뭉뚱그려서 이야기하면 워크플로우는 결국 Job들의 집합입니다.\n\n Job들은 서로 다른 독립된 환경을 가집니다. 각 작업은 독립적으로 실행되며, 각기 다른 러너 환경에서 다른 버전의 도구를 사용할 수 있습니다. 이 방법을 사용하면, 하나의 워크플로우 안에서 서로 다른 환경, 서로 다른 패키지 세트를 사용하여 작업을 수행할 수 있습니다. \n\n#### 여러 Job의 선언\n\n예를 들어, 첫번째 잡과 두번째 잡이 각각 다른 node 버전을 사용해야 하는 경우 다음과 같이 구성할 수 있습니다. \n\n```yaml\nname: job1-job2\n\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Use Node.js (Version for Build)\n      uses: actions/setup-node@v2\n      with:\n        node-version: '12' # 첫 번째 작업에 필요한 노드 버전\n\n    - name: Install Dependencies for Build\n      run: npm install\n\n    - name: Build\n      run: npm run build\n\n    - name: Additional Build Steps\n      run: # 필요한 추가 빌드 스텝\n\n  job2:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Use Node.js (Version for Deploy)\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18' # 두 번째 작업에 필요한 노드 버전\n\n    - name: Install Dependencies for Build\n      run: npm install\n\n    - name: Build\n      run: npm run build\n\n    - name: Additional Build Steps\n      run: # 필요한 추가 빌드 스텝\n\n```\n\n### Step\n\nJob은 Step들의 집합입니다. Step은 실행될 셀 스크립트 또는 실행될 action들이이며, Step들은 정의된 순서대로 실행됩니다.\n\n각 Step은 동일한 runner에서 실행되므로(Job이 가진 환경을 공유하므로) step 마다 데이터를 공유할 수 있다. 즉 한 job 안에서 빌드하는 step, 이를 테스트 하는 step을 한 번에 가질 수 있습니다.\n\n\n\n## 참조\n\n[https://docs.github.com/ko/actions/learn-github-actions/understanding-github-actions](https://docs.github.com/ko/actions/learn-github-actions/understanding-github-actions)\n\n\n\n"},{"excerpt":"개요 기본적으로 아무 설정이 없다면 각 Job들은 병렬적으로 실행됩니다.  예를 들어 job1과 job2를 선언한 경우, 이 두 job은 워크플로우 실행 시 동시에 시작이 됩니다.  이 때,  를 사용해 Job들 사이에 종속성을 주어 Job 하나가 끝나야 다음 Job이 실행되도록 설정할 수 있습니다. 예제 순차적으로 실행하기 다음과 같이 needs에 다른…","fields":{"slug":"/Github-Actions-Job---needs/"},"frontmatter":{"date":"February 04, 2024","title":"Github Actions Job - needs","tags":["Github Actions"]},"rawMarkdownBody":"## 개요\n\n기본적으로 아무 설정이 없다면 각 Job들은 병렬적으로 실행됩니다.  예를 들어 job1과 job2를 선언한 경우, 이 두 job은 워크플로우 실행 시 동시에 시작이 됩니다. \n\n이 때, `needs` 를 사용해 Job들 사이에 종속성을 주어 Job 하나가 끝나야 다음 Job이 실행되도록 설정할 수 있습니다.\n\n## 예제\n\n### 순차적으로 실행하기\n\n다음과 같이 needs에 다른 job의 이름을 지정해주면, 해당 job이 끝나야 이 job이 실행되도록 할 수 있습니다. \n\n```yaml\nname: job1-job2\n\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    steps:\n\t\t\t# 첫번째 작업 실행...\n  job2:\n\t\tneeds: job1 # 'job1' 작업이 성공적으로 완료된 후에 실행\n    runs-on: ubuntu-latest\n    steps:\n\t\t\t# 두번째 작업 실행...\n```\n\n이 경우에는 job1이 실패할 경우, job2의 작업도 건너뛰게 됩니다. \n\n### 여러개의 needs 선언하기\n\n```yaml\njobs:\n  job1:\n  job2:\n    needs: job1\n  job3:\n    needs: [job1, job2]\n```\n\n위와 같이 선언되어있다고 할 때에, job1이 반드시 성공해야 job2가 실행이 되며, job1, job2가 모두 성공적으로 완료되어야 job3이 실행됩니다. \n\n### 성공-실패 여부 상관없이 순차적으로 실행하기\n\njob의 실행에도 조건문을 걸 수 있습니다. \n\n```yaml\njobs:\n  job1:\n  job2:\n    needs: job1\n  job3:\n    if: ${{ always() }}\n    needs: [job1, job2]\n```\n\n예를 들어 이 조건문은 job1, job2의 성공 여부와는 상관없이 단지 순차적으로 작업을 실행합니다. 즉, job2가 실패하더라도 job3은 job1, job2 완료 이후에 실행됩니다. \n\n## 참조\n\n[https://docs.github.com/ko/actions/using-jobs/using-jobs-in-a-workflow](https://docs.github.com/ko/actions/using-jobs/using-jobs-in-a-workflow)\n\n\n\n"},{"excerpt":"왜 필요한가? 깃허브 액션 워크플로우를 로컬에서 테스트하기가 쉽지 않습니다. 결국 깃허브 콘솔 또는 깃허브 CLI를 이용해서 테스트 하게 되는 경우가 대부분입니다.  단순히 Re-Run을 실행하면, 레포에 변경된 내용이 반영되지 않고 그 당시 환경에 대해서만 재실행이 되어 정상적인 테스트가 불가능합니다.  결국 트리거에 Push 조건을 걸어놓고, 변경 사…","fields":{"slug":"/Github-Actions-Workflow-수동으로-실행하기/"},"frontmatter":{"date":"February 04, 2024","title":"Github Actions Workflow 수동으로 실행하기","tags":["Github Actions"]},"rawMarkdownBody":"## 왜 필요한가?\n\n깃허브 액션 워크플로우를 로컬에서 테스트하기가 쉽지 않습니다. 결국 깃허브 콘솔 또는 깃허브 CLI를 이용해서 테스트 하게 되는 경우가 대부분입니다. \n\n단순히 Re-Run을 실행하면, 레포에 변경된 내용이 반영되지 않고 그 당시 환경에 대해서만 재실행이 되어 정상적인 테스트가 불가능합니다. \n\n결국 트리거에 Push 조건을 걸어놓고, 변경 사항을 만들어서 Push를 해서 워크플로우를 테스트 하고 있었습니다. \n\n그런데, 수동으로 깃허브 웹 콘솔에서 수동으로 워크플로우를 테스트 할 수 있게 해주는 방법이 있습니다. \n\n## 워크플로우 파일 수정\n\n트리거에 `workflow_dispatch` 이벤트를 추가해줍니다. \n\n```yaml\non:\n    workflow_dispatch:\n```\n\n## 수동으로 워크플로우 수동으로 실행\n\n위의 이벤트를 트리거에 추가해주고, 웹 콘솔에서 다음의 화면으로 이동합니다. \n\n![](image1.png)\nActions 탭 - 수정된 워크플로우로 이동하면 Run Workflow 버튼이 생긴 것을 볼 수 있습니다. \n\n특이사항으로, 혹시 워크플로우를 비활성화 한다면 해당 버튼도 활성화되지 않습니다. \n\n## 참조\n\n[https://docs.github.com/ko/actions/using-workflows/manually-running-a-workflow?tool=webui](https://docs.github.com/ko/actions/using-workflows/manually-running-a-workflow?tool=webui)\n\n"},{"excerpt":"요약 짬짬이 틈을 내서 하는 작업이다 보니 문서화 작업을 제대로 못했습니다. 짜잘짜잘한 오류 수정이 있었으며, 비교적 큰 수정도 있었고 완성이 됐다고 판단해 버전을 정의하고 릴리즈도 해봤습니다.  버그 수정 코드 블록의 들여쓰기가 제대로 적용되지 않는 문제가 수정되었습니다.    스타일 내에서 언더바가 제대로 인식되지 않는 문제가 수정되었습니다.  타이틀…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅11/"},"frontmatter":{"date":"February 04, 2024","title":"NotionAPI를 활용한 자동 포스팅(11)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"## 요약\n\n짬짬이 틈을 내서 하는 작업이다 보니 문서화 작업을 제대로 못했습니다. 짜잘짜잘한 오류 수정이 있었으며, 비교적 큰 수정도 있었고 완성이 됐다고 판단해 버전을 정의하고 릴리즈도 해봤습니다. \n\n## 버그 수정\n\n- 코드 블록의 들여쓰기가 제대로 적용되지 않는 문제가 수정되었습니다. \n\n- `코드`  스타일 내에서 언더바가 제대로 인식되지 않는 문제가 수정되었습니다. \n\n- 타이틀에 스타일이 포함된 경우(노션에서 확인하기 어려움) 타이틀이 짤리던 문제가 수정되었습니다. \n\n- 기타 일부 스타일이 적용되지 않던 문제가 해결됐습니다. \n\n- API 호출 시 타임아웃이 될 경우 대책없이 프로그램이 중단되던 문제가 해결됐습니다. \n\n## 업데이트\n\n- 이제 토글 블록이 지원됩니다.\n\n- 블로그 자동 배포가 워크플로우에 포함되었습니다. 이를 위해 깃허브 계정을 미리 설정할 수 있게 되었습니다. \n\n- 메타데이터 초기화 기능이 추가되었습니다. 이제 새로운 계정으로 세팅 시 메타데이터 파일이 초기화됩니다.\n\n- 코드 리팩토링의 일부로 일부 코드의 디렉토리가 분리되었습니다. \n\n- `v1.0.0-beta`가 릴리즈 되었습니다. Readme가 완성되었습니다. \n\n- 프로젝트 이름이 `nolog`로 확정되었습니다.\n\n- 라이센스가 명시되었습니다. \n\n## 타임아웃 문제의 해결\n\n기타 다른 버그는 짜잘짜잘한 수정이었지만 타임아웃 문제 해결은 나름대로 머리를 써서 구현했으므로 해당 과정을 별도로 기록합니다. \n\n### 문제 파악\n\n노션 API를 사용하고 있고, 해당 API를 사용하기 위해서 notionClient 라이브러리를 사용하고 있습니다. 그리고 주로 데이터베이스 정보를 읽어올 때, 페이지 안의 내용을 읽어올 때, 블록 안의 내용을 읽어올 때 해당 라이브러리를 사용하여 통신합니다. \n\n하지만 가끔 가다 간헐적으로 규칙성 없이 timeout이 발생하는 문제가 있었습니다. 이를 해결하기 위해 Notion Client 라이브러리를 상속한 NotionClientWithRetry 클래스를 생성하고, 세가지 메소드에 대해 재시도 로직을 하는 공통 메소드를 생성하고 코드상에서 Client 직접 호출 대신 해당 메소드를 이용해 Client를 호출하도록 수정해주었습니다. \n\n<details>\n<summary>NotionClientWithRetry.ts</summary>\n\n```typescript\nimport { Client } from '@notionhq/client';\n\nexport class NotionClientWithRetry extends Client {\n    private maxRetries: number;\n    private retryDelay: number;\n\n    constructor(options: {\n        auth: string;\n        maxRetries?: number;\n        retryDelay?: number;\n    }) {\n        super({ auth: options.auth });\n        this.maxRetries = options.maxRetries || 3;\n        this.retryDelay = options.retryDelay || 1000; // 기본 1초 대기\n    }\n\n    private async retryAPI<T>(\n        operation: () => Promise<T>,\n        retries: number = this.maxRetries,\n    ): Promise<T> {\n        try {\n            return await operation();\n        } catch (error) {\n            if (retries <= 0) throw error;\n            console.log(\n                `[retryAPI] ${this.maxRetries - retries} retrying.....`,\n            );\n            await new Promise((resolve) =>\n                setTimeout(resolve, this.retryDelay),\n            );\n            return this.retryAPI(operation, retries - 1);\n        }\n    }\n\n    public async blocksRetrieve(args: { block_id: string }) {\n        return this.retryAPI(() => this.blocks.retrieve(args));\n    }\n\n    public async databasesQuery(args: {\n        database_id: string;\n        filter?: any;\n        sorts?: any;\n    }) {\n        return this.retryAPI(() => this.databases.query(args));\n    }\n\n    public async pagesRetrieve(args: { page_id: string }) {\n        return this.retryAPI(() => this.pages.retrieve(args));\n    }\n}\n```\n\n\n</details>\n\n따로 사용자가 설정할 수 있지만 기본적으로는 1초에 한 번씩 최대 3번까지 재시도를 하도록 해주었습니다. \n\n해당 코드를 적용한 이후로는 API 호출에 실패해서 프로그램이 중단되는 일은 겪지 못했습니다. \n\n## 블로그 자동 배포가 포함된 워크플로우 작성\n\n블로그 자동 배포 워크플로우는 사실 [github.io](http://github.io/) 쪽 레포에 이미 구현이 되어 있어 추가 구현이 필요 없었으며, 깃허브 블로그 테마마다 배포 방식이 다를 수 있어 사실 꼭 구현이 필수인 영역은 아니었습니다. \n\n하지만 깃허브 액션에 대해 이해도가 깊어질 수 있고, 또 프로젝트의 목적 상 가능하면 하나로 합쳐지는 예시를 제공하는 것도 좋아보여 공부 겸 진행했습니다. \n\n\n\n"},{"excerpt":"타입스크립트에서  을 사용해 특정 값들의 집합을 정의하고 각 멤버에 문자열 값을 할당할 수 있다. 이는 열거형이라고 하며 관련된 상수 값들의 집합에 이름을 부여하여 코드의 가독성을 높이고 오류 가능성을 줄여준다.  임의의 열거형을 정의하면 다음과 같다.  이렇게 정의하면 PageStatus 타입의 변수를 사용할 때 PageStatus.Deleted 로 사…","fields":{"slug":"/Typescript의-열거형/"},"frontmatter":{"date":"February 03, 2024","title":"Typescript의 열거형","tags":["Typescript"]},"rawMarkdownBody":"타입스크립트에서 `enum` 을 사용해 특정 값들의 집합을 정의하고 각 멤버에 문자열 값을 할당할 수 있다. 이는 열거형이라고 하며 관련된 상수 값들의 집합에 이름을 부여하여 코드의 가독성을 높이고 오류 가능성을 줄여준다. \n\n임의의 열거형을 정의하면 다음과 같다. \n\n```typescript\nenum PageStatus {\n    Deleted = \"deleted\",\n    Writing = \"writing\"\n}\n```\n\n이렇게 정의하면 PageStatus 타입의 변수를 사용할 때 PageStatus.Deleted 로 사용해서 열거형 멤버게 접근할 수 있다. \n\n이렇게 얻어진 값은 각각에 할당된 문자열과 같다. \n\n사용법 예시 코드는 다음과 같다. \n\n```typescript\nlet currentPageStatus: PageStatus = PageStatus.Writing;\n\nif (currentPageStatus === PageStatus.Deleted) {\n    console.log(\"The page is deleted.\");\n} else if (currentPageStatus === PageStatus.Writing) {\n    console.log(\"The page is in writing status.\");\n}\n```\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n만약, 열거형을 정의할 때 값을 명시하지 않으면 열거형의 값은 기본적으로 0에서 시작해 순차적으로 증가한다.  예를 들어 다음과 같다. \n\n```typescript\nenum PageStatus {\n    Deleted, // 0\n    Writing  // 1\n}\n```\n\n물론 임의로 숫자를 지정할 수도 있다. \n\n```typescript\nenum StatusCode {\n    Success = 200,\n    NotFound = 404,\n    ServerError = 500\n}\n```\n\n또한 숫자 열거형에서 일부 멤버에만 값을 지정하고 다른 멤버에는 값을 지정하지 않는다면 타입스크립트는 자동으로 이전 멤버의 값에서 1을 더해 계산한다. \n\n```typescript\nenum Example {\n    Start = 1,\n    Middle, // 값이 2로 자동 할당됩니다.\n    End // 값이 3으로 자동 할당됩니다.\n}\n```\n\n\n\n"},{"excerpt":"메타데이터 구조 확정 지난시간, 메타데이터를 관리하기 위한 클래스 초안을 작성했다. 시간이 없어서 급하게 만들었는데, 제대로 매개변수 이름을 지어주고 메타데이터 구조를 확정지었다.  크게 변하지는 않았고, 이름을 정해주는 수준이었다.  그리고 추가로 저장된 디렉토리 명을 사용해 포스팅된 마크다운 디렉토리를 삭제하는 메소드를 추가해주었다.  Posting …","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅10/"},"frontmatter":{"date":"February 03, 2024","title":"NotionAPI를 활용한 자동 포스팅(10)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"## 메타데이터 구조 확정\n\n지난시간, [메타데이터를 관리하기 위한 클래스 초안](https://sharknia.github.io/NotionAPI를-활용한-자동-포스팅9)을 작성했다. 시간이 없어서 급하게 만들었는데, 제대로 매개변수 이름을 지어주고 메타데이터 구조를 확정지었다. \n\n크게 변하지는 않았고, 이름을 정해주는 수준이었다. \n\n그리고 추가로 저장된 디렉토리 명을 사용해 포스팅된 마크다운 디렉토리를 삭제하는 메소드를 추가해주었다. \n\n<details>\n<summary>수정된 코드 전문</summary>\n\n```typescript\nimport { promises as fs } from 'fs';\nimport { join } from 'path';\nimport { EnvConfig } from './envConfig';\n\nconst METADATA_FILE_PATH = './pageMetadata.json';\n\ninterface PageMetadata {\n    path: string;\n}\n\ninterface Metadata {\n    [pageIdx: string]: PageMetadata;\n}\n\nexport class MetadataManager {\n    private static instance: MetadataManager;\n    private metadata: Metadata | null;\n    private envConfig: EnvConfig;\n\n    private constructor() {\n        this.metadata = null;\n        this.envConfig = EnvConfig.create();\n    }\n\n    /**\n     * 인스턴스를 반환하는 메서드입니다.\n     * @returns {MetadataManager} MetadataManager 인스턴스\n     */\n    public static async getInstance(): Promise<MetadataManager> {\n        if (!this.instance) {\n            this.instance = new MetadataManager();\n            await this.instance.loadMetadata();\n        }\n        return this.instance;\n    }\n\n    /**\n     * 메타데이터를 로드합니다.\n     * @returns {Promise<void>} Promise 객체\n     */\n    public async loadMetadata(): Promise<void> {\n        try {\n            const data = await fs.readFile(METADATA_FILE_PATH, 'utf8');\n            this.metadata = JSON.parse(data) as Metadata;\n            console.log('메타데이터 파일 읽기 성공:', this.metadata);\n        } catch (error) {\n            console.error('메타데이터 파일 읽기 오류:', error);\n            this.metadata = {};\n        }\n    }\n\n    /**\n     * 메타데이터를 반환합니다.\n     * @returns 메타데이터 객체 또는 null\n     */\n    public getMetadata(): Metadata | null {\n        return this.metadata;\n    }\n\n    /**\n     * 페이지 메타데이터를 업데이트합니다.\n     *\n     * @param pageIdx 페이지 식별자\n     * @param pageData 페이지 메타데이터\n     */\n    public updatePageMetadata(pageIdx: string, pageData: PageMetadata): void {\n        if (!this.metadata) {\n            this.metadata = {};\n        }\n        this.metadata[pageIdx] = pageData;\n        console.log(`메타 데이터 업데이트 [${pageIdx}]`);\n    }\n\n    /**\n     * 페이지 메타데이터를 삭제합니다.\n     * @param pageIdx 삭제할 페이지의 ID\n     */\n    public deletePageMetadata(pageIdx: string): void {\n        if (this.metadata && this.metadata[pageIdx]) {\n            delete this.metadata[pageIdx];\n        }\n    }\n\n    /**\n     * 메타데이터를 파일에 저장합니다.\n     * @returns 메타데이터가 성공적으로 저장될 때 해결되는 Promise입니다.\n     */\n    public async saveMetadata(): Promise<void> {\n        if (this.metadata) {\n            try {\n                await fs.writeFile(\n                    METADATA_FILE_PATH,\n                    JSON.stringify(this.metadata, null, 2),\n                );\n                console;\n            } catch (error) {\n                console.error('메타데이터 파일 저장 오류:', error);\n            }\n        }\n    }\n\n    /**\n     * 지정된 페이지 인덱스에 대한 메타데이터를 삭제합니다.\n     * @param pageIdx 삭제할 페이지 인덱스\n     * @returns 삭제 작업이 완료된 후에는 아무 값도 반환하지 않습니다.\n     */\n    public async deleteFromMetadata(pageIdx: string): Promise<void> {\n        if (this.metadata && this.metadata[pageIdx]) {\n            let dir = join(\n                this.envConfig.saveDir!,\n                this.metadata[pageIdx].path,\n            );\n            try {\n                await fs.unlink(dir);\n                console.log('파일 삭제 성공:', dir);\n            } catch (error) {\n                console.error('파일 삭제 오류:', error);\n            }\n        }\n    }\n}\n```\n\n\n</details>\n\n## Posting 클래스 수정\n\n메타데이터 정보는 모아두었다가, 프로그램 종료시에 한 번에 파일에 저장하도록 종료되는 부분에서 saveMetadata 메소드를 호출해주었다. \n\n<details>\n<summary>수정된 Posting 클래스 코드</summary>\n\n```typescript\npublic async start(): Promise<void> {\n        console.log('[posting.ts] start!');\n        try {\n            this.metadataManager = await MetadataManager.getInstance();\n            this.EnvConfig = EnvConfig.create();\n            const notionkey: string = this.EnvConfig.notionKey || '';\n            const databaseid: string = this.EnvConfig.databaseid || '';\n            this.notionApi = await NotionAPI.create(notionkey);\n            this.dbInstance = await DataBase.create(databaseid, '');\n\n            console.log('[posting.ts] page 순회 시작');\n            for (const item of this.dbInstance.pageIds) {\n                const page: Page = await Page.create(item.pageId);\n            }\n            this.metadataManager.saveMetadata();\n        } catch (error) {\n            console.error('Error creating database instance:', error);\n        }\n    }\n```\n\n\n\n\n</details>\n\n## Database 클래스 수정\n\n기존에는 상태값이 Ready 인 것만 쿼리하고 있었는데, 이제는 삭제도 진행해야 하므로 상태가 ToBeDeleted인 것도 쿼리하도록 수정해주었다. \n\n<details>\n<summary>수정된 코드</summary>\n\n```typescript\npublic async queryDatabase(): Promise<QueryDatabaseResponse> {\n        try {\n            const response: QueryDatabaseResponse =\n                await this.notion.databasesQuery({\n                    database_id: this.databaseId,\n                    filter: {\n                        or: [\n                            {\n                                property: '상태',\n                                select: {\n                                    equals: PageStatus.Ready,\n                                },\n                            },\n                            {\n                                property: '상태',\n                                select: {\n                                    equals: PageStatus.ToBeDeleted,\n                                },\n                            },\n                        ],\n                    },\n                });\n            // pageId 리스트 업데이트\n            this.pageIds = response.results.map((page) => ({\n                pageId: page.id,\n            }));\n            return response;\n        } catch (error) {\n            console.error('Error querying the database:', error);\n            throw error;\n        }\n    }\n```\n\n\n</details>\n\n## Page 클래스 수정\n\n### init() 메소드 수정\n\n- pageIdx 속성을 추가하고 init() 메소드에서 해당 속성을 초기화해준다.\n\n### create() 메소드 수정\n\n- create 메소드에서 상태값에 따른 분기처리를 추가했다. \n\n    기존에는 ready상태만 있었으므로 일관된 처리를 진행하면 됐지만, 이제는 삭제 대기 상태가 추가되었으므로 상태값에 따른 분기처리를 추가하고 각각 다르게 작동하도록 해주었다. \n\n    1. Ready, ToBeDeleted 두 상태 모두 일단 해당하는 파일을 삭제한다. Ready 인데도 삭제하는 이유는 타이틀이 바뀐 경우에는 파일을 삭제해야 하기 때문이다. \n\n    1. Ready 상태인 경우에는 기존의 로직을 실행하고, Metadata를 업데이트 해준다. \n\n    1. ToBeDeleted 상태인 경우에는 메타데이터에서 해당하는 내용을 삭제한다. \n\n<details>\n<summary>create() 메소드 안쪽 수정된 내용 코드</summary>\n\n    ```typescript\n    public static async create(pageId: string) {\n            const notionApi: NotionAPI = await NotionAPI.create();\n            const page: Page = new Page(pageId, notionApi.client);\n            MarkdownConverter.imageCounter = 0;\n            await page.init(page);\n            const status = page.properties!['상태'];\n            console.log(\n                `[page.ts] start - pageTitle : (${status})${page.pageTitle}`,\n            );\n            // 저장하기 전에도 기존 파일을 삭제한다. 타이틀이 달라진 update 일 수 있기 때문이다.\n            await page.metadataManager?.deleteFromMetadata(page.pageIdx!);\n            if (status === PageStatus.ToBeDeleted) {\n                // 페이지가 삭제될 예정인 경우\n                await page.metadataManager?.deletePageMetadata(page.pageIdx!);\n                await page.updatePageStatus(PageStatus.Deleted);\n                return page;\n            } else if (status === PageStatus.Ready) {\n                // 포스팅이 준비된 경우\n                page.contentMarkdown = await page.fetchAndProcessBlocks();\n                await page.printMarkDown();\n                await page.metadataManager?.updatePageMetadata(page.pageIdx!, {\n                    path: page.pageUrl!,\n                });\n                await page.updatePageStatus(PageStatus.Updated);\n                return page;\n            } else {\n                console.error(`[page.ts] start - status : ${status}`);\n                throw new Error(`[page.ts] start - status : ${status}`);\n            }\n        }\n    ```\n\n\n</details>\n\n## 기타 변경 내용\n\n- 열거형 클래스를 새로 만들어 상태값 정의를 미리 해주었다. \n\n- 또한, 메타데이터 파일도 레포의 일부인데 깃허브 액션 워크플로우 작동 후 이 파일도 수정될 것이므로 한 번 더 커밋/푸시가 필요하다. 해당 내용을 워크플로우의 Run Script 바로 다음에 추가해주었다. \n\n<details>\n<summary>추가된 워크플로우</summary>\n\n    ```yaml\n    - name: Commit and Push Changes to Current Repository\n          run: |\n            git config --global user.name 'name'\n            git config --global user.email 'mail'\n            git add .\n            git commit -m \"Update contents\" || echo \"No changes to commit in current repo\"\n            git push\n    ```\n\n\n</details>\n\n## 주의할 점\n\n일단, 로컬에서 실행 시 권한 문제로 파일 삭제가 제대로 되지 않았다. 그래서 깃허브 액션에서도 파일 권한 문제가 생길 우려가 있어 권한을 조정하는 방법이 있나 찾아보았는데, 깃허브 액션에서는 외부를 컨트롤 하려는게 아닌 이상 별도의 권한 문제가 발생하지 않는다고 한다. \n\n따라서 로컬에서 발생하는 문제는 따로 수정하지 않았다. \n\n\n\n"},{"excerpt":"지난시간 지난시간, 신나게 자동 배포를 만들었다.  이제 새로 생성된 문서들에 대해서 자동으로 한시간마다 배포가 되어 글이 포스팅된다.  근데, 이런 경우 글을 어떻게 삭제하지? 현재 타이틀이 키 값인데 타이틀 명이 바뀌면 내용은 똑같은데 제목만 다른 글이 두 개?  그렇다, 삭제 및 수정 기능이 구현되어야 하는 것이다.  설계 어떤걸 키 값으로 해야 하…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅9/"},"frontmatter":{"date":"February 02, 2024","title":"NotionAPI를 활용한 자동 포스팅(9)","tags":["Blogging","Typescript","Hobby","Notion-API"]},"rawMarkdownBody":"## 지난시간\n\n지난시간, 신나게 자동 배포를 만들었다. \n\n이제 새로 생성된 문서들에 대해서 자동으로 한시간마다 배포가 되어 글이 포스팅된다. \n\n\n\n**근데, 이런 경우 글을 어떻게 삭제하지?**\n\n**현재 타이틀이 키 값인데 타이틀 명이 바뀌면 내용은 똑같은데 제목만 다른 글이 두 개?** \n\n\n\n그렇다, 삭제 및 수정 기능이 구현되어야 하는 것이다. \n\n## 설계\n\n어떤걸 키 값으로 해야 하나 고민이 많았는데, 답은 간단했다. 노션에 ID를 추가하는 기능이 있었다. 이렇게 되면, 내 노션 블로그를 위한 필수 속성은 타이틀, 상태, ID 세 가지가 된다. 이런게 너무 많이 생기는걸 원하지 않지만 어쩔 수 없는 부분이라고 생각이 든다. \n\n계획은 다음과 같다. \n\n- 노션 글들에 ID 속성을 추가해 키값으로 삼는다. \n\n- 상태값에 ToBeDeleted, Deleted를 추가한다. \n\n- 키 값 - 저장된 디렉토리가 매핑된 메타데이터 json 파일을 생성한다. \n\n- ToBeDeleted를 쿼리해와서, 매핑 테이블을 참조해 데이터를 삭제 후 매핑 정보도 삭제하고 Deleted 상태로 바꿔준다. \n\n- Ready를 쿼리해와서 매핑 테이블을 참조해 기존 데이터를 삭제 후 매핑 정보를 갱신하고 마크다운을 다시 저장한다. \n\n이렇게 하면 수정과 삭제에 모두 대응이 가능해보인다!\n\n## 구현\n\n우선, 전역적으로 메타데이터를 읽어오고, 메모리에 데이터를 저장한 다음 프로그램 종료 직전에 메타 데이터를 한 번에 수정해주는 역할을 할 클래스를 생성했다. \n\n<details>\n<summary>metadataManager.ts</summary>\n\n```typescript\nimport { promises as fs } from 'fs';\n\nconst METADATA_FILE_PATH = './pageMetadata.json';\n\ninterface PageMetadata {\n    url: string;\n}\n\ninterface Metadata {\n    [pageId: string]: PageMetadata;\n}\n\nexport class MetadataManager {\n    private static instance: MetadataManager;\n    private metadata: Metadata | null;\n\n    private constructor() {\n        this.metadata = null;\n    }\n\n    /**\n     * 인스턴스를 반환하는 메서드입니다.\n     * @returns {MetadataManager} MetadataManager 인스턴스\n     */\n    public static getInstance(): MetadataManager {\n        if (!this.instance) {\n            this.instance = new MetadataManager();\n        }\n        return this.instance;\n    }\n\n    /**\n     * 메타데이터를 로드합니다.\n     * @returns {Promise<void>} Promise 객체\n     */\n    public async loadMetadata(): Promise<void> {\n        try {\n            const data = await fs.readFile(METADATA_FILE_PATH, 'utf8');\n            this.metadata = JSON.parse(data) as Metadata;\n        } catch (error) {\n            console.error('메타데이터 파일 읽기 오류:', error);\n            this.metadata = {};\n        }\n    }\n\n    /**\n     * 메타데이터를 반환합니다.\n     * @returns 메타데이터 객체 또는 null\n     */\n    public getMetadata(): Metadata | null {\n        return this.metadata;\n    }\n\n    /**\n     * 페이지 메타데이터를 업데이트합니다.\n     *\n     * @param pageId 페이지 식별자\n     * @param pageData 페이지 메타데이터\n     */\n    public updatePageMetadata(pageId: string, pageData: PageMetadata): void {\n        if (!this.metadata) {\n            this.metadata = {};\n        }\n        this.metadata[pageId] = pageData;\n    }\n\n    /**\n     * 페이지 메타데이터를 삭제합니다.\n     * @param pageId 삭제할 페이지의 ID\n     */\n    public deletePageMetadata(pageId: string): void {\n        if (this.metadata && this.metadata[pageId]) {\n            delete this.metadata[pageId];\n        }\n    }\n\n    /**\n     * 메타데이터를 파일에 저장합니다.\n     * @returns 메타데이터가 성공적으로 저장될 때 해결되는 Promise입니다.\n     */\n    public async saveMetadata(): Promise<void> {\n        if (this.metadata) {\n            try {\n                await fs.writeFile(\n                    METADATA_FILE_PATH,\n                    JSON.stringify(this.metadata, null, 2),\n                );\n            } catch (error) {\n                console.error('메타데이터 파일 저장 오류:', error);\n            }\n        }\n    }\n}\n```\n\n\n</details>\n\n이 클래스는 프로그램 시작과 동시에 초기화되며, 끝날 때 메타데이터 파일을 갱신하는 역할을 할 것이다. \n\n정확한 매개변수는 아직 정하지 않았으며, 초안만 짜뒀다. \n\n이제 내일, 이 클래스를 명확히 정의하고 이 클래스를 사용해 수정/삭제를 구현하면 된다. \n\n"},{"excerpt":"목표 단계적으로 GitHub Action을 사용해 블로그 자동 배포를 하려고 한다.  일단, 구상한 깃허브 액션에서 실행할 플로우는 다음과 같다.  구상 블로그 레포지토리를 받아온다.  내 프로그램을 돌려 블로그 레포지토리를 업데이트한다.  블로그 레포지토리를 커밋/푸시한다.  해당 작업은 특정 시간에, 또는 특정 시간마다 이뤄져야 한다.  구현 Noti…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅8/"},"frontmatter":{"date":"February 01, 2024","title":"NotionAPI를 활용한 자동 포스팅(8)","tags":["Notion-API","Blogging","Hobby","GitHub","Typescript"]},"rawMarkdownBody":"## 목표\n\n단계적으로 GitHub Action을 사용해 블로그 자동 배포를 하려고 한다. \n\n일단, 구상한 깃허브 액션에서 실행할 플로우는 다음과 같다. \n\n## 구상\n\n1. 블로그 레포지토리를 받아온다. \n\n1. 내 프로그램을 돌려 블로그 레포지토리를 업데이트한다. \n\n1. 블로그 레포지토리를 커밋/푸시한다. \n\n1. 해당 작업은 특정 시간에, 또는 특정 시간마다 이뤄져야 한다. \n\n## 구현\n\n### Notion to Markdown 프로그램 실행\n\n#### 워크플로우 파일 생성\n\n단계적으로 구현해보려고 한다. 일단은 내 프로그램을 돌려보자. \n\n일단 대충 다음과 같이 초안을 짜주었다. \n\n```yaml\n# 이름을 정의한다. \nname: Run TypeScript\n# 실행 시기입니다. 이번엔 push, pull_request 될 때 이다. \non: [push, pull_request]\n\n# 깃허브 액션에서 실행할 작업을 정의하는 섹션이다.\njobs:\n\t# 단일 작업의 이름을 정의합니다. 이 경우에는 이름이 run 이다.\n  run:\n\t\t# 작업이 어떤 환경에서 실행될지 정의한다. \n    runs-on: ubuntu-latest\n\t\t# 각 작업에서 실행할 순차적인 단계들을 정의한다. \n    steps:\n\t\t# uses : 특정 액션을 사용할 때 사용한다.\n\t\t# name : step의 이름을 정의한다.\n\t\t# with : user로 지정된 액션에 추가적인 매개변수를 전달한다. \n\t\t# run : 쉘 명령어를 실행한다. \n\t\t# env : 환경변수를 설정한다. \n\t\t# if : 조건문\n    - uses: actions/checkout@v2\n    - name: Use Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18'\n\n    - name: Install Dependencies\n      run: npm install\n\n    - name: Install TypeScript\n      run: npm install --save-dev typescript\n\n    - name: Build\n      run: npx tsc\n\n    - name: Run Script\n      run: node ./index.js\n```\n\n이 파일을 프로젝트의 `.github/workflows/main.yml` 로 저장해준다.\n\n그리고 커밋-푸시를 하면 \n\n  \n\n![](image1.png)\n이렇게 실행이 된다. 그리고\n\n![](image2.png)\nenv 파일이 없기 때문에 바로 설정값 오류가 났다. \n\n#### Github secret을 이용한 환경변수 설정\n\n환경변수 설정을 워크플로우 파일 내에서 직접 설정하거나, 리포지토리 설정 또는 깃허브 환경에서 설정할 수 있다. 환경변수가 노출되지 않기를 원하지만 깃허브 전체에 적용될 필요는 없으므로 리포지토리 Setting에서 설정해주려고 한다. \n\n![](image3.png)\nSecrets는 로그에도 노출되지 않으며, variables는 로그에서 노출이 된다. 적절하게 선택하자. \n\nNOTION_KEY와 NOTION_DATABASE_ID는 시크릿에, 나머지는 변수들에 저장해주었다. \n\n다음과 같이 워크플로우 파일을 수정해주면 정상 작동된다. \n\n```yaml\nname: Run TypeScript\n\non: [push, pull_request]\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Use Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18'\n\n    - name: Install Dependencies\n      run: npm install\n\n    - name: Install TypeScript\n      run: npm install --save-dev typescript\n\n    - name: Build\n      run: npx tsc\n\n    - name: Run Script\n      run: node ./index.js\n\t\t\t# 환경변수 불러오기 추가 \n      env:\n        NOTION_KEY: ${{ secrets.NOTION_KEY }}\n        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}\n        BLOG_URL: ${{ var.BLOG_URL }}\n        SAVE_DIR: ${{ var.SAVE_DIR }}\n        SAVE_SUB_DIR: ${{ var.SAVE_SUB_DIR }}\n```\n\n### 블로그 Repository 추가 \n\n1, 3번 작업을 위해 블로그 레포를 체크아웃 하는 과정을 추가하자. \n\n이 전에 깃 권한을 얻기 위해 토큰을 발급해주었다. Setting - Develper 설정에 가서 토큰을 발급해주자. classic token으로 발행하고, 적당히 권한을 수정해주었다. \n\n그리고 해당 토큰도 레포의 Secret에 추가해주었다. \n\n그리고 나서 워크플로우를 아래와 같이 수정해주었다. \n\n```yaml\nname: Run TypeScript\n\non: [push, pull_request]\n\njobs:\n  run:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n\t\t# 블로그 레포 체크아웃\n    - name: Checkout Blog Repository\n      uses: actions/checkout@v2\n      with:\n        repository: 'Sharknia/Sharknia.github.io'\n        token: ${{ secrets.GITTOKEN }}\n        path: 'blog-repo'\n\n    - name: Use Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '18'\n\n    - name: Install Dependencies\n      run: npm install\n\n    - name: Install TypeScript\n      run: npm install --save-dev typescript\n\n    - name: Build\n      run: npx tsc\n\n    - name: Run Script\n      run: node ./index.js\n      env:\n        NOTION_KEY: ${{ secrets.NOTION_KEY }}\n        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}\n        BLOG_URL: ${{ var.BLOG_URL }}\n        SAVE_DIR: ${{ var.SAVE_DIR }}\n        SAVE_SUB_DIR: ${{ var.SAVE_SUB_DIR }}\n\t\t# 블로그 레포 커밋/푸쉬\n    - name: Set Git remote URL with token and Commit and Push Changes\n\t\t\t# 다른 저장소이므로 git remote set-url을 실행한다. \n      run: |\n        cd blog-repo\n        git remote set-url origin https://${{ secrets.GITTOKEN }}@github.com/name/name.github.io\n        git config user.name 'name'\n        git config user.email 'mail'\n        git add .\n        git commit -m \"Update blog contents\" || echo \"No changes to commit\"\n        git push\n```\n\n여기까지 해두면, 제대로 깃허브 블로그에 새로운 글이 커밋/푸쉬 되었다!\n\n## 마무리\n\n마지막으로, 테스트를 종료하고 자동 배포가 매 시각 되게 하기 위해 on 부분을 다음과 같이 수정해주었다. \n\n```yaml\non: \n  push:\n  pull_request:\n  schedule:\n    - cron: '0 * * * *'\n```\n\n### 다음 계획\n\n이 방법으로는 삭제가 되지 않는다는 사실을 깨달았다. \n\n당분간은 수동삭제를 ~~ㅠㅠ~~ 하기로 하고 삭제를 어떻게 할 것인지에 대해서는 구상을 해두었다. \n\n다음 시간은 삭제의 구현이다. \n\n\n\n"},{"excerpt":"editor.quickSuggestions 설정은 VSCode에서 코드를 작성하는 동안 자동완성을 어떻게 표시할지를 결정한다.  다음 세가지의 옵션이 있다.  string 문자열 내에서 자동 완성 제안을 활성화/비활성화 한다.  comments 주석 내에서 자동 완성 제안을 활성화/비활성화 한다.  other 코드(문자열이나 주석이 아닌 부분) 내에서 자…","fields":{"slug":"/vscode-quick-Suggestions/"},"frontmatter":{"date":"January 31, 2024","title":"vscode-quick Suggestions","tags":["VSCode"]},"rawMarkdownBody":"editor.quickSuggestions 설정은 VSCode에서 코드를 작성하는 동안 자동완성을 어떻게 표시할지를 결정한다. \n\n다음 세가지의 옵션이 있다. \n\n- string\n\n    문자열 내에서 자동 완성 제안을 활성화/비활성화 한다. \n\n- comments\n\n    주석 내에서 자동 완성 제안을 활성화/비활성화 한다. \n\n- other\n\n    코드(문자열이나 주석이 아닌 부분) 내에서 자동 완성 제안을 활성화/비활성화 한다. \n\nVSCode의 설정을 json으로 열어 다음의 내용을 입력하면 된다. \n\n```json\n\"editor.quickSuggestions\": {\n    \"strings\": true,\n    \"comments\": true,\n    \"other\": true\n}\n```\n\n\n\n"},{"excerpt":"func란? SQL 함수를 생성하고 호출하는 데 사용되는 기능이다. SQL 표준 함수 뿐 아니라 데이터베이스 별 특정 함수까지 다룰 수 있다.  Sqlalchemy의 유연한 기능으로 다양한 데이터베이스 작업을 보다 Pythonic한 방식으로 작성할 수 있게 해준다.  func의 특징 함수 생성기 func는 데이터베이스의 내장 함수나 사용자 정의 함수를 파…","fields":{"slug":"/Sqlalchemy의-func/"},"frontmatter":{"date":"January 30, 2024","title":"Sqlalchemy의 func","tags":["SqlAlchemy","Python","DataBase"]},"rawMarkdownBody":"## func란? \n\nSQL 함수를 생성하고 호출하는 데 사용되는 기능이다. SQL 표준 함수 뿐 아니라 데이터베이스 별 특정 함수까지 다룰 수 있다. \n\nSqlalchemy의 유연한 기능으로 다양한 데이터베이스 작업을 보다 Pythonic한 방식으로 작성할 수 있게 해준다. \n\n## func의 특징\n\n- 함수 생성기\n\n    func는 데이터베이스의 내장 함수나 사용자 정의 함수를 파이썬 코드 내에서 호출하기 위한 함수 생성기이다. \n\n- 동적 생성\n\n    func 객체에 접근할 때 Python의 속성 접근 매커니즘을 통해 동적으로 SQL 함수 호출을 생성한다. 예를 들어 func.count() 는 SQL의 COUNT() 함수 호출을 생성한다. \n\n- 데이터베이스 독립성\n\n    다양한 데이터베이스 시스템에서 사용할 수 있는 표준 SQL 함수를 추상화한다. 따라서 특정 데이터베이스에 종속적이지 않은 코드를 작성할 수 있다. \n\n- 다양한 함수 지원\n\n    func는 거의 모든 종류의 SQL 함수를 호출할 수 있도록 지원한다. \n\n## `__getattr__` method\n\nfunc 객체의 __getattr__ 메소드는 특정 속성에 접근할 때 호출된다. 이 메소드는 동적으로 SQL 함수 호출을 생성한다. \n\n예를 들어, func.pg_try_advisory_lock에 접근하면 pg_try_advisory_lock 이름으로 _FunctionGenerator 객체를 생성한다. 이 객체는 최종적으로 SQL 쿼리 내에서 해당 함수 호출을 나타낸다. \n\n## 사용방법\n\n### 기본 사용\n\n함수 이름을 func 객체의 속성으로 접근하여 사용한다. (__getattr__ 메소드로 연결된다.)\n\n```python\nfrom sqlalchemy import func\n\n# COUNT 함수 호출 예시\nquery = select([func.count()]).select_from(my_table)\n```\n\n### 함수 매개변수 전달\n\n함수 호출 시 매개변수를 전달할 수 있다. \n\n```python\n# LENGTH 함수에 문자열 전달\nquery = select([func.length('some string')])\n```\n\n### 복잡한 표현식\n\n복잡한 SQL 표현식을 만드는 데도 사용할 수 있다. \n\n```python\n# 조건부 SQL 함수 호출\nquery = select([func.coalesce(my_table.column, 'default value')])\n```\n\n\n\n"},{"excerpt":"문제 가~끔 네트워크 문제 때문인지 클라이언트에서 같은 요청이 0.01초 미만의 간격으로 두 번씩 들어오는 경우가 있다. 사실상 요청이 동시에 들어오는 것과 같다. 대부분의 경우에는 문제가 되지 않지만, 쿠폰 구매 같은 민감한 요청에 대해서는 회사의 손해 또는 사용자의 불편과 예민하게 직결되므로 문제가 커질 수 있다.  따라서 해당 문제를 완벽히 해결하고…","fields":{"slug":"/동시성-제어문제-해결/"},"frontmatter":{"date":"January 30, 2024","title":"동시성 제어문제 해결","tags":["DataBase","Postgresql","Work"]},"rawMarkdownBody":"## 문제\n\n가~끔 네트워크 문제 때문인지 클라이언트에서 같은 요청이 0.01초 미만의 간격으로 두 번씩 들어오는 경우가 있다. 사실상 요청이 동시에 들어오는 것과 같다. 대부분의 경우에는 문제가 되지 않지만, 쿠폰 구매 같은 민감한 요청에 대해서는 회사의 손해 또는 사용자의 불편과 예민하게 직결되므로 문제가 커질 수 있다. \n\n따라서 해당 문제를 완벽히 해결하고자 한다. \n\n현재 별다른 조치가 없는 상황에서 발생하는 문제 상황은 다음과 같다. \n\n```plain text\n1번 요청과 2번 요청이 0.005초 차이이고(사실상 동시),\n\n1번 요청 - 트랜잭션 시작\n2번 요청 - 트랜잭션 시작\n1번 요청 - 트랜잭션 완료, 커밋. 새로운 구매 내역이 테이블에 추가됨.\n2번 요청 - 트랜잭션 완료, 커밋. 새로운 구매 내역이 테이블에 추가됨. \n\n따라서 동일한 유저가 두 번 쿠폰을 구매한 것 처럼 처리되었다. \n```\n\n### 제약조건 확인\n\n현재 테이블의 키값이 user\\_id가 아니며, 난수 발생을 통해 생성된 값을 키 값으로 하고 있다. 또한 Redis와 같은 외부 라이브러리를 도입할 여유는 없다. 따라서 가능한 한 현재 시스템(Sqlalchemy, Postgresql)내에서 해결책을 찾고자 한다. \n\n## 해결방안 고민\n\n### 최근 구매 내역 확인(기각)\n\n가장 쉽게 생각할 수 있는 방법은 쿠폰 구매 로직에서 이 유저의 최근 구매 로직을 확인하는 것이다. 하지만 현재 이 상황에서는 사실상 요청이 동시에 들어오고 있으므로, 2번 요청이 시작됐을 때에는 해당 유저의 구매 내역이 아직 DB에 없을 것이므로, 이 방법은 사용할 수 없다. \n\n### Database-Level Optimistic Locking(기각)\n\n각 트랜잭션에 버전 번호를 추가하고 트랜잭션이 커밋되기 전에 해당 버전 번호가 여전히 유효한지 확인한다. 하지만 이 방법도 여전히 2번 요청이 1번 요청의 결과를 감지하지 못할 수 있다. \n\n### Application-Level Locking(기각)\n\n코드 내에서 직접 잠금 로직을 구현하는 방법이다. 데이터베이스가 아닌 애플리케이션의 메모리 내에서 잠금을 관리한다. 주로 멀티스레드 환경에서 특정 자원에 대한 동시 접근을 제어하는데 사용된다. 현재 FastAPI를 사용하고 있고, FastAPI는 기본적으로 비동기 방식으로 단일 스레드 환경에서 실행된다는 점을 고려하면 전통적인 스레드 기반의 잠금 매커니즘보다 async 라이브러리에 들어있는 비동기 프로그래밍에 적합한 잠금 메커니즘인 asyncio.Lock을 사용하는 것이 이상적이다.\n\n#### FastAPI에서의 Application-Level Locking\n\n```python\nimport asyncio\n\nlocks = {}\n\nasync def purchase_coupon(user_id: int):\n    lock = locks.setdefault(user_id, asyncio.Lock())\n\n    async with lock:\n        # 여기에 쿠폰 구매 로직을 구현합니다.\n        # 이 블록 내의 코드는 동시에 하나의 요청만 처리합니다.\n        pass\n\n# FastAPI 엔드포인트에서 이 함수를 호출합니다.\n```\n\n이 코드는 user\\_id에 대해 별도의 asyncio.Lock을 생성하고 관리한다. 해당 user\\_id에 대한 작업이 진행중인 경우 다른 요청들은 해당 블록이 해제될 때까지 대기한다. \n\n예를 들어 다음 코드를 가정하자. 예를 들기 위해 아주 러프하게 작성했다. \n\n```python\nasync def test_function(a: int, b: int):\n    lock = locks.setdefault(a, asyncio.Lock())\n\n    async with lock:\n        await asyncio.sleep(1)\n        print(a)\n```\n\n이 코드는 다음과 같이 호출된다. \n\n```plain text\ntest_function(1, 2) 호출 -> a 값이 1인 잠금 획득 -> 처리 시작\ntest_function(1, 2) 호출 -> a 값이 1인 동일한 잠금으로 인해 대기\ntest_function(2, 2) 호출 -> a 값이 2인 새로운 잠금 획득 -> 처리 시작\ntest_function(2, 2) 처리 완료 -> a 값이 2인 잠금 해제\ntest_function(1, 2)의 첫 번째 호출 처리 완료 -> a 값이 1인 잠금 해제\ntest_function(1, 2)의 두 번째 호출이 대기 상태에서 해제 -> 처리 시작\n\n따라서 print는 1, 2, 1의 순서가 된다. \n```\n\n다만, 비동기 프로그래밍, 특히 Python의 [asyncio](https://sharknia.github.io/FastAPI와-asyncio)를 사용할 때 코드의 실행순서는 이벤트 루프에 의해 관리된다. 작업의 일시 중단과 다른 작업의 실행이 이벤트 루프에 의해 어떻게 스케줄링 될지는 여러 요인에 따라 달라질 수 있다. \n\n### PostgreSQL Advisory Locks(당장 채용)\n\nAdvisory Locks는 애플리케이션에서 데이터베이스 레벨의 잠금을 관리할 수 있도록 해준다. 이를 통해 특정 user\\_id 에 대한 동시 요청을 제어할 수 있다. \n\nAdvisory Locks는 테이블이나 행에 대한 접근을 잠그는 개념이 아니다. 특정 값 (정수)에 대한 잠금을 제공한다. 즉, 작업 자체에 1번이라는 이름을 붙이고 1번을 잠근다면, 다시 요청되는 1번 작업은 잠금이 된다. \n\n#### **Advisory Locks의 작동 방식**\n\n- 잠금 설정 : 애플리케이션이 pg\\_advisory\\_lock(key) 함수를 호출하여 특정 키에 대한 잠금을 요청한다.\n\n- 동일 키 사용 시 대기 : 같은 키를 사용하는 다른 데이터베이스 작업이 잠금을 보유하고 있다면, 새로운 잠금 요청은 해당 잠금이 해제될 때까지 대기한다. \n\n- 잠금 해제 : 원래의 작업이 완료되면 pg\\_advisory\\_unlock(key) 함수를 호출하여 잠금을 해제한다. 이후 대기 중이던 다른 작업이 잠금을 획득하고 진행될 수 있다. \n\n#### **Advisory Locks의 잠금 레벨**\n\nAdvisory Locks는 두 가지 레벨에서 사용할 수 있다. \n\n- 세션 레벨 : 이 잠금은 데이터베이스 세션과 연결되어있다. 세션이 종료되면 자동으로 해제된다. 세션 레벨 잠금은 장시간 유지되어야 하는 경우에 유용하다. \n\n    ```python\n    async def use_session_level_lock(db: AsyncSession, lock_key: int):\n        # 세션 레벨 잠금 획득\n        await db.execute(f\"SELECT pg_advisory_lock({lock_key})\")\n    \n        # 데이터베이스 작업 수행\n        # ...\n    \n        # 필요한 경우, 명시적으로 잠금 해제\n        # await db.execute(f\"SELECT pg_advisory_unlock({lock_key})\")\n    \n        # 세션 종료 시 잠금이 자동으로 해제됩니다.\n    ```\n\n- 트랜잭션 레벨 잠금 : 트랜잭션 레벨 잠금은 현재 트랜잭션과 연결되어 있으며 트랜잭션이 커밋되거나 롤백 될 때 자동으로 해제된다. \n\n    ```python\n    from sqlalchemy.ext.asyncio import AsyncSession\n    \n    async def use_transaction_level_lock(db: AsyncSession, lock_key: int):\n        async with db.begin():\n            # 트랜잭션 레벨 잠금 획득\n            await db.execute(f\"SELECT pg_advisory_xact_lock({lock_key})\")\n            \n            # 데이터베이스 작업 수행\n            # ...\n    \n        # 트랜잭션이 종료되면 잠금이 자동으로 해제됩니다.\n    ```\n\n쿠폰 구매 서비스 로직에서 트랜잭션 시작 전에  user\\_id를 기반으로 Advisory Lock을 요청하면 1번 요청이 처리되는 동안 동일한 user\\_id 에 대한 다른 요청은 대기하게 된다. 쿠폰 구매 처리가 완료되면, Advisory Lock을 해제하면 된다. \n\n## 결론 - PostgreSQL Advisory Locks 채용\n\n이 방법을 사용하면 매우 짧은 간격의 요청도 효과적으로 처리할 수 있다. 특히 트랜잭션 레벨의 잠금은 각 트랜잭션이 커밋되거나 롤백될 때까지만 유지되므로 0.005초와 같은 짧은 간격의 요청을 동기화하는데 매우 적합하다. \n\n백엔드 팀에서 협의를 거쳐 해당 방법을 사용해 구현하기로 최종 결정하였다. \n\n\n\n"},{"excerpt":"퇴근을 하고 고치다보니 시간이 부족하다.. 어쩔 수 없다.  돌아보기 어제 상당수의 디버깅을 진행하고 기존 블로그에서 새 블로그로 글을 이전했다. 깃허브 블로그에 업로드까지 마치고, 오늘 다시 보니 짜잘짜잘한 오류가 다시 또(!!) 발견되었다.  유지보수 Callout 수정 내가 선택한 블로그의 기능인지, 원래 마크다운 기능인지 모르겠다. 굳이 태그를 넣…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅7/"},"frontmatter":{"date":"January 30, 2024","title":"NotionAPI를 활용한 자동 포스팅(7)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"퇴근을 하고 고치다보니 시간이 부족하다.. 어쩔 수 없다. \n\n## 돌아보기\n\n어제 상당수의 디버깅을 진행하고 기존 블로그에서 새 블로그로 글을 이전했다. 깃허브 블로그에 업로드까지 마치고, 오늘 다시 보니 짜잘짜잘한 오류가 다시 또(!!) 발견되었다. \n\n## 유지보수\n\n### Callout 수정\n\n내가 선택한 블로그의 기능인지, 원래 마크다운 기능인지 모르겠다. 굳이 태그를 넣을 필요가 없어서, 태그를 제거해주었다. \n\n### 이미지 캡션 수정\n\n이미지 캡션도 마찬가지였다. 혹시 하는 노파심에 따로 p태그를 넣어주고 있었는데, 아무 문제 없이 정상 작동 되었다. 아무래도 깃허브 블로그도 마크다운 기반으로 움직이다보니 마크다운에서 좀 잘 나오면 잘 이쁘게 지원하는 것 같다. \n\n### 넘버링 숫자 안올라감 & 들여쓰기 문제\n\n넘버링이 모두 1로 나오고, 글머리 기호와 넘버링 리스트를 제외한 나머지 부분에 들여쓰기를 넣지 않았다는 사실을 깨달았다. \n\n넘버링부터 고치려다가 문득 들여쓰기가 제대로 되면 숫자가 제대로 나오지 않을까? 해서 들여쓰기부터 수정했는데, 정답이었다. \n\n들여쓰기를 고치고 나서 마크다운에 1. 이라고 숫자가 고정되어 있어도 제대로 숫자가 증가되면서 출력된다. \n\n### 일부 부분 [objcet Promise] 출력\n\n이건 await을 까먹은거다. 추가해주었다. \n\n### 이미지 깨짐\n\n어제 디버깅을 하면서 이미지 카운터를 static에서 인스턴스로 변경해주었는데, 어제 내가 멍청하다고 썼는데 사실은 오늘의 내가 멍청한 것이었다. \n\n이미지 카운터를 컨버터에 넣어놓을거면 거기서 인스턴스 변수로 선언하면 안됐다. 이미지가 거의 모든 경우에 항상 1번으로 저장된다.. \n\n페이지에 이미지 카운터 변수를 심고 블록-컨버터까지 내려줄 생각을 하니 아득해졌는데 생각해보니 그냥 컨버터 클래스에 넣어놓고 static 변수로 변경한 다음 페이지를 새로 만들 때마다 해당 값을 초기화 해주면 되지 않나 싶어서 그렇게 구현 해보려고 한다. \n\n### 언더바 또는 *가 포함된 경우 스타일 깨짐\n\n예를 들어 _db_connection_serializable 라고 입력된 텍스트가 *db*connection*serializable* 로 마음대로 스타일이 변경되고 있었다. 이를 예방하기 위해 일단 \n\n```typescript\n// 언더스코어 이스케이프 처리 함수\n    private escapeMarkdownUnderscores(text: string): string {\n        return text.replace(/(\\w)_(\\w)/g, '$1\\\\_$2');\n    }\n```\n\n해당 메소드를 사용해 언더바 앞에 백슬래시를 붙여주었다. \n\n잘 될지는 배포 후 테스트를 해봐야 한다. \n\n## 신규 기능\n\n### 테이블 지원\n\n이제 테이블을 지원한다.. 지금 고치느라 기록할 시간이 없는데 많은 것을 고쳤다.. \n\n근본적인 설계가 아쉽다. 굳이 block을 별도의 클래스로 뺄 필요도 없었을 것 같고, MarkdownConverter 클래스가 굳이 인스턴스 메소드가 이렇게 많아야 할 이유도 모르겠다. \n\n여유가 된다면 전체적으로 코드 리팩토링을 (다시 만드는 수준으로) 진행해야 할 것 같다. \n\n\n\n"},{"excerpt":"서론 동시성 제어 문제를 해결하기 위한 여러가지 방법을 고민 끝에 PostgreSQL Advisory Locks를 사용한 방법으로 구현하기로 했다.  구현을 하면서 겪은 과정을 여기에 기록한다.  구현 목표 현재 회사에서는 FastAPI와 SqlAlchemy 2.0, PostgreSQL을 사용하고 있다. 아주 짧은 텀으로 중복으로 온 API 요청에 대해 …","fields":{"slug":"/PostgreSQL-Advisory-Locks-트랜잭션-레벨에서-구현/"},"frontmatter":{"date":"January 30, 2024","title":"PostgreSQL Advisory Locks 트랜잭션 레벨에서 구현","tags":["Postgresql","SqlAlchemy","Python","DataBase","Work"]},"rawMarkdownBody":"## 서론\n\n[동시성 제어 문제를 해결하기 위한 여러가지 방법을 고민](https://sharknia.github.io/동시성-제어문제-해결) 끝에 PostgreSQL Advisory Locks를 사용한 방법으로 구현하기로 했다. \n\n구현을 하면서 겪은 과정을 여기에 기록한다. \n\n## 구현 목표\n\n현재 회사에서는 FastAPI와 SqlAlchemy 2.0, PostgreSQL을 사용하고 있다. 아주 짧은 텀으로 중복으로 온 API 요청에 대해 중복 구매가 되지 않도록 동시성 제어를 하려고 한다. \n\n유저의 아이디 값 또는 API 고유의 값 + 유저의 아이디 값을 사용해 트랜잭션 레벨에서 PostgreSQL Advisory Locks를 생성하고, 동일한 유저가 PostgreSQL Advisory Locks에 걸린 경우에는 중복 요청이라고 판단하고 두번째 요청은 중단 하거나, 또는 반드시 유효성 검사를 하도록 해 중복 구매가 되는 일이 없게 사전에 차단하려고 한다. \n\n## 구현 사전단계\n\n[공식문서](https://www.postgresql.org/docs/9.1/functions-admin.html)를 살펴보니 트랜잭션 레벨에서의 락을 위해 내가 사용할 수 있는 함수는 두 가지가 있었다. 트랜잭션 레벨의 락은 공통적으로 명시적인 언락이 불가능하다. \n\n아래의 두가지 함수는 모두 트랜잭션 레벨의 함수로 Advisory Lock을 획득할 수 있다는 점은 같지만, Advisory Lock을 바로 획득하지 못했을 때의 반응이 다르다. \n\n### pg\\_advisory\\_xact\\_lock\n\n지정된 키에 대한 Advisory Lock을 획득할 때까지 호출을 블로킹(대기)한다. 만약 다른 세션이 이미 해당 키에 대한 Advisory Lock을 갖고 있다면 그 Advisory Lock이 해제될 때까지 현재 세션에서의 처리가 중단된다. \n\nAdvisory Lock을 반드시 획득해야 해서 대기해야 할 경우에 적합하다.\n\n### pg\\_try\\_advisory\\_xact\\_lock\n\n이 함수는 즉시 Advisory Lock을 시도하고 성공하면 True, 실패하면 False를 반환한다. 이 함수는 잠금 획득을 위해 대기하지 않으며, 잠금이 이미 다른 세션에 의해 보유되고 있는 경우 즉시 실패한다.\n\nAdvisory Lock을 반드시 획득할 필요가 없고 대기하지 않고 다른 작업을 수행해야 할 경우에 적합하다. \n\n### 결정\n\n우리의 경우에는 쿠폰 중복 발급을 막으려고 하는 것이므로 만약 이미 잠금이 걸린 경우에는 굳이 나머지 DB 작업을 실행할 필요가 없다. 따라서 pg\\_try\\_advisory\\_xact\\_lock를 사용하는 것이 적절해보인다. \n\n## 구현\n\n따라서, 서비스 로직의 시작 지점에 다음의 코드를 넣어 락을 얻거나, 락을 얻지 못한 경우 해당 요청은 실행을 중단하려고 한다. \n\n```python\n# Advisory Lock 획득\n    result = await db.execute(f\"SELECT pg_try_advisory_xact_lock({lock})\")\n    # Advisory Lock을 획득하지 못한 경우에는 이미 선제 작업이 있는 것이므로 DB 작업을 계속할 필요가 없으므로\n    # 바로 중단한다.\n    if not result.scalar():\n        raise Exception(\"Unable to acquire lock\")\n```\n\nSQL 인젝션과 같은 보안 문제를 줄이고 쿼리 구성과 관련된 오류를 예방하기 위해 다음과 같이 코드를 수정했다. \n\n```python\n# Advisory Lock 획득\n    result = await db.execute(func.pg_try_advisory_xact_lock(100, lock))\n    # Advisory Lock을 획득하지 못한 경우에는 이미 선제 작업이 있는 것이므로 DB 작업을 계속할 필요가 없으므로\n    # 바로 중단한다.\n    if not result.scalar():\n        raise Exception(\"Unable to acquire lock\")\n```\n\n## 잠금 해제 지점은 어떻게 될까? \n\n막연히 트랜잭션이 커밋되거나 롤백될 때라고 알아둬도 충분할 것 같다(세션 주입 방식을 통해 에러가 발생하거나 요청이 정상정이 끝나지 않는 경우에 세션을 롤백하며, 정상적으로 요청이 정리된 경우에는 세션을 커밋하는 과정이 포함되어 있을 것이므로 충분할 것이다).\n\n다만, 트랜잭션 레벨에서의 락은 명시적으로 언락이 불가능하므로 정확한 언락 지점을 알아둬야 할 필요성이 있다고 느꼈다. \n\n- 트랜잭션 커밋 또는 롤백\n\n    함수 내부에서 명시적으로 await db.commit() 또는 await db.rollback()이 호출되는 경우에 같은 세션이라고 하더라도 언락된다. \n\n- 비동기 세션 컨텍스트 종료\n\n    AsyncSession 인스턴스가 컨텍스트 매니저(async with) 내에서 사용되고, 해당 컨텍스트 블록이 종료되는 경우\n\n- 예외 발생\n\n    함수 실행 중 예외가 발생하여 처리 흐름이 중단되는 경우. SQLAlchemy의 세션 관리는 트랜잭션이 커밋되지 않으면 자동으로 롤백을 수행하므로 사실상 트랜잭션이 롤백되는 시점과 일치한다. \n\n\n\n"},{"excerpt":"구현 목적 상품 구매에 관련된 API를 구현하려고 한다. DynamoDB를 사용할 때에 동시성 이슈로 쿠폰 중복 구매 이슈가 있었으므로 이번에 RDS로 옮긴 김에 해당 문제를 완벽하게 해결하기 위해 다각도로 방법을 고민했다.  그 방안 중 하나가 트랜잭션 격리 수준(Transaction Isolation Level) 을 이용한 것이다.  SqlAlchem…","fields":{"slug":"/Sqlalchemy에서의-트랜잭션-격리-수준-구현/"},"frontmatter":{"date":"January 29, 2024","title":"Sqlalchemy에서의 트랜잭션 격리 수준 구현","tags":["SqlAlchemy","DataBase","Python","Work"]},"rawMarkdownBody":"## 구현 목적\n\n상품 구매에 관련된 API를 구현하려고 한다. DynamoDB를 사용할 때에 동시성 이슈로 쿠폰 중복 구매 이슈가 있었으므로 이번에 RDS로 옮긴 김에 해당 문제를 완벽하게 해결하기 위해 다각도로 방법을 고민했다. \n\n그 방안 중 하나가 [트랜잭션 격리 수준(Transaction Isolation Level)](https://sharknia.github.io/트랜잭션-격리-수준Transaction-Isolation-Level) 을 이용한 것이다. \n\nSqlAlchemy - Postgresql을 사용하고 있는데, 이 라이브러리에서 트랜잭션 격리 수준을 어떻게 구현했는지 기록한다. \n\n## 현재\n\n현재에는 기본 격리 수준을 사용한(별다른 옵션값이 없는) 엔진만 사용하고 있다. API 별로 별도의 세션을 사용하기 위해 의존성 주입 방식을 사용하며, 이를 위해 `AsyncIterable[AsyncSession]`을 생성한다. 대략적인 코드는 다음과 같다. \n\n```python\n_db_connection: AsyncEngine\n\n...\n\nasync def on_startup():\n\t\t....\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        pool_size=pool_size,\n        max_overflow=max_overflow,\n        echo=echo,\n    )\n\t\t....\n\n....\n\nasync def get_db_connection() -> AsyncEngine:\n    assert _db_connection is not None\n    return _db_connection\n\n....\n\nasync def get_db_session(\n    db_conn: AsyncEngine = Depends(get_db_connection),\n) -> AsyncIterable[AsyncSession]:\n    session = None\n    try:\n        async with sessionmaker(\n            db_conn,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )() as session:\n            yield session\n    except Exception as e:\n        logger.error(f\"[get_db_session] {e}\")\n        await session.rollback()\n        raise\n    finally:\n        if session:\n            await session.close()\n```\n\n그리고 각 API의 엔드포인트에서 get_db_session을 주입받아 사용한다. \n\n이 엔진은 기본적으로 Read Committed 격리수준만 지원한다. \n\n## 모험\n\n###  sessionmaker 레벨에서 트랜잭션 격리 수준 설정?\n\n이렇게 내용이 조금만 깊어져도 챗지피티는 믿을 놈이 못된다. 챗지피티에서는 sessionmaker 레벨에서 트랜잭션 격리 수준 설정이 가능하다며, sessionmaker의 매개변수에 `isolation_level=\"SERIALIZABLE”` 를 추가해주면 된다고 주장한다. \n\n하지만 sessionmaker의 생성자에는 `isolation_level` 매개 변수가 없어 해당 설정은 바로 오류를 낸다. 이럴 경우 답은 스스로 해결하는 것 밖에 없다. \n\n### 트랜잭션 별 엔진 별도 생성?\n\n첫 구현은 단순하게 생각해서 엔진을 여러개를 만들었다. 즉, `isolation_level` 옵션을 각각 다르게 준create_async_engine을 여러번 하는 것이다. 이러면 간단하게 해결이 된다. 하지만 이렇게 할 경우에는 문제가 있다. 각각의 엔진이 모두 별도의 풀을 생성하면서 의도치 않게 커넥션이 증가할 위험이 있는 것이다. 안그래도 [SqlAlchemy의 QueuePool](https://sharknia.github.io/SqlAlchemy의-QueuePool) 를 겪었었기 때문에 해당 이슈는 꼭 피하고 싶었다. \n\n## 개선(해결)\n\n코드를 짜다보면 “이건 있어야 하는데?”라고 느낄때가 있다. 그런것들은 대부분, 너무 미완성인 라이브러리나 프레임워크가 아니라면 반드시 나온다. 구현 가능한데 필요성을 느끼는 것은 반드시 누군가 만들어둔 것이다. \n\n명확하게 다른 엔진이면서 풀을 공유하는 방법이 존재한다. `execution_options()` 를 사용하면 된다. \n\n### execution_options()\n\n`AsyncEngine`에서 `execution_options()` 메소드를 사용하면 반환되는 엔진도 `AsyncEngine` 타입이 된다. 이 메소드는 Engine, Connection, Session 객체에서도 사용할 수 있으며, 특정 실행 옵션을 동적으로 설정하거나 변경하기 위해 사용된다. \n\n이 메소드를 사용하면 동일한 연결 풀을 공유하면서도 다른 실행 옵션을 가진 엔진을 생성할 수 있다. \n\n개선된 코드는 다음과 같다. \n\n_db_connection 설정 후, _db_connection_serializable를 `execution_options()` 를 사용해 정의한다. \n\n```python\n_db_connection_serializable = _db_connection.execution_options(\n        isolation_level=\"SERIALIZABLE\",\n    )\n```\n\n해당 값을 사용한 의존성 주입용 메소드를 선언한다. \n\n```python\nasync def get_db_connection_serializable() -> AsyncEngine:\n    assert _db_connection_serializable is not None\n    return _db_connection_serializable\n\nasync def get_serializable_db_session(\n    db_conn: AsyncEngine = Depends(get_db_connection_serializable),\n) -> AsyncIterable[AsyncSession]:\n    session = None\n    try:\n        async with sessionmaker(\n            db_conn,\n            class_=AsyncSession,\n            expire_on_commit=False,\n        )() as session:\n            yield session\n    except Exception as e:\n        logger.error(f\"[get_serializable_db_session] {e}\")\n        if session:\n            await session.rollback()\n        raise\n    finally:\n        if session:\n            await session.close()\n```\n\n이제 엔드포인트에서 기존 get_db_session 대신 이 메소드를 사용하면 다른 옵션의 격리 레벨을 사용할 수 있다!\n\n"},{"excerpt":"트랜잭션 격리 수준(Transaction Isolation Level)이란? 트랜잭션 격리 수준은 데이터베이스 시스템에서 동시에 여러 트랜잭션이 실행될 때, 트랜잭션 간에 데이터를 어떻게 고립시킬지를 결정하는 설정이다. 이 설정은 트랜잭션이 다른 트랜잭션의 작업에 영향을 받지 않도록 보장하는 동시에 여러 트랜잭션이 데이터베이스의 동일한 데이터에 접근할 때…","fields":{"slug":"/트랜잭션-격리-수준Transaction-Isolation-Level/"},"frontmatter":{"date":"January 29, 2024","title":"트랜잭션 격리 수준(Transaction Isolation Level)","tags":["DataBase","Postgresql"]},"rawMarkdownBody":"## 트랜잭션 격리 수준(Transaction Isolation Level)이란?\n\n트랜잭션 격리 수준은 데이터베이스 시스템에서 동시에 여러 트랜잭션이 실행될 때, 트랜잭션 간에 데이터를 어떻게 고립시킬지를 결정하는 설정이다. 이 설정은 트랜잭션이 다른 트랜잭션의 작업에 영향을 받지 않도록 보장하는 동시에 여러 트랜잭션이 데이터베이스의 동일한 데이터에 접근할 때 발생하는 문제들을 관리하는데 중요한 역할을 한다. \n\n격리 수준이 높을 수록 트랜잭션 간의 고립 정도가 증가하지만, 그만큼 데이터베이스의 처리량(throughput)과 동시성(concurrency)이 감소할 수 있다. 반대로 격리 수준이 낮으면 처리량과 동시성은 증가하지만 데이터 무결성 문제가 발생할 위험이 높아진다. 따라서 적절한 격리 수준을 선택해야 한다. \n\n데이터베이스의 동일한 데이터에 접근할 때 발생할 수 있는 문제들은 다음과 같다. \n\n### Dirty Read\n\n한 트랜잭션이 아직 커밋되지 않은 데이터를 읽는 경우\n\n### Non-repeatable Read\n\n한 트랜잭션이 동일한 쿼리를 여러 번 실행할 떄마다 다른 결과를 얻는 경우. 이는 다른 트랜잭션이 데이터를 변경하거나 삭제했기 때문이다. \n\n### Phantom Read\n\n한 트랜잭션이 동일한 쿼리를 여러 번 실행할 때, 새로 삽입된 행이 결과에 나타나는 현상이다. \n\n### Serialization Anomaly\n\n트랜잭션들이 서로 다른 순서로 실행될 때 발생할 수 있는 일관성 없는 결과를 말한다. \n\n## 격리 수준의 종류\n\n각 데이터베이스 시스템은 아래의 격리 수준을 제공하며, 이는 ANSI SQL 표준에 의해 정의된다. \n\n### Read Uncommitted\n\n- 가장 낮은 격리 수준이다. \n\n- 다른 트랜잭션에서 아직 커밋되지 않은 변경 사항을 조회할 수 있다. (Dirty Read)\n\n- Postgreql에서는 구현되지 있지 않아 Read Committed로 자동 처리된다. \n\n#### 장점\n\n높은 동시성과 성능을 보여준다. \n\n#### 단점\n\n데이터 무결성 문제가 발생할 수 있다. \n\n#### 고려사항\n\n- 데이터의 정확성보다 동시성이 중요한 경우에 고려될 수 있으나, 일반적으로 권장되지 않는다. \n\n### Read Committed\n\n- 각 쿼리가 실행될 때 커밋된 데이터만 읽는다. \n\n- 다른 트랜잭션에서 커밋된 변경 사항을 쿼리 시점에 볼 수 있다. (Non-repeatable Read이 발생할 수 있다. )\n\n#### 장점\n\n적당한 수준의 동시성과 성능을 보여준다. \n\n#### 단점\n\n같은 트랜잭션 내에서 같은 데이터를 두 번 읽을 때 다른 결과가 나올 수 있다. \n\n#### 고려사항\n\n- 일반적인 응용 프로그램에서 광범위하게 사용된다. \n\n- 대부분의 경우에 적합한 격리 수준이다. \n\n### Repeatable Read\n\n- 트랜잭션이 시작될 때의 데이터 상태를 유지한다. \n\n- 트랜잭션 중에 다른 트랜잭션이 커밋한 변경 사항이 보이지 않음(Phantom Read이 발생할 수 있다.)\n\n#### 장점\n\n하나의 트랜잭션 내에서 일관된 조회 결과를 보장한다. \n\n#### 단점\n\n트랜잭션이 길어질수록 다른 트랜잭션과의 충돌 가능성이 증가한다. \n\n#### 고려사항\n\n- 일관된 데이터 읽기가 필요할 때 사용한다. \n\n- 데이터 무결정이 중요한 경우 적합하지만, 긴 트랜잭션은 피해야 한다. \n\n### Serializable\n\n- 데이터베이스 트랜잭션에서 가장 높은 격리 수준이다. \n\n- 트랜잭션이 서로 독립적으로 작동하며, 동시에 여러 트랜잭션이 수행되는 경우에도 하나의 트랜잭션이 완료된 것 처럼 작동한다. \n\n- 다른 트랜잭션이 동시에 같은 데이터를 수정하지 못하도록 하여 데이터의 일관성을 보장한다. \n\n- 하나의 트랜잭션 동안 새로 삽입되거나 삭제된 행이 다른 트랜잭션에 의해 보이지 않도록 해서 Phantom Read를 방지한다. \n\n- 각 트랜잭션이 독립적인 환경에서 실행되며 다른 트랜잭션의 중간 결과를 볼 수 없다. \n\n#### 장점\n\n높은 수준의 데이터 무결성을 제공한다. 데이터베이스 내의 데이터는 모든 트랜잭션에 대해 완전히 일관된 상태를 유지한다. \n\n따라서 동시성 문제도 최소화된다. 동시에 여러 트랜잭션이 실행되는 환경에서 발생할 수 있는 대부분의 동시성 문제를 예방한다. \n\n#### 단점\n\n트랜잭션의 격리 수준이 높을 수록 데이터베이스의 처리량이 감소할 수 있다. 락 경합(lock contention)이 증가하고, 이는 성능저하로 이어질 가능성이 높다. \n\n서로 다른 트랜잭션이 같은 자원을 기다리는 상황이 발생할 수 있으며, 이는 데드락으로 이어질 수 있다. \n\n#### 고려사항\n\n- 데이터 무결성이 중요한 경우에 적합하지만 성능상의 영향을 고려해야 한다. \n\n- 데드락을 방지하기 위해 트랜잭션 설계 시 주의가 필요하다. \n\n- 읽기만 수행하는 트랜잭션의 경우 더 낮은 격리 수준을 고려하는 것이 성능상 이점이 있을 수 있다. \n\n\n\n"},{"excerpt":"기능 구현이 거의 다 되었기 때문에, 오늘은 일단 짜잘짜잘한 오류 수정을 진행했다.  유지보수 내역 Front Matter-프로퍼티 연계 수정 프로퍼티가 Front Matter에 추가되는데, 값이 빈 것도 추가되어서 제대로 필터링 되지 않는 문제가 있었다. 값이 존재하는 프로퍼티만 Front Matter에 추가되도록 수정해주었다.  추가로, 프로퍼티가 고…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅6/"},"frontmatter":{"date":"January 29, 2024","title":"NotionAPI를 활용한 자동 포스팅(6)","tags":["Blogging","Typescript","Hobby","Notion-API"]},"rawMarkdownBody":"기능 구현이 거의 다 되었기 때문에, 오늘은 일단 짜잘짜잘한 오류 수정을 진행했다. \n\n## 유지보수 내역\n\n### Front Matter-프로퍼티 연계 수정\n\n프로퍼티가 Front Matter에 추가되는데, 값이 빈 것도 추가되어서 제대로 필터링 되지 않는 문제가 있었다. 값이 존재하는 프로퍼티만 Front Matter에 추가되도록 수정해주었다. \n\n추가로, 프로퍼티가 고정값이 아니라 커스텀에 따라서 유연하게 대응해 Front Matter에 추가되도록 해주었다. \n\n### 줄바꿈 주의!\n\n마크다운은 기본적으로 엔터를 두 번씩 쳐야 줄바꿈이 되는 모양이다. 리스트 컨버터 일부에 줄바꿈이 하나만 들어가있던 것을 두 개가 들어가도록 수정해주었다. \n\n### 이미지 다운로드 안되는 현상\n\n분명 동기적으로 작동하게끔 짜놓았는데도 불구하고 이미지가 제대로 들어가지 않는 현상이 있었다.  axios 재시도 패키지를 설치하고, 재시도 로직을 추가하고 타임아웃 10초 설정도 해주었다. 뭐가 문제였는지는 정확히 파악이 안됐는데, (오류 로그조차 없었다) 일단 이 이후로는 이미지 다운로드에 실패하는 현상은 없다. \n\n추가로, 이미지 다운로드 시에도 디렉토리가 없다면 만들도록 해주었다. \n\n### 스타일이 두 개 적용된 경우 \n\n스타일이 두 개가 적용된 경우는 더 띄어쓰기에 민감했다. \n\n```plain text\n*~~안녕하세요 ~~*\n```\n\n처럼 애매하게 띄어쓰기가 들어가면 하나만 적용된다. \n\n그래서 스타일이 있는 경우에는 공백을 제거 한 후에, 스타일을 붙이고 다시 스타일 뒤로 공백을 붙여서 자연스럽게 포맷이 유지되도록 수정해주었다. \n\n### ImageCounter 변수 수정\n\n이미지가 디렉토리가 변경되어도 숫자가 계속 늘어나는 현상을 겪었는데… 멍청하게 내가 이미지 카운터를 static 변수로 선언해서 모든 인스턴스가 값을 공유하고 있는 것 뿐이었다. 바로 인스턴스 변수로 변경 해주었다. \n\n### 멘션 링크 이후 줄바꿈 문제 해결\n\n멘션 링크를 북마크로 바꿔주는 메소드에 줄바꿈이 고정되어있어서, paragraph 안에 들어가 있는 멘션의 경우에도 줄바꿈이 들어가버렸다. 번거롭지만, 메소드를 수정해서 리스트로 처리되는 경우에만 줄바꿈이 들어가도록 수정해주었다. \n\n### notion url만 주어지는 경우\n\n노션의 페이지 링크를 따온 다음에, 아무 글자 또는 문장을 드래그 하고 붙여넣기 하면 해당 글자 또는 문장에 페이지 링크로 연결되는 북마크가 생성된다. \n\n![이렇게, 이 하이퍼링크는 노션 내부의 모 페이지로 연결된다. /xxxx… 만의 주소를 갖고 있다. ](image1.png)\n근데 애석하게도 이 링크는 단순히 32자리 문자열만 가지고 있다. (페이지의 아이디가 아니다). 따라서 해당 경우에는 북마크를 연결할 수가 없었다… \n\n인 줄 알았는데, 32자리인게 이상해서 혹시나 해서 UUID 형식으로 변환해봤더니 그게 내부 페이지의 아이디였다. 그러니까, 각 블록의 아이디가 UUID로 주어지는데 거기에서 하이픈만 제외하고 내부 URL로 사용중이었던 것이다. \n\n얼씨구나~ 하고 바로 해당 케이스도 북마크 기능으로 연결해주었다. \n\n### URL 형식 변경\n\n예를 들어 `노션 소개 페이지` 라는 타이틀을 `노션소개페이지` 로 변환해서 URL로 사용하고 있었는데, 공백을 완전히 제거하는 대신 하이픈을 넣도록 수정해주었다. \n\n## 다음 단계는? \n\n이제 다음 단계는 자동화가 목적이다. 어느정도 기능 구현은 90프로 이상 됐다고 생각하고, 당장 내가 블로그 글을 쓰는데 큰 문제가 없어보인다. 이제 github.io와 합쳐서 실질적인 사용이 가능하도록 해보려고 한다!\n\n"},{"excerpt":"타입스크립트의 Union Type 타입스크립트에는 유니언 타입이라는게 있다. 쉽게 말해 값이 여러 타입을 or 로 가질 수 있는 것이다. 막나간다.  두 개의 이상의 타입을  기호를 사용해 결합하면 유니언 타입이 된다. 이를 통해 변수가 함수 매개변수가 여러 타입 중 하나의 타입을 가질 수 있음을 나타낼 수 있다.  장점 유연성 다양한 타입을 하나의 변수…","fields":{"slug":"/Union-Type/"},"frontmatter":{"date":"January 28, 2024","title":"Union Type","tags":["Typescript","Python"]},"rawMarkdownBody":"## 타입스크립트의 Union Type\n\n타입스크립트에는 유니언 타입이라는게 있다. 쉽게 말해 값이 여러 타입을 or 로 가질 수 있는 것이다. ~~*막나간다.*~~ \n\n두 개의 이상의 타입을 `|` 기호를 사용해 결합하면 유니언 타입이 된다. 이를 통해 변수가 함수 매개변수가 여러 타입 중 하나의 타입을 가질 수 있음을 나타낼 수 있다. \n\n```typescript\ntype StringOrNumber = string | number;\n\nlet value: StringOrNumber;\n\nvalue = 'Hello'; // 유효함\nvalue = 123;     // 유효함\n```\n\n### 장점\n\n#### 유연성\n\n다양한 타입을 하나의 변수에 할당할 수 있어 다양한 시나리오에 대응할 수 있다. \n\n#### 타입 안정성 보장\n\n유니언 타입을 사용하면 타입스크립트 컴파일러가 타입 안정성을 체크해준다. 즉, 할당된 값이 유니언 타입에 명시된 타입 중 하나와 일치하지 않으면 오류를 발생시킨다. \n\n#### 코드 간결성\n\n복잡한 조건에 대한 타입을 간결하게 표현할 수 있다. \n\n### 유니언 타입 사용 시 고려할 사항\n\n#### 타입 가드\n\n유니언 타입은 여러 타입을 허용하기 때문에 실행 시점에 정확한 타입을 확인하기 위해 타입 가드를 사용해야 한다. \n\n```typescript\nfunction process(value: StringOrNumber) {\n    if (typeof value === 'string') {\n        // value는 여기서 string 타입입니다.\n    } else {\n        // value는 여기서 number 타입입니다.\n    }\n}\n```\n\n#### 공통 필드 사용\n\n유니언 타입의 모든 구성원이 공통으로 가진 필드나 메소드에만 접근할 수 있다. 공통되지 않은 필드에 접근하려면 타입 가드를 사용해야 한다. \n\n#### 복잡한 유니언 타입\n\n유니언 타입이 복잡해질 수록 그 타입을 사용하는 코드는 더 복잡해질 수 있다. 따라서 타입을 너무 복잡하게 만들지 않도록 주의해야 한다. \n\n## 파이썬의 Union Type\n\n여기까지 알아보다가 파이썬에서도 왠지 비슷한게 있을 것 같다는 생각이 들어서 찾아봤고, 역시 있음을 알게 됐다. \n\n파이썬은 타입스크립트와 다르게 동적 타이핑 언어이지만 3.5 이상에서는 타입 힌팅을 사용하여 비슷한 기능을 구현할 수 있고, 3.10 이상에서는 `Union` 대신 `|` 연산자를 이용하여 유니언 타입을 정의할 수 있다. \n\n### 파이썬 3.9 이전\n\ntyping 모듈의 Union 을 사용하여 나타낼 수 있다. \n\n```python\nfrom typing import Union\n\ndef process(value: Union[str, int]):\n    print(value)\n```\n\n### 파이썬 3.10 이상\n\n```python\ndef process(value: str | int):\n    print(value)\n```\n\n### 타입스크립트의 Union Type과 다른 점\n\n파이썬은 동적 타이핑 언어이므로 이 제약은 파이썬의 실행에 영향을 미치지 않는다. 즉 런타임에서 타입 안전을 강제하지 않는다. \n\n단순히 개발자의 가독성 향상, IDE의 자동완성 지원, mypy등 정적 타입 체킹 도구를 위한 것이다. \n\n저렇게 해두어도 런타임에서 다른 타입의 값이 할당될 수 있음을 주의해야 한다. \n\n## 사담\n\n노션 API를 활용한 블로그 쉽게 하는걸 만들다가 타입이 복잡하게 선언되어있는걸 보고 혹시나 해서 찾아봤더니 내 예상이 맞았다. \n\nC로 개발을 시작했어서 그런지 처음에 자바스크립트를 접하고 아주 재미있고 자유도가 높다고 생각했는데, 갈 수록 나 자신에게 쇠사슬이 묶여있는게 오히려 편하다고 느낀다. \n\n프로젝트 크기가 커질수록 더 그렇게 느끼는 것 같다. \n\n물론 이런 기능을 잘 활용하면 더 편할때도 많긴 하다. 코드가 짧아지는 것도 맞고..\n\n결국 내 자신이 잘하면 아무일도 안생길거라는 생각도 든다. \n\n\n\n"},{"excerpt":"개인적으로 나스를 하나 운영하고 있다.  이것저것 설정해서 쓰고 있는데, 어느 날 갑자기 토렌트 다운로드가 용량이 없다고 작동하지 않았다.  용량은 다음의 명령어를 통해 확인할 수 있다.  외장하든 왕 큰 놈을 달아서 사용하고 있는데, 오드로이드다보니 메인 OS가 깔리는 드라이브는 15gb짜리 작은 용량이다.. 그 드라이브가 100퍼센트 꽉 차 있었다. …","fields":{"slug":"/우분투-용량-관리/"},"frontmatter":{"date":"January 28, 2024","title":"우분투 용량 관리","tags":["Ubuntu"]},"rawMarkdownBody":"개인적으로 나스를 하나 운영하고 있다. \n\n이것저것 설정해서 쓰고 있는데, 어느 날 갑자기 토렌트 다운로드가 용량이 없다고 작동하지 않았다. \n\n용량은 다음의 명령어를 통해 확인할 수 있다. \n\n```bash\nfurychick@odroid:/HDD2/myHomePage$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           200M   29M  171M  15% /run\n/dev/mmcblk1p2   15G   15G     0 100% /\ntmpfs           996M     0  996M   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/mmcblk1p1  128M   14M  114M  11% /media/boot\n/dev/sdb1       2.7T  656G  2.0T  26% /HDD2\n/dev/sda1        11T  8.3T  2.1T  80% /HDD\ntmpfs           200M     0  200M   0% /run/user/1000\n```\n\n외장하든 왕 큰 놈을 달아서 사용하고 있는데, 오드로이드다보니 메인 OS가 깔리는 드라이브는 15gb짜리 작은 용량이다.. 그 드라이브가 100퍼센트 꽉 차 있었다. \n\n다음의 명령어를 사용해 용량이 큰 디렉토리를 검색했다. 외장하드 경로는 제외하고 검색했다. \n\n```bash\nfurychick@odroid:/$ sudo du -h / --exclude=/HDD2 --exclude=/HDD | sort -hr | head -n 10\ndu: cannot read directory '/proc/sys/fs/binfmt_misc': No such device\ndu: cannot access '/proc/19035/task/19035/fd/3': No such file or directory\ndu: cannot access '/proc/19035/task/19035/fdinfo/3': No such file or directory\ndu: cannot access '/proc/19035/fd/4': No such file or directory\ndu: cannot access '/proc/19035/fdinfo/4': No such file or directory\n15G     /\n11G     /home/furychick\n11G     /home\n8.7G    /home/furychick/.forever\n2.2G    /var\n1.7G    /usr\n1.6G    /var/log\n1.5G    /var/log/journal/dc87f36fc06c441a85ff7269ba4d50fb\n1.5G    /var/log/journal\n1.3G    /usr/lib\n```\n\n아~ 개인 홈페이지를 돌리는 로그가.. 몇 년 째 사이트를 그냥 켜둔채로 방치하다 보니 9기가에 달하게 크게 자라 내 서버를 억누르고 있었다. \n\n당장 로그를 삭제해주었다. \n\n해피엔딩~\n\n\n\n"},{"excerpt":"열정 추가로 더 진행해버렸다. 정말로 여기까지만 하려고 한다. 오랜만에 탄력 받으니 계속 하게 되어버렸다.  구현 내용 콜아웃, 디바이더, 인용문, 코드, 번호 매기기 , 글머리 기호 목록 컨버터를 추가했다.  콜아웃, 디바이더 html로 구현했다. 인용문은 hr 태그로 처리했으며, 콜아웃은 div 태그를 사용했다. 다만 콜아웃은 스타일 처리가 필요하다.…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅5/"},"frontmatter":{"date":"January 28, 2024","title":"NotionAPI를 활용한 자동 포스팅(5)","tags":["Blogging","Notion-API","Typescript","Hobby"]},"rawMarkdownBody":"## 열정\n\n추가로 더 진행해버렸다. 정말로 여기까지만 하려고 한다. 오랜만에 탄력 받으니 계속 하게 되어버렸다. \n\n## 구현 내용\n\n콜아웃, 디바이더, 인용문, 코드, 번호 매기기 , 글머리 기호 목록 컨버터를 추가했다. \n\n### 콜아웃, 디바이더\n\nhtml로 구현했다. 인용문은 hr 태그로 처리했으며, 콜아웃은 div 태그를 사용했다. 다만 콜아웃은 스타일 처리가 필요하다. 지금은 대충 태그만 만들어놨다. 따라서 이건 미완성이나 다름없다. \n\n```typescript\nprivate convertCallout(calloutBlock: any): string {\n        const textContent = calloutBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n        const icon = calloutBlock.icon ? calloutBlock.icon.emoji : '';\n        const color = calloutBlock.color\n            ? calloutBlock.color\n            : 'gray_background';\n\n        return `\n    <div class=\"callout ${color}\">\n        ${icon} <span>${textContent}</span>\n    </div>\\n`;\n    }\n\nprivate convertDivider(): string {\n        return `<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\\n`;\n    }\n```\n\n### 인용문\n\n인용문은 있을 줄 몰랐는데, 마크다운에서 지원을 해서 쉽게 구현했다. \n\n무엇은 지원하고 무엇은 지원안하고, 기준을 명확히 모르겠긴하다. \n\n```typescript\nprivate convertQuote(quoteBlock: any): string {\n        const quoteText = quoteBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n\n        return `> ${quoteText}\\n\\n`;\n    }\n```\n\n### 번호 매기기, 글머리 기호 \n\n둘은 구현이 상당히 비슷했다. 마치 세트와 같다. 이걸 구현할 때에, 들여쓰기를 하려면 계층 개념이 필요해서 전체적인 코드 수정이 한 번 이뤄졌다. \n\n```typescript\nprivate formatListItemContent(listItemBlock: any): string {\n        return listItemBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n    }\n\n    private convertNumberedList(listItemBlock: any): string {\n        const listItemContent = this.formatListItemContent(listItemBlock);\n        const indent = ' '.repeat(this.indentLevel * 2);\n        return `${indent}1. ${listItemContent}\\n`;\n    }\n\n    private convertBulletedList(listItemBlock: any): string {\n        const listItemContent = this.formatListItemContent(listItemBlock);\n        const indent = ' '.repeat(this.indentLevel * 2);\n        return `${indent}- ${listItemContent}\\n`;\n    }\n```\n\n띄어쓰기 네 번으로 계층을 구분한다. \n\n### 코드 \n\n코드도 쉬웠다. 마크다운에서 지원하는 것은 기본적으로 구현이 쉽다. \n\n```typescript\nprivate convertCode(codeBlock: any): string {\n        const codeText = codeBlock.rich_text\n            .map((textElement: any) => textElement.plain_text)\n            .join('');\n\n        const language = codeBlock.language || '';\n\n        return `\\`\\`\\`${language}\\n${codeText}\\n\\`\\`\\`\\n\\n`;\n    }\n```\n\n### to_do\n\nto_do도 간단하게 구현했다. 마크다운에서는 체크박스인데, 노션에서는 할 일 목록이다.. 그래서 체크박스를 체크하면 스타일이 추가되는데.. 이것까진 따로 구현하지 않았다. \n\n```typescript\nprivate convertToDo(toDoBlock: any): string {\n        const quoteText = toDoBlock.rich_text\n            .map((textElement: any) => this.formatTextElement(textElement))\n            .join('');\n        let pre = '- [ ]';\n        if (toDoBlock.checked == true) {\n            pre = '- [x]';\n        }\n        return `${pre} ${quoteText}\\n\\n`;\n    }\n```\n\n## 정리 \n\n### 작업 완료 블록\n\n일단 1차적으로 작업이 완료되었으며, 완료된 지원하는 블록 타입은 다음과 같다. \n\n- paragraph\n\n- heading_1, heading_2, heading_3\n\n- bookmark\n\n- link_to_page\n\n- image\n\n- callout\n\n- divider\n\n- quote\n\n- code\n\n- numbered_list_item\n\n- bulleted_list_item\n\n- to_do\n\n### 작업 예정 블록\n\n#### table, table-row\n\n꼭 필요해보이지만, 구현을 위해서는 현재 구현된 블록의 재귀 호출 구조를 바꿔야 한다. 상대적으로 까다로워 오늘 작업하진 않겠다. \n\n### 작업 미정 블록\n\n#### column (n개의 열로 구성된 블록 생성)\n\n가로로 여러 블록을 놓는 기능이다. 굳이 필요한가? 싶기도 하고 또 구현이 꽤나 까다로워 보인다. 다만 table, table-row를 작업할 때에 호출 구조를 잘 짜놓는다면 또 별 노력 없이 잘 될 것 같기도 한데, 마크다운에 원래 가로를 나누는 기능이 있는지는 또 모르겠다. 그래서 미정이다. \n\n#### 미디어 관련 블록\n\n미디어 관련 기능도 지금보니 노션에 있다. 이건.. 할만하지 않을까? 고려해보겠다. 아직 자세히 살펴보지 못했다. \n\n### 미구현 확정 블록\n\n#### 토글\n\n토글은 미구현 확정이다. 토글은 마크다운에서 지원하지 않고, 만약 하려면 부트스트랩의 collapse 같은 기능을 직접 구현해야 할 것 같다. 이건 쉽지 않다. \n\n#### 임베드 관련 블록\n\n기각이다. 블로그 글에 임베드는 쓰지 말자. \n\n#### 고급 블록\n\n지금보니 토글도 고급에 들어있다. 마크다운 기본 기능이 아닌 것들은 기본적으로 구현에 한계가 있다. 다시 보니 column도 고급에 있다. \n\n나중에 추후 시간이 되면 만들만한 것들은 한 번 고려해보겠다. \n\n#### 데이터베이스 관련 블록\n\n웹사이트에 넣을 수 있는 기능이 (아마도) 아니다. 기각이다. \n\n## 다음 작업 예정은? \n\n이제 배포 자동화를 목표로 해야 한다. 자동화까지 해둬야, 이 프로젝트를 만든 목적 달성이 아닐까? 얼른 자동화도 하고 싶다..\n\n자동화의 자세한 로직은 다음번에 설계 하도록 하겠다.. \n\n"},{"excerpt":"지난 이야기 NotionAPI를 활용한 자동 포스팅(3) 간만에 복귀를 했다. 별건 아니고.. 그냥 복습을 했다.  오늘의 작업 block.ts 나머지 작업 지난번에 block.ts를 미완성 된 상태로 두었다. 타입 검사에서 걸린 상태로 일단 두었고, Union Type이라는 것이 있다는 것을 알게 되었다. 해당 내용에 대해서는 따로 정리 해두었다.  U…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅4/"},"frontmatter":{"date":"January 27, 2024","title":"NotionAPI를 활용한 자동 포스팅(4)","tags":["Blogging","Notion-API","Typescript","Hobby"]},"rawMarkdownBody":"## 지난 이야기\n\n[NotionAPI를 활용한 자동 포스팅(3)](https://sharknia.github.io/NotionAPI를-활용한-자동-포스팅3)\n\n간만에 복귀를 했다. 별건 아니고.. 그냥 복습을 했다. \n\n## 오늘의 작업\n\n### block.ts 나머지 작업\n\n지난번에 block.ts를 미완성 된 상태로 두었다. 타입 검사에서 걸린 상태로 일단 두었고,\n\n```typescript\nexport type GetBlockResponse = PartialBlockObjectResponse | BlockObjectResponse;\n```\n\nUnion Type이라는 것이 있다는 것을 알게 되었다. 해당 내용에 대해서는 따로 정리 해두었다. \n\n[Union Type](https://sharknia.github.io/Union-Type)  \n\n### 로그 강화\n\n로그도 작업 순서에 맞게 찍히도록 강화했다. json 형태의 데이터여서 [object Object] 와 같이 찍히던 것도 제대로 내용물이 출력되도록 수정했다. 다음과 같은 코드로 json 형태의 데이터도 이쁘게 출력할 수 있다. \n\n```typescript\nconsole.log(\n            `convertParagraph - paragraph : ${JSON.stringify(\n                paragraph,\n                null,\n                2,\n            )}`,\n        );\n```\n\n### ConvertParagraph 완성\n\nparagraph는 노션 블록 타입 주으이 하나로, 텍스트의 기본 단위이다. 텍스트를 입력할 때 기본적으로 생성되는 블록 유형이다. \n\n이 블록 안에 들어있는 내용을 마크다운으로 바꿔주는 메소드를 완성했다. \n\n원본 노션의 내용은 다음과 같다. 일부러 여러가지 케이스를 집어넣었다. \n\n![](image1.png)\n이 paragraph블록의 데이터 형태는 다음과 같다. \n\n#### <u>노</u><u>~~션이~~</u>  **좋습*****니다***..\n\n```json\n{\n  \"rich_text\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"노\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": true,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"노\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"션이\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": true,\n        \"underline\": true,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"션이\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \" \",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \" \",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"좋습\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": true,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"좋습\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"니다\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": true,\n        \"italic\": true,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"니다\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"..\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"..\",\n      \"href\": null\n    }\n  ],\n  \"color\": \"default\"\n}\n```\n\n#### `정말로`.. <span style=\"color: pink;\">좋아합니다</span>.. \n\n```json\n{\n  \"rich_text\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"정말로\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": true,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \"정말로\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \".. \",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \".. \",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \"좋아합니다\",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"pink\"\n      },\n      \"plain_text\": \"좋아합니다\",\n      \"href\": null\n    },\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"content\": \".. \",\n        \"link\": null\n      },\n      \"annotations\": {\n        \"bold\": false,\n        \"italic\": false,\n        \"strikethrough\": false,\n        \"underline\": false,\n        \"code\": false,\n        \"color\": \"default\"\n      },\n      \"plain_text\": \".. \",\n      \"href\": null\n    }\n  ],\n  \"color\": \"default\"\n}\n```\n\n이 중에서 밑줄과 색상은 기본적으로 마크다운에서 지원하지 않는 기능이므로 html 태그를 사용했다. 코드는 다음과 같다. \n\n```typescript\nprivate convertParagraph(paragraph: any): string {\n        let markdown = '';\n\n        for (const textElement of paragraph.rich_text) {\n            let textContent = textElement.plain_text;\n\n            // 텍스트 스타일링 처리\n            if (textElement.annotations.bold) {\n                textContent = `**${textContent}**`;\n            }\n            if (textElement.annotations.italic) {\n                textContent = `*${textContent}*`;\n            }\n            if (textElement.annotations.strikethrough) {\n                textContent = `~~${textContent}~~`;\n            }\n            if (textElement.annotations.code) {\n                textContent = `\\`${textContent}\\``;\n            }\n            if (textElement.annotations.underline) {\n                // 마크다운은 기본적으로 밑줄을 지원하지 않으므로, HTML 태그 사용\n                textContent = `<u>${textContent}</u>`;\n            }\n            // 색상 처리 (HTML 스타일을 사용)\n            if (textElement.annotations.color !== 'default') {\n                textContent = `<span style=\"color: ${textElement.annotations.color};\">${textContent}</span>`;\n            }\n            if (textElement.href) {\n                textContent = `[${textContent}](${textElement.href})`;\n            }\n            markdown += textContent;\n        }\n        console.log(\n            `convertParagraph - paragraph : ${JSON.stringify(\n                paragraph,\n                null,\n                2,\n            )}`,\n        );\n        markdown = markdown + '\\n\\n';\n        console.log(`convertParagraph - markdown : ${markdown}`);\n        return markdown; // 문단 끝에 줄바꿈 추가\n    }\n```\n\n각 문장은 각각 다음과 같이 변환된다. \n\n```bash\nconvertParagraph - markdown : <u>노</u><u>~~션이~~</u> **좋습*****니다***..\nconvertParagraph - markdown : `정말로`.. <span style=\"color: pink;\">좋아합니다</span>..\n```\n\n이를 vscode의 마크다운 편집 기능을 통해서 확인하면 다음과 같이 정상적으로 출력되는 것을 확인할 수 있다. \n\n![](image2.png)\n### 마크다운 파일 저장 기능 추가 \n\npage.ts에 block들로부터 받아온 마크다운 내용과 properties에 저장된 내용들을 합해 마크다운 파일로 저장하는 메소드를 완성했다. \n\n```typescript\npublic async printMarkDown() {\n        //contentMarkdown과 properties의 내용을 마크다운 파일로 저장한다.\n        try {\n            // 파일 이름 설정 (페이지 제목으로)\n            const filename = `${this.pageTitle}.md`;\n\n            // 마크다운 메타데이터 생성\n            const markdownMetadata = this.formatMarkdownMetadata();\n\n            // 마크다운 메타데이터와 contentMarkdown을 결합\n            const fullMarkdown = `${markdownMetadata}${this.contentMarkdown}`;\n\n            // 디렉토리 생성 (이미 존재하는 경우 오류를 무시함)\n            await fs.mkdir(this.pageUrl ?? '', { recursive: true });\n            const filePath = join(this.pageUrl ?? '', 'index.md');\n            // 결합된 내용을 파일에 쓰기 (이미 존재하는 경우 덮어쓰기)\n            await fs.writeFile(filePath, fullMarkdown);\n            console.log(`[page.ts] Markdown 파일 저장됨: ${filePath}`);\n        } catch (error) {\n            console.error(`[page.ts] 파일 저장 중 오류 발생: ${error}`);\n        }\n    }\n\n    private formatMarkdownMetadata(): string {\n        // properties를 마크다운 메타데이터로 변환\n        const metadata = [\n            '---',\n            `title: \"${this.properties?.title ?? ''}\"`,\n            `description: \"${this.properties?.description ?? ''}\"`,\n            `date: ${this.properties?.date ?? ''}`,\n            `update: ${this.properties?.update ?? ''}`,\n            // tags가 배열인 경우에만 join 메소드를 호출\n            `tags:\\n  - ${\n                Array.isArray(this.properties?.tags)\n                    ? this.properties.tags.join('\\n  - ')\n                    : ''\n            }`,\n            `series: \"${this.properties?.series ?? ''}\"`,\n            '---',\n            '',\n        ].join('\\n');\n\n        return metadata;\n    }\n```\n\ngithub.io의 블로그 형식에 맞춰서 properties를 바꿔주고, 블로그 형식에 맞는 위치에 일단 마크다운 파일을 저장하게끔 해주었다. 아마, 추후 배포 방식에 따라 해당 위치는 바뀔 수 있을 것이다. 아직 정확하게 어떤 방식으로 블로그를 배포할 것인지까지는 고려하지 않았다. \n\n위 코드를 포함해서 실행하면, \n\n![](image3.png)\n이렇게 저장이 되고, \n\n![](image4.png)\n이렇게 깔끔하게 저장이 된다. 이제 앞으로는 각 블록 타입들에 대한 변환을 추가하면 된다!\n\n### BlockObjectResponse 분석\n\n실질적으로 컨텐츠들은 BlockObjectResponse 타입들로 이루어져있다. BlockObjectResponse는 다음의 Union Type이다. \n\n```typescript\nexport type BlockObjectResponse = ParagraphBlockObjectResponse | Heading1BlockObjectResponse | Heading2BlockObjectResponse | Heading3BlockObjectResponse | BulletedListItemBlockObjectResponse | NumberedListItemBlockObjectResponse | QuoteBlockObjectResponse | ToDoBlockObjectResponse | ToggleBlockObjectResponse | TemplateBlockObjectResponse | SyncedBlockBlockObjectResponse | ChildPageBlockObjectResponse | ChildDatabaseBlockObjectResponse | EquationBlockObjectResponse | CodeBlockObjectResponse | CalloutBlockObjectResponse | DividerBlockObjectResponse | BreadcrumbBlockObjectResponse | TableOfContentsBlockObjectResponse | ColumnListBlockObjectResponse | ColumnBlockObjectResponse | LinkToPageBlockObjectResponse | TableBlockObjectResponse | TableRowBlockObjectResponse | EmbedBlockObjectResponse | BookmarkBlockObjectResponse | ImageBlockObjectResponse | VideoBlockObjectResponse | PdfBlockObjectResponse | FileBlockObjectResponse | AudioBlockObjectResponse | LinkPreviewBlockObjectResponse | UnsupportedBlockObjectResponse;\n```\n\n~~너무 많다~~\n\n일단, 많이 쓸 것 같은 블록들을 예제 파일로 만들고 해당 노션 파일들을 불러와보고 어떤 타입들을 사용하는지 살펴보기로 했다. 나머지는 모르겠다 아직은 그냥 미지원이다. \n\n![](image5.png)\n이것들을 넘는것은 내가 아직은 노션에 쓸 것 같지가 않다. 따라서 위의 블록들을 중점적으로 먼저 변환하기로 하자. \n\n다음의 녀석들이 그 녀석들이다. \n\n#### heading_ 시리즈\n\n제목1, 제목2, 제목3 들이다. heading_1부터 heading_3까지를 노션에서는 사용할 수 있는데, 각각 h2, h3, h4로 변환하면 될 것 같다. 이건 쉬울 것 같은 예감이 든다. \n\n#### bookmark\n\n북마크는 간단할 것 같지만, 간단하지 않은 점이 있다. 노션에는 페이지 링크 기능이 있는데, 이것도 가능하면 (페이지 링크도 어차피 내 블로그 글 일테니까, 아니 사용자가 그렇게 사용해야 한다. ) 블로그 글의 링크로 변환하고 싶다. 이게 가능할까? 이건 쉽지 않을 수도 있을 것 같은 생각이 지금은 든다. 생성된 페이지 링크와 페이지 아이디가 다르다면 근본적으로 불가능한 일이다. \n\n#### code\n\n코드 블록이다. 이건 쉬울 것 같다. \n\n#### table, table_row\n\n마크다운은 표 그리기가 까다롭다. 그래도 어떻게든 할 수 있지 않을까? \n\n#### bulleted_list_item\n\n글머리 기호이다. 이건.. 이건 쉽지 않을까? \n\n#### numbered_list_item\n\n번호 이것도 쉽지 않을까?? \n\n#### toggle\n\n토글도 어떻게든 되지 않을까? 다른것보단 복잡하겠지만 특별한 건 없을 것 같다. \n\n#### quote, divider, callout\n\n이 놈들도 특별한 어려움은 예상되지 않는다. 그냥 스타일이나 이쁘게 주면 될 것 같다. \n\n결국, 표나 북마크를 제외하면 나머지는 그냥 단순 노가다로 예상이 된다. \n\n### heading 시리즈 컨버터 생성\n\n이건 간단했다. 다만, 컨버팅 관련 내용이 길어질 것 같아 해당 역할을 하는 클래스 MarkdownConverter을 만들어서 이걸 이용하기로 했다. 이 클래스는 블록의 내용을 받아 markdown 문자열로 변환해서 리턴하는 역할을 한다. \n\nparagraph와 heading 시리즈에 겹치는 내용이 많다. 사실상 pre 태그만 다르다. 따라서 중복되는 내용을 formatTextElement 메소드로 분리했다. 아마 다른 타입에 대해서도 사용할 수 있을 것 같은 예감이 든다. \n\n완성된 코드는 다음과 같다. \n\n#### 코드 \n\n```typescript\nimport { BlockObjectResponse } from '@notionhq/client/build/src/api-endpoints';\n\nexport class MarkdownConverter {\n    private block: BlockObjectResponse;\n\n    private constructor(block: BlockObjectResponse) {\n        this.block = block;\n    }\n\n    public static async create(block: BlockObjectResponse): Promise<string> {\n        const converter: MarkdownConverter = new MarkdownConverter(block);\n        const result = await converter.makeMarkDown();\n        return result;\n    }\n\n    private async makeMarkDown(): Promise<string> {\n        let block = this.block;\n        let markdown: string = '';\n\n        console.log(\n            `[markdownConverter.ts] makeMarkDown : ${\n                block.type\n            } : ${JSON.stringify(block, null, 2)}`,\n        );\n\n        switch (block.type) {\n            // 텍스트의 기본 단위,텍스트를 입력할 때 기본적으로 생성되는 블록 유형\n            case 'paragraph':\n                markdown += this.convertParagraph(block.paragraph);\n                break;\n            case 'heading_1':\n                markdown += this.convertHeading(block.heading_1, 1);\n                break;\n            case 'heading_2':\n                markdown += this.convertHeading(block.heading_2, 2);\n                break;\n            case 'heading_3':\n                markdown += this.convertHeading(block.heading_3, 3);\n            // 다른 블록 유형에 대한 처리를 여기에 추가...\n            default:\n                console.warn(\n                    `[markdownConverter.ts] makeMarkDown : Unsupported block type - ${block.type}`,\n                );\n        }\n        return markdown;\n    }\n\n    private convertParagraph(paragraph: any): string {\n        let markdown = '';\n        for (const textElement of paragraph.rich_text) {\n            markdown += this.formatTextElement(textElement);\n        }\n        return markdown + '\\n\\n';\n    }\n\n    private convertHeading(heading: any, level: number): string {\n        let markdown = '';\n        const prefix = '#'.repeat(level + 1) + ' ';\n        for (const textElement of heading.rich_text) {\n            markdown += this.formatTextElement(textElement);\n        }\n        return prefix + markdown + '\\n\\n';\n    }\n\n    private formatTextElement(textElement: any): string {\n        let textContent = textElement.plain_text;\n\n        // 텍스트 스타일링 처리\n        if (textElement.annotations.bold) {\n            textContent = `**${textContent}**`;\n        }\n        if (textElement.annotations.italic) {\n            textContent = `*${textContent}*`;\n        }\n        if (textElement.annotations.strikethrough) {\n            textContent = `~~${textContent}~~`;\n        }\n        if (textElement.annotations.code) {\n            textContent = `\\`${textContent}\\``;\n        }\n        if (textElement.annotations.underline) {\n            textContent = `<u>${textContent}</u>`;\n        }\n        if (textElement.annotations.color !== 'default') {\n            textContent = `<span style=\"color: ${textElement.annotations.color};\">${textContent}</span>`;\n        }\n        if (textElement.href) {\n            textContent = `[${textContent}](${textElement.href})`;\n        }\n\n        return textContent;\n    }\n}\n```\n\n### convertLinkToPage 생성\n\n아까 북마크에 대해서 고려할 때, 페이지 링크가 어렵지 않을까? 고민했었는데 해당 문제는 해결이 가능했다. link_to_page 타입에서 페이지 아이디를 제공하고 있었고, 페이지 아이디를 API를 통해서 호출을 하면 해당 데이터를 가져오는 것이 가능했다. 이 경우에는 마크다운 파일은 필요없고 URL만 필요하므로, Page 클래스에 간단한 정보만 가져오는 메소드를 만들고 해당 메소드를 활용해 북마크를 생성해주었다. 또, 설정 파일에 블로그의 주소를 설정하도록 했다. \n\n```typescript\nexport class Page {\n    private pageId: string;\n    private notion: Client;\n\n    public properties?: Record<string, PropertyValue>;\n    public pageTitle?: string;\n    public pageUrl?: string;\n    public contentMarkdown?: string;\n\n    private constructor(pageId: string, notion: Client) {\n        this.pageId = pageId;\n        this.notion = notion;\n    }\n\n    private async init(page: Page) {\n        const properties = await page.getProperties();\n        page.properties = await page.extractDataFromProperties(properties);\n        page.pageUrl = `${\n            page.pageTitle\n                ?.trim()\n                .replace(/[^가-힣\\w\\-_~]/g, '') // 한글, 영어, 숫자, '-', '_', '.', '~'를 제외한 모든 문자 제거\n                .replace(/\\s+/g, '-') ?? // 공백을 하이픈으로 치환\n            ''\n        }`;\n    }\n\n\t\t.... 생략 ....\n\n    public static async getSimpleData(pageId: string) {\n        const notionApi: NotionAPI = await NotionAPI.create();\n        const page: Page = new Page(pageId, notionApi.client);\n        await page.init(page);\n        return {\n            pageTitle: page.pageTitle ?? '',\n            pageUrl: page.pageUrl ?? '',\n        };\n    }\n\n\t\t.... 생략 ....\n}\n```\n\n기존의 생성자 create 메소드와 중복되는 부분을 init 메소드로 분리하고, getSimpleData에서는 title, url만 리턴하도록 수정해주었다. \n\n```typescript\nprivate async convertLinkToPage(linkToPage: any): Promise<string> {\n        try {\n            const envConfig = EnvConfig.create(); // EnvConfig 인스턴스 생성\n            const blogUrl = envConfig.blogUrl; // blogUrl 가져오기\n            const pageId = linkToPage.page_id;\n            const pageData = await Page.getSimpleData(pageId);\n\n            const pageTitle = pageData.pageTitle;\n            const pageUrl = pageData.pageUrl;\n\n            return `[${pageTitle}](${blogUrl}/${pageUrl})\\n\\n`;\n        } catch (error) {\n            console.error('Error converting link_to_page:', error);\n            return '';\n        }\n    }\n```\n\n그리고 이렇게 해당 값과 설정값을 이용해 북마크를 연결할 주소를 만들었다. \n\n### Page 멘션 기능 대응\n\nPage Mention의 경우에는 특이하게 paragraph에 링크 관련 정보가 담겨서 온다. 해당 경우에 대응하기 위해 convertParagraph를 다음과 같이 수정해주었다. \n\n```typescript\nprivate async convertParagraph(paragraph: any): Promise<string> {\n        let markdown = '';\n        for (const textElement of paragraph.rich_text) {\n            if (\n                textElement.type === 'mention' &&\n                textElement.mention.type === 'page'\n            ) {\n                // mention 타입이고, page를 참조하는 경우\n                markdown += await this.convertMentionToPageLink(\n                    textElement.mention.page.id,\n                );\n            } else {\n                // 기타 텍스트 요소\n                markdown += this.formatTextElement(textElement);\n            }\n        }\n        return markdown + '\\n\\n';\n    }\n\n    private async convertMentionToPageLink(pageId: string): Promise<string> {\n        try {\n            const envConfig = EnvConfig.create(); // EnvConfig 인스턴스 생성\n            const blogUrl = envConfig.blogUrl; // blogUrl 가져오기\n            const pageData = await Page.getSimpleData(pageId);\n\n            const pageTitle = pageData.pageTitle;\n            const pageUrl = pageData.pageUrl;\n\n            return `[${pageTitle}](${blogUrl}/${pageUrl})`;\n        } catch (error) {\n            console.error('Error converting mention to page link:', error);\n            return '';\n        }\n    }\n```\n\n이로써 까다로울 것 같았던 두 가지에 대한 대응이 끝났고, 나머지는 노가다만 남은 것 같다!\n\n### imageConverter 구현\n\n이미지도 구현이 쉬웠다. 마크다운과 같은 폴더에 이미지를 다운로드 받고(amazone 저장소 주소가 API에서 제공된다. ) 캡션 형식으로 넣어주면 된다. \n\n```typescript\nprivate async convertImage(imageBlock: any): Promise<string> {\n        try {\n            const imageUrl = imageBlock.file.url;\n            const imageCaption =\n                imageBlock.caption.length > 0\n                    ? this.formatRichText(imageBlock.caption)\n                    : '';\n\n            // 이미지 이름을 순서대로 할당 (image1, image2, ...)\n            const imageName = `image${++MarkdownConverter.imageCounter}.png`;\n            const imageDownDir = `/${this.pageUrl}/${imageName}`;\n            const imagePath = join('contents/post', imageDownDir);\n\n            // 이미지 다운로드 및 로컬에 저장\n            const response = await axios.get(imageUrl, {\n                responseType: 'arraybuffer',\n            });\n            await fs.writeFile(imagePath, response.data);\n\n            // 마크다운 이미지 문자열 생성\n            let markdownImage = `![${imageCaption}](${imageName})\\n`;\n            if (imageCaption) {\n                markdownImage += `<p style=\"text-align:center;\"><small>${imageCaption}</small></p>\\n`;\n            }\n\n            return markdownImage;\n        } catch (error) {\n            console.error('Error converting image:', error);\n            return '';\n        }\n    }\n```\n\n단, 이 클래스는 상당히 분리되어있는 클래스여서 따로 저장될 곳의 디렉토리 명을 알 수 있는 방법이 없었다.. 어쩔 수 없이 Page클래스부터 계속해서 디렉토리 명을 던져줬다.. 깔끔하지 않다.. \n\n또, 저장될 디렉토리 (contents/post) 도 설정의 영역에 넣어야 할 것 같다. \n\n## 오늘의 마무리 \n\n오늘은 이 정도면 될 것 같다. 이미 핵심 부분은 모두 진행이 됐고, 앞으로는\n\n### 남은 타입별 구현\n\n콜아웃, devider, quote는 html, css의 영역으로 생각된다. 이건 추후 따로 정적 파일에 html, css를 추가하거나 해야 할 것 같다. 자세한 방법은 나중에 생각해보려고 한다. 오늘 다섯시간? 정도를 쉬지 않고 했더니 집중력이 많이 떨어진 게 느껴진다. \n\n토글도 의외로 까다로울지도? 뭐 초반엔 내가 그냥 토글을 노션에 안쓰면 되는거 아닐까? \n\n까다로운 부분은 많이 구현된 것 같다. 빠르면 내일, 열심히 한다면 이번주 안에 직접 사용을 해볼 수 있을 것 같다.\n\n\n\n"},{"excerpt":"지난시간 https://sharknia.github.io/Notion-Api-2/ 문제점 아무것도.. 기억이 나지 않는다.. 지난날의 나는 무엇이었나? 5개월만의 복귀가 이렇게 어렵다. 이래서 사람은 꾸준해야 한다.  잡설 최근 업무에 약~간의 여유가 생기면서 IDE를 파이참에서 vs code로 갈아탔다. 파이썬만 할 때에는 파이참이 유리한 것이 사실이지…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅3/"},"frontmatter":{"date":"January 26, 2024","title":"NotionAPI를 활용한 자동 포스팅(3)","tags":["Blogging","Notion-API","Hobby"]},"rawMarkdownBody":"## 지난시간\n\n[https://sharknia.github.io/Notion-Api-2/](https://sharknia.github.io/Notion-Api-2/)\n\n## 문제점\n\n아무것도.. 기억이 나지 않는다.. 지난날의 나는 무엇이었나? 5개월만의 복귀가 이렇게 어렵다. 이래서 사람은 꾸준해야 한다. \n\n## 잡설\n\n최근 업무에 약~간의 여유가 생기면서 IDE를 파이참에서 vs code로 갈아탔다. 파이썬만 할 때에는 파이참이 유리한 것이 사실이지만(심지어 속도도 파이참이 더 빠르다고 느꼈다. ) 추후 여러 언어를 다루게 될 경우 vs code가 유리한 점이 있다고 생각되어 갈아탔고, 환경 세팅을 다시 했다. \n\nformatter나 기타 여러가지 등등.. 그리고 타입스크립트 코드를 오랜만에 보면서 여기에도 formatter 설정을 추가해주었다. \n\n## 설계 되새김질\n\n코드를 다시 읽는데에만 시간을 꽤 투자했다. 예전의 나는 코드를 꽤 잘 짠 것 같다. \n\n블로그 글을 복습하지 않고 코드를 읽고 설계를 다시 했는데 오늘의 나는 예전의 나와 의견이 똑같다. ~~(진작 복습할걸)~~\n\n### 그래서 어떻게 할 것이냐? \n\n노션은 블록 기반의 구조로 이루어져있다. 그래서 나는 노션의 메인 데이터베이스를 블로그 홈이라고 가정하고, 그 안에 있는 블록들의 리스트는 페이지라고 정의하여, 페이지의 양식이나 속성은 고정해두려고 한다. 그리고 비로소 페이지안의 컨텐츠, 블록들을 마크다운으로 변환하려고 한다. \n\n그래서, Page.ts에서 페이지 들을 관리하고, Block.ts에서 블록들을 관리하려고 한다. 블록들은 각자 하위 블록을 다시 가질 수 있는 재귀적 형태를 가진다. \n\n콘텐츠들은 문단, 이미지, 리스트 등 여러 타입을 가지므로 경우에 따라서 block 클래스를 상속한 하위 클래스가 생길수도 있겠다. \n\n각자 블록들은 자신의 내용들을 마크다운으로 변환해서 상위 블록에게 전달하는 메소드를 갖는다. 가장 하위 블록부터 전달된 변경된 마크다운 내용들은 상위로 타고 올라가 결국 페이지 클래스에 전달된다. \n\n페이지 클래스는 이 마크다운들을 모아 파일로 저장한다. \n\n계획은 완벽해 보인다. \n\n하지만, 타입이 아주 많고 이걸 마크다운으로 변환하는 작업은 노가다 그 자체일것이다.. \n\n그래서 일단은 제한된 타입들에 대해서만 변환을 지원하려고 한다. \n\n## 그래서 오늘 작업은? \n\n일단 코드를 읽었고, 분석했고, 설계를 굳이 다시 하고 예전의 나와 의견이 같다는 점을 뒤늦게 확인했으며, \n\nblock.ts의 초안을 작성했다. \n\n```typescript\nimport { Client } from '@notionhq/client';\nimport { BlockObjectResponse } from '@notionhq/client/build/src/api-endpoints';\nimport { GetBlockResponse } from '@notionhq/client/build/src/api-endpoints';\n\nexport class Block {\n    private notion: Client;\n    private blockId: string;\n    private blockData?: GetBlockResponse;\n\n    constructor(notion: Client, blockId: string) {\n        this.notion = notion;\n        this.blockId = blockId;\n    }\n\n    public async getMarkdown(): Promise<string> {\n        this.blockData = await this.fetchBlockData();\n        return await this.processBlock(this.blockData);\n    }\n\n    private async fetchBlockData(): Promise<GetBlockResponse> {\n        return await this.notion.blocks.retrieve({ block_id: this.blockId });\n    }\n\n    private async processBlock(block: BlockObjectResponse): Promise<string> {\n        let markdown = '';\n\n        switch (block.type) {\n            case 'paragraph':\n                markdown += this.convertParagraph(block.paragraph);\n                break;\n            case 'heading_1':\n                markdown += `# ${block.heading_1.rich_text\n                    .map((t) => t.plain_text)\n                    .join('')}\\n\\n`;\n                break;\n            // 다른 블록 유형에 대한 처리를 여기에 추가...\n            default:\n                console.warn(`Unsupported block type: ${block.type}`);\n        }\n\n        // 하위 블록 처리 (재귀적)\n        if (block.has_children) {\n            markdown += await this.processChildBlocks(block.id);\n        }\n\n        return markdown;\n    }\n\n    private async processChildBlocks(blockId: string): Promise<string> {\n        const children = await this.notion.blocks.children.list({\n            block_id: blockId,\n        });\n        let markdown = '';\n        for (const child of children.results) {\n            markdown += await this.processBlock(child as BlockObjectResponse);\n        }\n        return markdown;\n    }\n\n    private convertParagraph(paragraph: any): string {\n        console.log('paragraph:' + paragraph);\n        return paragraph.rich_text;\n        // return paragraph.text.map((t) => t.plain_text).join('') + '\\n\\n';\n    }\n\n    // 기타 블록 유형에 대한 변환 함수를 여기에 추가...\n}\n```\n\n완전히 지극히 초안 그 자체이다. 기초적인 형태만 잡았다. 아직 리턴값에 대한 이해가 충분하지 않아, 해당 부분에 대한 조정이 필요하다. (심지어 오류가 나는 상태이다)\n\n블록의 타입이 모두 정의가 되어있으므로, 해당 타입에 따른 각 클래스를 따로 생성할 필요도 느낀다. 모두 여기에 집중된다면 코드가 너무 길어질 것 같다. \n\n## 결론\n\n결국 오늘은 한 일이 별로 없다.. 복귀에 의의를 두자. \n\n"},{"excerpt":"문제 기존에는 프론트와 주고 받을 데이터 모델을 정의하는데에 dataclass를 사용하고 있었다. 그러다가 FastAPI로 넘어오면서 Pydantic model을 도입했다.  dataclass로는 해결할 수 없는 문제가 있었다. 프론트엔드 개발에서는 주로 카멜케이스를 사용하기 때문에, 파이썬에서는 주로 스네이크 케이스를 사용하는데 (네이밍 규칙(namin…","fields":{"slug":"/Pydantic-Model의-응용/"},"frontmatter":{"date":"January 26, 2024","title":"Pydantic Model의 응용","tags":["Work","Python"]},"rawMarkdownBody":"## 문제\n\n기존에는 프론트와 주고 받을 데이터 모델을 정의하는데에 [dataclass](https://sharknia.github.io/dataclass)를 사용하고 있었다. 그러다가 FastAPI로 넘어오면서 [Pydantic model](https://sharknia.github.io/Pydantic-모델)을 도입했다. \n\ndataclass로는 해결할 수 없는 문제가 있었다. 프론트엔드 개발에서는 주로 카멜케이스를 사용하기 때문에, 파이썬에서는 주로 스네이크 케이스를 사용하는데 ([네이밍 규칙(naming conventions)](https://sharknia.github.io/네이밍-규칙naming-conventions)), 프론트로 나갈 변수 명을 카멜케이스로 통일 하기 위해 어쩔 수 없이 백엔드의 코드 컨벤션을 해치면서 Response로 나갈 모델의 속성만 Camel 케이스로 선언하고 있었다. \n\n하지만 난 이런게 너무 싫다. \n\n## 해결\n\nPydantic Model을 이를 해결하기 위한 옵션이 있다. \n\n### alias_generator\n\n필드에 대한 별칭을 자동으로 생성하는 함수를 지정한다. 주로 모델 필드 이름을 snake 케이스에서 camel케이스로 매핑할 때 사용된다. \n\n### populator_by_name\n\n이 옵션을 True로 설정하면 Pydantic은 별칭 대신 모델 필드의 원래 이름을 사용하여 인스턴스를 생성하고 할당한다. 이는 필드의 별칭과 원래 이름을 혼용하여 사용할 수 있게 한다. 기본적으로는 False로 되어있어 만약 이 설정을 수정하지 않는다면 속성을 snake case로 정의했어도 인스턴스 생성시 변수를 camelCase로 넣어줘야 하는 불상사가 일어난다. \n\n이 설정을 True로 해두면 모델 인스턴스가 직렬화, 역직렬화 되는 과정에서만 별칭을 이용하게 된다. \n\n### 두 옵션을 엮어서..\n\n이 두 옵션을 엮어서 Request로 받을 경우, Response로 내려갈 경우에만 별칭을 사용하고, 별칭은 camelCase에 따라 생성되도록 설정을 할 수 있다. \n\n그리고 이 설정값을 넣은 BastDtoModel 클래스를 생성해서 앞으로 우리가 데이터 통신에 사용할 PydanticModel은 모두 이 클래스를 상속받아 구현하기로 했다. \n\n<details>\n<summary>base_dto_model.py</summary>\n\n```python\nfrom pydantic import BaseModel\n\n\ndef to_camel(string: str) -> str:\n    components = string.split('_')\n    return components[0] + ''.join(x.title() for x in components[1:])\n\n\nclass BaseDtoModel(BaseModel):\n    class Config:\n        alias_generator = to_camel\n        populate_by_name = True\n```\n\n\n</details>\n\nalias_generator에는 to_camel 메소드를 정의해주었다. to_camel 메소드는 snake_case로 작성된 문자열을 camelCase로 변환해주는 함수이다. \n\n\n\n"},{"excerpt":"개발중인 서비스에서 Postgresql을 적용한지 이제 한달 조금 더 지났다.  아직 DB 마이그레이션 작업은 거의 되지 않았으므로, 실제로 Postgresql DB를 이용하는 서비스는 그다지 많지 않았다.  새로 업데이트 하는 기능들에 대해서는 적극적으로 Postgresql을 이용하기로 했고, 이번에 새로 퀴즈 기능을 개발하면서 이 기능은 전부 RDB …","fields":{"slug":"/SqlAlchemy의-QueuePool/"},"frontmatter":{"date":"January 18, 2024","title":"SqlAlchemy의 QueuePool","tags":["SqlAlchemy","DataBase","Python","Work"]},"rawMarkdownBody":"개발중인 서비스에서 Postgresql을 적용한지 이제 한달 조금 더 지났다. \n\n아직 DB 마이그레이션 작업은 거의 되지 않았으므로, 실제로 Postgresql DB를 이용하는 서비스는 그다지 많지 않았다. \n\n새로 업데이트 하는 기능들에 대해서는 적극적으로 Postgresql을 이용하기로 했고, 이번에 새로 퀴즈 기능을 개발하면서 이 기능은 전부 RDB 기반으로 만들어졌다. \n\n퀴즈 기능의 개발은 유저의 접속률을 늘리기 위한 것으로, 모든 테스트를 마친 후 실서버에 배포가 되고 클라이언트까지 패치가 된 후 당당하게 퀴즈 풀러 오시라고 사용자들에게 푸시를 날렸다. \n\n그리고 펑! 서버가 터졌다. \n\n## 현상\n\n실제 서비스가 엄청나게 느리거나 거의 작동하지 않았다. \n\nPostgresql DB와 연결된 API들이 작동하지 않았다. \n\n서버에서는 `QueuePool limit of size 5 overflow 10 reached, connection timed out, timeout 30.00` 오류가 대량으로 발생했다. \n\n서버 모니터링 결과 푸쉬 직후 대량의 트래픽이 집중적으로 발생했다. \n\nFlarelane을 통해 확인한 결과, 2000명 정도가 동시 접속을 시도했다. \n\nSupabase의 DB 커넥션이 순간적으로 크게 늘어났다. (풀러를 사용중인데도 불구하고)\n\n\n\n시간이 지나면서 (약 10분이 안되는 짧은 시간) 트래픽이 줄어들고 자연스럽게 오류가 줄고, 서비스가 정상화되었다. \n\n## 원인 파악 및 해결\n\nSqlalchemy의 QueuePool을 현재 지금 pool size 5, max overflow 10으로 맞춰놓았었는데, 트래픽이 증가하면서 QueuePool의 한도가 다 차고, 대다수 요청이 최대 대기 시간 30초를 꽉 채워 대기하다가 오류가 발생했다. \n\n조사 결과, 대다수 많은 실제 서비스에서 해당 설정값은 대체적으로 너무 적어 값을 늘려서 사용한다고 한다. 정확한 값을 찾기 위해서는 여러번의 테스트가 필요하며, 점진적으로 늘려가면서 적당한 값을 찾아야 한다고 한다. \n\nsupabase 기반의 postgresql을 사용하고 있는데, 일단 postgresql의 connection 여유는 충분한 상황이었으므로 일단 설정값을 pool size 10, max overflow 20으로 각각 두배로 늘려주었다. \n\n이벤트나, 푸쉬가 있을 경우 추가적인 모니터링이 필요할 것이다. \n\n## Sqlalchemy의 QueuePool\n\n데이터베이스 Connection은 리소스가 많이 필요하다. QueuePool은 이런 Connection을 효율적으로 관리하여 성능을 향상시키기 위해 만들어졌다. Connection을 빠르게 재사용함으로써, 애플리케이션의 반응 시간을 단축 시키고 부하 상황에서도 안정적으로 동작하도록 한다. \n\nQuepool은 내부적으로 Connection 객체들을 큐로 관리한다. 동시성을 고려하여 설계되어 여러 스레드 또는 프로세스에서 안전하게 사용될 수 있다. \n\n### QueuePool의 생명주기 \n\n#### 생성\n\nQueuePool은 Sqlalchemy 엔진이 생성될 때 함께 생성된다. 이 때 데이터베이스와의 Connection 설정이 정의된다. \n\n#### Connection 관리 \n\n애플리케이션에서 데이터베이스 Connection이 필요할 때, QueuePool이 Connection을 제공한다. 사용 가능한 Connection이 없으면 새 Connection을 생성하거나 대기열에서 Connection을 기다린다. \n\n#### Connection 반환\n\n작업이 완료되면 Connection은 Pool에 반환되어 재사용된다. \n\n#### 종료\n\n애플리케이션 종료와 함께 Pool에 있는 모든 Connection이 안전하게 종료된다. \n\n### 작동\n\nQueuePool은 기본적으로 미리 설정된 `pool_size` 내에서 Connection을 관리한다. 사용 가능한 Connection이 있다면 그 연결을 제공하고, 없으면 새로은 Connection을 생성한다. \n\n`pool_size`는 동시에 활성화 할 수 있는 최대 연결수이다. 만약 이 한계를 초과한다면, `max_overflow` 에 설정된 값에 따라 추가 Connection을 생성한다. `max_overflow` 는 `pool_size` 를 초과하여 생성할 수 있는 추가 연결의 최대 수이다. \n\n`max_overflow` 까지 초과한다면 추가 Connection 연결 요청은 대기열에 들어가며 사용 가능한 Connection 이 생길때까지 대기한다. 이 때, 연결 대기 시간이 `timeout` 설정을 초과하면 연결 요청은 실패한다. \n\n#### pool_size\n\n이 설정은 가능한 최대 연결의 최대 수를 정의 하는 것이므로, 5로 설정했다고 해서 실제 연결이 5개가 유지되는 것은 아니다. 사용되지 않는 Connection은 pool에 반환되며, 유휴 상태로 남아있다가 `recycle` 설정 시간을 넘어서면 끊어진다. \n\n#### max_overflow\n\npool_size를 초과하는 최대 개수를 정의한다. pool_size 가 5이고, max_overflowrk 10이라면 동시에 최대 5+10 = 15개의 연결을 허용하는 것이다. max_overflow에 해당되어 생성된 Connection이라 할지라도 사용 후에 풀에 반환된다(바로 끊어지는 것이 아니다). 다만, 해당 Connection은 pool_size의 기본 한계를 넘어선 것이므로 풀에서 더 높은 우선 순위로 종료될 수 있다. \n\n### Sqlalchemy의 풀링 전략\n\nSqlalchemy의 기본 풀링 전략은 Queuepool이지만, 이외에도 다른 전략들이 있다. \n\n### StaticPool\n\n연결의 고정된 집합을 유지한다. 모든 Connection 요청은 이 고정된 집합에서 제공된다. 단일 사용자 또는 단일 프로세스 어플리케이션에 적합하다. \n\n### NullPool\n\nConnection Pooling을 사용하지 않는다. 매 요청마다 새로운 DB Connection이 열리고 작업 종료 후 Connection이 닫힌다. \n\n매우 드문 요청이 있는 어플리케이션에 적합하다. \n\n### Singleton ThreadPool\n\n각 스레드에 대해 하나의 Connection을 유지한다. 스레드마다 고유한 Connection을 사용한다. \n\n멀티 스레드 환경에서 각 스레드가 자체 Connection을 가질 필요가 있을 때 유용하다. \n\n\n\n"},{"excerpt":"서론 FatAPI에는 페이징을 위한 공식 라이브러리가 존재한다. 하지만 예제대로 진행해도 코드는 오류를 내뿜었다. 왜냐하면, FastAPI의 페이지네이션 라이브러리는 SqlAlchemy 2.0의 비동기 엔진을 지원하지 않기 때문이다.  그래서 직접 구현했다.  (이 라이브러리를 쓰면 Async pagination 지원합니다. 여러분은 이거 쓰세요.) 목적…","fields":{"slug":"/FastAPI의-Pagenation/"},"frontmatter":{"date":"January 17, 2024","title":"FastAPI의 Pagenation","tags":["Work","FastAPI","SqlAlchemy","Python"]},"rawMarkdownBody":"## 서론\n\nFatAPI에는 페이징을 위한 [공식 라이브러리](https://uriyyo-fastapi-pagination.netlify.app/)가 존재한다. 하지만 예제대로 진행해도 코드는 오류를 내뿜었다. 왜냐하면, FastAPI의 페이지네이션 라이브러리는 SqlAlchemy 2.0의 비동기 엔진을 지원하지 않기 때문이다. \n\n그래서 직접 구현했다. \n\n~~(이~~ [~~라이브러리~~](https://pypi.org/project/fastapi-sqla/)~~를 쓰면 Async pagination 지원합니다. 여러분은 이거 쓰세요.)~~\n\n## 목적\n\n### 응답값 고정\n\n모든 페이징 쿼리에 대해 동일한 응답값을 제공하기 위해 Response에 사용할 Pydantic Model도 함께 정의한다. \n\n### 가능한 한 간단한 사용 방법\n\nSqlalchemy의 Select 객체를 받아 바로 처리할 수 있도록 한다. 즉, 개발자는 쿼리문만 생성하고 사이즈/페이지만 정해주면 바로 고정된 응답값을 받을 수 있다. \n\n### 현재 서버에 필요한 사양에 맞춘 기능\n\n복잡한 쿼리나 여러 층의 정렬을 현재는 필요로 하지 않는다. 이에 발맞춰 기본적인 relationship을 활용한 조인, 단일 컬럼 sort만 지원하여 사용법을 간략화 한다. \n\n## 구현\n\n### PaginationReturn Class 구현\n\n페이지네이션의 결과의 데이터 모델을 미리 정의해 둔 클래스이다. \n\n응답값은 다음의 내용들로 고정된다.\n\n- itemList : 페이지에 포함된 Row의 리스트\n\n- totalCount : 전체 항목의 개수\n\n- totalPage : 전체 페이지의 개수 \n\n- nowPage : 현재 페이지 번호\n\n- nowSize : 현재 설정한 사이즈 \n\n- prevPage : 이전 페이지 번호\n\n- nextPage : 다음 페이지 번호\n\n<details>\n<summary>전체 코드</summary>\n\n```yaml\nclass PaginationReturn(BaseModel, Generic[T]):\n    \"\"\"\n    PaginationReturn 클래스는 페이지네이션 결과를 나타내는 모델입니다.\n\n    Attributes:\n        itemList (List[T]): 페이지에 포함된 항목의 리스트입니다.\n        totalCount (int): 전체 항목의 개수입니다.\n        totalPages (int): 전체 페이지의 개수입니다.\n        nowPage (int): 현재 페이지 번호입니다.\n        nowSize (int): 현재 페이지에 포함된 항목의 개수입니다.\n        prevPage (Optional[int], optional): 이전 페이지 번호입니다. 기본값은 None입니다.\n        nextPage (Optional[int], optional): 다음 페이지 번호입니다. 기본값은 None입니다.\n    \"\"\"\n    itemList: List[T]\n    totalCount: int\n    totalPages: int\n    nowPage: int\n    nowSize: int\n    prevPage: Optional[int] = None\n    nextPage: Optional[int] = None\n```\n\n\n</details>\n\n### Pagination Class 구현\n\npagination은 static 메소드로 만들어서 필요한 값을 넣으면 바로 PaginationReturn을 받을 수 있게 만들었다. static method로 만든것은 FastAPI의 Pagination 라이브러리를 본딴것이다. 이 방법이 가장 사용하기 편해보이기도 했다. \n\n#### `get_paginated_list`\n\n처음 구현한 메소드이다. 모델을 파라미터로 받아 페이징을 한다. 따로 select문을 만들 필요 없이 모델과 where절만 만들어서 넣어줘도 정의된 응답값 데이터 모델에 리스트를 담아 반환해준다. \n\n<details>\n<summary>전체 코드</summary>\n\n```python\n@classmethod\n    async def get_paginated_list(\n        cls,\n        db: AsyncSession,\n        model: T,\n        filters: Optional[List[BinaryExpression]] = None,\n        order_column: Optional[str] = None,\n        order_direction: str = \"desc\",\n        size: int = 10,\n        page: int = 1,\n    ) -> PaginationReturn:\n        # page 와 size 기본 유효성 검증\n        if page < 1 or size < 1:\n            raise PagingException(\"Page and size parameters must be greater than 0\")\n\n        # 기본 쿼리 생성\n        query = select(model)\n\n        # fileter 처리\n        if filters and len(filters) > 0:\n            query = query.filter(*filters)\n\n        # 정렬 처리\n        if order_column:\n            if order_direction.lower() == \"asc\":\n                query = query.order_by(asc(getattr(model, order_column)))\n            else:\n                query = query.order_by(desc(getattr(model, order_column)))\n\n        total_count = 0\n        try:\n            # 페이징\n            offset_value = (page - 1) * size\n            query = query.offset(offset_value).limit(size)\n            result = await db.execute(query)\n            items = result.scalars().all()\n            logger.info(f\"[Pagination Query] {query}\")\n            # total_count 계산\n            total_query = select(func.count()).select_from(model)\n            if filters:\n                total_query = total_query.filter(*filters)\n            total_result = await db.execute(total_query)\n            total_count = total_result.scalar_one()\n        except SQLAlchemyError as e:\n            raise PagingException(f\"An error occurred while fetching data from the database : {e}\")\n        except Exception as e:\n            raise PagingException(f\"An unexpected error occurred : {e}\")\n\n        total_pages, prev_page, next_page = calculate_pagination(total_count, size, page)\n\n        # 응답값 생성\n        items_dict = [serialize_sqlalchemy_obj(item) for item in items]\n\n        res = PaginationReturn(\n            itemList=items_dict,\n            totalCount=total_count,\n            totalPages=total_pages,\n            nowPage=page,\n            nowSize=size,\n            prevPage=prev_page,\n            nextPage=next_page,\n        )\n        return res\n```\n\n\n</details>\n\nBaseModel 객체는 직렬화 기능이 없으므로, Response로 응답을 내려보낼때에 귀찮아지는 문제가 있다. 모델의 키값과 밸류를 사용해 dictionary로 바꾸면 해당 문제를 해결할 수 있는데, 만약 리스트에 들어있는 값들에 대해 커스텀이 필요하다면 dictionary를 그대로 다루는 것은 아무래도 모델을 커스텀 하는 것보다 불편한 문제가 발생한다. \n\n이 문제를 해결하기 위해 모델 객체를 기반으로 Pydantic Model 객체를 동적으로 생성하여 Pydantic model의 리스트를 결과값에 담아 리턴하기로 했다. 이렇게 하면 응답값을 바로 Response로 내려보내도 직렬화가 가능해 문제가 발생하지 않으며, 혹시 아이템 내용을 커스텀 해야 할 때에도 모델을 다룰 때와 같은 방법으로 사용할 수 있으므로 훨씬 간편하다. 개선된 코드는 아래와 같다. \n\n<details>\n<summary>개선된 코드</summary>\n\n```python\n@classmethod\n    async def get_paginated_list(\n        cls,\n        db: AsyncSession,\n        model: T,\n        filters: Optional[List[BinaryExpression]] = None,\n        order_column: Optional[str] = None,\n        order_direction: str = \"desc\",\n        size: int = 10,\n        page: int = 1,\n    ) -> PaginationReturn:\n        \"\"\"\n        페이징을 해서 아이템을 뽑아내서 결과를 반환한다.\n        Args:\n            db: AsyncSession\n            model: 데이터를 가져올 모델\n            filters: [InitQuizList.is_deleted == False, InitQuizList.is_active == True] 의 꼴 where절\n            order_column: 정렬할 컬럼\n            order_direction: 'asc' or 'desc'\n            size: 페이지에 나타낼 아이템의 개수 (기본값 10)\n            page: page 번호 (기본값 1)\n        Returns:\n            PaginationReturn\n        \"\"\"\n\n        # page 와 size 기본 유효성 검증\n        if page < 1 or size < 1:\n            raise PagingException(\"Page and size parameters must be greater than 0\")\n\n        # 기본 쿼리 생성\n        query = select(model)\n\n        # fileter 처리\n        if filters and len(filters) > 0:\n            query = query.filter(*filters)\n\n        # 정렬 처리\n        if order_column:\n            if order_direction.lower() == \"asc\":\n                query = query.order_by(asc(getattr(model, order_column)))\n            else:\n                query = query.order_by(desc(getattr(model, order_column)))\n\n        total_count = 0\n        try:\n            # 페이징\n            offset_value = (page - 1) * size\n            query = query.offset(offset_value).limit(size)\n            result = await db.execute(query)\n            items = result.scalars().all()\n            logger.info(f\"[Pagination Query] {query}\")\n            # total_count 계산\n            total_query = select(func.count()).select_from(model)\n            if filters:\n                total_query = total_query.filter(*filters)\n            total_result = await db.execute(total_query)\n            total_count = total_result.scalar_one()\n        except SQLAlchemyError as e:\n            raise PagingException(f\"An error occurred while fetching data from the database : {e}\")\n        except Exception as e:\n            raise PagingException(f\"An unexpected error occurred : {e}\")\n\n        total_pages, prev_page, next_page = calculate_pagination(total_count, size, page)\n\n        # 응답값 생성\n        # 동적 Pydantic 모델 생성\n        PydanticModel = sqlalchemy_to_pydantic(model)\n        # 페이징 처리된 쿼리 결과를 Pydantic 모델 리스트로 변환\n        pydantic_items = [PydanticModel.model_validate(item.__dict__) for item in items]\n\n        res = PaginationReturn(\n            itemList=pydantic_items,\n            totalCount=total_count,\n            totalPages=total_pages,\n            nowPage=page,\n            nowSize=size,\n            prevPage=prev_page,\n            nextPage=next_page,\n        )\n        return res\n\ndef sqlalchemy_to_pydantic(db_model: Type[DeclarativeMeta]) -> Type[BaseModel]:\n    \"\"\"\n    SQLAlchemy 모델을 동적으로 생성된 동일한 스키마의 Pydantic 모델로 변환합니다.\n    :param db_model: SQLAlchemy 모델 클래스\n    :return: 생성된 Pydantic 모델 클래스\n    \"\"\"\n    fields = {}\n    for column in inspect(db_model).c:\n        python_type = column.type.python_type\n        default = None if column.default is None else column.default.arg\n        if column.nullable:\n            python_type = Optional[python_type]\n        fields[column.name] = (python_type, default)\n\n    pydantic_model = create_model(db_model.__name__ + \"Pydantic\", **fields)\n    return pydantic_model\n```\n\n모델의 컬럼값으로 동적으로 pydantic model을 생성하는 sqlalchemy_to_pydantic 메소드를 생성하고 해당 메소드를 활용해 응답값에 담도록 해주었다. \n\n\n</details>\n\n#### `get_paginated_list_by_query`\n\n처음에 신나서 만들었는데, 기존  `get_paginated_list` 메소드는 치명적인 문제가 있다. 쿼리가 복잡한 경우를 전혀 생각하지 않았다. 말 그대로 단일 테이블에서만 값을 가져올 수 있는 것이다. 그래서 이 점을 개선해야했다. 그래서 개발자가 조금 더 귀찮아지지만 쿼리문은 이 메소드를 이용할 작업자가 직접 구현을 하고, 그 쿼리문을 넣으면 리스트를 담아서 돌려주는 형식으로 다시 만들기로 했다. \n\n기존 메소드에서는 모델만을 조회하는 것이므로 모델을 기반으로 Pydantic model을 생성하면 문제가 없었는데, 이번에는 모델에서 컬럼값을 추출해서 사용할 수 없으므로 키 값을 동적으로 정의하는 코드를 따로 작성해주었다. \n\n쿼리된 내용에서 컬럼값을 조회해서 사용하였으며, 쿼리된 내용에는 시스템에서 사용하는 속성값도 존재하는데 해당 속성값들은 `_`로 시작하므로 해당 내용은 제거하고 동적으로 Pydantic model에 추가해주었다. relationship도 대응할 수 있게 별도의 예외처리를 추가해주었다. relationship 속성은 Select 객체의 컬럼에는 포함이 되지 않는데 조회된 결과물의 속성에는 들어있으므로 해당 값이 relationship 속성이라고 가정하고 따로 예외처리를 해주었다. \n\n<details>\n<summary>전체 코드</summary>\n\n```python\n@classmethod\n    async def get_paginated_list_by_query(\n        cls,\n        db: AsyncSession,\n        query: Select[Any],\n        size: int = 10,\n        page: int = 1,\n    ):\n        \"\"\"\n        Query에 대한 paging 생성\n        Args:\n            db: AsyncSession\n            query: sqlalchemy의 Select의 리턴값\n            size: 한 번에 보여줄 아이템의 개수(기본값 10)\n            page: 페이지(기본값 1)\n        Returns:\n            PaginationReturn\n        \"\"\"\n        # 매개변수 검증\n        if page < 1 or size < 1:\n            raise PagingException(\"Page and size parameters must be greater than 0\")\n\n        # 페이징 적용\n        offset_value = (page - 1) * size\n        paginated_query = query.offset(offset_value).limit(size)\n\n        # 쿼리 실행\n        try:\n            result = await db.execute(paginated_query)\n        except SQLAlchemyError as e:\n            raise PagingException(f\"An error occurred while fetching data from the database : {e}\")\n        except Exception as e:\n            raise PagingException(f\"An unexpected error occurred : {e}\")\n        items = result.scalars().all()\n\n        pydantic_items = []\n        total_count = 0\n        if items:\n            # 컬럼과 데이터 타입 파악\n            columns = query.columns\n            fields = {col.name: (Optional[col.type.python_type], None) for col in columns}\n\n            # relationship 처리\n            relationship_keys = set()\n            sample_item = items[0]\n            item_dict = sample_item.__dict__\n            for key in item_dict.keys():\n                if key not in fields and not key.startswith(\"_\"):\n                    # 관계형 속성의 키를 저장\n                    relationship_keys.add(key)\n\n            # 관계형 속성에 대한 필드 정의 추가\n            for key in relationship_keys:\n                fields[key] = (Optional[Any], None)\n            # 관계형 속성을 포함한 동적 Pydantic 모델 생성\n            DynamicPydanticModel = create_model(\"DynamicPydanticModel\", **fields)\n\n            for item in items:\n                item_dict = {k: v for k, v in item.__dict__.items() if not k.startswith(\"_\")}\n                # 관계형 속성을 딕셔너리로 변환\n                for key in relationship_keys:\n                    relation = getattr(item, key, None)\n                    if relation:\n                        # SQLAlchemy 내부 상태 정보를 제외하고 변환\n                        item_dict[key] = {\n                            k: v for k, v in relation.__dict__.items() if not k.startswith(\"_\")\n                        }\n                    else:\n                        item_dict[key] = None\n\n                pydantic_items.append(DynamicPydanticModel.model_validate(item_dict))\n\n            # 총 개수 계산\n            total_count_query = select(func.count()).select_from(query.subquery())\n            total_count_result = await db.execute(total_count_query)\n            total_count = total_count_result.scalar_one()\n\n        # 결과 반환\n        total_pages, prev_page, next_page = calculate_pagination(total_count, size, page)\n\n        return PaginationReturn(\n            itemList=pydantic_items,\n            totalCount=total_count,\n            totalPages=total_pages,\n            nowPage=page,\n            nowSize=size,\n            prevPage=prev_page,\n            nextPage=next_page,\n        )\n```\n\n\n</details>\n\n## 결론\n\n아직 기능이 많이 부족하지만, 현재 회사에서 사용을 하면서 필요한 기능은 대부분 구현을 했다고 생각이 든다. 그래서 일단 개발은 여기서 멈추었다. \n\n개발에 아주 오랜 시간이 걸리지는 않았다. 집중도와 실제 사용 시간을 따지면 이틀 좀 안되게 썼다고 생각이 든다. \n\n만드는것은 재미있는 과정이었고, 필요한 것이 잘 만들어져 뿌듯함도 있었지만 역시 있는 라이브러리를 쓰는게 시간 최적화에서는 더 좋았던 것 같다. \n\n\n\n"},{"excerpt":"결론 supabase 기준, Sqlalchemy의 비동기 엔진에서 Postgresql pooler (supavisor)에 오류 없이 연결하기 위해서는  을 다음과 같이 구성해주면 된다.  statementcachesize, preparedstatementcache_size를 둘 다 모두 0으로 줘야 한다. 그리고 왜 발생하는지 모르겠는 간헐적인 오류(캐싱…","fields":{"slug":"/Sqlalchemy-비동기-엔진에서의-Postgresql-Pooler/"},"frontmatter":{"date":"January 09, 2024","title":"Sqlalchemy 비동기 엔진에서의 Postgresql Pooler","tags":["DataBase","SqlAlchemy","Python","Work"]},"rawMarkdownBody":"## 결론\n\nsupabase 기준, Sqlalchemy의 비동기 엔진에서 Postgresql pooler (supavisor)에 오류 없이 연결하기 위해서는 `create_async_engine` 을 다음과 같이 구성해주면 된다. \n\n```python\nfrom uuid import uuid4\nfrom asyncpg import Connection\n\n...(생략)...\n\nclass CConnection(Connection):\n    def _get_unique_id(self, prefix: str) -> str:\n        \"\"\"\n        캐싱을 만들 시에 캐싱 아이디가 중복으로 생성되어서 오류가 발생하는 경우가 있다.\n        statement_cache_size를 0으로 해도 일단 캐싱을 만들고 오류를 낸다.\n        따라서 캐싱을 사용하지 않을 것이므로, 아예 중복되지 않게 UUID로 생성해버린다.\n        \"\"\"\n        return f'__asyncpg_{prefix}_{uuid4()}__'\n\n...(생략)...\n\nDATABASE_URL = f\"postgresql+asyncpg://postgres.[Project ID]:[Password]@aws-0-[Region].pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        connect_args={\n            \"statement_cache_size\": 0,\n            'connection_class': CConnection,\n        },\n    )\n```\n\nstatement_cache_size, prepared_statement_cache_size를 둘 다 모두 0으로 줘야 한다.\n\n그리고 왜 발생하는지 모르겠는 간헐적인 오류(캐싱 ID를 중복되게 생성하려고 시도함)를 해결하기 위해 Connection 클래스를 상속받아 _get_unique_id 메소드를 오버라이딩 해주고, 해당 클래스를 connection_class로 사용하도록 했다. 이렇게 하면 id 중복 생성 오류를 방지할 수 있다. \n\n\n\n일반적인 글이라면 결론이 서두에 나온다면 너무나 시시하겠지만 기술 블로그이니 괜찮다. \n\n## 문제 해결에 관한 글\n\n### 발단\n\nsupabase에서 메일이 왔다. \n\n![](image1.png)\n여태 본 기억이 없었는데, Final reminder라도 되어있는걸 보니 이미 여러번 고지했던 모양이다. \n\n읽어보니 Pgbouncer 지원이 종료된다는 내용이다. 지원이 종료되는건 알고 있었는데, 아예 Connection string을 죽여버리는 모양이다. 잔인하다는 생각이 들었지만 어차피 우리는 Pooler는 진작에 포기했으므로 상관이 없었다. \n\n문제는 다음이었는데, DB Direct access에 대한 IPv4 주소 지원도 끝난다니 이건 좀 문제의 소지가 있었다. \n\n마침 어제 지난번 DB 세팅에 대한 회고를 끝내서 다행이었다. \n\n### 전개\n\n선택지는 두가지이다. Supavisor 도입, 네트워크가 IPv6를 지원하도록 설정하고 Direct 연결 방법을 계속해서 사용. \n\n개인적으로 \n\n- 네트워크 설정에 상대적으로 약함\n\n- WebRTC를 하던 때의 기억 때문에 IPv6에 안좋은 감정이 있음 \n\n- 현재 백엔드의 네트워크 설정을 직접 진행하지 않았었음 \n\n- 혼자 출근한 백엔드는 나 혼자임\n\n- Pooler와 씨름을 한 기억이 최근이고, 회고는 더더욱 최근임\n\n의 이유로 Supavisor 도입에 대해 다른 백엔드가 출근하기 전 잠깐 살펴보기로 했다.\n\n그런데 왠걸? 직접 설치부터 해야 하나라고 막연히 생각을 했었는데(supabase의 공식 문서에는 아직 supavisor에 대한 내용이 적어도 찾기 쉬운 곳에 있지는 않다. ) 그렇지 않았다. \n\n[https://github.com/orgs/supabase/discussions/17817](https://github.com/orgs/supabase/discussions/17817)\n\n해당 링크를 보아하니 이미 꽤 지원이 진행된 상태였는지, connection string만 바꾸면 간단하게 Pgbouncer가 아니라 Supavisor를 사용하여 연결할 수 있다고 적혀 있었다. \n\n그래서 해당 링크를 참조해 Connection string은 약간 수정을 해야 했지만, 어쨌든 Supavisor 연결에 간단하게 성공해버렸다. \n\n### 위기\n\n곧장 과거 겪었던 것과 같은 문제가 발생했다. Sqlalchemy 입장에서는 Pgbouncer와 Supavisor를 따로 구분하지 못하는지 Transaction 또는 Session 풀 모드에서는 캐싱이 지원되지 않는다는 에러 문구가 발생했다. \n\n여기서 잠깐, 포기하고 얌전히 IPv6 설정을 보러 갈까 했는데, 아직 다른 백엔드가 아무도 출근하지 않았다. \n\n일단 Pool mode를 바꿔야 다음 단계의 설정을 진행할 수 있었는데, Pgbouncer의 Pool mode는 supabase 콘솔에서 세팅을 지원하지만 supavisor는 어떻게 바꿔야 할지 의문이었다. \n\n해당 의문은 말도 못하게 간단하게 풀렸다. 콘솔에 들어가보니 어느새 쥐도 새도 모르게 Pgbouncer 관련 섹션이 supavisor 관련 섹션으로 바뀌어있었다. \n\n![](image2.png)\n위 이슈에서 설명된 supavisor 관련 connection string은 `user` 로 시작하는 부분이 옳지 않아서 수정이 필요했는데, 콘솔의 pgbouncer 관련 연결 connection string 에는 제대로 적혀있었다. 아무래도 메일에 적어보낸 이슈이지만 따로 수정을 하지는 않은 모양이다.. \n\n아무튼간 pool mode를 변경 후 테스트를 할 수 있었다. \n\n### 절정1\n\nPool mode를 바꿨다고 해서 바로 연결이 가능하지는 않다. 서버 인스턴스를 실행하면 짜잔! 성공적인 DB 연결 대신 다음과 같은 문구를 볼 수 있다. \n\n```plain text\nsqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.InvalidSQLStatementNameError'>: prepared statement \"__asyncpg_stmt_b__\" does not exist\nHINT:  \nNOTE: pgbouncer with pool_mode set to \"transaction\" or\n\"statement\" does not support prepared statements properly.\nYou have two options:\n* if you are using pgbouncer for connection pooling to a\n  single server, switch to the connection pool functionality\n  provided by asyncpg, it is a much better option for this\n  purpose;\n* if you have no option of avoiding the use of pgbouncer,\n  then you can set statement_cache_size to 0 when creating\n  the asyncpg connection object.\n```\n\nSqlalchemy를 사용중이기 때문에 첫번째 방법은 자연스럽게 더 고려할것도 없이 기각이며, statement_cache_size를 0으로 수정하는 것이 현재 내 인프라에서 선택할 수 있는 방법이었다.\n\n[https://docs.sqlalchemy.org/en/20/dialects/postgresql.html](https://docs.sqlalchemy.org/en/20/dialects/postgresql.html)\n\n지난번 기나긴 고생 이후 공식 문서를 항상 참조하는 습관을 들이게 됐다. 아주 긍정적인 방향이라고 생각한다. \n\n공식 문서를 참조하면,\n\n```plain text\nengine = create_async_engine(\"postgresql+asyncpg://user:pass@hostname/dbname?prepared_statement_cache_size=0\")\n```\n\n의 방식으로 statement_cache_size를 0으로 설정할 수 있는 것을 알 수 있다. \n\n이 설정을 해준 후, 다시 서버 인스턴스를 실행하면 짜잔! DB 연결이 아주 잘 된다. 쿼리도 잘 날리고, 결과도 잘 받아온다.\n\n이 와중에 Cache 생성 시에 같은 인덱스로 중복 생성되어서 오류가 발생하기도 했다. 그래서 SqlAlchemy의 Connection 클래스를 상속받아 캐시의 아이디를 생성하는 메소드를 수정해주었다. \n\n```python\nfrom asyncpg import Connection\nfrom uuid import uuid4\n\nclass CConnection(Connection):\n    def _get_unique_id(self, prefix: str) -> str:\n        \"\"\"\n        캐싱을 만들 시에 캐싱 아이디가 중복으로 생성되어서 오류가 발생하는 경우가 있다.\n        statement_cache_size를 0으로 해도 일단 캐싱을 만들고 오류를 낸다.\n        따라서 캐싱을 사용하지 않을 것이므로, 아예 중복되지 않게 UUID로 생성해버린다.\n        \"\"\"\n        return f'__asyncpg_{prefix}_{uuid4()}__'\n```\n\n이후 DB 커넥션을 만들 때에 해당 클래스를 기본 Connection 클래스 대신 사용하도록 설정해주면 된다. \n\n```python\nDATABASE_URL = f\"postgresql+asyncpg://postgres.[Project ID]:[Password]@aws-0-[Region].pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        connect_args={\n            'connection_class': CConnection,\n        },\n    )\n```\n\n여기까지 작업을 해줘서 오류를 해결하고 같은 쿼리를 연속으로 날리면? \n\n짜잔! 다음과 같은 오류를 볼 수 있다. \n\n```plain text\nsqlalchemy.exc.DBAPIError: (sqlalchemy.dialects.postgresql.asyncpg.Error) <class 'asyncpg.exceptions.InvalidSQLStatementNameError'>: prepared statement \"__asyncpg_stmt_9462b5fe-88dc-46f5-98fb-57981ea56de0__\" does not exist\nHINT:  \nNOTE: pgbouncer with pool_mode set to \"transaction\" or\n\"statement\" does not support prepared statements properly.\nYou have two options:\n* if you are using pgbouncer for connection pooling to a\n  single server, switch to the connection pool functionality\n  provided by asyncpg, it is a much better option for this\n  purpose;\n* if you have no option of avoiding the use of pgbouncer,\n  then you can set statement_cache_size to 0 when creating\n  the asyncpg connection object.\n```\n\n그렇다, 오류는 반복된다. \n\n### 절정2\n\n이미 여기까지 시간을 꽤 쓴 상태였기 때문에 조금만 더 무언가를 해보기로 했다. \n\n캐시 사용을 아무리 꺼도, 도대체가 캐시를 계속 만들고 거기서 오류를 내는 이 상황. \n\n다행히 비슷한 오류를 겪는 사람들을 깃허브 이슈에서 찾을 수 있었다. \n\n이미 나의 신뢰를 많이 잃어버린 챗지피티에게 이 문제를 상담하면 `statement_cache_size` 옵션을 0으로 설정해서 connect_args에 넣으라고 한다. PgBouncer로 붙이려고 애써봤었을 때에 해당 옵션을 넣어도 아무런 변화가 없이 그대로 오류가 발생했었고, 공식문서에는 존재하지 않는 옵션이어서 이번에는 무시하고 넘어갔었다. \n\n근데 충격적이게도 한가지 옵션을 썼을 땐 되지 않고 두 옵션을 동시에 사용하면 된다고 한다.. 이게 무슨 일일까? 나는 왜 두 옵션을 동시에 쓸 생각을 못했을까?(이 생각을 어떻게 하나요?)\n\n최종적으로 DB Connection String을 다음과 같이 설정해주었다. \n\n```python\nfrom uuid import uuid4\nfrom asyncpg import Connection\n\n...(생략)...\n\nclass CConnection(Connection):\n    def _get_unique_id(self, prefix: str) -> str:\n        \"\"\"\n        캐싱을 만들 시에 캐싱 아이디가 중복으로 생성되어서 오류가 발생하는 경우가 있다.\n        statement_cache_size를 0으로 해도 일단 캐싱을 만들고 오류를 낸다.\n        따라서 캐싱을 사용하지 않을 것이므로, 아예 중복되지 않게 UUID로 생성해버린다.\n        \"\"\"\n        return f'__asyncpg_{prefix}_{uuid4()}__'\n\n...(생략)...\n\nDATABASE_URL = f\"postgresql+asyncpg://postgres.[Project ID]:[Password]@aws-0-[Region].pooler.supabase.com:6543/postgres?prepared_statement_cache_size=0\"\n    _db_connection = create_async_engine(\n        DATABASE_URL,\n        connect_args={\n            \"statement_cache_size\": 0,\n            'connection_class': CConnection,\n        },\n    )\n```\n\n### 결말 & 에필로그\n\n지난번 기나긴 고생과 삽질을 통해서 챗지피티에 대한 신뢰를 덜고 공식문서를 신뢰해야 한다는 사실을 깨달았다. \n\n그리고 이번 짧은(반나절 정도) 삽질을 통해서 공식문서도 100% 정답은 아니라는 사실을 깨달았다. (나는 무엇을 믿어야 할까?)\n\n생각해보면, Database url 에다가 옵션을 넣는다고 해서 내부적으로 뭔가 변경되는 것일리가 없다. 결국 엔진을 만들 때에 옵션값을 넣어줘야 내부적으로 뭔가 알고 다른 작동을 하지 않을까? 이 생각을 막바지에 와서 떠올렸다. \n\n항상 시야를 넓게 하고, 의심하면서 만들어야 한다는 것을 다시 한 번 깨달았다. 남들이 시키는대로 해서는 되는것이 없는 것이다. \n\n별개로, 내가 잘못을 한 것인지 뭔지 Sqlalchemy도 FastAPI도 Postgresql도 모두 유명한 것들인데 어째서 이렇게까지 불완전하게 작동하는지 의문이 든다. 역사가 길진 않지만 그래도 수년의 시간은 지난 것들인데, 가장 기본적인 부분이 이렇게 불안정할지는 예상을 못했다. FastAPI의 메모리 누수 문제도 뒤늦게 알게 되었는데, 메모리 누수 문제에 이어서 비동기 엔진에서의 DB Pooler 연결에 이렇게 문제가 있을 줄 알았다면 FastAPI와 SqlAlchemy를 절대 선택하지 않았을 것 같다. \n\n고찰, 의심, 경험.. 등등이 중요하다고 다시 한 번 느꼈다. \n\n"},{"excerpt":"현재 회사의 서비스에서 RDB를 사용하지 않고 NoSQL(DynamoDB)만 사용해서 어플리케이션 서비스를 하고 있었다.  필연적으로 RDB를 도입하게 되었고, supabase를 사용해보고 싶어서 DB는 깊은 고찰 없이 아무도 사용해본 적이 없었던 Postgresql을 고르게 되었다. 이것이 고생의 시작이었다.  그 고생을 기록해둔다.  1. 커넥션 직접…","fields":{"slug":"/FastAPI에서-Postgresql의-커넥션-관리/"},"frontmatter":{"date":"December 15, 2023","title":"FastAPI에서 Postgresql의 커넥션 관리","tags":["Python","FastAPI","DataBase","Work","Postgresql"]},"rawMarkdownBody":"현재 회사의 서비스에서 RDB를 사용하지 않고 NoSQL(DynamoDB)만 사용해서 어플리케이션 서비스를 하고 있었다. \n\n필연적으로 RDB를 도입하게 되었고, supabase를 사용해보고 싶어서 DB는 깊은 고찰 없이 아무도 사용해본 적이 없었던 Postgresql을 고르게 되었다.\n\n이것이 고생의 시작이었다. \n\n그 고생을 기록해둔다. \n\n### 1. 커넥션 직접 연결\n\nMySQL, Oracle, MsSQL등을 사용해봤지만 Postgresql은 처음이었다. 아무 생각없이 커넥션을 바로 연결했고 곧장 문제가 발생했다. \n\nsupabase에서 사용하는 요금제에서 커넥션을 65개까지 만들 수 있게끔 되어있었는데, 수시로 이 수치를 넘어버렸다. 작업 시작 당시 테스트 서버에서는 인스턴스를 5개를 사용하고 있었고, 기본값으로 인스턴스 하나 당 연결 5개를 사용하고 있었서 단순히 산술적으로는 65개를 넘어서는 안되는데 계속해서 65개를 넘겨서 DB 서버가 지속적으로 죽고 있었다. \n\n### 2. 커넥션 폭증, 첫번째 추측\n\n첫번째 추측으로는 세션의 생명주기를 관리하는 코드가 정상적으로 작동하는지 여부를 의심했다. 비동기에서의 세션 관리를 위해 의존성 주입 방식을 통한 세션 관리 방법을 선택하고 있었는데, 해당 코드를 다음과 같이 강화해주었다. \n\n#### 수정 전 코드\n\n```python\nasync def get_db_session(db_conn: Engine = Depends(get_db_connection)) -> AsyncIterable[Session]:\n    async with sessionmaker(db_conn, class_=AsyncSession, expire_on_commit=False)() as session:\n        yield session\n```\n\n#### 수정 후 코드\n\n```python\nasync def get_db_session(db_conn: Engine = Depends(get_db_connection)) -> AsyncIterable[Session]:\n    try:\n        async with sessionmaker(db_conn, class_=AsyncSession, expire_on_commit=False)() as session:\n            yield session\n    except Exception as e:\n        logger.error(f\"[get_db_session] {e}\")\n        await session.rollback()\n        raise\n    finally:\n        await session.close()\n```\n\n이론적으로는 위 코드만 사용해도 정상적으로 세션의 생명주기가 관리가 되어야 하는데, 해외에도 나와 같은 이슈를 겪는 사람이 있는 것으로 보였다. 아래처럼 수정 후에 눈에 띄게 커넥션 숫자가 줄었지만 여전히 DB 서버는 개복치 그 자체였다. 조금 나아지긴 했지만 숨만 쉬면 죽어나가는 상황이 반복되었고 여전히 실사용이 불가능한 심각한 상태의 환자였다. \n\n### 3. 커넥션 폭증, 두번째 추측\n\n두번째 추측으로는 dev는 테스트를 위해서 인스턴스 재생성이 꽤 잦았는데, 이 과정에서 Connection이 계속해서 다시 만들어지면서 연결이 폭증하는 것으로 보였다. \n\nPostgresql 설정에서 IDLE 상태로 들어간지 오래된 커넥션은 강제로 죽이는 옵션도 찾았는데, supabase에서는 해당 옵션값 설정은 지원하지 않았다. 아무래도 일반적인 대응 방법은 아닌것으로 보였다. \n인스턴스가 재생성 될때마다 연결이 안끊긴다면? 애초에 연결을 많이 안만들면 되는게 아닌가? 라는 결론에 다다랐다. 따라서 자연스럽게 Pooler를 사용하기로 했다. Supabase에서도 공식적으로 Pooler를 (당연히) 지원하므로 이는 완벽한 해결책으로 생각됐다. \n\n\n        Pooler란?\n\n데이터베이스 서버와 클라이언트 애플리케이션 간의 연결을 관리하는 소프트웨어이다. 이는 데이터베이스 서버에 대한 연결 요청을 효율적으로 처리하고 성능을 최적화하는 데 도움을 준다. \n\n#### 연결 재사용\n\n풀러는 데이터베이스 서버에 대한 연결을 재사용 할 수 있게 함으로써 매번 새로운 연결을 생성하는 오버헤드를 감소시킨다. \n\n#### 부하 관리\n\n동시에 많은 클라이언트 요청을 처리할 수 있도록 도와주며 데이터베이스 서버에 가해지는 부하를 분산시킨다. \n\n#### 성능 최적화\n\n연결 설정과 해제에 소요되는 시간을 줄임으로써 전반적인 애플리케이션 성능을 향상시킨다. \n\n#### 자원 관리\n\n데이터베이스 연결에 사용되는 리소스를 효율적으로 관리한다. \n\n#### 스케일링\n\n애플리케이션 확장성을 지원하며 사용자 요청 증가 시에도 안정적인 서비스 제공을 돕는다. \n\n풀러의 사용은 특히 많은 수의 동시 사용자와 트랜잭션이 있는 대규모 시스템에서 매우 중요하다. 데이터베이스에 대한 연결을 효과적으로 관리함으로써 시스템의 안정성, 성능, 그리고 확장성을 향상시킬 수 있다. \n\n### 4. Pooler 도입\n\nsupabase의 pooler는 PgBouncer를 제공하며, 사용법이 매우 간단했다. 기존에 사용하던 5432번 포트 대신 6543 포트로 바꿔서 연결해주기만 하면 됐다. \n\n또 다시 곧장 문제가 발생했다. Supabase의 PgBouncer  Poolmode는 기본적으로 Transation 모드였는데, 해당 모드에서는 SqlAlchemy의 비동기 엔진의 캐싱 기능을 지원하지 않았다. \n\n\n        Pgbouncer의 Pool Mode\n\n#### Session\n\n가장 기본적인 풀링 모드로 클라이언트가 연결을 끊을 때까지 PgBouncer는 해당 클라이언트 연결을 데이터베이스 서버에 계속 연결 상태로 유지한다. \n\n#### Transaction\n\n이 모드에서 PgBouncer는 각 SQL 트랜잭션이 완료될 때마다 연결을 풀로 반환한다. 이는 트랜잭션간에 서버 세션 상태를 유지하지 않아므로 세션 수준의 상태 설정이 트랜잭션간에 유지되지 않는다. Supabase의 Postgresql의 Pool mode는 기본적으로 Transacion 모드로 설정되어있다. \n\n#### Statement\n\n가장 제한적인 풀링 모드로 각 SQL 문(statement)이 완료될 때마다 연결을 풀로 반환한다. Supabase의 Postgresql은 해당 모드를 지원하지 않는다. \n\n### 5. Pooler 사용시 인스턴스 생성 에러 발생\n\n이번엔 문제가 바로 발생하지는 않았다. 당일은 지나갔으나 다음날이 되어서야 발견했다. 인스턴스 오류가 발생해서 항사 연결이 끊기는 것도 아니고 간헐적으로 연결이 끊기고 있었다. \n\n#### 당시 발생한 인스턴스 생성 에러\n\n```plain text\nLog ID\n5c0963cf-d16a-4cf3-b3fe-33c99147f879\nLog Timestamp (UTC)\n2023-12-11T02:36:52.732Z\nLog Event Message\npassword authentication failed for user \"postgres\"\nLog Metadata\n[\n  {\n    \"file\": null,\n    \"host\": \"db-fskkusemekciragirqyd\",\n    \"metadata\": [],\n    \"parsed\": [\n      {\n        \"application_name\": null,\n        \"backend_type\": \"client backend\",\n        \"command_tag\": \"authentication\",\n        \"connection_from\": \"61.1.172.3:56746\",\n        \"context\": null,\n        \"database_name\": \"postgres\",\n        \"detail\": \"Connection matched pg_hba.conf line 91: \\\"host  all  all  0.0.0.0/0     scram-sha-256\\\"\",\n        \"error_severity\": \"FATAL\",\n        \"hint\": null,\n        \"internal_query\": null,\n        \"internal_query_pos\": null,\n        \"leader_pid\": null,\n        \"location\": null,\n        \"process_id\": 89111,\n        \"query\": null,\n        \"query_id\": 0,\n        \"query_pos\": null,\n        \"session_id\": \"657675c4.15c17\",\n        \"session_line_num\": 2,\n        \"session_start_time\": \"2023-12-11 02:36:52 UTC\",\n        \"sql_state_code\": \"28P01\",\n        \"timestamp\": \"2023-12-11 02:36:52.732 UTC\",\n        \"transaction_id\": 0,\n        \"user_name\": \"postgres\",\n        \"virtual_transaction_id\": \"47/2627\"\n      }\n    ],\n    \"parsed_from\": null,\n    \"project\": null,\n    \"source_type\": null\n  }\n]\n```\n\n같은 설정에 같은 서버인데 뜬금없이 간헐적으로 패스워드 에러가 발생하니 죽을맛이었다. \n\n그러면 안되지만, 오류의 빈도수라도 적다면 그냥 무시하고 넘어갈텐데 무시하기엔 높은 빈도로 에러가 발생했다. PgBouncer의 지원이 2024년 1월로 끝난다고 하니 애초에 비동기와의 호환성 자체가 좋지 않아보였다. 이 시점에서 Pooler 사용을 포기하고 다른 방법을 찾기로 했다. \n\nPgBouncer 지원이 끊기고 대안으로 Superbase에서는 [https://github.com/supabase/supavisor](https://github.com/supabase/supavisor)를 제시하고 있었는데, 아직 그다지 완성도가 높지 않아보였으며(본인들의 공식 문서에도 없으므로) 시간이 넉넉하지 않아 도입이 망설여졌다. 그래서 다시 커넥션 직접 연결을 선택하고, 커넥션이 폭증하는 문제 자체를 해결하기로 했다. \n\n### 6. 커넥션 직접 연결로 회귀\n\n여담으로, 일을 하면서 챗지피티의 도움을 참 많이 받는다. 그런데 이번 업무를 처리하면서 Deep한 업무일수록 챗지피티가 4.0이라도 헛소리를 많이 한다는 사실을 알게됐다. 어떻게 알았냐고? 나도 알고 싶지 않았다. \n\n아예 처음으로 돌아와서 커넥션 폭증에 대해서 다시 한 번 알아보기로 했다. 이쯤에서 이미 실서버는 오픈을 해야 했기 때문에, 커넥션 직접 연결로 바꾸고 추가 결제를 해서 커넥션 총 개수를 200여개로 늘려버렸다. 실서버에서는 인스턴스가 바뀌는 일이 자주 있는 일이 아니었으므로, 모니터링 결과 IDLE 상태로 접어든 죽은 커넥션도 12시간 안에 종료가 되는 것을 경험적으로 확인했으므로 실서버에서는 200개 정도로 커넥션 연결 가능 개수를 늘려두면 어쨌건 서비스가 가능했다. \n\n이렇게 실서버를 열어두고 문제를 해결하기 위해 시간을 녹이기 시작했다. \n\n커넥션 자체의 관리는 이론적으로는 아주 간단했다. 앱을 시작할 때에 커넥션을 열며, 앱을 종료할 때 커넥션을 닫는다. 이를 위해서 FastAPI의 `on_event(”startup”)`과 `on_event(”shutdown”)` 을 사용했다. 챗지피티도 이 코드를 알려줬고, 어느 블로그를 봐도 모두 이 코드를 사용하고 있었다. \n\n[https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated](https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated)\n\n근데 충격적이게도 이 방법은 이미 지원이 중단된 방법이었다.  이 방법 대신 lifespan 방법을 선택해야 했는데, 이 방법에 대해서 챗지피티는 알고 있음에도 불구하고 여전히 지원 중단된 방법을 추천하는 것을 보고  이 시점에서 챗지피티에 대한 신뢰를 상당 부분 잃어버리고 사용 빈도가 확 줄게 되었다. 나중에 알게 되지만 이 코드의 문제는 아니었지만! 아무튼 신뢰가 엄청나게 없어져버렸다. \n\n해당 코드를 lifespan을 활용한 코드로도 변경하고도 여전히 문제는 반복됐다. 앱이 시작될 때는 코드가 정상적으로 작동했지만, 컨테이너 안에서 앱이 종료될 때에는 종료 코드가 정상적으로 작동하지 않았다. \n\n이를 위해서 sigterm을 활용해 직접 종료 코드를 잡아서 실행하는 방법도 고려했지만 이 방법 역시 작동하지 않았다. \n\n하지만 아주 삽질은 아니어서, 로컬에서 실행한 경우에는 정상적으로 종료 코드가 작동하고 커넥션이 끊어지는 것을 확인해버렸다. \n\n### 7. 문제 해결\n\n힞;민 모드를 바꿨음에도 캐싱은 여전히 작동하지 않아서 캐싱을 강제로 꺼야 했고 잘 꺼지지도 않았는지 이후에도 Sqlalchemy의 캐싱 과정에서 키값이 중복되게 생성되는 문제가 있어 해당 부분을 오버라이드 해서 키 값이 중복되지 않게 만들게 해주는 등 문제가 있었지만 일단은 연결이 끊기거나 커넥션이 폭증하지 않아 해결이 된 것으로 보였다. \n\n사실 이 전에 종료 신호가 정상적으로 어플리케이션에 도달하지 못하는 것이 아닌가? 에 대해서 인지를 하고 있었다. 도커 컨테이너를 강제 종료 하면 기본적으로 docker는 1번 프로세스에만 종료 명령을 정상적으로 전달하고 나머지 프로세스에게는 종료 명령을 정상 전달하지 않는다. 도커 작업은 다른 개발자가 진행했는데, 컨테이너 안에서 실행되는 작업이라곤 Uvicorn 하나이므로 당연히 1번 프로세스일것이다 라는 말을 들어서 이 부분을 점검해볼 생각을 못하고 있었다. \n\n근데, 로컬에서 실행했을때에는 정상적으로 커넥션이 끊어지는데 도커 컨테이너를 통해서 실행한 경우에는 정상 종료가 되지 않는다? 이 문제를 확인하고 나서야 이 부분을 점검해볼 생각이 들었고, 점검 결과 충격적이게도 Uvicorn이 1번 프로세스로 실행되지 않고 있었다. \n\nDumb-init이라는 Docker 컨테이너의 모든 프로세스에게 종료 신호를 보내주는 소프트웨어도 잠깐 고려했지만, 간단하게 Docker exec 부분을 수정해 Uvicorn이 1번 프로세스로 컨테이너 안에서 실행되게끔 하자 거짓말처럼 모든 문제가 해결되었다. \n\n### 8. 회고\n\n전 회사에서도 이미 만들어진 커넥션을 쓰거나, 그냥 가이드대로 연결하면 대충 됐기 때문에 DB와 서버의 연결에 관해서 깊게 고찰해본 적이 없었다. 이번 일을 겪으면서 일단 DB와 서버의 연결에 대해.. 대충 알고 있던 부분들에 대해서 깊은 학습이 되었다. \n\n문제해결 방법을 가장 마지막에 발견했지만, 사실 꼼꼼히 살폈다면 초장에 해결할 수 있던 간단한 문제였다. 돌고 돌아 삥 돌아 해결을 했지만 최종적인 코드 수정은 굉장히 적었다. 공부 했으니 한잔해~ 하고 넘어가기에는 무보수 야근이 너무 잦았기 때문에 (덕분인지 시원하게 A형 독감도 걸렸다. 아무튼 야근 탓이다. ) 억울한 부분이 크다. \n\n챗지피티에 대한 신뢰가 컸는데, 이번 일을 진행하면서는 챗지피티 때문에 뱅뱅 돌아가게 된 부분이 많았다. 확실히 겉핥기 부분만 챗지피티에게 기대야 하며(또는 반복적인 일) 딥 한 문제나 조금이라도 인터넷에 자료가 적은 문제들에 대해서는 공식문서를 신뢰해야 한다는 사실을 다시 한 번 뼈저리게 느꼈다. 이후로 나는 예전처럼 챗지피티를 많이 쓰지는 않는다.. 쓰더라도 조심히 사용한다. \n\n한편으로, 커넥션 직접 연결 방법을 선택해 문제를 해결했지만 이게 완벽히 올바른 방법이라고는 생각이 들지 않는다. 아직은 서비스의 크기가 완벽하게 크지는 않기 때문에 이 방법으로도 가능하지만, Pooler를 사용하는 것이 올바른 해결 방법이 아닐까? 지금도 단순히 문제를 피해 도망친 것이라고밖에 생각이 들지 않는다. 특히 Pooler로 인해 발생한 문제들은 풀리지 않은 미스테리가 너무 많이 남았다. \n\n추후 기회가 있다면 supavisor를 기반으로 다시 만들어보고 싶은 생각이 있다. 아니, 반드시 그래야 한다고 생각이 든다.\n\n\n\n"},{"excerpt":"회사에서 DynamoDB를 사용할 때에, PynamoDB ORM을 사용하여 연결을 하고 있다.  Postgresql을 도입했을 때에 커넥션 문제가 지속적으로 발생해 (연결이.. 끊어지지 않는다!) 이를 처리하던 도중 PynamoDB는 어떻게 커넥션을 관리하는지 궁금해서 찾아보게 되었다.  PynamoDB란? PynamoDB는 DynamoDB를 위한 Pyt…","fields":{"slug":"/PynamoDB와-boto3-PynamoDB의-커넥션/"},"frontmatter":{"date":"December 08, 2023","title":"PynamoDB와 boto3, PynamoDB의 커넥션","tags":["DataBase","AWS","Python","Work"]},"rawMarkdownBody":"회사에서 DynamoDB를 사용할 때에, PynamoDB ORM을 사용하여 연결을 하고 있다. \n\nPostgresql을 도입했을 때에 커넥션 문제가 지속적으로 발생해 (연결이.. 끊어지지 않는다!) 이를 처리하던 도중 PynamoDB는 어떻게 커넥션을 관리하는지 궁금해서 찾아보게 되었다. \n\n### PynamoDB란? \n\nPynamoDB는 DynamoDB를 위한 Python 라이브러리이다. \n\n#### Pythonic Interface\n\n파이썬 개발자들이 익숙한 객체 지향 접근 방식을 제공한다. \n\n#### 간편한 데이터 모델링\n\nPynamoDB를 사용하면 DynamoDB 테이블과 항목을 Python 클래스로 모델링 할 수 있다. \n\n#### CRUD Operation\n\nPynamoDB는 Create, Read, Update, Delete 작업을 간편하게 수행할 수 있는 메소드를 제공한다. \n\n### PynamoDB의 커넥션 관리\n\nPynamoDB에서 커넥션은 boto3를 사용해서 관리하며, Low Level의 API를 사용하면 커넥션을 직접 연결할 수 있지만 기본적으로는 자동으로 관리를 한다. 설정 몇가지만 해주면(또는 하지 않아도) 필요에 따라서 알아서 커넥션을 맺고 끊는다. \n\n[https://pynamodb.readthedocs.io/en/stable/settings.html](https://pynamodb.readthedocs.io/en/stable/settings.html)\n\n### boto3란? \n\nAWS를 위한 AWS 공식 Python SDK이다. 이 라이브러리를 통해 Python 어플리케이션에서 AWS를 쉽게 사용 및 관리 할 수 있다. \n\n#### 광범위한 AWS 서비스 지원\n\nS3, EC2, DynamoDB, IAM 등 AWS에서 제공하는 대부분의 서비스를 지원한다. \n\n#### 높은 수준의 객체 지향 API와 low level API 제공\n\n`Resource API`는 높은 수준의 객체 지향 인터페이스를 제공하여 사용자가 AWS 리소스를 직관적으로 다룰 수 있게 해준다. \n\n`Client API` 는 낮은 수준, 서비스 중심의 인터페이스를 제공하여 더 세밀한 제어가 필요할 때 사용할 수 있다. \n\n#### 자격 증명 관리와 보안\n\nAWS의 자격 증명을 효과적으로 관리할 수 잇는 기능을 제공한다.    \n\n"},{"excerpt":"소개 애플 단축어를 활용해 간단하게 슬랙 상태를 원클릭으로 변경할 수 있습니다. 슬랙의 API를 이용합니다.  저는 개인적으로 raycast의 뽀모도로 익스텐션과 함께 다음과 같이 사용하고 있습니다. \n집중을 시작할 때에  집중 모드 킴 음량 조절 노래 재생 슬랙 방해금지 상태 변경 \n집중을 끝낼 때에 집중 모드 끔 슬랙 방해금지 상태 해제 집중 모드에서…","fields":{"slug":"/단축어로-SLACK-프로필-변경하기/"},"frontmatter":{"date":"November 17, 2023","title":"단축어로 SLACK 프로필 변경하기","tags":["ETC","Work"]},"rawMarkdownBody":"## 소개\n\n애플 단축어를 활용해 간단하게 슬랙 상태를 원클릭으로 변경할 수 있습니다. 슬랙의 API를 이용합니다. \n\n저는 개인적으로 raycast의 뽀모도로 익스텐션과 함께 다음과 같이 사용하고 있습니다.\n\n![](image1.png)\n집중을 시작할 때에 \n\n- 집중 모드 킴\n\n- 음량 조절\n\n- 노래 재생\n\n- 슬랙 방해금지 상태 변경\n\n![](image2.png)\n집중을 끝낼 때에\n\n- 집중 모드 끔\n\n- 슬랙 방해금지 상태 해제\n\n\n\n집중 모드에서는 다른 알림을 모두 꺼두고 슬랙 알림만 켜서 업무 흐름은 놓치지 않게 해두었습니다. \n\n![](image3.png)\n메뉴바에서 원클릭으로 설정할 수 있어 편리합니다. \n\n## 설정법\n\n### 슬랙의 API App 생성\n\n먼저 슬랙의 API App을 만들어야 합니다. \n\n[https://api.slack.com/](https://api.slack.com/)\n\n해당 페이지로 이동 후, 우측 상단의 Your apps으로 이동합니다. \n\n`Create New App` 을 클릭합니다. \n\n`From an app manifest` 를 선택 후, workspace를 고르고 Next 합니다. \n\n`YAML`을 고르고, 다음의 내용을 입력합니다. \n\n```yaml\ndisplay_information:\n  name: Apple 상태변경 단축어\noauth_config:\n  scopes:\n    user:\n      - users.profile:write\nsettings:\n  org_deploy_enabled: false\n  socket_mode_enabled: false\n  token_rotation_enabled: false\n```\n\n권한을 더 원한다면 더 많이 입력해주셔도 됩니다. \n\n`Create`를 눌러 앱을 생성합니다. \n\n\n\n그 후 바로 그 화면에서 `Install to Workspace`를 클릭해 앱을 인스톨합니다. \n\n\n\n그리고, 좌측 메뉴에서 `OAuth & Permissions` 메뉴로 이동합니다. \n\n**OAuth Token** 값을 복사해줍니다. 슬랙 페이지에서 할 일은 여기까지입니다. \n\n### 단축어 설정\n\n단축어 앱을 열고 새로운 단축어를 추가합니다. \n\n`URL 콘텐츠 가져오기` 를 검색해서 다음과 같이 설정합니다. \n\n![](image4.png)\n입력해야 하는 값은 다음과 같습니다. \n\n- 헤더\n\n    키 : Authorization\n\n    값 : Bearer xoxp-xxxxx-xxxx…\n\n    \n\n    키 : Content-Type\n\n    값 : application/json; charset=UTF-8\n\n- 바디\n\n    키 : profile\n\n    값 : \n\n    ```json\n    {\n    \t\t\"status_text\": \"방해금지\",\n    \t\t\"status_emoji\": \":no_entry_sign:\"\n    }\n    ```\n\n    원하는 이모지와 텍스트를 넣어주시면 됩니다. \n\n    \n\n이렇게 하면 단축어가 완성됩니다. \n\n## 참고\n\n[https://api.slack.com/apps?deleted=1](https://api.slack.com/apps?deleted=1)\n\n"},{"excerpt":"aiohttp는 파이썬의 비동기 HTTP 네트워킹 라이브러리이다. 이 라이브러리는 asyncio를 사용하여 비동기 I/O를 수행하고 클라이언트와 서버 양쪽 모두에 대한 HTTP 지원을 제공한다.  즉 aiohttp를 사용하면 비동기적으로 HTTP 요청을 보내고 응답을 받을 수 있다.  주요 특징 비동기/동시성 지원 async, await를 사용하여 동시에…","fields":{"slug":"/aiohttp/"},"frontmatter":{"date":"November 08, 2023","title":"aiohttp","tags":["Python"]},"rawMarkdownBody":"aiohttp는 파이썬의 비동기 HTTP 네트워킹 라이브러리이다. 이 라이브러리는 asyncio를 사용하여 비동기 I/O를 수행하고 클라이언트와 서버 양쪽 모두에 대한 HTTP 지원을 제공한다. \n\n즉 aiohttp를 사용하면 비동기적으로 HTTP 요청을 보내고 응답을 받을 수 있다. \n\n## 주요 특징\n\n#### 비동기/동시성 지원\n\nasync, await를 사용하여 동시에 여러 HTTP 요청을 비동기적으로 처리할 수 있다. \n\n#### 서버와 클라이언트 사이드 지원\n\nHTTP 클라이언트 기능과 HTTP 서버 기능을 모두 제공한다. \n\n#### 웹소켓 지원\n\n웹소켓 연결 및 통신을 지원한다. \n\n#### 신호 및 슬롯 매커니즘\n\n요청 처리 과정에서 다양한 이벤트에 대응할 수 있게 해준다. \n\n## 설치\n\n파이썬 설치에 기본적으로 포함되지 않은 외부 라이브러리이다. \n\n```bash\npip install aiohttp\n```\n\npip를 이용해 간단하게 설치할 수 있다. \n\n## 예제\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(session, url):\n    async with session.get(url) as response:\n        return await response.text()\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        html = await fetch(session, 'http://python.org')\n        print(html)\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n```\n\npython 공식 홈페이지에서 HTML을 비동기적으로 가져오고 출력하는 간단한 예제이다. \n\n## 세션 관리\n\n**ClientSession 객체를 사용하여 HTTP 클라이언트 세션을 관리할 수 있다.** \n\n#### 연결 재사용\n\n여러 요청을 수행할 때, 세션은 자동으로 재연결을 사용한다.\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.get('http://httpbin.org/get') as resp:\n            print(await resp.text())\n        async with session.get('http://httpbin.org/get') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 쿠키 관리\n\n세션은 자동으로 쿠키를 관리한다. 첫 번째 요청에서 서버로부터 받은 쿠키는 자동으로 저장되고 다음 요청에 사용된다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        # 첫 번째 요청\n        async with session.get('http://httpbin.org/cookies/set?cookie_name=cookie_value') as resp:\n            print(await resp.text())\n        # 두 번째 요청 - 첫 번째 요청에서 설정된 쿠키가 포함됩니다.\n        async with session.get('http://httpbin.org/cookies') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 헤더의 기본값 설정\n\n세션은 생성할 때 기본 헤더를 설정할 수 있으며 이후의 모든 요청에 이 헤더가 포함된다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    headers = {'Authorization': 'Bearer your_token'}\n    async with aiohttp.ClientSession(headers=headers) as session:\n        async with session.get('http://httpbin.org/headers') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 타임아웃 관리\n\n세션 또는 개별 요청에 대한 타임아웃을 설정할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\nfrom aiohttp import ClientTimeout\n\nasync def main():\n    timeout = ClientTimeout(total=5)  # 전체 요청에 대한 타임아웃을 5초로 설정\n    async with aiohttp.ClientSession(timeout=timeout) as session:\n        async with session.get('http://httpbin.org/delay/10') as resp:\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 커스텀 설정\n\nSSL 검증 비활성화, 프록시 설정 등의 커스텀 설정을 할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.get('https://self-signed.badssl.com/', ssl=False) as resp:  # SSL 검증 비활성화\n            print(await resp.text())\n\nasyncio.run(main())\n```\n\n#### 자원 해제\n\n`async with` 문을 사용하면 세션 사용이 끝날 때 자동으로 자원을 해제한다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:  # 세션 생성 및 자동 자원 해제\n        async with session.get(url) as response:\n            return await response.text()\n\nasync def main():\n    html = await fetch('http://python.org')\n    print(html)\n\nasyncio.run(main())\n```\n\n## 에러 핸들링\n\n#### HTTP 응답 에러\n\n서버가 4XX 클라이언트 에러 또는 5XX 서버 에러를 반환하는 경우, `ClientResponseError` 가 발생할 수 있다. 이는 `raise_for_status()` 메서드를 사용하여 처리할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url) as response:\n                response.raise_for_status()\n                return await response.text()\n        except aiohttp.ClientResponseError as e:\n            print(f\"HTTP Response Error: {e.status} {e.message}\")\n        except aiohttp.ClientError as e:\n            print(f\"HTTP Client Error: {str(e)}\")\n        except Exception as e:\n            print(f\"Unexpected Error: {str(e)}\")\n\nasync def main():\n    await fetch('http://httpbin.org/status/400')  # 이 URL은 400 Bad Request를 반환합니다.\n\nasyncio.run(main())\n```\n\n#### 연결 에러\n\n네트워크 문제 또는 DNS 문제로 인해 연결을 설정할 수 없는 경우 `ClientConnectError` 예외가 발생할 수 있다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url) as response:\n                return await response.text()\n        except aiohttp.ClientConnectorError as e:\n            print(f\"Connection Error: {str(e)}\")\n\nasync def main():\n    await fetch('http://nonexistent.url')  # 존재하지 않는 URL\n\nasyncio.run(main())\n```\n\n#### 타임아웃 에러\n\n지정된 타임아웃 내에 서버로부터 응답을 받지 못하는 경우 `asyncio.TimeoutError` 예외가 발생한다.\n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url, timeout=1) as response:  # 1초 타임아웃 설정\n                return await response.text()\n        except asyncio.TimeoutError:\n            print(\"Timeout Error: The request timed out\")\n\nasync def main():\n    await fetch('http://httpbin.org/delay/10')  # 응답 지연 URL\n\nasyncio.run(main())\n```\n\n#### 일반 HTTP 클라이언트 에러\n\naiohttp가 발생시킬 수 있는 다른 클라이언트 에러를 처리한다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def fetch(url):\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(url) as response:\n                return await response.text()\n        except aiohttp.ClientError as e:\n            print(f\"Client Error: {str(e)}\")\n\nasync def main():\n    await fetch('http://httpbin.org/status/500')  # 서버 에러 URL\n\nasyncio.run(main())\n```\n\n## 스트리밍 응답\n\n스트리밍 응답을 처리하는 것은 큰 데이터를 다룰 때 특히 유용하다. 스트리밍을 사용하여 응답의 전체 내용을 메모리에 한 번에 로드하지 않고 데이터를 조각으로 나누어 처리할 수 있다. 이 방법은 메모리 사용량을 최적화하고 대용량 응답을 효율적으로 처리할 수 있도록 한다. \n\n```python\nimport aiohttp\nimport asyncio\n\nasync def stream_response(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            # 스트리밍 응답 처리\n            async for data in response.content.iter_any():\n                # 데이터 조각을 처리 (예: 파일에 쓰기, 출력 등)\n                print(data)\n\nasync def main():\n    await stream_response('http://httpbin.org/stream/20')\n\nasyncio.run(main())\n```\n\n이 예제에서는 `response.content.iter_any()` 를 사용하여 응답 스트림에서 데이터를 순차적으로 읽는다. \n\n스트리밍 응답을 사용할 때는 데이터의 양이 많거나 응답을 받는 동안 다른 처리를 동시에 해야 하는 경우에 매우 효과적이다. 예를 들어 다운로드한 데이터를 파일에 쓰면서 동시에 다음 데이터 청크를 받거나 데이터를 받아서 실시간으로 사용자에게 전송하는 경우 등에 유용하다. \n\n스트리밍을 사용할 때에는 네트워크 상황이나 서버의 응답 특성에 따라 데이터를 받는 속도가 일정하지 않을 수 있음을 인지하고 있어야 한다. 데이터를 처리하는 로직이 블로킹되지 않도록 주의해야 하며, 가능하면 각 청크를 처리하는데 시간이 너무 오래 걸리지 않도록 설계해야 한다.\n\n## 테스트\n\naiohttp의 비동기적인 특성 때문에 전통적인 동기 테스트 방식을 그대로 사용할 수 없다. 대신 aiohttp는 비동기 테스트를 지원하기 위해 pytest와 함께 사용할 수 있는 `aitohttp.test_utils` 모듈을 제공한다. 이를 통해 웹서버와 클라이언트 코드를 테스트 할 수 있다. \n\n#### 클라이언트 코드 테스트\n\n클라이언트 코드를 테스트 하기 위해서는 일반적으로 pytest와 pytest-aiohttp 플러그인을 사용한다. 이를 통해 aiohttp 클라이언트 세션을 비동기적으로 테스트 할 수 있다. \n\n```python\nimport aiohttp\nimport pytest\nfrom aiohttp import web\n\nasync def test_example_client(aiohttp_client):\n    async def hello(request):\n        return web.Response(text='Hello, world')\n    \n    app = web.Application()\n    app.router.add_get('/', hello)\n    \n    client = await aiohttp_client(app)\n    resp = await client.get('/')\n    assert resp.status == 200\n    text = await resp.text()\n    assert text == \"Hello, world\"\n```\n\naiohttp_client는 pytest-aiohttp 플러그인에서 제공하는 fixture로, 테스트용 애플리케이션 서버와 상호작용하는 데 사용할 수 있는 aiohttp.ClientSession 객체를 생성한다. \n\n\n        fixture란?\n테스트를 실행하기 전에 필요한 준비 작업과 설정을 의미한다. 일반적으로 fixture는 테스트 환경을 설정하고 테스트가 실행되는 동안 필요한 리소스나 상태를 생성하며, 테스트가 완료된 후에 정리 작업을 수행한다. \n\n#### 서버 코드 테스트\n\naiohttp 웹서버를 테스트 할 때에는 aiohttp.test_utils.TestClient를 사용하여 요청을 보내고 응답을 검사한다. \n\n```python\nfrom aiohttp import web\nfrom aiohttp.test_utils import AioHTTPTestCase, unittest_run_loop\nfrom myapp import create_app  # 가정: myapp 모듈에서 앱을 생성하는 함수\n\nclass MyAppTestCase(AioHTTPTestCase):\n\n    async def get_application(self):\n        \"\"\"\n        AioHTTPTestCase에서 애플리케이션 인스턴스를 생성하기 위해 오버라이드\n        \"\"\"\n        return create_app()\n\n    @unittest_run_loop\n    async def test_example(self):\n        resp = await self.client.request(\"GET\", \"/\")\n        assert resp.status == 200\n        text = await resp.text()\n        assert 'Hello, world' in text\n```\n\nAioHTTPTestCase는 aiohttp.test_utils에서 제공하는 기본 테스트 케이스 클래스이며, unittest_run_loop 데코레이터는 테스트 코루틴을 이벤트 루프에서 실행할 수 있게 해준다.\n\naiohttp를 테스트할 때 주의해야 할 점은 테스트 환경에서도 비동기 코드를 올바르게 실행하기 위해 적절한 테스트 실행기를 설정해야 한다는 것이다. pytest는 비동기 테스트를 위한 좋은 선택이며, pytest-asyncio 플러그인을 사용하면 pytest에서 async def 테스트 함수를 직접 사용할 수 있다.\n\n또한, 실제 HTTP 호출을 모킹하기 위해 aiohttp의 pytest_plugin이 제공하는 aioresponses와 같은 도구를 사용할 수도 있다. 이를 통해 실제 외부 서비스와의 통신 없이 HTTP 요청과 응답을 시뮬레이션할 수 있어, 테스트의 견고성을 높이고 실행 시간을 단축시킬 수 있다.\n\n\n\n"},{"excerpt":"Dependency Injection 시스템과 데코레이터 시스템은 유사한 목적을 가질 수 있다. 하지만 구현과 사용법에서 몇 가지 차이점이 있다.  공통점 명시적인 인증 체크 인증 로직을 명시적으로 라우트에 적용하여 추가 기능이 필요한 지점을 쉽게 식별할 수 있다.  중앙 집중식 관리 로직을 한 곳에서 관리함으로써 코드의 중복을 줄이고 일관성을 유지할 수…","fields":{"slug":"/FastAPI에서-데코레이터와-Dependency/"},"frontmatter":{"date":"November 07, 2023","title":"FastAPI에서 데코레이터와 Dependency","tags":["FastAPI","Python"]},"rawMarkdownBody":"Dependency Injection 시스템과 데코레이터 시스템은 유사한 목적을 가질 수 있다. 하지만 구현과 사용법에서 몇 가지 차이점이 있다. \n\n## 공통점\n\n#### 명시적인 인증 체크\n\n인증 로직을 명시적으로 라우트에 적용하여 추가 기능이 필요한 지점을 쉽게 식별할 수 있다. \n\n#### 중앙 집중식 관리\n\n로직을 한 곳에서 관리함으로써 코드의 중복을 줄이고 일관성을 유지할 수 있다. \n\n## 각 기능 별 장점\n\n### Dependency Injection (의존성 주입)\n\n#### 재사용성\n\nDependency는 여러 라우트에서 재사용 될 수 있으며 필요에 따라 다양한 파라미터와 함께 주입될 수 있다. \n\n#### 자동문서화\n\nFastAPI의 스웨거 UI에서 Dependency가 자동으로 문서화된다. \n\n#### 형식 안정성\n\nFastAPI의 형식 추론을 사용하여 파라미터의 유효성 검사와 변환을 자동으로 수행할 수 있다. \n\n### Decorator (데코레이터)\n\n#### 간결성\n\n데코레이터는 종종 코드를 더 간결하게 만들어주며, 의도를 명확히 표현하는 경우가 많다. \n\n#### 융통성\n\n데코레이터는 함수를 감싸는 방식으로 작동하므로 더 복잡한 로직을 내부에 포함할 수 있다. \n\n## 사용 시 고려 사항\n\n#### 비즈니스 로직의 분리 \n\n데코레이터는 추가를 원하는 로직과 비즈니스 로직을 물리적으로 분리하는데 유용할 수 있다. \n\n#### 코드의 명확성\n\nDependency는 의존성이 명시적이므로 코드를 읽고 이해하기 쉬워진다. \n\n#### 비동기 지원\n\n비동기 코드를 사용할 때에는 데코레이터 내부에서 async 함수를 올바르게 처리할 수 있는지 확인해야 한다. \n\n"},{"excerpt":"asyncio란? 소개 파이썬에서 비동기 프로그래밍을 위한 표준 라이브러리이다. 이 라이브러리는  을 사용하여 동시성 코드를 작성하는 데 필요한 구조를 제공한다. 단일 스레드 내에서도 여러 I/O 바운드 작업과 고수준의 구조화된 네트워크 코드를 동시에 실행할 수 있으며, 이는 효율성과 속도에서 큰 이점을 제공한다.  주요 컴포넌트 Event Loop 프로…","fields":{"slug":"/FastAPI와-asyncio/"},"frontmatter":{"date":"November 07, 2023","title":"FastAPI와 asyncio","tags":["Python","FastAPI"]},"rawMarkdownBody":"## asyncio란?\n\n### 소개\n\n파이썬에서 비동기 프로그래밍을 위한 표준 라이브러리이다. 이 라이브러리는 `coroutine` 을 사용하여 동시성 코드를 작성하는 데 필요한 구조를 제공한다. 단일 스레드 내에서도 여러 I/O 바운드 작업과 고수준의 구조화된 네트워크 코드를 동시에 실행할 수 있으며, 이는 효율성과 속도에서 큰 이점을 제공한다. \n\n### 주요 컴포넌트\n\n#### Event Loop\n\n프로그램의 진입점으로써, 비동기적으로 실행될 다양한 작업들을 관리한다. 작업이 실행 준비가 되면 이벤트 루프는 해당 작업을 실행하고 완료되기를 기다리는 다른 작업으로 제어를 전환한다. \n\n#### Coroutine\n\n코루틴은 `async def` 로 정의되는 비동기 함수이다. 코루틴은 `await`  키워드를 사용하여 실행을 일시 중지하고 이벤트 루프에 제어를 반환할 수 있어 다른 코루틴이 실행될 수 있게 한다. \n\n#### Future\n\n아직 완료되지 않은 작업을 의미한다. 코루틴이 완료될 때 결과를 저장하거나 예외를 전달할 수 있는데 이를 통해 비동기 작업의 최종 상태를 알 수 있다. \n\n#### Task\n\n`Future` 를 상속한다. 이는 이벤트 루프에서 코루틴의 실행을 가능하게 한다. Task는 코루틴을 스케줄링하고 실행 결과를 추적한다. \n\n### 사용 예시\n\n```python\nimport asyncio\n\nasync def main():\n    await asyncio.sleep(1)\n    print('Hello, World!')\n\n# 이벤트 루프 실행\nasyncio.run(main())\n```\n\n이 예제에서 `asyncio.run()` 함수는 `main()` 코루틴을 이벤트 루프에서 실행한다. `main()` 내부에서 `asyncio.sleep(1)` 은 비동기적으로 1초간 대기하도록 한다. 이 대기 시간동안 이벤트 루프는 다른 코루틴을 실행할 수 있다. \n\n### 동기화 기능\n\nasyncio는 비동기 프로그래밍 환경에서 동기화를 위한 여러 도구를 제공한다. asyncio.Lock은 공유 리소스에 대한 동시 액세스를 방지하고 asyncio.Event, asyncio.Condition, asyncio.Semaphore와 같은 다른 동기화 기본 요소도 사용할 수 있다. \n\n### 네트워킹 지원\n\nTCP, UDP, SSL, TLS 등을 비롯한 다양한 네트워크 프로토콜에 대한 지원을 제공한다. asyncio 스트림을 사용하여 비동기적으로 네트워크 I/O 작업을 할 수 있으며 이를 통해 서버와 클라이언트 양쪽 모두에서 비동기 네트워크 어플리케이션을 만들 수 있다. \n\n### 성능과 제한 사항\n\n입출력 바운드 작업과 고수준 구조화된 네트워크 코드를 실행하는데 효율적이다. 입출력 작업이 블로킹 되는 것을 피하면서 동시에 여러 입출력 작업을 관리할 수 있기 때문이다. \n\n예를 들어 웹서버는 많은 수의 동시 http 요청을 처리해야 하며 각 요청은 네트워크 입출력에 의존적이다. asyncio는 각 네트워크 연산이 완료되기를 기다리는 동안 다른 연산으로 전환하여 리소스를 효율적으로 사용할 수 있다. \n\n그러나 계산을 많이 요구 하는 작업(CPU 바운드 작업)에는 적압하지 않다. 이 작업은 병렬 처리가 필요한데, 이는 프로세스나 스레드를 여러개 사용해야 함을 의미한다. asyncio는 싱글 스레드, 싱글 프로세스 디자인이므로 병렬 CPU 작업을 위해서는 별도의 모듈이나 멀티 스레딩을 사용해야 한다. \n\n### 생명 주기\n\n#### 이벤트 루프 생성\n\nasyncio 프로그램은 이벤트 루프라는 중앙 실행기를 사용하여 실행된다. 이벤트 루프는 프로그램의 진입점에서 생성된다. 특히 아래의 함수를 사용하면 적절한 이벤트 루프를 설정하고 프로그램을 실행할 수 있다. \n\n```python\nasyncio.run(main())\n```\n\n#### 코루틴 실행\n\n`async def` 를 통해 코루틴을 생성하고 `await` 를 사용하여 실행을 스케줄한다. 코루틴은 이벤트 루프에 의해 실행될 태스크로 변환된다. \n\n```python\nasync def main():\n    # 코루틴 실행\n    result = await some_async_function()\n```\n\n#### 태스크 스케쥴링\n\n이벤트 루프는 `await` 표현식을 만날 때마다 실행을 일시 중지하고 다른 태스크로 제어를 전환한다. 이를 통해 CPU가 I/O 작업으로 인해 차단되는 것을 방지하고 다른 태스크의 실행을 계속할 수 있다. \n\n#### 비동기 I/O 처리\n\n`asyncio`는 네트워크 I/O, 파일 I/O와 같은 비동기 작업을 효율적으로 관리한다. 이벤트 루프는 모든 I/O 이벤트에 대해 비블로킹 소켓과 파일 디스크립터를 사용한다. \n\n#### 이벤트와 콜백 처리\n\n이벤트 루프는 등록된 이벤트가 발생했을 때 적절한 콜백 함수를 실행한다. 콜백은 완료된 I/O 작업의 결과를 다루거나 타이머 이벤트 같은 다른 유형의 이벤트를 처리하기 위해 사용된다. \n\n#### 종료 처리\n\n모든 태스크가 완료되거나 특정 조건을 만족하면 이벤트 루프는 종료될 수 있다. 이벤트 루프를 종료하기 전에 `close()` 함수를 호출하여 자원을 정리할 수 있다. \n\n```python\nloop = asyncio.get_event_loop()\ntry:\n    loop.run_until_complete(main())\nfinally:\n    loop.close()\n```\n\n`asyncio.run()` 을 사용하는 경우에는 이벤트 루프의 생성과 종료가 자동으로 관리된다. \n\n#### 예외 처리\n\n`asyncio` 는 예외가 발생할 경우 적절한 예외 처리를 위한 매커니즘을 제공한다. 코루틴 내부에서 발생한 예외는 해당 코루틴을 호출한 곳으로 전파되고 이벤트 루프는 처리되지 않은 예외를 포착하여 프로그래머에게 알린다. \n\n## FastAPI와 Async 함수\n\nFastAP의 가장 큰 특징 중 하나는 asyncio 라이브러리를 기반으로 한 비동기 프로그래밍 지원이다. 이를 통해 개발자는 비동기 파이썬 코드를 작성할 수 있으며 이는 특히 I/O 바운드 작업(네트워크 요청 처리, 디스크에서의 데이터 읽기 /쓰기 등)을 비동기적으로 처리하는데 유리하다. \n\n### 비동기 함수의 장점\n\n#### 성능 향상\n\nI/O 작업이 완료되기를 기다리는 동안 다른 코드를 실행할 수 있으므로 I/O 바운드 시스템에서 동시성을 크게 향상시킬 수 있다. \n\n#### 스케일성\n\n적은 수의 워커로 많은 수의 요청을 처리할 수 있으므로 더 효율적으로 리소스를 사용하고 스케일링 할 수 있다. \n\n#### 비용 효율성\n\n리소스 사용의 효율성은 특히 클라우드 환경에서 환경 절감으로 이어질 수 있다. \n\n#### 응답성 향상\n\n서버가 요청에 더 빨리 응답할 수 있기 때문에 사용자 경험이 향상된다. \n\n### 언제 비동기 함수를 쓰는 것이 더 유리할까? \n\n#### 네트워크 요청 처리\n\n웹 API 호출, 원격 데이터베이스 쿼리 등 네트워크 I/O 관련 작업을 처리할 때\n\n#### 고비용 I/O 작업\n\n디스크 작업이나 네트워크를 통한 파일 전송과 같은 I/O 집중적인 작업을 비동기적으로 실행할 때 \n\n#### 동시성이 필요한 작업\n\n다수의 클라이언트나 서비스로부터 오는 동시 요청을 효과적으로 처리해야 할 때\n\n#### WebSocket을 사용할 때 \n\n실시간 통신을 위해 WebSocket 연결을 관리하는 경우 \n\n### 비동기 함수 사용 시 주의할 점\n\n#### 데드락\n\n비동기 프로그래밍은 데드락에 빠질 위험이 있으므로 올바른 `await` 사용과 태스크 관리가 필요하다. \n\n\n        데드락이란?\n두 개 이상의 프로세스나 스레드가 서로의 작업 완료를 무한히 기다리게 되는 상태\n\n#### 에러 핸들링\n\n비동기 코드에서는 예외 처리가 더 복잡해질 수 있으므로 주의가 필요하다. \n\n#### 디버깅의 복잡성\n\n비동기 코드는 디버깅이 더 어려울 수 있다. \n\n#### 블로킹 코드와의 혼용\n\n비동기 코드 내에서 블로킹 동기 코드를 호출하면 성능이 저하될 수 있으므로 주의해야 한다. \n\n### `async`\n\n이 키워드는 두 가지 주요 상황에서 사용된다. \n\n#### 코루틴 함수를 정의할 때\n\n```python\nasync def fetch_data():\n    # 비동기 작업을 수행하는 코드\n```\n\n`async def` 는 이 함수가 코루틴 함수임을 나타낸다. 코루틴 함수는 호출될 때 즉시 실행되지 않고 대신에 코루틴 객체를 반환한다. 이 객체는 나중에 이벤트 루프에 의해 실행된다. \n\n#### async with와 async for를 사용할 때\n\n비동기 컨텍스트 매니저와 비동기 이터레이터를 사용할 때 필요하다. \n\n### `await`\n\n이 키워드는 코루틴의 실행을 일시 중지하고 코루틴의 결과가 준비될 때까지 기다린다. 코루틴, 태스크, 미래 객체 등이 뒤에 올 수 있다. \n\n```python\nasync def fetch_data():\n    data = await some_async_function()\n    return data\n```\n\n여기서 await는 some_async_function이 완료될 때까지 fetch_data 코루틴의 실행을 중지시킨다. 완료되면 그 결과값을 data에 할당하고 실행을 계속한다. \n\n이러한 비동기 프로그래밍 방식의 장점은 I/O 작업이 완료되기를 기다리는 동안 프로그램이 다른 작업을 계속할 수 있어 프로그램의 전반적인 실행 효율을 개선한다는 것이다.\n\n"},{"excerpt":"소개 파이썬의 중요한 특징 중 하나로 이터레이터 프로토콜을 사용하여 데이터의 시퀀스를 느긋하게(lazily) 생성하는데 사용된다. 즉, 생성기는 시퀀스의 전체 항목을 메모리에 한 번에 로드하지 않고 반복(iteration) 할 때마다 하나씩 항목을 생성한다.  이를 통해 메모리 사용을 줄이고 대용량 또는 무한한 시퀀스를 다룰 수 있다.  이터레이터 프로토…","fields":{"slug":"/생성기generate-패턴/"},"frontmatter":{"date":"November 03, 2023","title":"생성기(generate) 패턴","tags":["Python"]},"rawMarkdownBody":"## 소개\n\n파이썬의 중요한 특징 중 하나로 이터레이터 프로토콜을 사용하여 데이터의 시퀀스를 느긋하게(lazily) 생성하는데 사용된다. 즉, 생성기는 시퀀스의 전체 항목을 메모리에 한 번에 로드하지 않고 반복(iteration) 할 때마다 하나씩 항목을 생성한다. \n\n이를 통해 메모리 사용을 줄이고 대용량 또는 무한한 시퀀스를 다룰 수 있다. \n\n## 이터레이터 프로토콜\n\n파이썬에서 반복 가능한 객체(컬렉션)를 순회하기 위한 규약이다. 이 프로토콜은 반복(iteration) 동작을 정의하며 일련의 요소들에 대해 순차적으로 접근할 수 있게 해준다. 파이썬의 모든 반복 가능한 객체는 이 이터레이터 프로토콜을 구현해야 하며 이는 주로 두 가지 매직 매소드(magic method)로 구성된다. \n\n#### `__iter__()`\n\n이터레이터 객체를 반환해야 하며, 보통 self를 반환하여 객체 자체가 이터레이터인 경우를 처리한다. 이 메소드는 for 루프와 같은 반복 연산이 시작될 때 호출된다. \n\n### `__next__()`\n\n컬렉션의 다음 요소를 반환해야 한다. 만약 더 이상 요소가 없으면 `StopIteration` 예외를 발생시켜 반복을 종료해야 한다. \n\n## 생성기 만드는 방법\n\n### 생성기 함수 (Generator function)\n\n일반 함수와 비슷하지만 `return` 대신 `yield` 문을 사용하여 값을 하나씩 반환한다. `yield` 를 사용하면 함수는 해당 상태에서 실행을 일시 정지하고 다음 값이 요청될 때까지 대기 상태가 된다. \n\n### 생성기 표현식 (Generator expression)\n\n리스트 컴프리헨션과 유사하지만 괄호를 사용하여 정의한다. 이 방식은 단순한 경우에 대한 생성기를 간결하게 작성할 수 있게 해준다. \n\n## 예제\n\n### 생성기 함수 예제\n\n```python\ndef countdown(n):\n    print(\"Starting to count from\", n)\n    while n > 0:\n        yield n\n        n -= 1\n    print(\"Done!\")\n\n# 생성기 객체를 생성\ncd = countdown(3)\n\n# 생성기를 사용\nfor count in cd:\n    print(count)\n```\n\n이 코드를 실행하면 3, 2, 1이 출력되고 각 숫자 사이에 다른 코드를 실행할 수 있는 기회가 주어진다. 각 yield 문마다 함수의 상태는 일시 중단되고 다음 반복에서 계속된다. \n\n### 생성기 표현식 예제\n\n```python\nsquares = (x*x for x in range(10))\n\n# 생성기를 사용\nfor square in squares:\n    print(square)\n```\n\n이 코드는 0부터 9까지의 숫자에 대한 제곱을 출력한다. 반복(iteration)할 때마다 다음 제곱값이 생성되고 모든 값이 한 번에 메모리에 저장되지 않는다. \n\n## yield\n\nyield는 함수가 값을 반환(return)하면서도 그 상태를 기억하고 일시 중지(pause)되는 것을 가능하게 한다. 이는 함수의 실행을 중단시키지만 함수의 로컬 변수와 실행 상태가 유지되어 나중에 이 함수가 다시 호출될 때 이전 상태에서부터 실행을 재개할 수 있다. \n\n일반적인 return은 함수의 실행을 완전히 종료하고 해당 함수의 모든 상태(로컬 변수, 실행 컨텍스트)를 버리지만 yeild는 함수가 여전히 살아있다는 점에서 return과 다르다. yeild를 통해 반환된 함수(즉 생성기)는 `__next__()` 메소드를 호출함으로써 (또는 `next()` 함수를 사용하여) 다시 실행될 수 있으며, yield문 바로 다음부터 실행이 이어진다. \n\n## 이터러블(iterable) 객체\n\n### 소개\n\n멤버를 한 번에 하나씩 반환할 수 있는 객체로, 파이썬에선 주로 순회 가능한 모든 객체를 의미한다. 이터러블 객체는 for 루프와 같은 반복문을 사용해 순회할 수 있으며 이는 그 객체가 반복 가능한(iterable) 인터페이스를 구현하고 있기 때문이다. \n\n이터러블 객체가 되기 위해서는 객체 내에 `__iterm__()` 메소드를 구현해야 한다. 이 메소드는 이터레이터를 반환하고 이터레이터는 `__next__()` 메소드를 구현하여 순차적으로 값을 반환할 수 있어야 한다. `topIteration` 예외는 이터레이터가 더 이상 반환할 값이 없을 때 발생시켜 반복이 끝났음을 알린다.  \n\n### 파이썬의 이터러블 객체\n\n리스트, 튜플, 문자열, 딕셔너리, 집합 등이 있다. 또한 파일(file) 객체도 이러터블하며 파일의 각 줄을 순회할 수 있다. \n\n### 예제\n\n```python\nmy_list = [1, 2, 3, 4]  # 리스트는 이터러블 객체입니다.\nfor item in my_list:\n    print(item)  # 1, 2, 3, 4가 순차적으로 출력됩니다.\n```\n\n직접 이터러블 객체를 만들고 싶다면 `__iter__()` 메소드를 포함하는 클래스를 정의할 수 있다. \n\n```python\nclass Counter:\n    def __init__(self, low, high):\n        self.current = low\n        self.high = high\n\n    def __iter__(self):\n        return self  # self.__next__()를 호출할 이터레이터로 자신을 반환합니다.\n\n    def __next__(self):\n        if self.current < self.high:\n            num = self.current\n            self.current += 1\n            return num\n        else:\n            raise StopIteration\n\n# Counter 인스턴스는 이터러블 객체입니다.\ncounter = Counter(1, 4)\nfor num in counter:\n    print(num)  # 1, 2, 3이 출력됩니다.\n```\n\n위는 사용자 정의 이터러블 객체이다. \n\n위의 `Counter` 클래스는 `__iter__()`와 `__next__()` 메소드를 정의하여 이터러블 인터페이스를 구현한다. `Counter` 인스턴스를 순회할 때마다 `__next__()` 메소드가 호출되어 숫자를 하나씩 반환하고, 더 이상 반환할 숫자가 없을 때 `StopIteration` 예외가 발생한다.\n\n\n\n"},{"excerpt":"개요 FastAPI의 데코레이터는 파이썬 데코레이터 패턴을 활용하여 FastAPI 프레임워크에서 제공하는 여러 기능을 함수나 클래스에 적용하는 구문이다. 이 데코레이터들은 FastAPI에서 매우 중요한 역할을 한다.  데코레이터는  기호를 사용하여 함수나 클래스의 위에 선언된다. 데코레이터는 그 아래에 정의된 함수에 추가적인 기능을 부여하거나 특정 작업을…","fields":{"slug":"/FastAPI의-데코레이터/"},"frontmatter":{"date":"November 03, 2023","title":"FastAPI의 데코레이터","tags":["FastAPI","Python"]},"rawMarkdownBody":"## 개요\n\nFastAPI의 데코레이터는 파이썬 데코레이터 패턴을 활용하여 FastAPI 프레임워크에서 제공하는 여러 기능을 함수나 클래스에 적용하는 구문이다. 이 데코레이터들은 FastAPI에서 매우 중요한 역할을 한다. \n\n데코레이터는 `@` 기호를 사용하여 함수나 클래스의 위에 선언된다. 데코레이터는 그 아래에 정의된 함수에 추가적인 기능을 부여하거나 특정 작업을 수행하도록 지시한다. \n\n데코레이터를 사용하여 개발자는 복잡한 로직을 함수에 직접 쓰지 않고 프레임워크가 제공하는 데코레이터를 사용하여 빠르고 쉽게 웹 애플리케이션을 구현할 수 있다. \n\n## 주요 데코레이터\n\n### `@app.middleware(\"http\")`\n\nHTTP 요청-응답 사이클에 관여하는 미들웨어를 등록하는데 사용된다.  이 데코레이터 아래에 정의된 함수는 애플리케이션으로 들어오는 모든 http 요청에 대해 처리되고 그리고 해당 요청에 대한 응답을 반환하기 전에 호출된다. \n\n\n        미들웨어란?\n요청과 응답을 처리하는 과정 사이에 위치하여 들어오는 요청을 가로채 그 요청에 대해 특정 작업을 수행하거나 응답을 조작하는 구성 요소이다. \n\n#### 예제\n\n```python\n@app.middleware(\"http\")\nasync def custom_middleware(request: Request, call_next):\n    # 요청 전에 실행할 코드\n    response = await call_next(request)\n    # 응답 전에 실행할 코드\n    return response\n```\n\n크게 두 부분으로 나눠진다. \n\n- 요청 전에 실행할 코드 : `call_next` 함수에 요청을 전달하기 전에 실행할 코드를 작성한다. \n\n- 응답 전에 실행할 코드 : `await call_next(request)` 는 다음 미들웨어나 실제 요청을 처리하는 엔드포인트를 호출한다. 이후 응답이 반환되면 그 응답에 추가적인 처리를 하고 싶을 때 사용할 수 있다. \n\n#### 미들웨어 체인\n\n만약 `@app.middleware(\"http\")` 가 여러개 정의되어 있다면 FastAPI는 그것을 선언된 순서대로 실행한다. 각 미들웨어는 이전 미들웨어에서 `await call_next(request)` 를 호출한 후의 응답을 받아 처리한다. 이를 미들웨어 체인이라고 하며, 요청이 엔드포인트에 도달하기 전에 여러 미들웨어를 통과한다. \n\n따라서 미들웨어는 가벼운 로직을 수행하는 것이 좋으며, 무거운 작업은 미들웨어에서 피해야 한다. 또한 각 미들웨어는 만드시 `await call_next(request)` 를 호출하여 체인을 계속 진행할 수 있도록 해야 한다. \n\n### `@app.get`**,** `@app.post`**,** `@app.put`**,** `@app.delete`**,** `@app.options`**,** `@app.head`\n\nHTTP 메소드에 맞게 라우트를 설정하는 데코레이터이다. \n\n각각의 데코레이터는 해당 함수가 지정된 HTTP 메서드의 요청을 처리하는 엔드포인트임을 알려준다. 엔드포인트 함수 내부에서는 파라미터 검증, 비즈니스 로직, 데이터 반환 등의 작업을 수행할 수 있다. \n\n```python\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    return {\"item_id\": item_id}\n```\n\n### `@app.api_route`\n\n모든 http 메소드를 하나의 함수로 라우트 할 수 있게 해주는 데코레이터이다. \n\n하나의 엔드포인트에 여러 http 메소드를 지정할 수 있도록 한다. 예를 들어 같은 경로에 대해 Get과 Post 요청을 모두 처리하고 싶은 경우에 사용할 수 있다. 이 데코레이터를 사용하면 각 메서드에 대한 처리 로직을 한 함수에서 정의할 수 있다. \n\n```python\n@app.api_route(\"/items/{item_id}\", methods=[\"GET\", \"POST\"])\nasync def handle_item(item_id: int):\n    # 여기에 GET과 POST를 처리하는 로직을 구현\n    pass\n```\n\n### `@app.websocket`\n\n웹소켓 연결을 처리하는 엔드포인트를 선언하는 데코레이터이다. 클라이언트가 해당 경로로 웹소켓 연결을 시도하면 FastAPI는 해당 함수를 실행해 웹소켓 핸드셰이크를 처리하고 연결을 유지한다. \n\n### `@app.on_event(\"startup\" | \"shutdown\")`\n\n애플리케이션의 시작 시 또는 종료 시 실행할 함수를 등록하는 데코레이터이다. \n\n### `@app.exception_handler(Exc)`\n\n특정 예외를 처리하는 핸들러를 등록하는 데코레이터이다. 특정 예외 유형이 발생했을 때 실행될 커스텀 핸들러를 등록하는 데 사용된다. 표준 예외 로직을 오버라이드 하거나 특정 예외 유형에 대해 특별한 처리를 구현할 수 있다. \n\n예를 들어 ValueError가 발생했을 때, 표준 HTTP 500 Error 대신 더 구체적인 오류 메세지와 HTTP 400 코드를 반환할 수 있다. \n\n#### 예제\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom starlette.requests import Request\n\napp = FastAPI()\n\n@app.exception_handler(ValueError)\nasync def value_error_exception_handler(request: Request, exc: ValueError):\n    return JSONResponse(\n        status_code=400,\n        content={\"message\": str(exc)},\n    )\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    if item_id < 0:\n        raise ValueError(\"Item ID must be positive\")\n    return {\"item_id\": item_id}\n```\n\n이를 이용해 다음과 같은 이점을 얻을 수 있다. \n\n1. 유저 친화적인 오류 메시지 제공\n\n1. 로그 이록\n\n1. 오류 리포팅\n\n1. 커스텀 http 상태 코드 반환 : 기본적으로 변경된 메시지 대신 사용자에게 안내를 줄 수 있다. \n\n이러한 예외 핸들러는 API 의 로버스트성을 늘려준다. \n\n\n        로버스트성이란?\n소프트웨어가 예기치않은 입력이나 사용 상황에서도 안정적으로 작동하는 성질을 의미한다. \n\n### `@app.dependency`\n\n함수가 종속성으로 작동하게 하며 해당 함수가 다른 경로 작업에서 호출될 때마다 실행되게 한다. 이를 이용해 공통 기능을 중앙에서 관리하고 경로 작업에서 필요한 데이터를 제공하거나 사전 처리를 수행할 수 있다. \n\n#### 예제\n\n모든 경로에서 공통으로 사용되는 데이터베이스 세션을 생성하는 경우를 가정하자. 아래는 해당 데코레이터를 사용하여 데이터베이스 세션을 경로에 주입하는 예제이다. \n\n```python\nfrom fastapi import FastAPI, Depends\n\napp = FastAPI()\n\nclass DBSession:\n    # DBSession 클래스는 데이터베이스 세션을 관리합니다.\n    def __init__(self):\n        self.session = \"DB Connection\"\n\n    def close(self):\n        self.session = \"DB Connection Closed\"\n\n# 종속성으로 사용될 함수\n@app.dependency\nasync def get_db_session():\n    db_session = DBSession()\n    try:\n        yield db_session.session\n    finally:\n        db_session.close()\n\n# 경로 작업에서 종속성 사용\n@app.get(\"/items/\")\nasync def read_items(db: str = Depends(get_db_session)):\n    return {\"db_session\": db}\n```\n\nget_db_session 함수가 @app.dependency 데코레이터로 마크되어있다. 이 함수는 호출될 때마다 새로운 DBSession 인스턴스를 생성하고 요청 처리가 완료된 후 세션을 정리한다. \n\n이 함수는 경로 작업 함수 read_items에 Depends를 사용하여 주입된다. 경로 작업에서는 반환된 데이터베이스 세션을 db라는 변수로 받아 사용할 수 있다. \n\nDepends를 사용함으로써 FastAPI는 get_db_session 함수를 실행하고 그 반환값을 read_items 경로 작업의 매개변수로 전달한다. 이 패턴은 서비스 계층, 데이터 접근 계층 등에서 특히 유용하며 코드 중복을 줄이고 테스트 용이성을 높여준다. \n\nFastAPI에서 해당 데코레이터를 사용해 정의된 함수는 생성기(generator) 패턴을 사용한다. yield 키워드를 사용해 이 함수는 값을 반환하기 전과 후에 코드를 실행할 수 있다. \n\n1. yield를 만날 때까지 함수를 실행한다. 이 때, DBSession 인스턴스가 생성되고, 세션이 초기화된다. \n\n1. yield에서 함수는 호출한 측에 db_session.session 값을 넘겨주고 실행을 일시 중지(pause) 한다. \n\n1. 이제 read_items 경로 함수의 본문을 실행한다. 이 때, db 매개변수로 전달된 값을 사용한다. \n\n1. read_items 함수가 완료되고 응답이 반환되면 yield문 이후의 코드가 실행된다. 이 코드에서는 finally 블록이다. \n\n즉 finally 블록은 http 요청 처리가 완전히 끝나고 응답이 클라이언트에게 전송된 후에 실행된다. 이는 DBSession 객체의 리소스를 안전하게 정리할 수 있게 해준다. yield를 사용하는 이 패턴은 파이썬의 컨텍스트 매니저와 유사한 방식으로 자원의 정리를 보장한다. \n\n### `@Query`**,** `@Path`**,** `@Header`**,** `@Cookie`**,** `@Body`**,** `@Form`\n\n엔드포인트의 각 파라미터를 특정 데이터 위치(쿼리 파라미터, 경로 파라미터, 헤더, 쿠키, 요청 본문, 폼 데이터)에 연결한다. \n\n### `@Response`**,** `@JSONResponse`**,** `@HTMLResponse`**,** `@FileResponse`\n\n특정 응답 클래스를 사용하여 응답을 반환한다. 예를 들어 `@JSONResponse`는 JSON 형식의 응답을 반환할 때 사용된다. \n\n\n\n"},{"excerpt":"특징 및 장점 타입 힌트를 활용한 API 선언 Python 3.6 이상의 타입 힌트를 활용하여 API 매개변수 및 응답 모델을 선언한다. 이를 통해 데이터 검증, 직렬화, 문서화를 자동화한다. 이러한 방식은 깔끔한 코드 작성과 함께 명확한 API 명세를 제공한다.  속도 Starlette(ASGI 기반) 및 Pydanic의 결합으로 다른 파이썬 프레임워크…","fields":{"slug":"/FastAPI/"},"frontmatter":{"date":"November 02, 2023","title":"FastAPI","tags":["FastAPI","Python"]},"rawMarkdownBody":"## 특징 및 장점\n\n### 타입 힌트를 활용한 API 선언\n\nPython 3.6 이상의 타입 힌트를 활용하여 API 매개변수 및 응답 모델을 선언한다. 이를 통해 데이터 검증, 직렬화, 문서화를 자동화한다. 이러한 방식은 깔끔한 코드 작성과 함께 명확한 API 명세를 제공한다. \n\n### 속도\n\nStarlette(ASGI 기반) 및 Pydanic의 결합으로 다른 파이썬 프레임워크에 비해 월등히 높은 성능을 자랑한다. \n\n### 자동화된 문서\n\n타입 힌트와 함께 추가된 FateAPI 데코레이터를 사용하여 자동으로 실시간 API 문서(Swagger UI 및 ReCod)를 생성한다. \n\n### 데이터 검증\n\nPydantic을 이용하여 입력 데이터의 유효성을 자동으로 검사하고 오류에 대한 명확한 정보를 반환한다. 유효하지 않은 데이터에 대한 명확하고 읽기 쉬운 오류 메시지를 자동으로 제공한다. \n\n### 비동기 지원\n\n`async/await` 문법을 사용하여 DB 작업, 파일 작업 및 네트워크 작업 등의 비동기 코드를 쉽게 통합할 수 있다. \n\n### 보안 기능\n\nOAuth2, 비밀번호 해싱, JWT 토큰 생성 및 검증, CORS 등 웹 API 보안에 필요한 기능들이 아웃 오브 더 박스에서 제공된다. \n\n### 확장성\n\n사용자의 요구에 따라 쉽게 확장할 수 있으며 다양한 서드파티 라이브러리와 통합이 용이하다. \n\n### 의존성 주입 시스템\n\n요청 처리 함수의 매개변수로 의존성을 선언하면 이를 자동으로 처리하고 제공한다. 이를 통해 공통적인 기능(데이터베이스 세션, 토큰 검증 등)을 재사용할 수 있다. \n\n## 다른 웹 프레임워크와의 비교\n\n- Flask : 가볍고 간단하며 확장 가능하다. 하지만 기본적으로 동기식이며 ASGI를 사용한 비동기 처리를 지원하지 않는다. 데이터 검증이나 자동 문서화 같은 기능을 위해서는 추가 확장 또는 라이브러리가 필요하다. \n\n- Django : 많은 기능(관리자 인터페이스, ORM, 인증..)등이 내장되어 있다. 큰 커뮤니티와 문서화가 잘 되어있다. 그렇지만 무거울 수 있으며 프로젝트의 구조나 방식이 명확하게 정해져있어 높은 수준의 커스터마이제이션을 원하는 개발자는 제약이 될 수 있다. 기본적으로 동기식이다.\n\n- Tornado : 비동기 네트워크 라이브러로 시작하여 웹 프레임워크로 발전했다. WenSocket과 같은 비동기 작업에 적합하다. 다만 최신 ASGI 프레임워크보다는 성능면에서 뒤쳐질 수 있다. 자동 문서화나 타입 힌트 기반의 데이터 검증과 같은 기능이 없다. \n\n## 단점\n\n### 상대적으로 작은 커뮤니티의 크기 \n\nFlask나 Django처럼 오랜 기간 사용된 프레임워크에 비해 커뮤니티가 상대적으로 작아 확장 기능, 플러그인, 해결책을 찾는데 어려움을 겪을 수 있다. \n\n### 기술 부채\n\n비교적 최근에 개발되어 예전의 기존 시스템과 통합하거나 레거시 코드와 병합할 때 어려움을 겪을 수 있다. \n\n### 학습 곡선\n\n타입 힌트, Pydantix, ASGI 등 다양한 기능들이 초기 사용자에게는 약간의 학습 곡선을 만들 수 있다. 특히 비동기 프로그래밍에 익숙하지 않다면 어려움을 겪을 수 있다. \n\n### 완전한 ORM의 부재\n\n데이터베이스 통합을 위해 SQLAlchemy나 Tortoise-ORM과 같은 외부 라이브러리를 사용한다. 프레임워크 자체의 통합된 ORM을 제공하지 않는다. \n\n### 제한된 동기 지원\n\n주로 비동기 환경을 위해 설계되어 동기 코드를 사용하는 라이브러리 또는 시스템과 통합할 때 추가적인 노력이 필요할 수 있다. \n\n## WSGI와 ASGI\n\nWSGI와 ASGI는 모두 웹서버와 파이썬 웹 어플리케이션 사이의 표준 인터페이스를 정의하는 규격이다. \n\n### WSGI (Web Server Gateway Interface)\n\n동기적인 웹 어플리케이션을 위한 표준 인터페이스를 제공하며, 전통적인 웹 어플리케이션 개발에 널리 사용된다. \n\n- 목적 : 웹서버와 파이썬 웹 어플리케이션 혹은 프레임워크가 서로 통신할 수 있도록 하는 간단한 호출 규약을 정의한다. \n\n- 동작 방식 : 동기적이며, 한 번에 하나의 요청만 처리할 수 있다. 이로 인해 여러 동시 요청을 처리하기 위해서는 다중 프로세스나 다중 스레드 방식을 사용해야 한다. \n\n- 대표적인 서버 및 미들웨어 : uWSGI, gunicorn, mod_wsgi\n\n- 사용되는 프레임워크 : Flask, Django 등 대부분의 전통적인 파이썬 웹 프레임워크\n\n### ASGI (Asynchronous Server Gateway Interface)\n\n비동기 웹 어플리케이션과 다양한 웹 프로토콜을 위한 확장된 인터페이스를 제공한다. 최근의 웹 트렌드에 맞게 설계되었다. \n\n- 목적 : WSGI의 확장 버전으로 웹소켓과 같은 비동기 프로토콜의 지원과 더 빠른 성능을 목표로 한다. \n\n- 동작 방식 : 비동기이며, 이를 통해 한 번에 여러 요청을 처리할 수 있다. 주로 `async/awit` 구문을 활용한 비동기 프로그래밍을 가능하게 한다. \n\n- 대표적인 서버 : Daphne, Uvicorn, Hypercorn 등\n\n- 사용되는 프레임 워크 : FastAPI, Starlette 등\n\n## FastAPI의 작동 플로우\n\n### 1. API 선언\n\nAPI 엔드포인트를 선언하기 위해 파이썬 함수를 사용한다. 타입 힌트와 함께 Pydantic 모델을 사용하여 요청 및 응답 객체를 선언한다. \n\n이렇게 선언된 엔드포인트는 HTTP 메서드에 맞게 데코레이터로 데코레이트 된다. \n\n### 2. 데이터 검증 및 직렬화\n\n요청이 들어오면 Pydantic 모델을 사용하여 들어온 데이터를 검증한다. \n\n유효하지 않은 요청 데이터가 감지되면 자동으로 오류 응답을 생성하여 클라이언트에 반환한다. \n\n### 3. 의존성 해결\n\n각 엔드포인트에 선언된 의존성(예: 데이터베이스 세션, 인증 토큰 확인)을 자동으로 해결한다. \n\n이를 통해 반복적인 작업 없이 재사용 가능한 컴포넌트들을 생성할 수 있다. \n\n### 4. 요청 처리\n\n데이터 검증 및 의존성 해결이 완료되면 해당 엔드포인트 함수를 호출하고 처리한다. \n\n이 때, 비동기 함수(`async def`)를 사용하면 비동기 방식으로 요청을 처리할 수 있다. \n\n### 5. 응답 생성\n\n엔드포인트 함수의 반환값은 자동으로 JSON으로 직렬화된다. \n\n반환값이 Pydantic 모델 인스턴스라면 해당 모델을 사용하여 데이터 직렬화 및 검증이 이루어진다. \n\n### 6. 자동 문서화\n\n타입 힌트와 함께 선언된 엔드포인트를 기반으로 자동으로 API 문서를 생성한다. \n\n이 때 Swagger UI 및 ReDoc와 같은 도구를 사용하여 실시간 API 문서를 제공한다. \n\n### 7. 응답 반환\n\n최종적으로 처리된 응답이 클라이언트에 반환된다. 이 때 HTTP 상태 코드, 헤더, 쿠키 등도 함께 설정할 수 있다. \n\n## Uvicorn\n\nUvicorn은 ASGI(Asynchronous Server Gateway Interface)를 기반으로 한 빠르고 경량화된 웹 서버이다. Uvicorn은 주로 파이썬으로 작성된 비동기 웹 애플리케이션과 프레임워크의 서빙에 사용된다. \n\n### 특징 및 장점\n\n#### 비동기\n\nUvicorn은 비동기 I/O를 지원하며 이를 통해 동시에 여러 요청을 효율적으로 처리할 수 있다. 파이썬 3.6 이상에서 `async/await` 문법을 활용한다. \n\n#### 성능\n\nuvloop와 httptools를 사용하여 성능을 최적화한다. uvloop는 파이썬의 asyncio 모듈을 대체하는 빠른 이벤트 루프 구현이며 httptools는 http 파싱을 위한 라이브러리이다. \n\n#### ASGI 호환성\n\nASGI 3.0 사양을 구현하여 다양한 ASGI 호환 웹 애플리케이션 및 프레임워크와 함께 작동할 수 있다. \n\n#### 단순성\n\n코드 기반이 단순하며 웹 서버의 기본 기능에 집중한다. 보다 복잡한 배포나 튜닝이 필요한 경우 Uvicorn을 Gunicorn과 같은 다른 웹 서버 래퍼와 함께 사용하는 것이 일반적이다. \n\n### 단점\n\n#### 기능 제한\n\n핵심적인 웹 서버 기능에 중점을 둬 복잡한 배포 또는 다양한 설정, 최적화 옵션과 같은 기능이 제한적이다. \n\n#### 비동기 환경의 복잡성\n\n비동기 애플리케이션을 위한 웹 서버로 설계되어 비동기 코드와 동기 코드의 혼합 사용, 특히 데이터베이스와의 연결과 같은 경우 문제가 발생할 수 있다. 모든 코드를 비동기적으로 작성하거나 동기 코드가 올바르게 스레드 또는 프로세스에서 실행되도록 관리되어야 한다.\n\n## FastAPI 시작하기\n\n### 1. 설치\n\n```python\npip install fastapi[all] uvicorn\n```\n\n### 2. 기본 애플리케이션 생성\n\n```python\n# main.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n```\n\n### 3. 애플리케이션 실행\n\n```bash\nuvicorn main:app --reload\n```\n\n`—reload` : 개발 중 코드 변경을 자동으로 반영하기 위해 사용한다. \n\n### 4. 자동 문서화\n\nFastAPI는 자동으로 API 문서를 생성해준다. 애플리케이션을 실행 후 브라우저에서 다음 URL을 방문하여 문서를 확인할 수 있다. \n\n- Swagger UI: `http://127.0.0.1:8000/docs`\n\n- ReDoc: `http://127.0.0.1:8000/redoc`\n\n\n\n"},{"excerpt":"은 파이썬의 타입 힌트 시스템을 기반으로 데이터 검증 및 설정 관리를 제공하는 라이브러리이다. 특히 FastAPI에서는 Pydantic을 주로 요청 및 응답 객체의 데이터 검증, 직렬화 및 역직렬화에 사용한다.  주요 특징 타입 힌트 기반 Pydantic 모델은 파이썬의 타입 힌트를 활용하여 선언된다. 이를 통해 코드 내에서 명확하게 데이터의 구조와 타입…","fields":{"slug":"/Pydantic-모델/"},"frontmatter":{"date":"November 02, 2023","title":"Pydantic 모델","tags":["Python"]},"rawMarkdownBody":"`Pydantic` 은 파이썬의 타입 힌트 시스템을 기반으로 데이터 검증 및 설정 관리를 제공하는 라이브러리이다. 특히 FastAPI에서는 Pydantic을 주로 요청 및 응답 객체의 데이터 검증, 직렬화 및 역직렬화에 사용한다. \n\n## 주요 특징\n\n### 타입 힌트 기반\n\nPydantic 모델은 파이썬의 타입 힌트를 활용하여 선언된다. 이를 통해 코드 내에서 명확하게 데이터의 구조와 타입을 지정할 수 있다. \n\n### 자동 데이터 검증\n\n모델에 데이터를 할당할 때에 자동으로 데이터 검증이 수행된다. 유효하지 않은 데이터는 오류를 발생시킨다. \n\n### 데이터 변환\n\n입력 데이터를 적절한 타입으로 자동 변환한다. 예를 들어 문자열로 들어온 숫자 데이터를 정수로 변환할 수 있다. \n\n### 기본값 및 유효성 검사\n\n모델 필드에 기본값을 제공하거나 유효성 검사 규칙을 추가할 수 있다. \n\n### JSON 직렬화 및 역직렬화\n\n자동으로 JSON 형태로 직렬화되며 JSON 데이터를 역직렬화 하여 모델 객체로 변환할 수 있다. \n\n## 예시\n\n```python\nfrom pydantic import BaseModel, ValidationError\n\nclass User(BaseModel):\n    id: int\n    name: str\n    age: int\n    email: str\n\n# 사용 예\nuser_data = {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"age\": 30,\n    \"email\": \"johndoe@example.com\"\n}\n\nuser = User(**user_data)\nprint(user.json())  # 모델 객체를 JSON 형태로 출력\n\ntry:\n    invalid_data = {\n        \"id\": \"invalid\",\n        \"name\": \"John\",\n        \"age\": \"invalid\",\n        \"email\": \"johndoe\"\n    }\n    User(**invalid_data)\nexcept ValidationError as e:\n    print(e.errors())  # 데이터 검증 오류 출력\n```\n\n\n\n"},{"excerpt":"데이터베이스의 동시성 제어란? 여러 트랜잭션 또는 연산이 동시에 수행될 때 데이터의 일관성을 유지하고 충돌을 방지하기 위한 매커니즘을 말한다. 잘못된 동시성 제어는 데이터의 무결성을 해치고 시스템의 성능을 저하시키며 예측할 수 없는 결과를 초래할 수 있다.  동시성 제어의 주요 개념 락 (Locking) 데이터베이스 항목에 대한 동시 액세스를 제어하기 위…","fields":{"slug":"/DynamoDB의-동시성-제어Concurrency-Control/"},"frontmatter":{"date":"November 01, 2023","title":"DynamoDB의 동시성 제어(Concurrency Control)","tags":["AWS","DataBase"]},"rawMarkdownBody":"## 데이터베이스의 동시성 제어란?\n\n여러 트랜잭션 또는 연산이 동시에 수행될 때 데이터의 일관성을 유지하고 충돌을 방지하기 위한 매커니즘을 말한다. 잘못된 동시성 제어는 데이터의 무결성을 해치고 시스템의 성능을 저하시키며 예측할 수 없는 결과를 초래할 수 있다. \n\n### 동시성 제어의 주요 개념\n\n#### 락 (Locking)\n\n데이터베이스 항목에 대한 동시 액세스를 제어하기 위해 사용된다. 트랜잭션은 데이터에 액세스 하기 전에 락을 획득해야 하며, 작업이 완료되면 락을 해제해야 한다. \n\n일반적으로 비관적 잠금과 낙관적 잠금의 두 형태로 구분된다. \n\n#### 트랜잭션 (Transaction)\n\n일련의 데이터베이스 작업을 하나의 논리적 단위로 묶는 것이다. 트랜잭션은 모두 성공하거나 모두 실패해야 하며, 중단 상태에서 중단되어서는 안된다. \n\n#### 일관성 (Consistency)\n\n데이터베이스의 모든 트랜잭션이 데이터를 올바른 상태에서 유지하도록 보장하는 속성이다. \n\n- 강력한 일관성 (Strong Consistency)\n\n    데이터베이스의 모든 노드가 항상 동일한 데이터를 보게 됨을 보장한다. \n\n    데이터베이스에 쓰기 작업이 수행된 후 모든 후속 읽기 작업이 그 쓰기 작업의 결과를 반영해야 한다. \n\n    실시간 시스템, 금융 시스템 또는 기타 높은 무결성이 필요한 시스템에서 중요하다. \n\n    일반적으로 높은 네트워크 지연과 함께 오며, 시스템의 전반적인 성능과 확장성에 영향을 미칠 수 있다. \n\n- 최종 일관성 (Eventual Consistency)\n\n    시간이 지나면서 데이터베이스의 모든 노드가 동일한 데이터를 볼 수 있게 됨을 보장한다. \n\n    쓰기 작업이 수행된 후 일정 시간이 지나면 모든 읽기 작업이 쓰기 작업의 결과를 반영한다. \n\n    일반적으로 높은 확장성과 낮은 지연 시간을 제공하며 분산 시스템에서 높은 성능을 달성하는데 유리하다. \n\n    일시적인 데이터 불일치를 허용하므로 애플리케이션의 로직이 이러한 불일치를 처리할 수 있도록 설계되어야 한다. \n\n#### 분리성 (Isolation)\n\n각 트랜잭션이 독립적으로 실행되도록 보장하는 속성이다. 트랜잭션들 사이에 어떤 중간 상태도 다른 트랜잭션에게 보여져셔는 안된다. \n\n#### 지속성 (Durability)\n\n트랜잭션이 성공적으로 완료된 후에는 시스템의 어떤 실패에도 영향 받지 않고 데이터베이스 변경이 유지되도록 보장하는 속성이다. \n\n## DynamoDB의 동시성 제어 매커니즘\n\nDynamoDB는 동시성 제어와 데이터 일관성을 관리하기 위한 여러 매커니즘을 제공한다. \n\n### 조건식 사용 (Conditional Writes)\n\n항목을 작성하거나 업데이트 할 때 조건식을 사용하여 해당 항목이 변경되지 않았음을 확인할 수 있다. 이것은 항목이 기존에 있던 상태에 따라 연산을 수행하도록 할 때 유용하다. 예를 들어 항목이 특정값에 해당하지 않으면 업데이트 하지 않도록 요청할 수 있다. \n\n### 컨설턴트 (Consistent Reads)\n\n기본적으로 최종적으로 일관성 있는 읽기를 제공한다. 이것은 약간의 지연을 허용하면서 빠른 응답 시간을 제공한다. 하지만 바로 이전의 쓰기 작업 이후에 항목의 최신 상태를 확인하려면 Strongly Consistent Read 옵션을 사용할 수 있다. 이 옵션은 데이터의 최신 상태를 보장한다.\n\n<hr style=\"border: none; height: 1px; background-color: #e0e0e0; margin: 16px 0;\" />\n하지만 Dynamo DB에서 동시성은 주로 다음의 두 가지 방법을 사용하여 수행된다. \n\n## 낙관적 잠금 (Optimistic Locking)\n\n낙관적 잠금은 클라이언트가 업데이트(또는 삭제) 하려는 항목이 DynamoDB의 항목과 동일한지 확인하는 전략이다. 이 전략을 사용하면 데이터베이스 쓰기는 다른 사용자의 쓰기에 의해 덮여지는 일을 방지할 수 있다. \n\n이 전략은 DynamoDB의 테이블에 `버전 번호` 라는 속성을 사용하여 구현된다. 클라이언트가 데이터 항목을 수정할 때 클라이언트 측의 버전 번호가 테이블 항목의 버전 번호와 동일해야 한다. 버전 번호가 동일하면 다른 사용자가 레코드를 변경하지 않았음을 의미하며 쓰기를 수행할 수 있다. \n\n그러나 버전 번호가 다르면 다른 사용자가 이미 레코드를 업데이트 했을 가능성이 있으며, 이 경우 DynamoDB는 `ConditionalCheckFailedException` 예외를 발생시켜 쓰기를 거부한다. \n\n낙관적 잠금을 사용하기 위해서는 코드에서 `ConditionalCheckFailedException` 을 포함하여 `put`, `delete`, `update` 작업에 대한 조건부 쓰기를 구현해야 한다. \n\n## 비관적 잠금 (Pessimistic Locking)\n\n비관적 잠금은 동시 업데이트를 방지하기 위해 사용하는 또 다른 전략이다. 낙관적 잠금이 버전 번호를 클라이언트에 전송하는 반면, 비관적 잠금은 버전 번호를 유지하지 않고 동시 업데이트를 방지하려고 한다. \n\n대부분의 데이터베이스 서버는 트랜잭션 시작 시 잠금을 획득하고 트랜잭션이 완료된 후에만 잠금을 해제하지만, DynamoDB는 이를 약간 다르게 처리한다. DynamoDB는 데이터를 잠그지 않고, 대신 트랜잭션을 사용하여 검토 중인 데이터에 대한 다른 스레드의 변경을 식별하며, 변경이 감지되면 DynamoDB는 트랜잭션을 취소하고 오류를 발생시킨다. \n\n비관적 잠금을 구현하기 위해 `taransacWrite` 라는 메소드를 제공한다. 이를 사용하면 처리 중인 항목을 잠그지 않지만 항목을 모니터링하고, 다른 스레드가 데이터틀 수정하면 전체 트랜잭션이 데이터 변경으로 인해 실패하고 데이터를 롤백한다. \n\n## RDB 와의 차이점\n\n### 데이터 모델\n\n- RDB\n\n    관계형 데이터 모델을 사용하여 테이블, 행, 열의 데이터로 형태를 데이터 구조하화며, 테이블 간의 관계를 정의하여 데이터의 무결성을 유지한다. \n\n- DynamoDB\n\n    NoSQL 데이터베이스로 키-값과 문서 데이터 모델을 사용한다. 데이터 무결성 보장을 위해 관계형 데이터베이스와 다른 매커니즘을 사용한다. \n\n### 동시성 제어\n\n- RDB\n\n    트랜잭션을 사용하여 동시성을 제어한다. 트랜잭션을 시작할 때 데이터에 잠금을 걸고, 트랜잭션이 완료될 때까지 잠음을 유지하여 다른 트랜잭션에 의한 동시 수정을 방지한다. \n\n- DynamoDB\n\n    낙관적 잠금과 조건부 업데이트를 사용하여 동시성을 제어하거나 `taransacWrite` API를 사용하여 트랜잭션을 지원한다. \n\n### 일관성\n\n- RDB\n\n    보다 강력한 일관성(Strong Consistency)을 지원한다. 트랜잭션의 모든 변경이 즉시 모든 다른 트랜잭션에 표시된다. \n\n- DynamoDB\n\n    기본적으로 최종 일관성(Eventual Consistency)을 제공한다. 필요한 경우 강력한 일관성(Strong Consistency)을 요청할 수 있다. \n\n### 분리성\n\n- RDB\n\n    트랜잭션의 분리성을 보장하여 각 트랜잭션이 독립적으로 실행되도록 한다. \n\n- DynamoDB\n\n    `TransactWrite` API를 통해 여러 항목에 대한 트랜잭션을 지원하여 분리성을 제공한다. \n\n### 지속성\n\n두 DB 모두 트랜잭션이 성공적으로 완료된 후에는 시스템의 어떤 실패에도 영향을 받지 않고 데이터베이셔 변경이 유지되도록 지속성을 보장한다. \n\n## 참고 문헌\n\n[https://dynobase.dev/dynamodb-locking/](https://dynobase.dev/dynamodb-locking/)\n\n"},{"excerpt":"개요 Poetry는 파이썬 프로젝트의 의존성 관리와 패키징을 도와주는 도구이다. Poetry의 주요 목표는 프로젝트의 설정과 관리를 단순화하고, 이를 위한 통합된 솔루션을 제공하는 것이다.  파이썬 개발자에게 일관된 개발 환경을 제공하며, 의존성 해결, 버전 관리, 패키징 배포와 같은 일반적인 작업을 처리하는 통합된 도구를 제공하여 파이썬 프로젝트의 생명…","fields":{"slug":"/Poetry/"},"frontmatter":{"date":"October 31, 2023","title":"Poetry","tags":["Python"]},"rawMarkdownBody":"## 개요\n\nPoetry는 파이썬 프로젝트의 의존성 관리와 패키징을 도와주는 도구이다. Poetry의 주요 목표는 프로젝트의 설정과 관리를 단순화하고, 이를 위한 통합된 솔루션을 제공하는 것이다. \n\n파이썬 개발자에게 일관된 개발 환경을 제공하며, 의존성 해결, 버전 관리, 패키징 배포와 같은 일반적인 작업을 처리하는 통합된 도구를 제공하여 파이썬 프로젝트의 생명 주기를 관리하는데 필요한 모든 기능을 제공한다. \n\n기존의 requirements.txt, venv, setup.py, wheel, twine 등을 모두 통합해 하나의 인터페이스와 프로세스로 간소화하여 제공한다. \n\n## 기능과 이점\n\n### 의존성 관리\n\n`pyproject.toml` 파일을 사용하여 프로젝트의 의존성을 선언적으로 정의하고 관리한다. \n\n의존성이 충돌하지 않고 서로 호환되는지 자동으로 확인하여 프로젝트의 의존성 그래프를 안정적으로 유지한다. \n\n### 패키징\n\n프로젝트를 패키지화하고 배포할 수 있는 도구를 제공한다. \n\n`build` 및 `publish` 명령을 사용하여 프로젝트를 빌드하고 PyPi와 같은 패키지 저장소에 배포할 수 있다. \n\n### 가상 환경 관리\n\n프로젝트에 대한 가상 환경을 자동으로 생성하고 관리하여 프로젝트의 의존성이 시스템 전역의 Python 환경으로부터 격리되도록 한다. \n\n`poetry shell` 과 `poetry run` 명령어를 사용하여 가상 환경 내에서 명령을 실행할 수 있다. \n\n### 버전 관리\n\n프로젝트의 버전을 쉽게 관리할 수 있도록 돕는다. \n\n`version` 명령어를 사용하여 프로젝트의 버전을 업데이트 할 수 있다. \n\n### 프로젝트 생성과 설정\n\n`new` 명령을 사용하여 프로젝트를 생성하고 기본 구조와 설정 파일을 자동으로 생성한다. \n\n### 플러그인 시스템\n\n사용자 정의 플러그인을 지원하여 기능을 확장하고 사용자 정의 작업을 수행할 수 있다. \n\n### 다양한 설정 옵션\n\n다양한 설정 옵션을 제공하여 프로젝트의 특별한 요구 사항에 따라 도구의 동작을 사용자 정의할 수 있다.  \n\n## requirements.txt 와의 비교\n\nrequirements.txt는 파이썬 프로젝트에서 아요되는 의존성과 그 버전을 나열하는 텍스트 파일이다. 프로젝트에 필요한 모든 패키지와 버전을 명시하며, 이를 통해 다른 개발자가 다른 환경에서 동일한 의존성을 쉽게 유지할 수 있다. \n\n- requirements.txt는 단순히 프로젝트의 의존성과 해당 버전을 나열하는 데 사용되며, 이를 넘어서는 확장성을 제공하지 않는다.\n\n- 의존성 간의 충돌을 자동으로 해결하지 않는다. \n\n- 가상 환경을 자동으로 생성하거나 관리하지 않는다. \n\n- 프로젝트 패키징, 배포 기능을 제공하지 않는다. \n\n- 프로젝트의 구조나 설정에 대한 정보를 제공하지 않는다. \n\n## venv와의 비교\n\nvenv는 파이썬 가상환경을 생성하고 관리하는 기본 도구이다. venv의 주요 목적은 프로젝트의 의존성을 시스템 전역의 파이썬 환경으로부터 격리하는 것이다. \n\n- venv는 의존성을 관리하지 않는다. 가상 환경 내에서 pip를 사용하여 수동으로 패키지를 설치하고 관리해야 한다. \n\n- 프로젝트 패키징, 배포 기능을 제공하지 않는다. \n\n- 프로젝트의 구조나 설정에 대한 정보를 제공하지 않는다. \n\n- poetry는 install 명령을 실행할 때에 자동으로 가상환경을 생성하지만, venv는 별도의 명령어를 사용해 가상환경을 생성해야한다. \n\n- 기본적인 명령줄 인터페이스만 제공한다. \n\n## 단점\n\n### 학습곡선\n\n그 자체로 너무 많은 기능이 들어있기 때문에 기존 도구에들에 익숙한 개발자들에게는 적응 시간이 필요할 수 있다. \n\n### 호환성 문제\n\n일부 레거시 프로젝트나 특정 시스템에서는 호환성 문제가 발생할 수 있다. \n\n### 성능\n\n큰 프로젝트에서 종속성의 해결이 다소 느릴 수 있다. \n\n### 통합 문제\n\n아직 모든 플랫폼이나 서비스에 완벽하게 통합되지 않았을 수 있다. CI/CD 도구와의 통합, 또는 특정 클라우드 서비스와의 통합에서 약간의 설정 작업이 필요할 수 있다. \n\n## 주요 사용법 및 명령어\n\n### 설치\n\n```bash\ncurl -sSL https://install.python-poetry.org | python3 -\n```\n\n### 새 프로젝트 시작\n\n```bash\npoetry new project-name\n```\n\n### 기존 프로젝트에서 시작\n\n이미 존재하는 프로젝트에서도 poetry를 사용할 수 있다. 프로젝트의 루트 디렉토리에서 다음 명령어를 실행한다. \n\n```bash\npoetry init\n```\n\n### 종속성 추가\n\n```bash\npoetry add package-name\n```\n\n한편 개발중에만 필요한 패키지를 추가하려면 다음을 실행한다. \n\n```bash\npoetry add package-name --dev\n```\n\n### 종속성 제거\n\n```bash\npoetry remove package-name\n```\n\n### 종속성 설치\n\n`pyproject.toml`에 정의된 종속성을 설치하려면 다음 명령어를 실행한다. \n\n```bash\npoetry install\n```\n\n### 스크립트 실행\n\n```bash\npoetry run python your_script.py\n```\n\n### 빌드 및 배포\n\n프로젝트를 패키징 하려면 다음의 명령어를 실행한다. 이렇게 하면 `dist`  디렉토리에 패키지가 생성된다. \n\n```bash\npoetry build\n```\n\n### 가상환경\n\npoetry는 자동으로 가상환경을 생성한다. 가상 환경에 들어가기 위해서는 다음의 명령어를 실행한다. \n\n```bash\npoetry shell\n```\n\n### 기능 탐색\n\n```bash\npoetry --help\n```\n\n\n\n"},{"excerpt":"소개 Blue-Green 배포 전략은 지속적인 통합 및 지속적인 배포 환경에서 자주 사용되는 소프트웨어 배포 패턴 중 하나이다. 이 전략의 주요 목표는 시스템의 중단 없이 안전하게 애플리케이션을 배포하고 업데이트 하는 것이다.  핵심 아이디어 두 개의 독립적인 환경 Blue와 Green이라는 두 개의 별도의 환경(또는 색상)이 있다. 일반적으로 한 환경(…","fields":{"slug":"/Blue-Green-Deploy-전략/"},"frontmatter":{"date":"October 30, 2023","title":"Blue Green Deploy 전략","tags":["ETC","BackEnd"]},"rawMarkdownBody":"## 소개\n\nBlue-Green 배포 전략은 지속적인 통합 및 지속적인 배포 환경에서 자주 사용되는 소프트웨어 배포 패턴 중 하나이다. 이 전략의 주요 목표는 시스템의 중단 없이 안전하게 애플리케이션을 배포하고 업데이트 하는 것이다. \n\n## 핵심 아이디어 \n\n### 두 개의 독립적인 환경\n\nBlue와 Green이라는 두 개의 별도의 환경(또는 색상)이 있다. 일반적으로 한 환경(Blue)은 실제 사용자에게 서비스를 제공하는 라이브 환경이며, 다른 환경(Green)은 새 버전의 애플리케이션을 배포하기 위한 준비 환경이다. \n\n### 스위치 오버\n\n새 버전의 애플리케이션을 Green 환경에 배포한 후, 모든 테스트와 검증 절차를 거쳐 문제가 없다고 판단되면 트래픽을 Blue 환경에서 Green 환경으로 빠르게 전환(또는 스위치 오버) 한다. 이렇게 하면 실제 사용자는 서비스 중단 없이 새 버전의 어플리케이션을 사용할 수 있다. \n\n### 롤백 용이\n\n만약 Green 환경의 새 버전에 문제가 발생하면, 트래픽을 다시 Blue 환경으로 즉시 전환하여 이전 버전의 애플리케이션을 사용하게 할 수 있다. 이는 문제가 발생했을 때 빠르게 롤백하는데 큰 도움이 된다. \n\n## 개요\n\nBlue-Green 배포 전략에서는 실제로 “배포” 하는 작업을 여러번 반복하지 않는다. 대신, 두 개의 독립적인 환경을 유지하고 트래픽을 이 두 환경 사이에서 스위칭한다. 이를 구체적으로 설명하면 다음과 같다. \n\n1. 초기 상태\n\n    Blue 환경이 라이브 상태이고 실제 사용자 트래픽을 처리하고 있다고 가정한다. \n\n1. 새 버전 배포\n\n    Green 환경에 새로운 버전의 애플리케이션을 배포한다. 이 시점에서 아직 Blue가 여전히 실제 트래픽을 처리하고 있다. Green은 배포 및 테스트 준비 단계에 있다. \n\n1. 스위칭\n\n    Green 환경에서의 테스트와 검증이 완료되면 트래픽을 Blue에서 Green으로 스위칭한다. 이제 Green이 라이브 상태가 되어 사용자 트래픽을 처리한다. \n\n1. 다음 배포\n\n    다음 번 배포 시, Blue 환경을 업데이트하고 검증한 후 트래픽을 다시 Blue로 스위칭한다. \n\n## 장점\n\n### 빠른 롤백\n\n문제가 발생하면 즉시 이전 버전으로 롤백할 수 있다. \n\n### 중단시간 없음\n\n사용자에게 서비스 중단 없이 애플리케이션을 배포할 수 있다. \n\n### 통합 테스트\n\nGreen 환경에서 새 버전의 애플리케이션을 배포하기 전에 광범위한 테스트와 검증을 수행할 수 있다. \n\n## 단점\n\n### 자원 중복\n\n두 개의 독립적인 환경을 유지해야 하므로 추가적인 인프라와 자원이 필요하다. \n\n### 데이터베이스 동기화\n\n데이터베이스 스키마나 데이터에 변동이 있는 경우 Blue와 Green 환경 사이의 동기화 문제가 발생할 수 있다. \n\n## Blue-Green은 테스트 서버일까? \n\n배포 과정에서 한 환경이 새 버전의 검증을 위한 임시 테스트 역할을 하게 되는 것은 맞다. 이렇게 설명하면 테스트 서버와 실서버 역할을 그린과 블루가 번갈아가면서 한다고 착각하기 쉽지만 이보다는 둘 다 실서버의 역할을 할 수 있는 독립적인 프로덕션 환경임을 알아야 한다. 몇 가지 중요한 차이점이 있다. \n\n### 프로덕션 준비\n\nBlue와 Green 둘 다 프로덕션 트래픽을 처리할 준비가 되어있다. 그러나 일반적인 테스트 서버는 프로덕션 수준의 리소스나 성능을 가지고 있지 않을 수 있다. \n\n### 스케일\n\nBlue와 Green은 실제 사용자의 트래픽을 처리할 수 있을 만큼 큰 규모로 구축되어야 한다. 반면 테썹은 보통 작은 규모로 운영된다. \n\n### 데이터 \n\nBlue와 Green은 실제 데이터를 다룬다. \n\n## Blue-Green 배포에서 DB 관리\n\nBlue-Green 배포 전략에서 데이터베이스의 관리는 중요한 고려사항 중 하나이다. 데이터베이스는 상태를 가지고 있으므로 애플리케이션 코드와는 달리 간단히 스위칭하는게 어렵다. \n\n### 단일 데이터베이스 사용\n\n같은 DB를 사용하는 것은 가능하다. 하지만 이렇게 할 때 고려해야 할 몇가지 중요한 문제가 있다. \n\n#### 스키마 변경\n\n데이터베이스 스키마에 변경이 필요한 새 버전의 애플리케이션을 배포할 때 문제가 발생할 수 있다. 새 버전에서 필요한 스키마 변경이 이전 버전과 호환되지 않으면 Green 환경에서의 배포 동안 Blue 환경의 애플리케이션에 문제가 발생할 수 있다. \n\n#### 데이터 무결성\n\n두 환경이 동일한 데이터베이스를 공유할 경우, 새 버전의 애플리케이션에서 예상치 못한 데이터 변경이 발생하면 이전 버전의 실행에도 영향을 줄 수 있다. 따라서, 롤백에도 영향을 줄 수 있다. \n\n\n\n이를 극복하기 위한 여러 전략이 있다. 예를 들면,\n\n#### 스키마 변경 전략\n\n데이터 베이스 스키마 변경을 두 단계로 나누어 수행한다. 먼저 호환성이 있는 스키마 변경을 실시한 후, 새 버전의 애플리케이션을 배포한다. 그 다음 이전 버전과 호환되지 않는 스키마 변경을 실시한다. \n\n#### 피쳐 토글\n\n데이터베이스에 영향을 주는 새로운 기능이나 변경 사항을 피쳐 토글로 구현하여, 실제로 데이터베이스에 적용되기 전까지는 활성화하지 않을 수 있다. \n\n#### 결론\n\n만약 이 방법을 선택할 경우, 새로운 버전의 애플리케이션은 데이터베이스 스키마에 대한 변경이 없거나 하위 호환성을 유지해야 한다. \n\n### 데이터베이스 마이그레이션\n\n- 배포 전에 필요한 데이터베이스 마이그레이션을 신중하게 계획하고 실행한다. \n\n- 마이그레이션은 트래픽을 Green 환경으로 전환하기 전에 실행되어야 한다. \n\n- 롤백 시나리오에 대한 계획도 있어야 한다. \n\n### 데이터 동기화\n\n- 동기화를 위해 DynamoDB Streams와 Lambda를 사용하여 두 테이블 간의 데이터를 동기화 할 수 있다. \n\n## ALB-ECS 환경에서의 Blue-Green 배포 전략 시나리오\n\n### 1. 초기 환경 설정\n\n- ECS 클러스터에서 두 개의 서비스(Blue와 Green)을 생성한다. 초기에는 Blue만 활성 상태(현재 트래픽을 받고 있는 상태)이다. \n\n- ALB 설정: ECS 서비스에 트래픽을 라우팅하기 위해 사용된다. \n\n- Route 53에 도메인 이름을 설정하고 ALB의 DNS 이름을 지정한다. \n\n### 2. CodePipeline 설정\n\n- Source: 예를 들면 깃허브 또는 CodeCommit의 소스 변경을 감지하여 파이프라인을 트리거한다. \n\n- Build: CodeBuild를 사용하여 Docker 이미지를 빌드하고 ECR(Elastic Container Registry)에 푸시한다. \n\n- Deploy: CodeDeploy를 사용하여 ECS에 새로운 이미지를 배포한다. \n\n### 3. Blue-Green 배포\n\n- 새로운 코드 변경이 파이프라인을 트리거하면 CodeBuild가 새 Docker 이미지를 빌드하고 ECR에 푸시한다. \n\n- CodeDeploy는 Green ECS 서비스를 대상으로 새 이미지를 사용하여 작업 정의를 업데이트한다. \n\n- Green 서비스의 태스크가 성공적으로 시작되면 Blue-Green 배포 설정에 따라 CodeDeploy는 자동으로 ALB의 대상 그룹을 업데이트하여 Green 서비스로 트래픽을 전환한다. \n\n### 4. 검증 및 롤백\n\n- Green 환경에서 서비스의 동작을 검증한다. \n\n- CodeDeploy는 선택적으로 사용자 지정 후크를 사용하여 배포 후 검증 스크립트를 실행할 수 있다. \n\n- 문제가 발생한 경우 원래의 Blue 서비스로 빠르게 전환하여 롤백한다. CodeDeploy는 자동 롤백을 지원하여 이전 상태로 되돌릴 수 있다. \n\n### 5. 다음 배포 준비\n\n- 다음 배포는 Blue가 새 환경이고 Green이 이전 환경이 된다. 이렇게 두 환경을 번갈아 사용하면서 배포를 진행한다. \n\n## 기타 배포 전략\n\n1. 카나리아 배포\n\n1. 롤링 배포\n\n1. 기능 토글\n\n1. A/B 테스팅\n\n1. 섀도우 배포\n\n\n\n"},{"excerpt":"Amazon CloudFront 아마존에서 제공하는 CDN(Content Delivery Network) 서비스이다.  CDN이란? 사용자에게 웹 콘텐츠를 더 빠르게 제공하기 위해 전 세계 여러 위치에 콘텐츠를 분산 및 저장하는 네트워크이다. 주요 기능과 특징 글로벌 엣지 네트워크 전 세계에 200개가 넘는 엣지 로케이션과 리전을 갖추고 있다.  동적 및…","fields":{"slug":"/CloudFront/"},"frontmatter":{"date":"October 30, 2023","title":"CloudFront","tags":["AWS"]},"rawMarkdownBody":"## Amazon CloudFront\n\n아마존에서 제공하는 CDN(Content Delivery Network) 서비스이다. \n\n#### CDN이란?\n\n사용자에게 웹 콘텐츠를 더 빠르게 제공하기 위해 전 세계 여러 위치에 콘텐츠를 분산 및 저장하는 네트워크이다.\n\n## 주요 기능과 특징\n\n#### 글로벌 엣지 네트워크\n\n전 세계에 200개가 넘는 엣지 로케이션과 리전을 갖추고 있다. \n\n#### 동적 및 정적 콘텐츠 전송\n\n정적 콘텐츠와 동적 콘텐츠를 모두 효과적으로 전송한다. \n\n#### 오리진 서버\n\n배포를 생성할 때 오리진 서버를 지정해야 한다. 이 오리진은 S3 버킷, EC2 인스턴스, ELB, 외부 서버 등이 될 수 있다. 오리진에서 컨텐츠를 가져와 사용자에게 제공한다. \n\n#### 성능향상\n\nCDN은 사용자에게 가까운 서버에서 콘텐츠를 제공하여 웹 사이트의 로딩시간을 단축시킨다. \n\n#### 가용성 및 장애 복구\n\n여러 서버에 콘텐츠를 복사하므로, 한 서버에 문제가 발생하더라도 다른 서버에서 콘텐츠를 제공할 수 있다. \n\n#### 부하 분산\n\n웹 트래픽을 여러 서버에 분산시켜 오리진 서버의 부하를 줄인다. \n\n#### 비용 절감\n\n캐시된 콘텐츠를 제공함으로써 오리진 서버의 대역폭 사용량과 비용을 줄일 수 있다. \n\n#### 보안 향상\n\n웹사이트에 대한 DDoS 공격과 같은 일부 보안 위협으로부터 보호할 수 있다. \n\n## 동작 플로우\n\n1. **배포 생성**\n\n    사용자는 AWS Management Console, SDK, CLI 등을 사용하여 CloudFront 배포를 생성한다. 이 때, 오리진 서버(S3 버킷, EC2 인스턴스, HTTP 서버 등)와 다양한 배포 설정(예: 캐싱 동작, SSL 인증서, 액세스 제한 등)을 지정한다. \n\n1. **DNS 등록**\n\n    CloudFront는 배포에 대해 고유한 도메인 이름을 제공한다. 사용자는 Route 53 같은 DNS 서비스를 사용해 이 도메인 이름을 웹 사이트나 애플리케이션에 연결할 수 있다. \n\n1. **사용자 요청**\n\n    사용자가 콘텐츠에 액세스 하기 위해 웹 브라우저나 애플리케이션으로 요청을 보내면 DNS 시스템은 해당 요청을 CloutFront 배포의 도메인 이름으로 Resolve 한다. \n\n1. **엣지 로케이션 라우팅**\n\n    요청은 사용자게에 가장 가까운 CloudFront 엣지 로케이션으로 라우팅된다. \n\n1. **캐시 확인**\n\n    해당 엣지 로케이션의 캐시에서 요청된 콘텐츠를 확인한다. \n\n    - 캐시에 콘텐츠가 있는 경우 (Cache Hit) : 캐시된 콘텐츠를 사용자에게 직접 반환한다. \n\n    - 캐시에 콘텐츠가 없는 경우 (Cache Miss) : CloudFront는 설정된 오리진 서버로 요청을 전달한다. \n\n1. **오리진에서 콘텐츠 가져오기**\n\n    오리진 서버에서 요청된 콘텐츠를 가져온다. \n\n1. **콘텐츠 반환 및 캐싱**\n\n    가져온 콘텐츠를 사용자에게 반환하면서 동시에 해당 콘텐츠를 엣지 로케이션의 캐시에 저장한다. 이후 동일한 콘텐츠에 대한 요청이 들어오면 캐시에서 빠르게 제공할 수 있다. \n\n1. **캐시 만료**\n\n    캐싱 동작은 TTL(Time to Live) 설정에 따라 관리된다. TTL이 만료되면 콘텐츠는 자동으로 캐시에서 제거된다. 다음 요청이 들어오면 다시 오리진에서 콘텐츠를 가져온다. \n\n## 동적 콘텐츠의 캐싱\n\n동적 콘텐츠는 사용자별로 또는 시간에 따라 달라지는 콘텐츠를 의미한다. 예를 들면 사용자 프로필 페이지, 실시간 주식 시세, 뉴스 피드 등이 있을 수 있다. CDN 서비스들은 동적 콘텐츠를 캐싱하기 위해 특별한 전략을 사용한다. \n\n동적 콘텐츠 캐싱의 핵심 아이디어는 “모든 동적 콘텐츠가 항상 동적인 것은 아니다.”라는 점이다. 즉, 일부 동적 콘텐츠는 짧은 시간 동안에는 변하지 않을 수도 있다. \n\n캐시의 동작을 잘못 설정하면 데이터 베이스의 변경사항이 반영된 새로운 응답이 캐시에 저장되지 않을 수 있으므로, 그 결과로 사용자는 오래된 또는 기대하지 않은 응답을 받을 수 있다. 이런 문제를 피하기 위해 캐싱은 항상 주의해야 한다. \n\n캐싱 전략을 변경하거나 새로운 캐싱 매커니즘을 도입할 때에는 항상 테스트 환경에서 먼저 검증하는 것이 좋다. \n\n동적 콘텐츠를 캐싱하는 방법은 다음과 같다. \n\n### 조정 가능한 TTL\n\n동적 콘텐츠에 대한 캐싱 정책의 TTL을 짧게 설정할 수 있다. 이렇게 하면 콘텐츠가 잠시동안만 캐싱되어 그 이후에는 다시 오리진에서 최신 데이터를 가져온다. \n\n### 캐시 키 매개변수\n\n쿼리 문자열, 헤더, 쿠키 등의 요청 요소를 기반으로 캐시 버전을 구분할 수 있다. 이를 사용해 동일한 URL에 대한 다양한 버전의 콘텐츠를 캐싱할 수 있다. \n\n### 컨텐츠 기반의 캐싱 정책\n\n오리진 서버는 `Cache-Control` 또는 `Expires` 헤더를 사용하여 콘텐츠의 캐싱 동작을 제어할 수 있다. \n\n#### Cache-Control 헤더\n\n반환된 응답에 Cache-Control 헤더가 포함되어 있는 경우 이 헤더의 지시어를 따라 캐싱한다. 예를 들어 `Cache-Control: max-age=3600` 은 해당 응답을 1시간 동안 캐싱하라는 의미이다. \n\n또는 `Cache-Control: no-store` 나 `Cache-Control: private` 헤더를 포함하고 있으면 해당 응답은 캐싱되지 않는다. \n\n#### Expires 헤더\n\n반환된 응답이 Expires 헤더를 포함하고 있으면 해당 시간까지 응답을 캐싱한다. \n\n### Lambda@Edge\n\nLambda@Egde를 이용하여 요청 및 응답을 가로채고 수정하여 캐싱 동작을 더욱 세밀하게 제어할 수 있다. \n\n### Bypass Cache\n\n특정 조건 하에서 캐싱을 우회하고 항상 오리진 서버로 요청을 전달할 수 있다. 이렇게 하면 특정 요청에 대해서는 항상 최신의 동적 콘텐츠를 제공할 수 있다. \n\n### 데이터 변경 시 캐시 무효화\n\n특정 콘텐츠의 캐시를 수동으로 무효화 하는 기능을 제공한다. 데이터베이스에 중요한 변경이 발생한 경우, 이 기능을 사용하여 해당 콘텐츠의 캐시를 즉시 제거할 수 있다. \n\n\n\n"},{"excerpt":"개요 AWS ECS는 Docker 컨테이너를 쉽게 실행, 중지 및 관리할 수 있도록 해주는 컨테이너 오케스트레이션 서비스이다. 컨테이너 배포, 작업 정의, 서비스 정의, 클러스터 관리 등을 지원한다.  오케스트레이션 서비스란? 컨테이너의 배포, 관리, 스케일링, 네트워킹 및 가용성을 자동화하는 프로세스 및 도구의 집합이다.  즉, 컨테이너화된 애플리케이션…","fields":{"slug":"/AWS-ECSElastic-Container-Service/"},"frontmatter":{"date":"October 25, 2023","title":"AWS ECS(Elastic Container Service)","tags":["AWS"]},"rawMarkdownBody":"## 개요\n\nAWS ECS는 Docker 컨테이너를 쉽게 실행, 중지 및 관리할 수 있도록 해주는 컨테이너 오케스트레이션 서비스이다. 컨테이너 배포, 작업 정의, 서비스 정의, 클러스터 관리 등을 지원한다. \n\n### 오케스트레이션 서비스란?\n\n- 컨테이너의 배포, 관리, 스케일링, 네트워킹 및 가용성을 자동화하는 프로세스 및 도구의 집합이다. \n- 즉, 컨테이너화된 애플리케이션의 라이프사이클을 관리하는 것을 의미한다. \n\n## 주요 특징\n\n### 서비스 \n\n여러 개의 작업 인스턴스를 관리하며, 로드 밸런싱과 함께 사용하여 트래픽 분산을 지원한다. \n\n### 클러스터\n\n컨테이너를 실행하는데 사용되는 논리적인 그룹이다. ECS에서 실행되는 리소스의 집합을 의미한다. 클러스터는 EC2 인스턴스나 Fargate 인스턴스를 포함할 수 있다. \n\n클러스터를 사용하여 워크로드와 관련된 리소스를 논리적으로 그룹화 할 수 있다. 예를 들면 **“production” 클러스터와 “development” 클러스터를 별도로 운영할 수 있다.** \n\n기본적으로 단순한 논리적 이름을 지닌 메타 데이터 구조체로, 클러스터 안의 실제 컴퓨팅 리소스와 독립적이다. \n\n### 네임스페이스\n\n리소스를 논리적으로 분리하고 관리하기 위한 도구이다. 동일한 도메인 이름 아래에서 운영되는 서비스들의 논리적 그룹의 형성에 사용된다. \n\n### 테스크\n\n단일 컨테이너 또는 여러 컨테이너의 그룹이다. \n\n`작업 정의` : 컨테이너의 속성을 정의하는 JSON 템플릿이다. CPU, 메모리 할당, 네트워크 모드, 사용할 도커 이미지 등을 포함한다. \n\n테스크는 작업 정의에 정의된 내용에 따라 실행된다. 예를 들면 웹 서버와 관련된 데이터베이스를 함께 실행하는 테스크를 생성할 수 있다. \n\n## 순서와 구조\n\n1. 컨테이너\n\n    독립적으로 실행 가능한 소프트웨어의 최소 단위. AWS ECS에서는 이 컨테이너를 EC2 인스턴스나 Fargate에서 실행할 수 있다.  \n\n1. 테스크\n\n    ECS의 실행 단위, 하나 이상의 컨테이너로 구성됨\n\n1. 클러스터\n\n    테스크와 서비스를 실행하고 관리하는 논리적 단위\n\n1. 네임스페이스\n\n    서비스와 관련된 논리적 그룹을 형성\n\n1. ECS\n\n각 단계는 이전 단계의 구성 요소를 더 큰 단위로 그룹화하거나 관리하는 역할을 한다. \n\n## 웹 애플리케이션의 개발 후 ECS 배포\n\n1. 웹 애플리케이션 개발\n\n    웹 애플리케이션을 개발한다. \n\n    로컬에서 테스트하여 작동을 확인한다. \n\n1. Docker 이미지 생성\n\n    `Dockerfile` 을 작성하여 웹애플리케이션을 컨테이너화 한다. \n\n    로컬에서 Docker를 사용하여 이미지를 빌드하고 테스트한다. \n\n1. Docker 이미지를 ECR(Elastioc Container Registry)에 푸시\n\n    AWS ECR에 저장소를 생성한다. \n\n    로컬에서 빌드한 Docker 이미지를 ECR 저장소에 PUSH 한다. \n\n1. ECS 클러스터 및 작업 정의 설정\n\n    ECS 클러스터를 생성한다. (Fargate 또는 EC2 인스턴스)\n\n    작업 정의(Task Definition)을 생성한다. 작업 정의에는 ECR에수 푸시한 Docker 이미지, 필요한 CPU와 메모리, 환경 변수, 포트 매핑 등을 지정한다. \n\n1. ECS 서비스 생성\n\n    ECS 클러스터 내에서 작업정의를 기반으로 서비스를 생성한다. 서비스를 사용하면 컨테이너의 실행 상태를 유지하거나 원하는 개수의 작업 인스턴스를 실행할 수 있다. \n\n1. 도메인 및 DNS 설정\n\n    AWS Route 53을 사용하여 도메인을 설정하고 ALB(Application Load Balancer) 의 주소를 해당 도메인에 연결한다. \n\n1. 보안 그룹 및 네트워크 설정\n\n1. 모니터링 및 로깅\n\n## 기존 서비스에서 ECS 기반 서비스로의 전환\n\n1. ECS, Fargate 설정\n\n1. ALB 설정\n\n    Application Load Balancer를 생성한다. \n\n    리스너와 타겟 그룹을 설정하여 ECS 서비스의 작업에 트래픽을 라우팅 하도록 한다. \n\n1. Route 53 업데이트\n\n    현재 서비스에 연결된 Route53 레코드를 수정하여 ALB의 DNS 이름을 가리키도록 변경한다. \n\n1. 모니터링 및 튜닝\n\n### 주의점\n\n기본적으로는 CES, Fargate, ALB를 설정한 후 마지막 단계로 Route 53의 레코드 설정만 변경하면 트래픽은 새로운 ECS-Fargate 기반의 서비스로 라우팅된다. \n\n그러나 다음과 같은 주의 사항이 있다. \n\n1. 테스트 \n\n    ALB와 ECS가 올바르게 작동하는지 사전에 충분히 테스트 해야 한다. \n\n1. DNS 전파 지연\n\n    DNS 업데이트는 즉시 전파되지 않을 수 있다. 일부 사용자는 변경 전의 엔드포인트로 트래픽이 라우팅 될 수 있으므로 업데이트 후에도 기존 서비스를 적어도 몇 시간 동안 실행 상태로 유지하는 것이 좋다. \n\n1. 롤백 계획\n\n    문제가 발생할 경우를 대비하여 롤백 계획을 준비해야 한다. 예를 들어 새로운 서비스에 문제가 발생한 경우 Route 53 설정을 원래대로 돌릴 수 있어야 한다. \n\n"},{"excerpt":"어느 순간 morethan-log에 노션 글 업로드가 잘 되지 않았다.  글을 그 동안 마구 마구 써버리긴 했다.. 그래서 찔리는 점이 있었는데, 최근 이직 후 일을 하면서 조금 덜 쓰면서, 그리고 잠깐 옵시디언으로 한 눈을 팔면서 고칠 생각을 안하고 있었다.  그러면서 일도 쪼끔 익숙해지고 옵시디언도 아 별로다 라고 결론을 내고 다시 여기에 글을 썼는데…","fields":{"slug":"/vercel-배포-자동화/"},"frontmatter":{"date":"October 23, 2023","title":"vercel 배포 자동화","tags":["Hobby","Blogging"]},"rawMarkdownBody":"어느 순간 morethan-log에 노션 글 업로드가 잘 되지 않았다. \n\n\n\n글을 그 동안 마구 마구 써버리긴 했다.. 그래서 찔리는 점이 있었는데, 최근 이직 후 일을 하면서 조금 덜 쓰면서, 그리고 잠깐 옵시디언으로 한 눈을 팔면서 고칠 생각을 안하고 있었다. \n\n그러면서 일도 쪼끔 익숙해지고 옵시디언도 아 별로다 라고 결론을 내고 다시 여기에 글을 썼는데, 업로드가 되지 않는걸 보니 결국은 고쳐야 할 때가 왔음을 알게 됐다. \n\n\n\nmorethan-log는 일정 시간마다 특정 api를 호출해서 노션에서 글을 긁어오는 함수를 실행한다. \n\n그리고 vercel에서는 그 api가 실행한 함수의 실행 로그를 볼 수 있다… \n\n\n\n로그를 봤더니 예상과 같았다. \n\n무료 플랜에서는 함수 실행 시간을 10초까지만 허용하는데, 10초를 당연한듯이 넘고 있었다. \n\n\n\nnotion 비공식 api를 호출하는건데.. \n\ndeploy를 직접 했을땐 제대로 긁어오는 것으로 보아 api가 막힌 것 같진 않고 글이 많아지면서 발생한 것 같다.. \n\n길게 고민하지 않고 Vercel의 Deploy hooks와 Pipedream의 기능의 무료 기능들만 사용해서 매일 새벽 두시에 자동 배포를 하기로 했다. \n\n### Vercel Deploy Hooks\n\nVercel의 Deploy Hooks 기능을 이용하면 특정 URL에 HTTP 요청이 이뤄질 때마다 사이트를 다시 빌드하고 배포할 수 있다. \n\n내 프로젝트를 고르고, Setting를 고른 다음 좌측의 Git을 고른다. \n\nDeploy Hooks로 이동해서 `Create Hook`을 해 url을 생성한다. 이제 해당 URL로 접속하면 deploy 되는것을 볼 수 있다. \n\n### Pipedream 설정\n\nPipedream은 서로 다른 애플리케이션, 데이터 및 API를 연결하고 자동화 하는데 사용할 수 있는 서버리스 통합 플랫폼이다. 단적으로, 이번엔 내가 원하는 시간에 Hook을 위한 URL을 호출하도록 스케쥴링 할 수 있다. \n\nPipedream 대시보드에서 `New`를 클릭하고 `Workflow`를 선택하여 새 워크플로우를 생성한다. \n\n트리거로 `Schedule` 을 선택하고 원하는 스케쥴을 설정한다. \n\n`Send Http Request` 를 선택하고, Vercel Deploy Hooks URL을 입력한다. \n\n테스트 후, 저장 및 배포한다. \n\n\n\n이로써 매일 사이트를 다시 배포하고 빌드함으로써 블로깅에 아무 문제가 없게 되었다. \n\n해피~\n\n"},{"excerpt":"기본 개념 DDD는 복잡한 애플리케이션과 시스템의 개발에 있어서 비즈니스 요구사항을 중심으로 설계하고 구현하는 접근법입니다. 이 방식은 비즈니스 도메인의 복잡성을 이해하고, 그 복잡성을 모델링하여 소프트웨어에 반영하는 것에 중점을 둡니다. 핵심 요소 Ubiquitous Language (모든 곳에서 사용되는 언어): 개발팀과 비즈니스 팀 간에 공통적으로 …","fields":{"slug":"/FastAPI와-DDD/"},"frontmatter":{"date":"October 20, 2023","title":"FastAPI와 DDD","tags":["FastAPI","DDD"]},"rawMarkdownBody":"## 기본 개념\n\nDDD는 복잡한 애플리케이션과 시스템의 개발에 있어서 비즈니스 요구사항을 중심으로 설계하고 구현하는 접근법입니다. 이 방식은 비즈니스 도메인의 복잡성을 이해하고, 그 복잡성을 모델링하여 소프트웨어에 반영하는 것에 중점을 둡니다.\n\n## 핵심 요소\n\n1. **Ubiquitous Language (모든 곳에서 사용되는 언어)**: 개발팀과 비즈니스 팀 간에 공통적으로 사용되는 언어. 이를 통해 불필요한 혼동을 줄이고, 의사소통의 효율을 높입니다.\n\n1. **Bounded Context (경계가 정의된 맥락)**: 특정한 맥락 내에서만 유효한 모델을 정의. 다른 맥락에서는 같은 용어도 다른 의미를 가질 수 있습니다.\n\n1. **Entities, Value Objects, Aggregates, Repositories**: 도메인 모델을 구성하는 주요 요소들입니다.\n\n1. **Domain Events**: 도메인 내에서 중요한 비즈니스 이벤트를 나타냅니다.\n\n## FastAPI에서 DDD의 기본 구조\n\n```bash\n├── api                               # 첫 번째 API 모듈\n│   ├── domain                        # 도메인 로직과 엔티티 정의\n│   │   ├── entities                  # 엔티티 정의 (비즈니스 모델)\n│   │   └── value_objects             # 값 객체 정의 (데이터 검증 및 전송 용도)\n│   ├── presentation                  # 애플리케이션의 사용자 인터페이스 레이어\n│   │   ├── dto                       # 데이터 전송 객체 정의 (Pydantic 모델)\n│   │   └── rest                      # REST 엔드포인트 정의\n│   │       └── controllers           # REST 컨트롤러 (FastAPI 경로 작업 처리)\n│   ├── repository                    # 영속성 관리를 위한 인터페이스 정의\n│   │   └── impl                      # 리포지토리 구현체 (영속성 레이어)\n│   │       └── sql                   # SQL 기반 리포지토리 구현체\n│   │           ├── models.py         # 데이터베이스 모델 (ORM 모델)\n│   │           ├── crud.py           # CRUD 연산 로직\n│   │           └── database.py       # 데이터베이스 세션 및 연결 관리\n│   └── service                       # 도메인 서비스 정의, 도메인 로직 구현\n│       └── impl                      # 서비스 구현체 (비즈니스 로직의 구현)\n├── api2                              # 두 번째 API 모듈\n│   └── ...                           # 같은 구조를 가져간다\n├── core                              # 공통 핵심 기능이나 공유 컴포넌트\n└── router                            # 애플리케이션의 라우팅 로직, URL 라우트 정의\n    └── routes                        # 개별 라우트 정의 파일\n```\n\nDDD의 원칙에 따라 애플리케이션을 조직화하면 각 도메인이 뚜렷하게 분리되고 관리하기 쉬워진다. 위에 제시된 디렉토리 구조는 DDD를 적용한 예시이다. \n\n도메인 레이어, 애플리케이션 레이어, 인프라 레이어로 분리되어 코드의 응집도를 높이고 결합도를 낮출 수 있다. 또한 각 레이어가 제공하는 인터페이스와 구현체를 분리함으로써 유지보수와 확장성에 유리한 구조를 만들 수 있다. \n\n### 도메인 레이어\n\n#### domain 디렉토리\n\nDDD에서 가장 중요한 부분으로 복잡한 비즈니스 로직과 애플리케이션 핵심 기능을 표현하는 모델을 담고 있다. \n\n- entities\n\n    - 비즈니스 도메인의 기본적인 데이터 단위로 고유한 식별자를 가지고 있다. \n- 비즈니스 규칙을 캡슐화하고 FastAPI 애플리케이션에서 데이터베이스 레코드와 1:1로 매핑되는 경우가 많다. \n- FastAPI는 이러한 엔티티에 직접적으로 연관되지능 않지만 ORM 도구를 사용하여 엔티티 클래스를 데이터 모델로 변환하고 데이터베이스와 상호작용을 할 수 있다. \n\n- value\\_objects\n\n    - 식별자를 가지지 않는 불변 객체로 엔티티의 속성을 표현한다. \n- FastAPI에서는 값 객체를 표현하기 위해 Pydantic의 기능을 활용할 수 있으며 요청 검증이나 응답 시리얼라이징에 유용하게 사용된다. \n\n### 애플리케이션 레이어(presentation, service)\n\n#### presentaion 디렉토리\n\n이 디렉토리는 사용자 인터페이스와 외부에 노출되는 API 엔드포인트의 정의를 담당한다. \n\n- DTO\n\n    - DTO는 클라이언트와 서버간에 데이터를 전달할 때 사용하는 객체이다.\n- FastAPI에서는 Pydantic 모델을 이용해 DTO를 정의한다. 이 모델들은 본문, 쿼리 파라미터, 경로 파라미터, 그리고 응답 모델을 정의하는데 사용된다.\n- DTO는 엔드포인트가 받아들이고 반환하는 데이터의 구조와 유효성 검증 규칙을 명시한다.\n\n- REST\n\n    - Restful 원칙을 따르는 API 엔드포인트를 정의한다. \n- FastAPI에서는 각 API 경로에 해당하는 함수를 작성하고 이를 HTTP 메서드에 매핑한다. \n- 각 함수는 Pydantic DTO를 매개변수로 사용하여 요청 데이터의 유효성을 검사하고 이를 도메인 모델로 변환하거나 서비스 레이어로 전달한다. \n\n- Controllers\n\n    - API 엔드포인트에 대한 요청을 처리하는 로직을 담당한다. \n- FastAPI에서는 주로 경로 작업 함수로 표현되며 경로 데코레이터를 사용하여 라우트를 설정한다. \n- 컨트롤러는 요청을 받아 DTO로 파싱하고 필요한 비즈니스 로직을 수행한 후 적절한 HTTP 응답을 반환한다.\n\n이 구성 요소들은 FastAP의 강점인 타입 힌트, 자동 문서화, 요청 검증 등을 활용하여 효율적인 API 인터페이스 레이어를 구성하는데 중요한 역할을 한다. \n\n#### service 디렉토리\n\n애플리케이션의 비즈니스 로직을 구현하는 부분을 담당한다. 컨트롤러와 데이터 액세스 계층 사이에서 중간 역할을 한다. 주요 목적은 비즈니스 규칙과 애플리케이션 로직을 캡슐화하여 재사용성을 높이고 애플리케이션의 다룬 부분과의 결합도를 낮추는 것이다. \n\n- Service Implementations (impl)\n\n    실제 로직을 구현한다. 이 구현체는 데이터베이스 호출, 외부 서비스 호출, 비즈니스 규칙 처리 등을 담당한다. \n\n### 인프라 레이어(repository)\n\n도메인 레이어와 애플리케이션 레이어의 기술적 세부 사항을 추상화하는 역할을 한다. 주로 데이터베이스, 메시징 시스템, 파일 시스템와 같은 외부 자원과의 통신을 담당한다. 애플리케이션의 나머지 부분이 기술적인 세부사항에 직접적으로 의존하지 않도록 추상화를 제공한다. \n\n"},{"excerpt":"단위 테스트는 소프트웨어 개발의 기본 구성 단위를 테스트하는 것을 말한다. 이 경우 기본 구성 단위는 보통 함수나 메소드, 작은 클래스 등이 될 수 있다. 단위 테스트의 주된 목적은 개별 구성 단위가 예상대로 동작하는지 확인하고, 코드의 특정 부분에 대한 논리적 오류를 식별하는 것이다.  특징과 목적 독립성 : 각 단위 테스트는 독립적으로 실행되어야 하며…","fields":{"slug":"/Unit-Test단위-테스트/"},"frontmatter":{"date":"October 17, 2023","title":"Unit Test(단위 테스트)","tags":["TDD"]},"rawMarkdownBody":"단위 테스트는 소프트웨어 개발의 기본 구성 단위를 테스트하는 것을 말한다. 이 경우 기본 구성 단위는 보통 함수나 메소드, 작은 클래스 등이 될 수 있다. 단위 테스트의 주된 목적은 개별 구성 단위가 예상대로 동작하는지 확인하고, 코드의 특정 부분에 대한 논리적 오류를 식별하는 것이다. \n\n### 특징과 목적\n\n1. 독립성 : 각 단위 테스트는 독립적으로 실행되어야 하며, 다른 테스트의 결과에 영향을 받지 않아야 한다. \n\n1. 작고 빠름 : 단위 테스트는 빠르게 실행되어야 하며, 코드의 작은 부분만을 테스트해야 한다. \n\n1. 예측 가능함 : 동일한 입력에 대해 항상 동일한 출력을 반환해야 한다. \n\n1. 자동화 가능 : 단위 테스트는 자동으로 실행되어야 하며 수동 테스트 없이도 결과를 제공해야 한다. \n\n1. 명확함 : 테스트의 결과는 명확하게 통과 또는 실패로 나타나야 하며 이유를 쉽게 파악할 수 있어야 한다. \n\n\n\n"},{"excerpt":"메소드는 Python의  모듈에 있는  데코레이터를 사용하여 클래스를 정의할 때 사용된다.   메소드는 객체가 초기화된 직후에 자동으로 호출된다. 즉,  메소드가 호출된 후에  메소드가 호출된다. 클래스의 인스턴스가 생성될 때  메소드가 호출되어 인스턴스 변수들을 초기화하고, 바로 이어서  메소드가 호출되어 추가적인 초기화 작업을 수행할 수 있다. 이러한…","fields":{"slug":"/__post_init__/"},"frontmatter":{"date":"October 17, 2023","title":"__post_init__","tags":["Python"]},"rawMarkdownBody":"`__post_init__` 메소드는 Python의 `dataclasses` 모듈에 있는 `dataclass` 데코레이터를 사용하여 클래스를 정의할 때 사용된다. \n\n`__post_init__` 메소드는 객체가 초기화된 직후에 자동으로 호출된다. 즉, `__init__` 메소드가 호출된 후에 `__post_init__` 메소드가 호출된다.\n\n클래스의 인스턴스가 생성될 때 `__init__` 메소드가 호출되어 인스턴스 변수들을 초기화하고, 바로 이어서 `__post_init__` 메소드가 호출되어 추가적인 초기화 작업을 수행할 수 있다. 이러한 방식으로, 객체의 초기화 과정 중에 추가적인 로직을 실행할 수 있다.\n\n```python\ndto = ContentListQueryDto(**payload)\n```\n\n`ContentListQueryDto` 클래스의 인스턴스를 생성하면서 `__init__` 메소드가 자동으로 호출되고, 이어서 `__post_init__` 메소드도 자동으로 호출된다. `__post_init__` 메소드는 `__init__` 메소드가 완료된 직후에 실행되므로, `__init__` 메소드에서 설정된 필드 값을 기반으로 추가적인 초기화 작업을 수행할 수 있다.\n\n이 경우에, `dto = ContentListQueryDto(**payload)` 코드를 실행하면 `__init__` 메소드가 호출되어 `payload` 딕셔너리의 키-값 쌍을 이용하여 인스턴스 변수들을 초기화하고, 이어서 `__post_init__` 메소드가 호출되어 해당 메소드에 정의된 작업들을 수행한다. \n\n\n\n"},{"excerpt":"dataclass는 [파이썬]3.7부터 표준 라이브러리의 일부로 도입된 데코레이터이다.\n주로 클래스를 정의할 때 반복적으로 필요한 특수 메서드들(, ,  등)을 자동으로 생성해주는 역할을 한다. 이를 통해 데이터를 저장하고 처리하는 클래스를 보다 간결하게 정의할 수 있다. 주요 특징 자동 생성된 생성자 자동 생성된 표현 메서드 자동 생성된 비교 메서드 불…","fields":{"slug":"/dataclass/"},"frontmatter":{"date":"October 17, 2023","title":"dataclass","tags":["Python"]},"rawMarkdownBody":"dataclass는 [[파이썬]]3.7부터 표준 라이브러리의 일부로 도입된 데코레이터이다.\n주로 클래스를 정의할 때 반복적으로 필요한 특수 메서드들(`__init__`, `__repr__`, `__eq__` 등)을 자동으로 생성해주는 역할을 한다. 이를 통해 데이터를 저장하고 처리하는 클래스를 보다 간결하게 정의할 수 있다.\n\n## 주요 특징\n\n1. 자동 생성된 생성자\n\n1. 자동 생성된 표현 메서드\n\n1. 자동 생성된 비교 메서드\n\n1. 불변성 옵션\n\n## 예제\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Point:\n    x: float\n    y: float\n\n\n```\n\n이 코드는 아래의 코드를 간결하게 표현한 것이다.\n\n```python\nclass Point:\n    def __init__(self, x: float, y: float):\n        self.x = x\n        self.y = y\n\n    def __repr__(self):\n        return f\"Point(x={self.x}, y={self.y})\"\n\n    def __eq__(self, other):\n        if not isinstance(other, Point):\n            return NotImplemented\n        return self.x == other.x and self.y == other.y\n\n\n```\n\n## 결론\n\ndataclass는 데이터 구조나 간단한 객체를 정의할 때 유용하게 사용된다. 그러나 복잡한 로직이나 특별한 초기화가 필요한 경우에는 일반 클래스를 사용하는 것이 더 나을 수 있다.\n\n\n\n"},{"excerpt":"개요 브랜치 관리 전략은 소프트웨어 개발에서 코드 베이스의 다양한 변경 사항을 조직하고 관리하기 위한 방법이나 원칙이다. 특히 버전관리 시스템을 사용하는 프로젝트에서 브랜치를 효과적으로 활용하면 여러 개발자나 팀이 동시에 다양한 기능, 버그 수정, 실험 등을 독립적으로 처리할 수 있다.  목적 코드의 안정성 유지 여러 개발자나 팀 간의 협업 촉진 배포와 …","fields":{"slug":"/브랜치-관리-전략/"},"frontmatter":{"date":"October 16, 2023","title":"브랜치 관리 전략","tags":["GitHub"]},"rawMarkdownBody":"### 개요\n\n브랜치 관리 전략은 소프트웨어 개발에서 코드 베이스의 다양한 변경 사항을 조직하고 관리하기 위한 방법이나 원칙이다. 특히 버전관리 시스템을 사용하는 프로젝트에서 브랜치를 효과적으로 활용하면 여러 개발자나 팀이 동시에 다양한 기능, 버그 수정, 실험 등을 독립적으로 처리할 수 있다. \n\n### 목적\n\n1. 코드의 안정성 유지\n\n1. 여러 개발자나 팀 간의 협업 촉진\n\n1. 배포와 통합을 위한 과정 단순화\n\n### 대표적인 전략\n\n#### Feature branching\n\n각 기능마다 별도의 브랜치를 만들어 개발한다. 기능이 완성되면 메인 브랜치에 병합한다. \n\n- 장점\n\n    각 기능이 독립적으로 개발되므로 다양한 기능과 버그 수정을 동시에 진행할 수 있다. \n\n    충돌이 발생할 경우 해당 기능의 브랜치에서만 처리하면 되므로 메인 브랜치는 상대적으로 안정적이다. \n\n- 단점\n\n    병합 시점에 복잡한 병합 충돌이 발생할 수 있다. \n\n    오랜 시간 분리된 브랜치에서 작업할 경우 메인 브랜치와의 동기화가 어려워질 수 있다. \n\n- 적합한 상황\n\n    개발자들이 독립적으로, 혹은 소규모 팀으로 작업을 진행하는 경우\n\n#### Gitflow\n\nmaster, develop, feture/, hotfix/ 등 여러 브랜치를 사용하여 작업의 단계와 종류를 구분한다. \n\n- 장점\n\n    명확한 브랜치 구조를 가지고 있어 프로젝트의 다양한 단계와 상태를 잘 구분한다. \n\n    핫픽스와 기능 개발을 동시에 진행할 수 있다. \n\n- 단점\n\n    브랜치 관리가 복잡하다. \n\n    많은 규칙과 워크 플로우를 따라야 하므로 초보자에게는 진입 장벽이 될 수 있다. \n\n- 적합한 상황\n\n    큰 규모의 프로젝트나, 여러 단계의 배포 과정을 가진 프로젝트에서 사용\n\n#### Github flow\n\n간단하게 master와 feature 브랜치만을 사용한다. 기능이 준비되면 master에 PR을 통해 병합 요청한다. \n\n- 장점\n\n    브랜치 관리가 간단하다. \n\n    지속적인 통합 및 배포 환경에 적합하다. \n\n- 단점\n\n    복잡한 프로젝트에서는 세부적인 브랜치 구분이 부족할 수 있다. \n\n- 적합한 상황\n\n    중소 규모의 프로젝트나 간단한 배포 과정을 가진 프로젝트에서 사용\n\n#### Trunk Based Deveolpment(TBD)\n\n모든 개발자가 하나의 trunk 혹은 main 브랜치에서 직접 작업한다. 기능 토글등을 활용하여 미완성 기능을 숨긴다. \n\n- 장점\n\n    브랜치 관리가 매우 간단하다. \n\n    지속적인 통합 및 배포 환경에 최적화되어 있다. \n\n- 단점\n\n    안정적인 CI/CD 파이프라인 및 테스트 환경이 필요하다. \n\n    기능 토글 등 추가적인 관리 도두가 필요할 수 있다. \n\n- 적합한 상황\n\n    빠른 개발 주기와 지속적인 배포를 원하는 프로젝트에서 사용\n\n    \n\n"},{"excerpt":"이 오류는 Python 클래스 또는 함수에서 기본값이 설정된 인자 뒤에 기본값이 설정되지 않은 인자가 위치할 때 발생한다. 예를 들어, 나는 다음과 같았다.   필드에 기본값 이 설정되어 있고, 이 필드 뒤에 기본값이 설정되지 않은 와  필드가 위치하고 있다. 해결 방법 이 문제가 발생한 이유는 기본값이 설정된 매개변수 뒤에 기본값이 설정되지 않은 매개변…","fields":{"slug":"/TypeError-non-default-argument-content-follows-default-argument/"},"frontmatter":{"date":"October 10, 2023","title":"TypeError: non-default argument 'content' follows default argument","tags":["Python"]},"rawMarkdownBody":"`TypeError: non-default argument 'content' follows default argument`\n\n\n\n이 오류는 Python 클래스 또는 함수에서 기본값이 설정된 인자 뒤에 기본값이 설정되지 않은 인자가 위치할 때 발생한다. 예를 들어, 나는 다음과 같았다. \n\n```python\n@dataclass\nclass CsDetailQueryReturnDto:\n    ...\n    occurrence_datetime: Optional[str] = None\n    ...\n    content: str\n    cs_type: CsType\n    ...\n```\n\n`occurrence_datetime` 필드에 기본값 `None`이 설정되어 있고, 이 필드 뒤에 기본값이 설정되지 않은 `content`와 `cs_type` 필드가 위치하고 있다.\n\n### 해결 방법\n\n이 문제가 발생한 이유는 기본값이 설정된 매개변수 뒤에 기본값이 설정되지 않은 매개변수가 올 수 없다는 규칙 때문이다. \n\n함수를 호출할 때, Python은 매개변수를 위치나 키워드로 판별할 수 있어야 한다. 만약 기본값이 없는 매개변수가 기본값이 있는 매개변수 뒤에 온다면, Python은 어떤 매개변수에 값을 할당해야 하는지 혼동할 수 있다.\n\n\n\n"},{"excerpt":"1. 등장 배경 RDBMS는 정해진 스키마와 트랜잭션의 ACID 속성을 기반으로 설계되었다. 하지만 웹 2.0 시대가 도래하면서 대용량 데이터와 급격한 트래픽 증가, 다양한 데이터 구조의 요구가 생기면서 RDBMS의 한계가 드러나 NoSQL이 등장하게 되었다. 2. RDBMS와의 주요 차이점 스키마 RDBMS는 고정된 스키마를 가지고 있으나, NoSQL은…","fields":{"slug":"/NoSQL/"},"frontmatter":{"date":"September 20, 2023","title":"NoSQL","tags":["DataBase"]},"rawMarkdownBody":"## 1. 등장 배경\n\nRDBMS는 정해진 스키마와 트랜잭션의 ACID 속성을 기반으로 설계되었다. 하지만 웹 2.0 시대가 도래하면서 대용량 데이터와 급격한 트래픽 증가, 다양한 데이터 구조의 요구가 생기면서 RDBMS의 한계가 드러나 NoSQL이 등장하게 되었다.\n\n## 2. RDBMS와의 주요 차이점\n\n### 스키마\n\nRDBMS는 고정된 스키마를 가지고 있으나, NoSQL은 동적 스키마를 가진다.\n\n### 확장성\n\nRDBMS는 수직 확장을 위해 설계 되었지만 NoSQL은 수평 확장이 가능하다.\n\n### 데이터 모델\n\nRDBMS는 테이블 기반의 데이터 모델을 사용하지만 NoSQL은 키-값, 문서, 컬럼, 그래프 등 다양한 데이터 모델을 사용한다.\n\n### 트랜잭션\n\nRDMBS는 ACID 트랜잭션을 지원하지만 NoSQL은 일반적으로 이를 지원하지 않거나 부분적으로 지원한다.\n\n## 3. NoSQL의 일관성과 무결성\n\n### 일관성(Consistency)\n\nRDBMS에서는 ACID 트랜잭션 속성을 지원하여 강력한 일관성을 보장한다.\n반면, 많은 NoSQL 데이터베이스는 BASE 모델을 채용한다. 이는 즉각적인 일관성보다는 가용성과 복원력을 우선시하며 일시적인 불일관성을 허용하지만 결국 일관된 상태로 수렴한다는 의미이다.\n일부 NoSQL 시스템은 일관성 수준을 조정할 수 있도록 설정할 수 있다.\n\n### 무결성(Intergirty)\n\nRDBMS는 외래 키 제약 조건, 유니크 제약 조건 등을 통해 데이터 무결성을 보장한다.\nNoSQL 데이터베이스는 스키마가 유연하므로 같은 강도의 무결성 제약 조건을 갖지 않을 수 있다. 그러나 일부 NoSQL 시스템은 도큐먼트의 유효성 검사나 스키마 유효성 검사를 지원하여 무결성을 일정 수준 보장하기도 한다.\n\n### 정리\n\nNoSQL 데이터베이스는 일관성과 무결성을 \"없애는\" 것이 아니라 다양한 요구 사항과 환경에 맞게 그 특성과 규칙을 조정하고 최적화 한 것이다. 따라서 특정 응용 프로그램의 요구 사항에 따라 적절한 데이터베이스와 설정을 선택하는 것이 중요하다.\n\n## 4. 주요 특징 및 장점\n\n### 유연성\n\n다양한 데이터 및 동적 스키마를 지원함으로써 다양한 데이터 구조에 유연하게 대응할 수 있다.\n\n### 확장성\n\n수평 확장을 통해 대량의 데이터와 트래픽을 처리할 수 있다. NoSQL은 데이터를 여러 노드에 복제하거나 분할하는 매커니즘을 내장하고 있다. 이는 고가용성과 분산 처리를 위한 핵심 요소로 수평 확장을 자연스럽게 지원한다.\n\n### 빠른 성능\n\n간단한 쿼리와 특정 작업에 최적화된 데이터베이스 설계로 빠른 성능을 제공한다.\n\n### 다양한 데이터 처리\n\n구조화, 반구조화, 비구조화 데이터를 모두 처리할 수 있다.\n\n## 5. 단점\n\n### 표준 부재\n\n다양한 NoSQL 데이터베이스 중 특정 표준이 없어 학습 곡선이 높을 수 있다.\n\n### 일관성\n\nACID 트랜잭션의 일부를 희생하므로 데이터 일관성 문제가 발생할 수 있다.\n\n## 6. NoSQL이 필요한 경우\n\n### 빠른 개발\n\n동적 스키마로 개발과 변화에 빠르게 대응할 수 있다.\n\n### 대량의 데이터 처리\n\n수평 확장을 통해 대량의 데이터를 처리하는데 적합하다.\n\n### 다양한 데이터 구조\n\n구조화 된 데이터 뿐 아니라 로그, 소셜 미디어 포스트 등 다양한 데이터를 저장하고 처리할 때 유용하다.\n\n"},{"excerpt":"1. 정의 AWS에서 제공하는 완전 관리형 NoSQL 데이터베이스 서비스이다. 높은 트래픽의 웹 스케일 애플리케이션에 적합한 특성을 가지며, 밀리초 단위의 지연 시간으로 대량의 데이터를 처리할 수 있다.\n확장 가능하고 높은 성능의 NoSQL 데이터베이스로 다양한 웹 기반 애플리케이션과 서비스에서 데이터 관리를 위한 핵심 선택지이다. AWS의 다른 서비스와…","fields":{"slug":"/Dynamo-DB/"},"frontmatter":{"date":"September 20, 2023","title":"Dynamo DB","tags":["DataBase","AWS"]},"rawMarkdownBody":"## 1. 정의\n\nAWS에서 제공하는 완전 관리형 NoSQL 데이터베이스 서비스이다. 높은 트래픽의 웹 스케일 애플리케이션에 적합한 특성을 가지며, 밀리초 단위의 지연 시간으로 대량의 데이터를 처리할 수 있다.\n확장 가능하고 높은 성능의 NoSQL 데이터베이스로 다양한 웹 기반 애플리케이션과 서비스에서 데이터 관리를 위한 핵심 선택지이다. AWS의 다른 서비스와 통합이 잘 되어 있어 복잡한 애플리케이션 구축에 적합하다.\n\n## 2. 주요 특징\n\n### 성능\n\n밀리초 미만의 응답 시간을 제공한다.\n\n### 확장성\n\n사용자의 요구에 따라 자동으로 확장 및 축소 된다.\n\n### 내구성 및 가용성\n\n자동으로 여러 AWS 리전 및 가용 영역에 데이터를 복제하여 데이터 손실과 서비스 중단 없이 운영된다.\n\n### 완전관리형\n\n서버 인프라스트럭쳐 또는 관리 작업 없이 실행된다.\n\n### 이벤트 소싱\n\nDynamoDB Streams를 통해 테이블의 데이터 변경을 캡쳐할 수 있다.\n\n### 글로벌 테이블\n\n여러 AWS 리전에서 동일한 테이블에 액세스 할 수 있다.\n\n### 3. 데이터 모델\n\n- 테이블 : 데이터의 컬렉션. RDBMS와 유사하지만 스키마가 고정되어있지 않다.\n\n- 항목 : 테이블 내의 개별 데이터 항목. RDBMS의 Row와 유사하다.\n\n- 속성 : 항목의 데이터 요소. RDBMS의 칼럼과 유사하다.\n\n### 4. 주요 기능\n\n- 초당 읽기/쓰기 용량 조절 : 요구에 따라 자동으로 조정된다.\n\n- 보조 인덱스 : 다양한 쿼리 옵션을 제공한다.\n\n- 트랜잭션 : 여러 항목 및 테이블에 걸친 ACID 트랜잭션을 지원한다.\n\n- 보안 : IAM을 통해 데이터 액세스 및 암호화를 제어한다.\n\n### 5. 사용 사례\n\n- 웹 애플리케이션 : 세션 데이터, 사용자 프로필, 소셜 그래프 등의 저장\n\n- 모바일 애플리케이션 : 빠른 액세스와 확장성이 필요한 데이터를 저장\n\n- IoT 애플리케이션 : 대량의 장치 데이터를 실시간으로 수집 및 분석\n\n- 게임 : 게임 상태, 플레이어 프로필, 리더보드 등을 저장\n\n\n\n"},{"excerpt":"1. 정의 REST는 웹서비스를 설계하기 위한 아키텍쳐 스타일이다. 웹의 기본 프로토콜인 HTTP를 최대한 활용하기 위한 일련의 제약사항 및 원칙을 포함한다. 2. 기본 원칙 Stateless 각 요청은 모든 정보를 가지고 있어야 한다. 즉, 서버는 클라이언트의 상태 정보를 저장하면 안된다. 클라이언트-서버 구조 클라이언트와 서버가 별도로 존재하고 각각의…","fields":{"slug":"/REST/"},"frontmatter":{"date":"September 20, 2023","title":"REST","tags":["BackEnd"]},"rawMarkdownBody":"## 1. 정의\n\nREST는 웹서비스를 설계하기 위한 아키텍쳐 스타일이다. 웹의 기본 프로토콜인 HTTP를 최대한 활용하기 위한 일련의 제약사항 및 원칙을 포함한다.\n\n## 2. 기본 원칙\n\n### Stateless\n\n각 요청은 모든 정보를 가지고 있어야 한다. 즉, 서버는 클라이언트의 상태 정보를 저장하면 안된다.\n\n### 클라이언트-서버 구조\n\n클라이언트와 서버가 별도로 존재하고 각각의 역할이 분리되어 있어야 한다.\n\n### 캐시 가능\n\n클라이언트에서 캐시를 사용할 수 있어야 하며, 응답이 캐시 가능한지 아닌지를 명시해야 한다.\n\n### 계층화된 시스템 (Layered System)\n\n클라이언트는 중간 계층(서버)을 알 필요 없이, 끝단 서버에만 접속하면 된다.\n\n### 코드 온 디맨드 (optional)\n\n서버는 코드를 클라이언트에게 보낼 수 있으며, 클라이언트는 그 코드를 실행할 수 있다.\n\n### 인터페이스 일관성 (Uniform Interface)\n\n아키텍쳐의 규칙을 정의하며, 이를 통해 시스템의 분리와 통합이 용이해진다.\n\n- 자원의 식별 : 각 리소스는 URI로 식별되어야 한다.\n\n- 메시지를 통한 자원 조작 : 리소스는 HTTP Method를 통해 조작된다.\n\n- 자기 설명적 메시지(Self-descriptive message): 각 메시지는 자신을 어떻게 처리해야 하는지에 대한 충분한 정보를 포함해야 한다.\n\n- HATEOAS(Hypermedia as the Engine of Application State): 응답에 다음 가능한 액션에 대한 정보도 포함되어야 한다.\n\n## 3. HTTP 메서드\n\n### GET\n\n리소스를 조회하는데 사용된다.\n\n### POST\n\n리소스를 생성한다.\n\n### PUT\n\n리소스를 수정한다.\n\n### DELETE\n\n리소스를 삭제한다.\n\n### PATCH\n\n리소스의 일부를 수정한다.\n\n## 4. 상태코드 (Status Code)\n\n### 2xx (Successful)\n\n요청이 성공적으로 수행됨\n\n### 3xx (Redirection)\n\n요청을 완료하기 위해 추가 동작이 필요함\n\n### 4xx (Client Error)\n\n클라이언트의 잘못된 요청으로 인한 오류\n\n### 5xx (Server Error)\n\n서버 오류\n\n## 5. URI 설계\n\n- 리소스를 명시하며, 명사를 사용해야 한다. (ex) `/users` , `/products`\n\n- 리소스의 계층 관계를 나타날 때는 `/`를 사용한다. (ex) `/users/123/orders`\n\n## 6. 미디어 타입\n\n- JSON, XML등 다양한 미디어 타입을 사용하여 데이터를 표현한다.\n\n- `Accept` 헤더를 통해 클라이언트가 원하는 미디어 타입을 서버에게 알릴 수 있다.\n\n\n\n"},{"excerpt":"서버가 \"없는\" 것이 아니다.\n서버리스는 개발자 또는 운영 팀이 서버의 운영, 관리, 확장 등에 대한 걱정 없이 어플리케이션 코드에만 집중할 수 있게 해주는 컴퓨팅 모델을 의미한다. 특징 자동확장 사용량에 따라 자동으로 리소스를 확장하거나 축소한다. 따라서 트래픽이 급증하더라도 시스템이 알아서 처리한다. 이벤트 주도 대부분의 서버리스 어플리케이션은 이벤트…","fields":{"slug":"/Serverless/"},"frontmatter":{"date":"September 19, 2023","title":"Serverless","tags":["BackEnd"]},"rawMarkdownBody":"서버가 \"없는\" 것이 아니다.\n서버리스는 개발자 또는 운영 팀이 서버의 운영, 관리, 확장 등에 대한 걱정 없이 어플리케이션 코드에만 집중할 수 있게 해주는 컴퓨팅 모델을 의미한다.\n\n## 특징\n\n### 자동확장\n\n사용량에 따라 자동으로 리소스를 확장하거나 축소한다. 따라서 트래픽이 급증하더라도 시스템이 알아서 처리한다.\n\n### 이벤트 주도\n\n대부분의 서버리스 어플리케이션은 이벤트에 응답하여 동작한다.\n\n### 비용 효율\n\n실제로 사용된 컴퓨팅 리소스만큼만 비용을 지불한다. 즉, 코드가 실행되지 않을 때에는 비용이 발생하지 않는다.\n\n### 서버 관리 불필요\n\n인프라의 설정, 관리, 유지보수와 같은 작업들이 서버리스 플랫폼 제공업체에 의해 자동으로 처리된다.\n\n- 대표적인 서버리스 서비스로는 AWS Lambda, Google Cloud Function, Azure Function 등이 있다.\n\n## 단점\n\n### ColdStart 문제\n\n### 실행 시간 제한\n\n대부분의 서버리스 플랫폼은 함수의 최대 실행 시간에 제한을 두고 있다.\n\n### 상태 관리\n\n서버리스 함수는 상태가 없기 때문에 외부 저장소나 캐싱 시스템을 통해 상태를 관리해야 한다.\n\n### 종속성 및 복잡성\n\n여러 서버리스 컴포넌트와 서비스의 상호작용은 의존성과 복잡성을 증가시킬 수 있다.\n\n### 보안 및 권한 관리\n\n특히 여러 서비스와 컴포넌트가 연동되는 환경에서 권한 설정과 보안 관리가 복잡해질 수 있다.\n\n### 로컬 개발 환경 설정 어려움\n\n"},{"excerpt":"AWS API Gateway는 개발자가 간편하게 RESTful API와 WebSocket API를 생성, 배포, 유지 관리 할 수 있도록 지원하는 AWS의 완전 관리형 서비스이다. 클라이언트와 백엔드 서비스 간의 호출을 중개한다. 1. 주요 기능 API 생성 및 배포 빠르게 API를 생성하고 배포할 수 있다. 호출 중개 클라이언트와 백엔드 서비스 간의 호…","fields":{"slug":"/AWS-API-Gateway/"},"frontmatter":{"date":"September 19, 2023","title":"AWS API Gateway","tags":["BackEnd","AWS"]},"rawMarkdownBody":"AWS API Gateway는 개발자가 간편하게 RESTful API와 WebSocket API를 생성, 배포, 유지 관리 할 수 있도록 지원하는 AWS의 완전 관리형 서비스이다. 클라이언트와 백엔드 서비스 간의 호출을 중개한다.\n\n## 1. 주요 기능\n\n### API 생성 및 배포\n\n빠르게 API를 생성하고 배포할 수 있다.\n\n### 호출 중개\n\n클라이언트와 백엔드 서비스 간의 호출을 중개한다.\n\n### 요청 및 응답 변환\n\n데이터 매핑 템플릿을 사용하여 호출 중 요청 및 응답 데이터를 변환할 수 있다.\n\n### 액세스 제어\n\nAWS Identity and Access Management (IAM) 또는 AWS Lambda 권한 기반 접근 제어를 사용하여 API 호출을 보호할 수 있다.\n\n### API 키 및 요금 제한\n\nAPI키를 생성하고 배포하고 사용량을 추적하여 API 호출에 요금을 부과할 수 있다.\n\n### API 캐싱\n\nAPI 호출의 성능을 향상시키기 위해 캐시를 사용할 수 있다.\n\n### API 버전 관리\n\nAPI의 다양한 버전 및 스테이지를 관리할 수 있다.\n\n## 2. 통합\n\n### AWS 서비스 통합\n\nAWS Lambda, Dynamo DB, Amazone S3등의 AWS 서비스와 손쉽게 통합할 수 있다.\n\n### 모니터 및 로깅\n\nAmazone CloudWatch와 통합하여 API 호출 및 오류 데이터를 모니터링 및 로깅할 수 있다.\n\n## 3. 보안\n\n### SSL/TLS 인증서\n\n커스텀 도메인 이름에 대한 SSL/TLS 인증서를 제공하고 관리한다.\n\n### 사용자 인증 및 인가\n\nAmazone Cognito와 통합하여 사용자 인증 및 인가를 수행할 수 있다. 또한, OAuth토큰을 사용하여 API 엑세스를 제어할 수 있다.\n\n## 4. 비용\n\n사용한 만큼의 비용만 지불한다. 요청 수 및 데이터 전송량에 따라 비용이 발생한다.\n\n## 5. 사용 사례\n\n### Serverless\n\nAWS Lambda와 결합하여 서버리스 애플레키션의 백엔드를 구축할 수 있다.\n\n### 모바일 및 웹 애플리케이션\n\n다양한 클라이언트에서 사용될 RESTful API를 제공한다.\n\n### 웹사이트의 서드파티 애플리케이션 통합\n\n다른 웹 서비스나 데이터 소스와의 통합을 위한 API를 제공한다.\n\n\n\n"},{"excerpt":"먼저 ColdStart를 이해하기 위해서는 Serverless의 동작방식을 이해할 필요가 있다. 간단히 알아보자.\nServerless 컴퓨팅에서 사용자의 요청이 오면 해당 요청을 처리하는 함수(Function)가 실행된다. 이 함수는 이벤트에 응답하여 실행되는데, 이 함수는 항상 실행 상태를 유지하고 있지 않다. 대신에 요청이 들어오면 새롭게 시작되거나 …","fields":{"slug":"/ColdStart/"},"frontmatter":{"date":"September 19, 2023","title":"ColdStart","tags":["AWS","BackEnd"]},"rawMarkdownBody":"## 먼저\n\nColdStart를 이해하기 위해서는 Serverless의 동작방식을 이해할 필요가 있다. 간단히 알아보자.\nServerless 컴퓨팅에서 사용자의 요청이 오면 해당 요청을 처리하는 함수(Function)가 실행된다. 이 함수는 이벤트에 응답하여 실행되는데, 이 함수는 항상 실행 상태를 유지하고 있지 않다. 대신에 요청이 들어오면 새롭게 시작되거나 재사용된다.\n\n이제 콜드 스타트의 개념에 대해서 알아볼 수 있다.\n\n## 콜드 스타트와 웜 스타트\n\n### 콜드 스타트\n\n함수가 처음 호출될 때, 아직 사용 가능한 인스턴스가 없거나 지나치게 오래된 인스턴스에 대한 요청이 들어왔을 때 함수 인스턴스가 새롭게 생성된다. 이런 상태에서 함수가 실행될 때를 '콜드 스타트' 라고 한다.\n콜드 스타트는 초기화에 필요한 시간과 리소스를 요구하기 때문에 함수의 실행 시작까지의 대기 시간이 길어질 수 있다. 이로 인한 지연은 사용자 경험에 영향을 줄 수 있다.\n\n### 웜 스타트\n\n이미 실행된 함수 인스턴스가 재사용될 때 발생한다. 초기화 시간이 필요 없기 때문에 대기 시간이 짧다.\n\n## 지연 최소화를 위한 전략\n\n### 최적의 함수 크기 할당\n\n메모리와 CPU를 적절히 할당하여 함수의 시작 시간을 줄일 수 있다.\n\n### 초기화 코드 최적화\n\n함수 내에서 필요한 라이브러리나 자원을 로드하는 초기화 코드를 최적화 하여 실행 시간을 줄인다.\n\n### 외부 연결 최소화\n\n외부 시스템이나 데이터베이스에 대한 연결을 미리 확립하거나 재사용하는 방식을 고려한다.\n\n### 함수 미리 실행\n\n일정 주리고 함수를 호출하여 웜 상태를 유지한다.\n\n서버리스 제공 업체에 따라 콜드 스타트의 영향도와 대응 전략이 다를 수 있다.\n\n"},{"excerpt":"지난 시간 지난시간에는 DataBase를 불러오는 비동기 로직 때문에 Factory Method를 사용해서 노션의 DataBase를 불러오는 코드를 작성했다.  기능만 점검 후, 전체적인 설계를 다시 하고 프로젝트의 디렉토리 구조도 다시 짰다.  새로운 설계 일단, 크게 네 가지의 클래스를 만들기로 했다.  DataBase 클래스 생성시에 날짜를 입력 받…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅2/"},"frontmatter":{"date":"September 04, 2023","title":"NotionAPI를 활용한 자동 포스팅(2)","tags":["Notion-API","Blogging","Hobby","Typescript"]},"rawMarkdownBody":"## 지난 시간\n\n지난시간에는 DataBase를 불러오는 비동기 로직 때문에 Factory Method를 사용해서 노션의 DataBase를 불러오는 코드를 작성했다. \n\n기능만 점검 후, 전체적인 설계를 다시 하고 프로젝트의 디렉토리 구조도 다시 짰다. \n\n## 새로운 설계\n\n일단, 크게 네 가지의 클래스를 만들기로 했다. \n\n### DataBase 클래스\n\n- 생성시에 날짜를 입력 받아 파라미터마다 다른 조건으로 쿼리 하여 노션 API에서 결과값을 받아오는 필터링 기능을 가진다. \n\n### Page 클래스\n\n- Page 타입의 Block이다. \n\n- 해당 페이지를 마크다운으로 저장(Print)하는 메소드를 가지고 있다. \n\n- Property들을 가지고 있다. (페이지의 속성)\n\n### Content 클래스\n\n- Page에 귀속된다. \n\n- 종류가 여러가지이다. h1, h2, h3.. 등등 직접적으로 포스팅의 내용이 될 블록이다.\n\n- 가장 상위의 Content는 부모가 Page이다. \n\n- 자기 자신을 마크다운으로 변환하여 부모에게 리턴하는 method를 가지고 있다. 만약 child가 있다면, 해당 child도 함께 변환하여 부모에게 리턴한다. \n\n- 일단 기본적으로 다음의 속성을 가진다. \n\n    - **id** : idx\n\n    - **parent** : 부모 ID\n\n    - **hasChidren** : 자식 요소가 있는지 여부 (true, false)\n\n    - **type** : `heading_1` 등등 블록의 종류. 종류마다 마크다운 변환 전략이 달라야 하므로 중요하다. \n\n    - **is_toggleable** : **** 토글 여부\n\n### Posting 클래스\n\n- 생성자에 날짜를 넣어서 생성하면 해당하는 DataBase를 조회하고, Page → Content 를 돌면서 마크다운으로 페이지를 변환하여 저장하는 클래스들의 메소드를 모두 여기서 실행한다. 메인함수 같은 개념이다. \n\n위 내용은 언제든지 달라질 수 있다. 실제로 매일매일 하루하루 숨 쉬듯이 달라지고 있다. \n\n## 디렉토리 구조\n\nsrc/models 디렉토리를 새로 만들고 여기에 모든 클래스, 비즈니스 로직을 넣기로 했다. \n\n## database.ts\n\n일단 이번에는 database.ts를 1차적으로 완성했다. \n\n```typescript\nimport { Client } from \"@notionhq/client\";\nimport { QueryDatabaseResponse } from \"@notionhq/client/build/src/api-endpoints\";\nimport * as dotenv from \"dotenv\";\n\n\nexport class DataBase {\n    private notion: Client;\n    private databaseId: string;\n    public database: QueryDatabaseResponse;\n\n    public pageIds: { pageId: string }[] = [];\n\n    private constructor(notion: Client, databaseId: string) {\n        this.notion = notion;\n        this.databaseId = databaseId;\n        this.database = {} as QueryDatabaseResponse;\n    }\n\n    public static async create(filterUdate?: string): Promise<DataBase> {\n        dotenv.config({ path: `${__dirname}/../../.env` });\n        const notionkey: string = process.env.NOTION_KEY || \"\";\n        const databaseid: string = process.env.NOTION_DATABASE_ID || \"\";\n\n        if (!notionkey || !databaseid) {\n            throw new Error(\"NOTION_KEY or NOTION_DATABASE_ID is missing in the environment variables.\");\n        }\n\n        const notion = new Client({ auth: notionkey });\n        const instance = new DataBase(notion, databaseid);\n        if (filterUdate == \"lastest\") {\n            const today = new Date(); // 현재 날짜와 시간을 가져옵니다.\n            today.setDate(today.getDate() - 1); // 날짜를 하루 전으로 설정합니다.\n\n            // YYYY-MM-DD 형식의 문자열로 날짜를 가져옵니다.\n            filterUdate = `${today.getFullYear()}-${String(today.getMonth() + 1).padStart(2, '0')}-${String(today.getDate()).padStart(2, '0')}`;\n        }\n        instance.database = await instance.queryDatabase(filterUdate);\n        return instance;\n    }\n\n    public async queryDatabase(filterUdate?: string): Promise<QueryDatabaseResponse> {\n        try {\n            const response = await this.notion.databases.query({\n                database_id: this.databaseId,\n                filter: {\n                    and: [\n                        {\n                            property: '상태',\n                            select: {\n                                equals: \"POST\",\n                            },\n                        },\n                        ...(filterUdate ? [{\n                            property: 'update',\n                            date: { on_or_after: filterUdate }\n                        }] : []),\n                    ]\n                },\n                sorts: [\n                    {\n                        property: 'update',\n                        direction: 'descending',\n                    },\n                ],\n            });\n            // pageId 리스트 업데이트\n            this.pageIds = response.results.map(page => ({ pageId: page.id }));\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n}\n```\n\n전에 제작한 클래스를 수정하여 만들었으며, 필터 기능을 추가하였다. \n\nNotion 서버에는 반드시 세계공통시? 로 저장이 되는 문제가 있다. 서울 시간대가 달라서 필터에도 현재 시간을 변환하여 조회하는 로직을 짜야 할 것 같다. \n\n`pageIds` 속성으로 DataBase에 저장된 페이지들의 ID를 받을 수 있다. \n\n## 코드 작성시 고려한 점\n\n### DataBase 클래스가 Page 인스턴스 리스트 관리 책임을 가진다면?\n\n- SRP(Single Responsibility Principle) 원칙에 의거해 하나의 클래스는 하나의 책임만을 져야 한다. DataBase 클래스가 Page 인스턴스 리스트 관리 책임도 가진다면 이 원칙에 어긋날 수 있다. \n\n- 두 클래스가 강한 연결성을 가지게 되어 유연성이 떨어질 수 있다. \n\n- 코드가 더 직관적으로 보일 수 있다. (추적이 쉽다)\n\n결론적으로, DataBase 클래스와 Page 클래스의 연결을 줄이고(유연성을 늘리고) SRP를 준수하기 위해 현재의 방식을 선택했다. \n\n### `databaseId` , `notionkey` 를 생성자의 파라미터로 전달한다면? \n\n- 해당 값을 생성자의 파라미터로 값을 전달할 경우 코드가 길어지는 단점이 있다. 또한, 다양한 notionkey나 databaseid로 클래스를 생성할 수 있다. \n\n- 하지만, 해당 프로젝트에서 Database는 여러개를 생성하거나 값을 바꿔가면서 클래스를 생성할 일이 없으므로 현재의 방법을 택했다. \n\n- 다만 지금처럼 할 경우 클래스 내부에서 외부 환경 변수에 직접 접근하므로 캡슐화 원칙에 어긋나기는 한다. 하지만 앞서 언급한대로 해당 변수가 바뀔일은 거의 없다고 여겨지므로 지금의 방법을 택했다. \n\n## 추가 수정\n\n### 고려사항\n\n- DataBase는 여러개일 필요가 없으므로, Singleton 패턴을 고려한다.\n\n- `DataBase` 클래스는 Notion과의 통신 뿐만 아니라, 환경 변수의 로딩 및 데이터베이스 ID 및 키의 유효성 검사까지 담당하고 있다. 이러한 기능들을 분리하여 각각의 책임을 명확히 하는 것이 좋다. \n\n- Typings: 현재 코드에서는 TypeScript를 사용하고 있다. 가능한 한 모든 변수, 함수 매개변수 및 반환 타입에 타입 주석을 추가하는 것이 좋다.\n\n## 완성코드\n\n위의 문제를 고려해 Notion과의 통신을 NotionAPI 클래스로 분리했다. Page나 Block 같은 다른 클래스에서도 동일한 Notion Client를 사용해야 하므로, 이는 아주 타당한 선택이었다. 또 하나의 프로젝트에 API는 유일하므로 싱글톤 패턴을 사용했다. 다음은 이를 위해 만들어진 notionapi.ts 코드이다. \n\n### notionapi.ts\n\n```typescript\nimport { Client } from \"@notionhq/client\";\n\nexport class NotionAPI {\n    private static instance: NotionAPI | null = null;\n    public client: Client;\n\n    private constructor(notionKey: string) {\n        this.client = new Client({ auth: notionKey });\n    }\n\n    public static async create(notionKey: string = \"\") {\n        if (!this.instance) {\n            if (!notionKey) {\n                throw new Error(\"NOTION_KE is missing\");\n            }\n            this.instance = new NotionAPI(notionKey);\n        }\n        return this.instance;\n    }\n}\n```\n\n최초 클래스 생성시에만 notionKey가 필요하도록 해두었다. 나중에 이 클래스를 호출할 때에는 번거롭게 환경변수를 조회할 필요가 없다. \n\n### database.ts\n\n```typescript\nimport { Client } from \"@notionhq/client\";\nimport { QueryDatabaseResponse } from \"@notionhq/client/build/src/api-endpoints\";\nimport * as dotenv from \"dotenv\";\nimport { NotionAPI } from \"./notionapi\";\n\nexport class DataBase {\n    private static instance: DataBase | null = null;\n\n    private notion: Client;\n    private databaseId: string;\n    public database: QueryDatabaseResponse;\n\n    public pageIds: { pageId: string }[] = [];\n\n    private constructor(notion: Client, databaseId: string) {\n        this.notion = notion;\n        this.databaseId = databaseId;\n        this.database = {} as QueryDatabaseResponse;\n    }\n\n    public static async create(filterUdate?: string): Promise<DataBase> {\n        if (!this.instance) {\n            dotenv.config({ path: `${__dirname}/../../.env` });\n            const notionkey: string = process.env.NOTION_KEY || \"\";\n            const databaseid: string = process.env.NOTION_DATABASE_ID || \"\";\n\n            if (!notionkey || !databaseid) {\n                throw new Error(\"NOTION_KEY or NOTION_DATABASE_ID is missing in the environment variables.\");\n            }\n\n            const notionApi: NotionAPI = await NotionAPI.create(notionkey);\n\n            this.instance = new DataBase(notionApi.client, databaseid);\n            if (filterUdate === \"lastest\") {\n                const today: Date = new Date();\n                today.setDate(today.getDate() - 1);\n                filterUdate = `${today.getFullYear()}-${String(today.getMonth() + 1).padStart(2, '0')}-${String(today.getDate()).padStart(2, '0')}`;\n            }\n            this.instance.database = await this.instance.queryDatabase(filterUdate);\n        }\n        return this.instance;\n    }\n\n    public async queryDatabase(filterUdate?: string): Promise<QueryDatabaseResponse> {\n        try {\n            const response: QueryDatabaseResponse = await this.notion.databases.query({\n                database_id: this.databaseId,\n                filter: {\n                    and: [\n                        {\n                            property: '상태',\n                            select: {\n                                equals: \"POST\",\n                            },\n                        },\n                        ...(filterUdate ? [{\n                            property: 'update',\n                            date: { on_or_after: filterUdate }\n                        }] : []),\n                    ]\n                },\n                sorts: [\n                    {\n                        property: 'update',\n                        direction: 'descending',\n                    },\n                ],\n            });\n            // pageId 리스트 업데이트\n            this.pageIds = response.results.map(page => ({ pageId: page.id }));\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n}\n```\n\n타입 주석을 모두 추가해주었고, 싱글톤 패턴으로 변경되었다. Notion Client의 생성부가 다른 곳으로 분리되었다. \n\n환경변수 역시 분리하는것을 고려중이지만, \n\n일단은 database.ts를 이 정도로 완성 하려고 한다. \n\ndatabase.ts 완성!\n\n[https://github.com/Sharknia/Notion-to-Markdown/tree/database-class-complete](https://github.com/Sharknia/Notion-to-Markdown/tree/database-class-complete)\n\n"},{"excerpt":"naming conventions는 클래스, 메소드 또는 변수의 이름을 지을 때 사용되는 명명 규칙이다.  camelCase 첫번째 단어는 소문자로 시작하고 그 후의 단어는 대문자로 시작한다. 변수 또는 함수 명으로 주로 사용된다.  예 :    PascalCase 모든 단어가 대문자로 시작한다. 주로 클래스명으로 사용된다.  예 :    snake_ca…","fields":{"slug":"/네이밍-규칙naming-conventions/"},"frontmatter":{"date":"September 03, 2023","title":"네이밍 규칙(naming conventions)","tags":["ETC"]},"rawMarkdownBody":"naming conventions는 클래스, 메소드 또는 변수의 이름을 지을 때 사용되는 명명 규칙이다. \n\n1. **camelCase**\n\n    첫번째 단어는 소문자로 시작하고 그 후의 단어는 대문자로 시작한다. 변수 또는 함수 명으로 주로 사용된다. \n\n    예 : `variableName`  `printMyName`\n\n1. **PascalCase**\n\n    모든 단어가 대문자로 시작한다. 주로 클래스명으로 사용된다. \n\n    예 : `ClassName`  `PrintMyName`\n\n1. **snake_case**\n\n    모든 단어를 소문자로 작성하고 단어 사이를 `_` 로 구분한다. 변수나 함수명 또는 데이터베이스 필드명으로 사용된다. \n\n    예 : `variable_name`  `print_my_name`\n\n1. **kebab-case**\n\n    모든 단어를 소문자로 작성하고 단어 사이를 `-` 로 구분한다. 주로 URL 또는 css 클래스 명에 사용된다. \n\n    예 : `variable-name`  `print-my-name`\n\n1. **UPPER_SNAKE_CASE**\n\n    모든 단어를 대문자로 작성하고 단어 사이를 `_` 로 구분한다. 주로 상수를 나타낼 때 사용된다. \n\n    예 : `CONSTANT_VALUE`\n\n물론 이것들은 강제되는 것은 아니다. 하지만 일관된 스타일을 여러 개발자 사이에 유지하면 협업이나 가독성에 도움이 될 것이다. \n\n"},{"excerpt":"팩토리 패턴은 객체 생성에 관련된 로직을 클래스 내부에 포함시키는 대신 별도의 클래스나 메소드에 위임하는 패턴이다.  개요 팩토리 패턴은 객체를 생성하는 코드와 객체의 사용 코드를 분리하는 역할을 한다.  객체를 생성하는 로직을 별도의 팩토리 클래스나 팩토리 메소드에 위임하여 객체 생성을 캡슐화 한다.  특징 유연성 : 객체 생성 과정에 변화가 생겼을 때…","fields":{"slug":"/Factory-Pattern/"},"frontmatter":{"date":"September 03, 2023","title":"Factory Pattern","tags":["DesignPattern"]},"rawMarkdownBody":"팩토리 패턴은 객체 생성에 관련된 로직을 클래스 내부에 포함시키는 대신 별도의 클래스나 메소드에 위임하는 패턴이다. \n\n### 개요\n\n- 팩토리 패턴은 객체를 생성하는 코드와 객체의 사용 코드를 분리하는 역할을 한다. \n\n- 객체를 생성하는 로직을 별도의 팩토리 클래스나 팩토리 메소드에 위임하여 객체 생성을 캡슐화 한다. \n\n### 특징\n\n- 유연성 : 객체 생성 과정에 변화가 생겼을 때, 클라이언트 코드는 변경되지 않고 팩토리 코드만 변경하면 된다. \n\n- 확장성 : 새로운 타입의 객체를 지원하기 위해 클라이언트 코드를 변경할 필요 없이 팩토리 클래스를 확장하면 된다. \n\n### 장점\n\n- 분리 및 재사용\n\n- 유지보수\n\n- 클라이언트 코드 간소화\n\n### 단점\n\n- 복잡도\n\n- 학습곡선\n\n### 사용 시나리오\n\n- 객체 생성 로직이 복잡하거나 초기화에 많은 설정이 필요한 경우\n\n- 다양한 타입의 객체를 생성해야 하며, 이런 객체들이 공통의 인터페이스나 추상 클래스를 가지고 있는 경우\n\n- 시스템에서 아용되는 객체의 구체적인 타입을 시스템에서 분리하고자 할 때\n\n"},{"excerpt":"갑자기 짚고 넘어가는 프로젝트의 목표 이번 프로젝트의 목표는 다음과 같다.  개발 내적인 목표 타입스크립트를 사용한다.  변경에는 닫혀있고, 확장에는 열린 코드를 작성한다.  이를 위해 구현에만 집중하지 않고 설계에 신경을 써서 진행해본다. 이를 위해 디자인 패턴을 가능한 한 적극적으로 활용해본다.  가능한 한 사용이 쉽도록 만들어본다. 명확한 명명 규칙…","fields":{"slug":"/NotionAPI를-활용한-자동-포스팅/"},"frontmatter":{"date":"September 03, 2023","title":"NotionAPI를 활용한 자동 포스팅","tags":["Hobby","Blogging","Notion-API","DesignPattern"]},"rawMarkdownBody":"### 갑자기 짚고 넘어가는 프로젝트의 목표\n\n이번 프로젝트의 목표는 다음과 같다. \n\n#### 개발 내적인 목표\n\n1. 타입스크립트를 사용한다. \n\n1. 변경에는 닫혀있고, 확장에는 열린 코드를 작성한다. \n\n1. 이를 위해 구현에만 집중하지 않고 설계에 신경을 써서 진행해본다.\n\n1. 이를 위해 디자인 패턴을 가능한 한 적극적으로 활용해본다. \n\n1. 가능한 한 사용이 쉽도록 만들어본다.\n\n1. 명확한 명명 규칙을 사용한다.\n\n    1. PascalCase를 클래스 이름에 사용한다.\n\n    1. camelCase를 메소드 및 변수에 사용한다.\n\n#### 개발 외적인 목표\n\n1. 노션으로 공부만 해도 포스팅/풀심기가 모두 되는 꿈의 프로그램을 만든다. \n\n### 그래서 이번에는?\n\n[Notion API(2)](https://sharknia.github.io/Notion-API2)  \n\n지난번에 API 정상 작동을 확인만 한 코드를 타입스크립트로 바꾸고, 하나의 클래스로 만들려고 한다. \n\n#### 최초 작성 코드\n\n```typescript\nimport * as dotenv from \"dotenv\";\nimport { Client } from \"@notionhq/client\";\nimport { QueryDatabaseResponse } from \"@notionhq/client/build/src/api-endpoints\";\n\nclass NotionToMarkdown {\n    private notion: Client;\n    private databaseID: string;\n    private database?: QueryDatabaseResponse;\n\n    constructor() {\n        dotenv.config();\n        \n        if (!process.env.NOTION_KEY) {\n            throw new Error(\"Environment variable NOTION_KEY is not defined.\");\n        }\n\n        if (!process.env.NOTION_PAGE_ID) {\n            throw new Error(\"Environment variable NOTION_PAGE_ID is not defined.\");\n        }\n\n        this.notion = new Client({ auth: process.env.NOTION_KEY });\n        this.databaseID = process.env.NOTION_PAGE_ID;\n        this.initializeDatabase();\n    }\n    \n    private async initializeDatabase(): Promise<void> {\n        this.database = await this.queryDatabase();\n    }\n    \n    public async queryDatabase(): Promise<QueryDatabaseResponse> {\n        try {\n            const response = await this.notion.databases.query({ database_id: this.databaseID });\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n    // ... 다른 메소드들 (retrievePage, retrieveBlock, listBlockChildren) ...\n}\n```\n\n이런 식으로 구성을 했는데 위 코드의 문제는 생성자에서 비동기 작업을 직접 실행하는 것에 있다. \n\n비동기 작업을 생성자에서 실행할 경우 잠재적으로 발생할 수 있는 문제는 다음과 같다.\n\n1. 비동기 로직이 실패할 경우, 객체 생성 자체에 문제가 생길 수 있다. 생성자는 객체 초기화를 수행하는 로직만 포함해야 하며, 부작용(side-effect)이 발생할 수 있는 코드는 포함되어서는 안된다. \n\n1. 비동기 로직이 포함되면 생성자의 수행 시간이 길어질 수 있다. \n\n이 문제는 Factory 패턴을 사용하여 해결할 수 있다. Factory 메소드를 사용하여 비동기 로직을 수행하고 완료되면 객체를 반환한다. 아래와 같이 Factory 패턴을 사용하여 코드를 수정했다.  \n\n#### Factory Pattern을 활용한 코드\n\n```typescript\nclass NotionToMarkdown {\n    private notion: Client;\n    private databaseId: string;\n    public database: QueryDatabaseResponse;\n\n    private constructor(notion: Client, databaseID: string, database: QueryDatabaseResponse) {\n        this.notion = notion;\n        this.databaseId = databaseID;\n        this.database = database;\n    }\n\n    public static async create(): Promise<NotionToMarkdown> {\n        dotenv.config();\n\n        if (!process.env.NOTION_KEY || !process.env.NOTION_DATABASE_ID) {\n            throw new Error(\"Environment variable is not defined.\");\n        }\n\n        const notion = new Client({ auth: process.env.NOTION_KEY });\n        const databaseId = process.env.NOTION_DATABASE_ID;\n        const database = await new NotionToMarkdown(notion, databaseId, {} as QueryDatabaseResponse).queryDatabase();\n\n        return new NotionToMarkdown(notion, databaseId, database);\n    }\n\n    public async queryDatabase(): Promise<QueryDatabaseResponse> {\n        try {\n            const response = await this.notion.databases.query({ database_id: this.databaseId });\n            return response;\n        } catch (error) {\n            console.error(\"Error querying the database:\", error);\n            throw error;\n        }\n    }\n\n    // ... 다른 메소드들 (retrievePage, retrieveBlock, listBlockChildren) ...\n}\n\n// 사용 예제:\nNotionToMarkdown.create().then(handler => {\n    console.log(handler.database);  // database 속성 출력\n});\n```\n\n위 코드에서 create 메소드가 비동기 팩토리 메소드이다. 이 메소드는 `NotionToMarkdown` 객체를 비동기적으로 생성하고 반환한다. 생성자는 private로 선언되어 직접 호출할 수 없고, 반드시 create 메소드를 통해 객체를 생성해야 한다. \n\n### 진행 코드\n\n[https://github.com/Sharknia/Notion-to-Markdown/blob/42911777c0146c4260a5769fe9a7d5b1f9ac4c32/notionApi.ts](https://github.com/Sharknia/Notion-to-Markdown/blob/42911777c0146c4260a5769fe9a7d5b1f9ac4c32/notionApi.ts)\n\n\n\n"},{"excerpt":"gitignore란?  는 Git에서 프로젝트에 특정 파일 또는 디렉터리를 추적하지 않도록 지시하는 파일이다.  파일에 나열된 패턴과 일치하는 파일/디렉터리는 Git에서 무시된다. 이는 민감한 데이터, 컴파일 된 바이너리, 로그 파일 등 Git 저장소에 저장할 필요가 없거나 저장해서는 안되는 파일들을 제외하는 데에 유용하다.  문법  파일에는 여러 패턴을…","fields":{"slug":"/gitignore/"},"frontmatter":{"date":"September 02, 2023","title":"gitignore","tags":["GitHub"]},"rawMarkdownBody":"## gitignore란?\n\n`.gitignore` 는 Git에서 프로젝트에 특정 파일 또는 디렉터리를 추적하지 않도록 지시하는 파일이다. `.gitignore` 파일에 나열된 패턴과 일치하는 파일/디렉터리는 Git에서 무시된다. 이는 민감한 데이터, 컴파일 된 바이너리, 로그 파일 등 Git 저장소에 저장할 필요가 없거나 저장해서는 안되는 파일들을 제외하는 데에 유용하다. \n\n## 문법\n\n`.gitignore` 파일에는 여러 패턴을 사용할 수 있는데, 이를 알고 있으면 파일과 디렉터리를 무시하는 데 더 유용하게 사용할 수 있다.\n\n1. `` : 어떤 문자든지 0개 이상을 의미한다. 예를 들어, `.log`는 모든 로그 파일을 무시한다.\n\n1. `/` : 디렉터리 구분자로 사용된다. `/logs`는 logs라는 이름의 디렉터리만 무시하며, `logs/`는 logs로 시작하는 모든 디렉터리를 무시한다.\n\n1. `!` : 패턴을 부정한다. 예를 들면, `.log` 아래에 `!important.log`를 추가하면, 모든 로그 파일은 무시되지만 important.log 파일은 예외로 추적된다.\n\n1. `#` : 주석을 작성하는 데 사용된다.\n\n## 전역 설정하기\n\n프로젝트마다 반복적으로 무시 해야하는 파일이나 디렉터리가 있다면, 전역 `.gitignore` 파일을 만들 수 있다.\n\n1. 전역 `.gitignore` 파일 생성: `touch ~/.gitignore_global`\n\n1. Git 설정에 전역 `.gitignore` 파일 추가: `git config --global core.excludesFile ~/.gitignore_global`\n\n## TypeScript 프로젝트에서의 gitignore\n\n일반적으로 아래와 같지만, 프로젝트의 상황과 요구에 따라 추가/제외되는 항목이 당연히 있을 수 있다. \n\n### 포함하지 말아야 할 파일들\n\n1. `/node_modules/`: `npm install`이나 `yarn` 명령으로 설치한 패키지들이 이 디렉터리에 저장된다. 이는 대부분의 경우에 매우 크며, 이를 Git에 저장하는 것은 비효율적이다. 대신, `package.json`에 의존성이 기록되므로 다른 개발자들은 동일한 패키지를 쉽게 설치할 수 있다.\n\n1. `/dist/` **또는** `/build/`: TypeScript를 JavaScript로 컴파일할 때 생성되는 출력 디렉터리이다. 이것은 원본 코드에서 파생된 것이므로 저장소에 포함할 필요가 없다.\n\n1. `.env`: 환경 변수가 포함된 파일이다. 이 파일은 민감한 정보 (API 키, 비밀번호 등)를 포함할 수 있으므로 공개 저장소에 저장되어서는 안된다.\n\n1. `.DS_Store`: macOS에서 파일 시스템이 생성하는 메타데이터 파일이다. 이는 프로젝트와 관련이 없으므로 무시해야 한다.\n\n### 포함되어야 할 파일들\n\n1. `.js`**와** `.js.map`: TypeScript를 JavaScript로 컴파일하면, 일반적으로 `.js`와 `.js.map` 파일이 생성된다. 이러한 파일은 `/dist/`나 `/build/` 디렉터리에 포함될 수 있지만, 그렇지 않은 경우에도 이러한 파일을 무시하는 것이 좋다.\n\n1. `.tsbuildinfo`: TypeScript 3.4 이후로 컴파일러는 프로젝트 빌드 정보를 이 파일에 저장할 수 있다. 이것은 증분 빌드(incremental build)를 빠르게 만들기 위한 것이므로 저장소에 포함시키지 않아도 된다.\n\n1. `/coverage/`: 테스트 코드의 코드 커버리지를 저장하는 디렉터리이다. 보통 이 디렉터리는 코드 커버리지 도구로 생성된다.\n\n1. **로그 파일**: 예를 들면, `.log` 확장자를 가진 파일들이다.\n\n## 정의되었는데, 추적되는 경우\n\n예를 들어 나의 경우에는 *.js를 정의했는데 특정 js 파일이 계속해서 추적당했다. 이는 몇가지 이유가 있을 수 있다. \n\n1. 이전에 추적되었던 파일\n\n1. 위치 문제\n\n1. 다른 `.gitignore` 규칙\n\n나의 경우에는 1번의 경우였다. `git rm --cached 파일명.js` 명령어를 사용해 추적에서 따로 제거해줘야 한다. \n\n\n\n"},{"excerpt":"TypeScript TypeScript란? TypeScript는 Microsoft에서 개발한 오픈 소스 프로그래밍 언어이다. JavaScript의 SuperSet(상위 집합)이므로, 기존의 자바스크립트 코드도 타입스크립트에서 동작한다. 타입스크립트의 주요 목적은 큰 규모의 어플리케이션 개발을 돕기 위해 정적 타입, 인터페이스, 클래스, 모듈 등의 기능을 …","fields":{"slug":"/Typescript-시작하기/"},"frontmatter":{"date":"September 02, 2023","title":"Typescript 시작하기","tags":["Typescript"]},"rawMarkdownBody":"## TypeScript\n\n### TypeScript란?\n\nTypeScript는 Microsoft에서 개발한 오픈 소스 프로그래밍 언어이다. JavaScript의 SuperSet(상위 집합)이므로, 기존의 자바스크립트 코드도 타입스크립트에서 동작한다. 타입스크립트의 주요 목적은 큰 규모의 어플리케이션 개발을 돕기 위해 정적 타입, 인터페이스, 클래스, 모듈 등의 기능을 자바스크립트에 추가하는 것이다. \n\n### 주요 특징\n\n1. 정적 타이핑 : 변수, 함수 매개변수, 함수 반환 값에 대한 타입을 지정할 수 있다. 이를 통해 컴파일 시점에 오류를 발견할 수 있으며, IDE와 에디터는 타입 정보를 사용하여 더 나은 코드 제안과 리팩토링을 제안할 수 있다. \n\n1. 객체 지향 프로그래밍 : 클래스, 인터페이스, 상속, 제네릭 등과 같은 객체 지향 프로그래밍 기능을 제공하여 코드의 구조와 디자인을 개선한다. \n\n1. 도구 및 에디터 지원 : 코드 자동완성, 타입검사, 코드 리팩토링과 같은 기능을 지원하는 다양한 도구 및 에디터에 통합된다. \n\n1. 다운레벨 타겟팅 : 최신 자바스크립트 기능을 사용하여 코드를 작성하고 ES3, ES5, ES6등 이전 버전의 자바스크립트로 컴파일 할 수 있다. 이를 통해 최신 기능을 사용하면서도 구버전 브라우저와의 호환성을 유지할 수 있다. \n\n1. 확장성 : 모듈화 된 방식으로 동작하여 외부 라이브러리와의 통합을 쉽게 만들어준다. \n\n## TypeScript의 설치\n\n1. TypeScript 설치\n\n    ```bash\n    npm install -g typescript\n    ```\n\n1. 새 프로젝트 시작\n\n    ```bash\n    mkdir my-ts-project\n    cd my-ts-project\n    npm init -y\n    ```\n\n1. TypeScript 설정 파일 생성\n\n    ```bash\n    tsc --init\n    ```\n\n    `tsconfig.json` 파일이 생성된다. 이 파일에서 TypeScript의 컴파일 옵션을 설정할 수 있다.\n\n1. 필요한 TypeScript 타입 정의 설치\n\n    Node.js와 TypeScript를 함 사용하기 위해 해당 타입 정의를 설치한다. \n\n    ```bash\n    npm install --save @types/node\n    ```\n\n1. 첫 TypeScript 파일 작성\n\n    `index.ts` 파일에 다음과 같은 내용을 작성해보자. \n\n    ```typescript\n    const greet = (name: string): string => {\n        return `Hello, ${name}!`;\n    }\n    \n    console.log(greet(\"TypeScript\"));\n    ```\n\n1. 컴파일 및 실행\n\n    아래 명령어로 컴파일 할 수 있다. \n\n    ```bash\n    tsc index.ts\n    ```\n\n    index.js가 생성되었다. 이제 Node.js로 실행할 수 있다. \n\n    ```bash\n    node index.js\n    ```\n\n1. 자동 컴파일 설정(선택사항)\n\n    `ts-node` 라는 패키지를 설치하여 TypeScript 코드를 바로 실행할 수 있다.\n\n    ```bash\n    npm install -g ts-node\n    ```\n\n    설치 후, 아래 명령어로 TypeScript 코드를 바로 실행할 수 있다.\n\n    ```bash\n    ts-node index.ts\n    ```\n\n## TypeScript 설정 파일(**tsconfig.json)**\n\n`tsconfig.json` 은 타입스크립트 프로젝트의 루트 디렉토리에 위치하는 설정 파일이다. 타입스크립트 컴파일러에 대한 구성 옵션을 정의한다. 이 파일을 통해 타입스크립트 코드가 자바스크립트 코드로 변환되는지, 어떤 파일들이 포함되거나 제외되는지 등을 정의할 수 있다. \n\n주요 옵션들은 다음과 같다. \n\n- **files**: 컴파일에 포함할 파일의 목록이다. 이 옵션을 사용하면 특정 파일만 명시적으로 포함시킬 수 있다.\n\n- **include**: 컴파일에 포함할 파일이나 디렉토리의 패턴 목록다. Glob 패턴을 사용할 수 있다. 예: `[\"src/**/*.ts\"]`\n\n- **exclude**: 컴파일에서 제외할 파일이나 디렉토리의 패턴 목록다. 예: `[\"node_modules\"]`\n\n- **extends**: 다른 `tsconfig.json` 파일을 기반으로 현재 설정을 확장하려면 파일 경로를 지정한다.\n\n- **compilerOptions**: 컴파일러에 대한 다양한 설정을 제공하는 핵심 옵션이다.\n\n- **typeRoots와 types**: 사용자 정의 타입 선언의 위치와 포함될 타입 선언 패키지를 지정한다.\n\ncompilerOptions에 대해 좀 더 자세히 알아보면 다음과 같다. \n\n- **target**: 컴파일된 JavaScript의 버전을 지정한다. 예: ES5, ES6 등\n\n- **module**: 모듈 시스템을 지정한다. 예: CommonJS, ESNext, UMD 등\n\n- **outDir**: 컴파일된 JavaScript 파일이 저장될 디렉토리를 지정한다.\n\n- **rootDir**: 입력 파일의 루트 디렉토리를 지정한다.\n\n- **strict**: 모든 엄격한 타입 검사 옵션을 활성화한다.\n\n- **noImplicitAny**: 암시적인 'any' 타입이 없는 경우 오류를 발생시킨다.\n\n- **esModuleInterop**: CommonJS와 ES 모듈 간의 상호 운용성을 위한 코드를 생성한다.\n\n- **skipLibCheck**: 선언 파일의 타입 검사를 건너뛴다.\n\n"},{"excerpt":"브랜치란? 브랜치는 코드의 다양한 버전을 동시에 관리하고 작업할 수 있게 해주는 도구이다. 개발자는 브랜치를 통해 작업을 병렬적으로 진행하고 나중에 원하는 시점에 이 작업들을 병합할 수 있다.  브랜치의 핵심 개념 Commit Git에서 변경점을 나타내는 스냅샷이다. 각 커밋은 이전 커밋에 대한 참조를 가진다.  HEAD 현재 작업중인 브랜치의 가장 최신…","fields":{"slug":"/Branch/"},"frontmatter":{"date":"September 01, 2023","title":"Branch","tags":["GitHub"]},"rawMarkdownBody":"### 브랜치란?\n\n브랜치는 코드의 다양한 버전을 동시에 관리하고 작업할 수 있게 해주는 도구이다. 개발자는 브랜치를 통해 작업을 병렬적으로 진행하고 나중에 원하는 시점에 이 작업들을 병합할 수 있다. \n\n### 브랜치의 핵심 개념\n\n#### Commit\n\nGit에서 변경점을 나타내는 스냅샷이다. 각 커밋은 이전 커밋에 대한 참조를 가진다. \n\n#### HEAD\n\n현재 작업중인 브랜치의 가장 최신 커밋이다. \n\n#### Merge\n\n두 브랜치의 변경 내용을 합치는 과정이다. \n\n### 브랜치의 사용목적\n\n브랜치를 사용하여 코드의 특정 시점을 백업하자. 이것은 Git의 핵심 기능 중 하나이다. \n\n브랜치를 사용하는 것은 코드의 특정 상태나 특정 기능을 분리하고, 나중에 필요한 경우 그 상태로 되돌아가거나 다른 브랜치와 병합하는 데 유용하다. \n\n브랜치를 백업 목적으로 사용하는 것의 장점은 다음과 같다. \n\n#### 장점\n\n1. 상태보존 : 특정 코드의 상태를 보존하고 싶을 때 브랜치를 생성하면 그 시점의 코드 상태를 유지할 수 있다. \n\n1. 안전한 테스트 : 새로운 기능이나 변경 사항을 시도하고 싶을 때 별도의 브랜치에서 작업하면 메인 브랜치를 안전하게 보호할 수 있다. \n\n1. 히스토리 관리 : 브랜치를 통해 어떤 변경 사항이 이루어졌는지의 히스토리를 명확하게 관리할 수 있다. \n\n#### 주의사항\n\n많은 수의 브랜치가 생성되면 관리가 복잡해질 수 있다. 정기적으로 더 이상 필요하지 않은 브랜치를 정리하는 습관을 들이는 것이 좋다. \n\n코드의 모든 작은 변경마다 브랜치를 생성한다면(그럴 리 없겠지만), 과도하게 많은 브랜치가 생성될 수 있다. 백업 목적으로 브랜치를 사용할 때에는 특히 실제로 중요한 시점에서만 브랜치를 생성하는 것이 바람직하다. \n\n### 브랜치 이름 짓기 \n\n브랜치 이름을 지을 때 고려할 수 있는 일반적인 가이드라인은 다음과 같다. \n\n1. 명확하게\n\n1. 짧게\n\n1. 소문자 사용 : 대소문자 구문을 위해 소문자로 통일하는 것이 좋다. \n\n1. 하이픈 사용 : 띄어쓰기가 안되므로 단어와 단어를 하이픈으로 구분한다. \n\n1. 카테고리/분류 사용 : 작업의 유형 또는 카테고리를 브랜치 이름의 접두사로 사용한다. 예시는 다음과 같다. \n\n    - `feat/` 새로운 기능 추가\n\n    - `fix/` 버그 수정\n\n    - `refactor/` 코드 리팩토링\n\n    - `docs/` 문서 관련 작업\n\n    예) `feat/user-authentication`\n\n1. 티켓 번호 포함 : 이슈 트래커나 프로젝트 관리도구와 연계된 작업의 경우 해당 IDX를 브랜치 이름에 포함시키는기 유용할 수 있다. \n\n1. 명사보다 동사를 선호 \n\n1. 확장자 사용 주의 : 브랜치 이름에 txt, md 같은 확장자를 포함하지 않도록 주의한다. \n\n"},{"excerpt":"DataTable 개요 DataTable은 .Net 프레임워크와 .Net Core, 그리고 .Net 5 이후 버전에서 제공하는 ADO.Net의 일부이다. 메모리 상에서 데이터의 테이블 형태를 관리하기 위한 .Net의 클래스이다.  DataTable은 클래스이다. 객체를 인스턴스화하고 메서드와 속성에 액세스 할 수 있다.  메모리상에서 데이터의 테이블 형태…","fields":{"slug":"/DataTable-클래스/"},"frontmatter":{"date":"September 01, 2023","title":"DataTable 클래스","tags":["ASP.Net"]},"rawMarkdownBody":"### DataTable 개요\n\nDataTable은 .Net 프레임워크와 .Net Core, 그리고 .Net 5 이후 버전에서 제공하는 ADO.Net의 일부이다. 메모리 상에서 데이터의 테이블 형태를 관리하기 위한 .Net의 클래스이다. \n\n1. DataTable은 클래스이다. 객체를 인스턴스화하고 메서드와 속성에 액세스 할 수 있다. \n\n1. 메모리상에서 데이터의 테이블 형태를 표현한다. 열(DataColumn)과 행(DataRow)로 이루어진다. \n\n1. 데이터베이스 쿼리 결과 저장, 사용자 인터페이스에 데이터 표시, 데이터 처리 및 변환, XML과 같은 다른 형식으로 데이터 직렬화 등 다양한 곳에 사용될 수 있다. \n\n1. DataTable은 DataSet 내에 존재할 수 있다. DataSet은 여러 DataTable 객체를 포함할 수 있으며, 이러한 테이블간에 관계(DataRelation)를 정의할 수 있다. \n\n### 장점과 단점\n\n#### 장점\n\n1. 자유로운 데이터 접근 : 행과 열을 사용하여 데이터에 액세스 할 수 있으므로 데이터를 쉽게 탐색/수정 할 수 있다. \n\n1. 자체적인 변경 추적 : 데이터 변경 내역을 추적할 수 있다. \n\n1. 연결 독립성 : 메모리 상의 데이터 구조이므로 데이터베이스 연결이 필요 없다. \n\n1. 스키마 정보 포함 : 스키마 정보를 포함하므로 데이터의 형식 및 관계를 정의하고 이해하기 쉽다. \n\n1. 필터 및 계산 기능 : 데이터를 필터링 하거나 계산 할 수 있다. \n\n#### 단점\n\n1. 메모리 사용량 : 메모리 기반의 구조이므로 큰 데이터 세트를 로딩할 경우 상당한 양의 메모리를 사용할 수 있다. \n\n1. 성능 : 엔터티나 객체와 같은 스트림라인된 모델에 비해 처리 및 직렬화에 상대적으로 더 많은 리소스를 요구한다. \n\n1. 타이핑 : 데이터를 액세스 할 때 타입 캐스팅이 필요할 수 있다. \n\n1. 학습 곡선 : ORM이나 LINQ 같은 현대적인 접근법에 익숙한 개발자에게는 학습곡선이 필요하다. \n\n1. 모듈성과 확장성 : 객체지향적인 접근법이나 도메인 주도 설계와 같은 설계 패턴에 직접 통합하기 어려울 수 있다. \n\n### DataTable.Compute의 사용 가능한 주요 표현식\n\n#### 집계함수\n\n| Sum | 합계 |\n| --- | --- |\n| Avg | 평균 |\n| Min | 최소값 |\n| Max | 최대값 |\n| Count | 항목의 수 |\n| StDev | 표준 편차 |\n| Var | 분산 |\n\n#### 산술 연산자\n\n`+`, `-`, `*`, `/`, `%`\n\n#### 비교 연산자\n\n`=`, `<>`, `>`, `<`, `>=`, `<=`\n\n#### 논리 연산자\n\n`AND`, `OR`, `NOT`\n\n#### 문자열 함수\n\n| LEN | 문자열의 길이 |\n| --- | --- |\n| SUBSTRING | 부분 문자열 |\n| TRIM | 앞 뒤 공백 제거 |\n| UPPER | 대문자로 변환 |\n| LOWER | 소문자로 변환 |\n\n#### 기타 함수\n\n| ISNULL | NULL일 경우 대체할 값 반환 |\n| --- | --- |\n| CONVERT | 데이터 타입 변환 |\n\n### 기타 다른 주요 메소드\n\n| Select | 특정 조건을 만족하는 행을 DataRow 배열로 반환 |\n| --- | --- |\n| NewRow | 새 DataRow를 생성 |\n| Rows.Add | 행을 추가 |\n| Rows.Remove | 행을 삭제 |\n| Rows.RemoveAt | 지정한 위치의 행을 삭제 |\n| Clear | 모든 행을 삭제 |\n| Copy | DataTable의 사본을 생성 |\n| Clone | 구조만 복사한 새 DataTable을 반환 |\n| GetChanges | 변경된 행들만을 포함하는 새 DataTable을 반환 |\n| AcceptChanges | 변경 내용을 확정 |\n| RejectChanges | 변경 내용을 취소 |\n\n"},{"excerpt":"무엇을 하는가? Notion API를 이용해서 내가 쓴 글 들을 불러와 MD 파일로 만들려고 한다. 그 과정을 실시간으로 기록하려고 한다. 명확하게 가능한가? 는 사실 아직 알아보지 않았다.  왜 하는가? 너무나 너무나 귀찮기 때문이다. 블로그 포스팅을 위해서 마크다운 에디터로 노션을 쓰고 싶기 때문이다. 아니? 노션으로 쓴 것을 그대로 블로그에 커밋해버…","fields":{"slug":"/Notion-API2/"},"frontmatter":{"date":"September 01, 2023","title":"Notion API(2)","tags":["Hobby","Blogging","Notion-API"]},"rawMarkdownBody":"## 무엇을 하는가? \n\nNotion API를 이용해서 내가 쓴 글 들을 불러와 MD 파일로 만들려고 한다. 그 과정을 실시간으로 기록하려고 한다. 명확하게 가능한가? 는 사실 아직 알아보지 않았다. \n\n## 왜 하는가? \n\n너무나 너무나 귀찮기 때문이다.\n\n블로그 포스팅을 위해서 마크다운 에디터로 노션을 쓰고 싶기 때문이다. 아니? 노션으로 쓴 것을 그대로 블로그에 커밋해버리고 싶기 때문이다. 그 과정을 자동화해버리고 싶기 때문이다. \n\n즉, 노션에 글만 써두면 자동으로 마크다운으로 받아져서 깃허브에 커밋이 되고 깃허브 페이지에 배포가 되는 원대한 자동화를 원한다. \n\n## 어떻게 하는가?\n\n### 환경\n\nNode.js 18.17.1 버전을 사용한다.\n\nNode.js 프로젝트 시작\n\n```bash\nnpm init\n```\n\n.env 생성\n\n```bash\nNOTION_KEY=<노션키>\nNOTION_PAGE_ID=<PageId>\n```\n\n.gitignore에 .env 등록\n\n```bash\n.env\n```\n\n.env 안의 내용을 가져올 수 있도록 `dotenv` 패키지 설치\n\n```bash\n$ npm install dotenv\n```\n\nNotion API를 이용할 수 있도록 패키지 설치\n\n```bash\n$ npm install @notionhq/client\n```\n\n### API 확인\n\n#### 코드\n\nNotion API reference에 아주 친절하게 JavaScript 예제 코드까지 전부 붙어있다. Node.js를 선택한 이유이기도 하다. 예제 코드가 자바스크립트로 되어있어 그대로 갖다 붙이기 쉬워 보였기 때문이다. 일단 API의 결과물이 어떻게 날아오는지 확인하기 위해 있는 대로 갖다 붙였다. \n\n```javascript\nrequire(\"dotenv\").config();\nconst { Client } = require(\"@notionhq/client\");\n\nconst pageId = process.env.NOTION_PAGE_ID;\nconst key = process.env.NOTION_KEY;\n\nconst notion = new Client({ auth: key });\n\n(async () => {\n    try {\n        const response = await notion.databases.query({ database_id: pageId });\n        // console.log(response);\n        //페이지들을 돌면서 id로 페이지를 읽어온다. \n        response.results.forEach(page => {\n            (async () => {\n                const pageId = page.id;\n                //page properties\n                const response = await notion.pages.retrieve({ page_id: pageId });\n                console.log(\"----------Page Properties----------\")\n                console.log(JSON.stringify(response, null, 2));\n                //page contents\n                (async () => {\n                    const blockId = pageId;\n                    const response = await notion.blocks.retrieve({\n                        block_id: blockId,\n                    });\n                    console.log(\"----------Page Contents as Block----------\")\n                    console.log(JSON.stringify(response, null, 2))\n                })();\n                (async () => {\n                    const blockId = pageId;\n                    const response = await notion.blocks.children.list({\n                        block_id: blockId,\n                        page_size: 50,\n                    });\n                    console.log(\"----------Page Block List----------\");\n                    console.log(JSON.stringify(response, null, 2));\n                })();\n            })();\n        });\n    } catch (error) {\n        console.error(\"Error querying the database:\", error);\n    }\n})();\n```\n\n#### 노션의 페이지 구성\n\n![](image1.png)\npage properties는 정확하게 마크다운의 Front Matter에 사용하면 될 것 같다. \n\n그리고, 마크다운의 콘텐츠에 page content가 들어가면 되겠다. \n\npage properties를 얻기 위해서는 notion.pages.retrieve method를 이용하면 된다. \n\n#### 코드 실행 결과문 분석\n\n- notion.pages.retrieve\n\n```json\n{\n  \"object\": \"page\",\n  \"id\": \"...id...\",\n  \"created_time\": \"2023-08-23T13:32:00.000Z\",\n  \"last_edited_time\": \"2023-09-01T12:45:00.000Z\",\n  \"created_by\": {\n    \"object\": \"user\",\n    \"id\": \"...id...\"\n  },\n  \"last_edited_by\": {\n    \"object\": \"user\",\n    \"id\": \"...id...\"\n  },\n  \"cover\": null,\n  \"icon\": null,\n  \"parent\": {\n    \"type\": \"database_id\",\n    \"database_id\": \"...id...\"\n  },\n  \"archived\": false,\n  \"properties\": {\n    \"tags\": {\n      \"id\": \"PHNF\",\n      \"type\": \"multi_select\",\n      \"multi_select\": [\n        {\n          \"id\": \"...id...\",\n          \"name\": \"AWS\",\n          \"color\": \"brown\"\n        },\n        {\n          \"id\": \"...id...\",\n          \"name\": \"TAG2\",\n          \"color\": \"pink\"\n        }\n      ]\n    },\n  },\n  \"url\": \"https://www.notion.so/...\",\n  \"public_url\": \"https://name.notion.site/...\"\n}\n```\n\n정확하게 페이지의 속성들을 가져오고 있다. Front Matter에 필요한 내용들을 속성으로 만들어두면 완벽할 것 같다. \n\n- notion.blocks.retrieve\n\nnotion.blocks.retrieve는 page ID를 넣으면 해당 페이지의 정보를 가져다 주는데 왜냐면 노션에서 모든 단위가 블록이기 때문이다. 해당 페이지의 블록으로서의 데이터를 주는 것이기 때문에 지금 내 상황에선 그다지 사용할 필요가 없어 보인다. \n\n- notion.blocks.children.list\n\n```json\n{\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"object\": \"block\",\n      \"id\": \"id\",\n      \"parent\": {\n        \"type\": \"page_id\",\n        \"page_id\": \"id\"\n      },\n      \"created_time\": \"2023-09-01T12:00:00.000Z\",\n      \"last_edited_time\": \"2023-09-01T12:12:00.000Z\",\n      \"created_by\": {\n        \"object\": \"user\",\n        \"id\": \"id\"\n      },\n      \"last_edited_by\": {\n        \"object\": \"user\",\n        \"id\": \"id\"\n      },\n      \"has_children\": false,\n      \"archived\": false,\n      \"type\": \"heading_1\",\n      \"heading_1\": {\n        \"rich_text\": [\n          {\n            \"type\": \"text\",\n            \"text\": {\n              \"content\": \"제목1\",\n              \"link\": null\n            },\n            \"annotations\": {\n              \"bold\": false,\n              \"italic\": false,\n              \"strikethrough\": false,\n              \"underline\": false,\n              \"code\": false,\n              \"color\": \"default\"\n            },\n            \"plain_text\": \"제목1\",\n            \"href\": null\n          }\n        ],\n        \"is_toggleable\": false,\n        \"color\": \"default\"\n      }\n    },\n\t...\n}\n```\n\n우리가 원하는 Page Content들의 내용이 드디어 보인다. 위의 결과에는 나타나지 않았지만, 들여쓰기가 된 경우에는 해당 들여쓰기가 된 block의 상위 요소에게 종속된다. 다시 말 내 다음 블록에 들여쓰기를 한 블록이 있다면 `has_children` 속성이 true가 된다. 그 안을 다시 파고 들어가면 될 것 같다. \n\n## 오늘의 결론\n\n#### 코드 기록\n\n[https://github.com/Sharknia/Notion-to-Markdown/tree/check-api-test-complete](https://github.com/Sharknia/Notion-to-Markdown/tree/check-api-test-complete)\n\n이렇게 해서, API를 통해서 내 노션에 올려진 글들을 가져오는 데에 성공했다. 이제 다음 시간에는 이 JSON들을 가공해서 Markdown으로 변환하려고 한다. \n\n방법은 세 가지 정도를 생각하고 있다. \n\n1. 노션 내부 API 사용\n\n    실제 노션 페이지를 띄울 때 오고 가는 요청값 들을 확인해 이를 사용하여 마크다운 파일을 바로 만들 수 있는 것으로 보인다. 이는 만들기가 가장 쉬워 보이나, 언제 폭파 될지 모른다는 문제점을 갖고 있다.  만약 폭파된다면 귀찮게도 다시 만들어야 한다는 치명적인 문제 때문에 지금 당장은 고려하고 있지 않지만 실패할 가능성도 가장 적어 보이므로, 귀찮아지면 언제든지 이쪽으로 틀도록 하자. \n\n1. 오픈 소스 사용\n\n    Notion 블록을 마크다운 형식으로 바꿔주는 오픈 소스가 있는 것으로 보인다. 아직 자세히 살펴보지는 않았다. \n\n1. 직접 구현\n\n    그냥.. 직접 만든다. 아마 많은 기능을 지원하지는 못하겠지만 노션을 쓰면서 느낀 건데 많은 기능을 딱히 사용하지 않는다. 정말 개인적인 용도라면 그냥 생각보다 쉽게 될 것 같기도 하다.\n\n일단 코드를 막 짜고 있는데, 이번에는 변화에 닫힌, 확장에 열린 코드를 짜보려고 한다. 설계도 신경써서 하고 싶다. \n\n또 여유가 허락된다면 타입스크립트도 써보고 싶다. \n\n## 참고문서\n\n[https://developers.notion.com/reference](https://developers.notion.com/reference)\n\n[https://blog.hwahae.co.kr/all/tech/10960](https://blog.hwahae.co.kr/all/tech/10960)\n\n"},{"excerpt":"npx란? npx는 Node.js와 함께 제공되는 패키지 실행도구이다.  npm에서 제공하는 패키지를 설치하지 않고 즉시 실행하게 해주는 역할을 한다.  npx의 특징 및 기능 전역설치 없이 실행 패키지를 전역으로 설치하지 않고 즉시 명령어를 바로 실행할 수 있다. 에를 들어  gh-pages와 같은 CLI 도구를 전역적으로 설치하지 않아도 npx gh-…","fields":{"slug":"/npx/"},"frontmatter":{"date":"August 31, 2023","title":"npx","tags":["Node.js"]},"rawMarkdownBody":"#### npx란?\n\nnpx는 Node.js와 함께 제공되는 패키지 실행도구이다.  npm에서 제공하는 패키지를 설치하지 않고 즉시 실행하게 해주는 역할을 한다. \n\n#### npx의 특징 및 기능\n\n1. 전역설치 없이 실행\n\n    패키지를 전역으로 설치하지 않고 즉시 명령어를 바로 실행할 수 있다.\n\n    에를 들어  gh-pages와 같은 CLI 도구를 전역적으로 설치하지 않아도 npx gh-pages로 바로 실행할 수 있다.\n\n1. 일회성 실행\n\n    npx를 사용하여 패키지를 실행하면 해당 패키지는 일시적으로 다운로드 되어 실행되고 실행 후에는 시스템에서 제거된다. \n\n1. 명령어 실행\n\n1. Github gist 실행\n\n1. 임시 패키지 실행\n\n    특정 버전의 패키지나 다른 레지스트리의 패키지를 실행하는데 npx를 사용할 수 있다. \n\n#### npx의 장점\n\n1. 디스크 공간 절약\n\n    사용 후에 패키지가 제거되므로, 불필요한 디스크 공간을 사용하지 않는다. \n\n1. 항상 최신 버전 사용\n\n    최신 버전의 패키지를 가져와 실행한다. \n\n1. 전역 충돌 방지\n\n    패키지를 전역 설치하지 않으므로 다른 버전의 동일한 패키지가 설치되어 있을 때 충돌을 피할 수 있다. \n\n#### npx의 단점\n\n매번 다시 받을 필요 없는 자주 사용하는 패키지나 용량이 큰 패키지의 경우 의미 없이 시간이 걸릴 수 있다. \n\n#### npx의 실행순서\n\n예를 들어 `npx gh-pages -d public` 를 실행하면 다음의 과정을 거친다. \n\n1. 시스템에 `gh-pages`가 설치되어 있는지 확인한다. \n\n1. 설치되어 있지 않다면, 일시적으로 `gh-pages`를 다운로드한다. \n\n1. 다운로드 된 `gh-pages` 패키지의 실행 파일을 사용하여 `-d public` 옵션과 함께 실행한다. \n\n1. 실행이 완료된 후에 `gh-pages` 패키지는 시스템에서 제거된다. \n\n\n\n"},{"excerpt":"왜 하는가 나는 귀찮은게 너무 귀찮다.  커밋 한 이후에  를 해야 배포가 되는 것도 너무 귀찮았다.  무엇을 하는가 그래서, 이걸  을 이용해서 자동화를 했다.  에 임의의 yml 파일을 넣어주면 해당 작업을 github action에서 진행한다.  어떻게 했는가 yml 파일 내용은 다음과 같다.  문제는 없었는가 처음에는 npm install을 사용했…","fields":{"slug":"/githubio-자동배포/"},"frontmatter":{"date":"August 30, 2023","title":"github.io 자동배포","tags":["Blogging","Hobby"]},"rawMarkdownBody":"#### 왜 하는가\n\n나는 귀찮은게 너무 귀찮다. \n\n커밋 한 이후에 \n\n```bash\nnpm run deploy-gh\n```\n\n를 해야 배포가 되는 것도 너무 귀찮았다. \n\n#### 무엇을 하는가\n\n그래서, 이걸 `github action` 을 이용해서 자동화를 했다. \n\n`.gitbub/workflows`에 임의의 yml 파일을 넣어주면 해당 작업을 github action에서 진행한다. \n\n#### 어떻게 했는가\n\nyml 파일 내용은 다음과 같다. \n\n```bash\nname: Deploy\n\non: # 어떤 작업이 수행될 때 deploy.yml 작업이 수행된다. (트리거)\n  push: # push 작업이 수행될 때\n    branches: # 특정 브랜치를 대상으로\n      - master\n\npermissions: # github action이 수행되는 환경에서 특정 권한을 준다\n  contents: write\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-node@v2\n        with:\n          node-version: 20.3.1\n\n      - name: Install node packages\n        run: yarn\n        \n      - name: Check lint\n        run: yarn check:lint\n        \n      # 아래와 같은 오류가 남\n      # error Command failed with exit code 1.\n      # info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n      # - name: Check prettier\n      #   run: yarn check:prettier\n      \n      - name: Build\n        run: yarn build\n        \n      - name: Set up GitHub token\n        env:\n          GITHUB_TOKEN: ${{ secrets.Token}}\n        run: git config --global user.email \"email입력\" && git config --global user.name \"name입력\"\n\n      - name: Set Git remote URL with token\n        run: git remote set-url origin https://${{ secrets.classicToken }}@github.com/Sharknia/Sharknia.github.io\n\n      - name: Deploy to GitHub Pages\n        run: npx gh-pages -d public\n```\n\n#### 문제는 없었는가\n\n처음에는 npm install을 사용했었는데, 패키지가 없다던지 여러가지 문제가 났다. 해당 문제는 npx 명령을 사용하는 것으로 해결돼\n\nDeploy 까지 자동으로 하기 위해서는 깃허브 설정 관련 명령어가 필요했다. \n\n특히, 반드시 토큰을 발행하고 Github Action의 환경 변수에 추가를 해주어야 한다. \n\n"},{"excerpt":"https://devhudi.github.io/gatsby-starter-hoodie/quick-start-kr/ 디자인이 마음에 들고, 필요로 하는 기능이 모두 들어가 있기 때문에 해당 테마를 선택했다. (시리즈 기능, 목차 기능, 댓글 기능) 5번까지는 무사히 테스트 했는데,  6번 특히 Netlify를 활용한 배포에서 막혔다.  Repository를…","fields":{"slug":"/githubio를-이용한-블로그/"},"frontmatter":{"date":"August 28, 2023","title":"github.io를 이용한 블로그","tags":["Blogging","Hobby"]},"rawMarkdownBody":"[https://devhudi.github.io/gatsby-starter-hoodie/quick-start-kr/](https://devhudi.github.io/gatsby-starter-hoodie/quick-start-kr/)\n\n디자인이 마음에 들고, 필요로 하는 기능이 모두 들어가 있기 때문에 해당 테마를 선택했다. (시리즈 기능, 목차 기능, 댓글 기능)\n\n\n\n5번까지는 무사히 테스트 했는데, \n\n6번 특히 Netlify를 활용한 배포에서 막혔다. \n\nRepository를 생성 후, 해당 저장소를 바로 Netlify에서 빌드하려고 하면 의존성 오류가 났다. \n\nNetlify를 활용하려고 한 이유는 Github Pages를 통해 배포할 경우 마스터 브랜치와 별도의 브랜치가 생성되는것이 번잡스러웠기 때문이다. \n\n\n\n조금 더 알아본 결과,\n\n```bash\n$ npm run build\n```\n\n를 실행하면, \n\n`/public` 에 빌드 결과물이 생성되고 해당 폴더만 배포하면 될 것 같긴하다. 또는 `github action` 을 이용할 수도 있겠다. \n\n\n\n조금 더 편하게 블로그를 하려는 길은 아직 멀고 험하다. \n\n"},{"excerpt":"print와 pprint는 모두 Python에서 사용되는 출력 관련 함수이다.  print 값을 터미널에 간단히 출력할 때 사용한다.  pprint pprint는 pretty print의 약자로, 복잡한 데이터 구조를 읽기 쉽게 출력하는 데에 사용된다. 데이터 구조를 계층적으로 출력하고 들여쓰기를 적용하여 가독성을 향상시킨다.  예를 들어 위와 같은 코드…","fields":{"slug":"/print와-pprint/"},"frontmatter":{"date":"August 23, 2023","title":"print와 pprint","tags":["Python"]},"rawMarkdownBody":"print와 pprint는 모두 Python에서 사용되는 출력 관련 함수이다. \n\n### print\n\n값을 터미널에 간단히 출력할 때 사용한다. \n\n```python\nx = 10\ny = \"Hello\"\nprint(x, y)  # 출력: 10 Hello\n```\n\n### pprint\n\npprint는 pretty print의 약자로, 복잡한 데이터 구조를 읽기 쉽게 출력하는 데에 사용된다. 데이터 구조를 계층적으로 출력하고 들여쓰기를 적용하여 가독성을 향상시킨다. \n\n```python\nimport pprint\n\ndata = {'name': 'John', 'age': 30, 'city': 'New York'}\npprint.pprint(data)\n```\n\n예를 들어 위와 같은 코드를 실행하면,\n\n```python\n{'age': 30,\n 'city': 'New York',\n 'name': 'John'}\n```\n\n로 출력이 된다. \n\n딕셔너리, 리스트, 중첩된 데이터 구조 등을 보기 좋게 출력해주어 디버깅이나 데이터 분석시에 유리하다. \n\n"},{"excerpt":"정규화 정규화는 데이터베이스 설계 과정 중에서 중복을 최소화하고 데이터의 일관성과 무결성을 유지하기 위한 기법이다. 정규화는 데이터를 더 작은 논리적 단위로 분할하고 이를 통해 중복 데이터를 제거하여 데이터베이스의 효율성과 유지보수 용이성을 향상 시킨다.  정규화는 대표적으로 업데이트 이상, 삽입 이상, 삭제 이상과 같은 무제를 해결한다.  주요 정규화 …","fields":{"slug":"/정규화와-역정규화/"},"frontmatter":{"date":"August 23, 2023","title":"정규화와 역정규화","tags":["DataBase"]},"rawMarkdownBody":"## 정규화\n\n정규화는 데이터베이스 설계 과정 중에서 중복을 최소화하고 데이터의 일관성과 무결성을 유지하기 위한 기법이다. 정규화는 데이터를 더 작은 논리적 단위로 분할하고 이를 통해 중복 데이터를 제거하여 데이터베이스의 효율성과 유지보수 용이성을 향상 시킨다. \n\n정규화는 대표적으로 업데이트 이상, 삽입 이상, 삭제 이상과 같은 무제를 해결한다. \n\n#### 주요 정규화 단계\n\n1. 제 1정규형(1NF)\n\n1. 제 2정규형(2NF)\n\n1. 제 3정규형(3NF)\n\n1. 보이스-코드 정규형(BCNF)\n\n#### 정규화의 필요성\n\n1. 데이터 중복 최소화\n\n    중복 데이터는 데이터베이스 내에서 일관성 문제를 일으킨다. 중복된 데이터가 변경될 경우 모든 복사본을 일관되게 업데이트 해야 하며, 이를 놓칠경우 불일치가 발생할 수 있다. \n\n    정규화는 데이터를 더 작은 단위로 분할하여 중복을 최소화 하고 이로 인한 불일치 가능성을 낮춘다. \n\n1. 데이터 일관성 유지\n\n    데이터를 여러 테이블에 나누고 관계를 설정함으로써 데이터 간의 의미적 관계를 명확하게 정의할 수 있다. \n\n1. 데이터 이상 방지\n\n    삽입, 업데이트, 삭제 시 발생할 수 있는 이상 현상을 방지한다. \n\n1. 검색 성능 개선\n\n1. 데이터베이스 구조의 명확성\n\n1. 유지보수 용이성\n\n    정규화된 데이터는 데이터베이스 변경이 필요한 경우에도 수정 작업이 해당 테이블에서만 이루어질 가능성이 높다. 이로써 수정이 다른 부분에 미치는 영향을 최소화 하고 유지보수를 용이하게 만든다. \n\n1. 데이터베이스 확장성\n\n    추후 DB 확장 또는 새로운 기능 추가할 때 더 유연하게 대응할 수 있는 기반이 된다. \n\n1. 데이터 무결성 강화\n\n    중복이 최소화 되고 일관성이 유지되면서 데이터의 무결성이 강화된다.\n\n#### 정규화의 단점\n\n- 정규화되면 데이터의 일부가 여러 테이블에 나눠져 저장되어 테이블이 증가하므로 아래와 같은 문제점들이 발생할 수 있다. \n\n1. 조인의 증가\n\n1. 읽기 작업의 복잡성\n\n1. 데이터 일관성 유지의 어려움\n\n1. 갱신 작업의 비효율성\n\n#### 정규화시 주의해야 할 점\n\n1. 과도한 정규화\n\n    너무 과도한 정규화는 복잡한 쿼리, 늘어난 조인으로 성능 저하의 원인이 될 수 있다. \n\n1. 데이터베이스 디자인의 복잡성\n\n    데이터가 분산되어 복잡한 조인과 서브쿼리가 늘어나면서 쿼리 작성, 유지보수가 어려워 질 수 있다. \n\n1. 역정규화의 필요성\n\n    때로 성능 향상이나 특정 요구사항을 위해 데이터 무결성을 해칠 위험을 각오하고도 역정규화가 필요할 수 있다. \n\n## 역정규화\n\n성능 향상 또는 특정 비즈니스 요구에 맞추기 위해 데이터베이스의 구조를 다시 조정하는 과정을 말한다. 정규화된 데이터를 다시 조합하여 중복 데이터를 허용하는 등의 작업이다.\n\n#### 역정규화가 필요한 이유\n\n1. 성능향상\n\n    과도한 정규화는 여러 테이블에 데이터를 나누어 저장하므로 데이터 추출 또는 조인 시 성능 저하의 우려가 있다. 데이터를 조합하여 빠른 검색과 조인을 가능하게 하여 성능을 향상 시킬 수 있다. \n\n1. 복잡한 쿼리 간소화\n\n    마찬가지로 과도한 정규화는 많은 조인으로 인해 쿼리문이 복잡해진다. 역정규화를 통해 쿼리문의 간소화를 노릴 수 있다. \n\n1. 비즈니스 요구 사항 충족\n\n    보고서 생성 및 분석이나 실시간 대시보드 처럼 데이터를 실시간으로 수집하거나 집계해야 하는 경우 데이터베이스 구조를 최적화 하거나 데이터를 임의로 중복저장 하여 성능향상을 노릴 수 있다. \n\n### 결론\n\n정규화와 역정규화는 서로 장점, 단점을 역으로 공유한다. 따라서 데이터의 무결성이나 성능, 일관성 등 여러가지를 신중하게 검토하여 최적의 전략을 선택해야 한다. \n\n[DB 튜닝 경험](https://sharknia.github.io/DB-튜닝-경험)  \n\n"},{"excerpt":"트래픽 튜닝(traffic tuning)은 네트워크 성능을 최적화하기 위한 방법 중 하나이다. 트래픽 튜닝은 대부분 네트워크의 병목 현상을 최소화하고, 서비스 응답 시간을 개선하며, 사용자의 경험을 향상시키는 데 목표를 둔다. 다음은 트래픽 튜닝과 관련된 몇 가지 주요 측면과 전략이다. 로드 밸런싱 (Load Balancing) 로드 밸런싱은 서비스의 안…","fields":{"slug":"/트래픽-튜닝/"},"frontmatter":{"date":"August 23, 2023","title":"트래픽 튜닝","tags":["Network"]},"rawMarkdownBody":"트래픽 튜닝(traffic tuning)은 네트워크 성능을 최적화하기 위한 방법 중 하나이다.\n\n트래픽 튜닝은 대부분 네트워크의 병목 현상을 최소화하고, 서비스 응답 시간을 개선하며, 사용자의 경험을 향상시키는 데 목표를 둔다. 다음은 트래픽 튜닝과 관련된 몇 가지 주요 측면과 전략이다.\n\n1. **로드 밸런싱 (Load Balancing)**\n\n    로드 밸런싱은 서비스의 안정성을 유지하는 핵심 요소이다. 서비스에 과부하가 걸리지 않도록 여러 서버간에 트래픽을 균등하게 분산시켜 주는 역할을 한다. 이는 서비스의 가용성을 향상시키며, 단일 서버의 과부하를 방지한다.\n\n    로드 밸런서는 일반적으로 클라이언트와 서버 중간에 위치하여 요청을 전달하는 역할을 한다. \n\n    - 로드 밸런싱의 특징 및 이점\n\n    1. 고가용성/장애복구/무정지 업그레이드\n\n        로드 밸런서의 도입으로 서버 중 하나에 문제가 발생해도 다른 서버가 작업을 처리하므로 서비스 중단을 최소화 가능하다. \n\n        또한 특정 서버에 장애가 발생했을 때에도 로드 밸런서가 그 사실을 인지하고 자동으로 정상 작동하는 서버로 트래픽을 전환할 수 있다. \n\n        또, 서버의 유지보수나 업그레이드시 서비스 중단 없이 서버를 순차적으로 점검 또는 업그레이드 할 수 있다. \n\n    1. 확장성\n\n        트래픽이 늘어나면 추가 서버를 쉽게 추가하여 트래픽을 처리할 수 있다. \n\n    1. 균등한 분산\n\n        서버 간의 트래픽이나 작업 부하를 균등하게 분산하여 각 서버의 효율적인 작동을 지원한다. \n\n    - 로드 밸런싱의 주요 유형\n\n    1. DNS 로드 밸런싱\n\n        DNS 쿼리를 기반으로 요청을 다양한 서버 IP 주소로 리디렉션한다. \n\n    1. 레이어 4 로드 밸런싱\n\n        전송 계층(TCP/UDP)에서 작동하며, IP 주소와 포트 정보를 기반으로 트래픽을 분산시킨다. \n\n    1. 레이어 7 로드 밸런싱\n\n        응용 프로그램 계층에서 작동하며 HTTP 헤더, 쿠키, URL 등의 내용을 기반으로 트래픽을 분산시킨다. \n\n    - 로드 밸런싱의 주의 또는 단점\n\n    1. 설정의 복잡성\n\n        올바르게 설정하지 않으면 문제 발생 가능\n\n    1. 단일 장애점\n\n        로드 밸런서 자체가 단일 장애점(SPOF, Single Point Of Failure)이 될 수 있다. 이를 예방하기 위해서 고가용성 구성을 가진 여러 로드 밸런서를 사용해야 한다. \n\n    1. 세션 유지 문제\n\n        로드 밸런서는 상태가 없는(stateless) 방식으로 작동하므로 특정 사용자의 세션을 특정 서버에 고정 시키기 위해 추가적인 설정이 필요\n\n    1. 데이터 동기화 문제\n\n        여러 서버에 걸쳐 데이터가 저장된다면 이를 동기화 하는 것이 중요하다. \n\n    1. 기타\n\n        비용, 지연 시간 등이 추가로 발생할 수 있다. 또한 로드 밸런서는 시스템 중앙에 위치하므로 보안 설정 및 업데이트에 주의를 기울여야한다. \n\n1. **데이터베이스 최적화**\n\n    대형 서비스에서는 많은 양의 데이터가 데이터베이스에 저장되기 때문에, 데이터베이스의 성능은 서비스의 성능에 직접적인 영향을 미친다. 쿼리 최적화, 인덱싱, 캐싱 등의 방법으로 데이터베이스 응답 시간을 최적화하는 것이 중요하다.\n\n    [DB 튜닝 경험](https://sharknia.github.io/DB-튜닝-경험)  \n\n1. **캐싱 (Caching)**\n\n    많은 사용자가 동일한 정보나 페이지에 접근할 가능성이 높다. 캐싱을 통해 빈번하게 접근되는 데이터나 페이지를 빠르게 제공할 수 있으며, 네트워크 트래픽과 데이터베이스 부하를 줄일 수 있다.\n\n1. **데이터 압축**\n\n    대규모 서비스에서는 데이터 전송량이 많을 수 있다. 압축을 통해 네트워크 부하를 줄이고 응답 속도를 향상시킬 수 있다.\n\n1. **로깅 및 모니터링**\n\n    실시간으로 서비스의 상태를 모니터링하고, 문제가 발생할 경우 빠르게 대응하기 위해 로깅 및 모니터링 시스템의 구축이 중요하다.\n\n1. **CDN 사용**\n\n    이미지나 정적 파일과 같은 컨텐츠를 전세계적으로 빠르게 제공하기 위해 CDN 사용을 고려할 수 있다.\n\n    \n\n"},{"excerpt":"Notion API를 사용해보려고 한다.  이것으로 무엇을 할 것인가? 는 생각해둔게 있지만, 가능하다고 생각된 시점에서 본격적으로 해보려고 하고, 일단 오늘은 Notion API를 살펴보려고 한다.  간단한 소개 https://developers.notion.com/ Notion의 공개 Rest API를 이용하여 Notion Workspace와 상호 작…","fields":{"slug":"/Notion-API1/"},"frontmatter":{"date":"August 23, 2023","title":"Notion API(1)","tags":["Notion-API","Blogging","Hobby"]},"rawMarkdownBody":"Notion API를 사용해보려고 한다. \n\n이것으로 무엇을 할 것인가? 는 생각해둔게 있지만, 가능하다고 생각된 시점에서 본격적으로 해보려고 하고, 일단 오늘은 Notion API를 살펴보려고 한다. \n\n#### 간단한 소개\n\n[https://developers.notion.com/](https://developers.notion.com/)\n\nNotion의 공개 Rest API를 이용하여 Notion Workspace와 상호 작용 할 수 있다.\n\n페이지, 데이터베이스, 사용자, 페이지 및 인라인 주석, Workspace의 포스팅에 대한 검색 등등.. \n\n점점 지원 영역이 늘어나는 것 같다. change-log가 꽤 활발해보인다. \n\n#### API 생성 및 권한 부여\n\n일단 우선, API를 생성하고 권한 부여 작업을 해야 한다. \n\n[https://www.notion.so/my-integrations](https://www.notion.so/my-integrations)\n\n올라와 있는 한글 블로그들과 현재의 API 권한 부여 방식이 달라지는 바람에  여기서 은근 헤맸다… 역시 공식 가이드를 우선적으로 봐야한다. \n\n위 페이지에서 새 `API 통합 만들기` 를 선택한다. \n\nAPI의 권한은 워크스페이스별로 관리되므로, `연결된 워크스페이스` 를 정확히 선택해준다. \n\n필수 항목을 입력해주고 `제출` 을 눌러주자. \n\n![](image1.png)\n그럼 바로 위와 같은 화면으로 넘어온다. 표시를 눌러서 시크릿 키를 복사해서 보관해주자. \n\n![](image2.png)\n기능도 커스텀 할 수 있는데, 일단 기본적인 기능을 넣어주었다. \n\n배포는 당연히 공개로 해두지 않았다. \n\n이후, 노션으로 돌아와 API와 연결된 워크스페이스를 선택하고 \n\n![](image3.png)\n오른쪽 상단의 … 를 눌러서 연결추가 를 선택한 다음, 아까 만들어준 API의 이름을 검색해 해당 API와 연결을 해주면 된다.\n\n이 부분이 UI가 달라진 부분이어서 공식 가이드를 보기 전까지 한참 헤맸던 부분이었다.\n\n이제, API와 Notion Workspace가 연결되었다!\n\n"},{"excerpt":"정의 PRG 패턴은 웹 어플리케이션에서 폼 제출 후의 중복 제출 문제를 해결하기 위한 웹 디자인 패턴입니다. 사용자가 웹 폼을 제출한 후 (Post), 서버가 해당 요청을 처리하고 사용자를 새로운 위치로 리다이렉트 (Redirect) 시킵니다. 이후 사용자는 리다이렉트된 위치의 정보를 (Get) 요청하여 표시합니다. 특징 중복 폼 제출 방지: 사용자가 새…","fields":{"slug":"/PRG-패턴-PostRedirectGet/"},"frontmatter":{"date":"August 23, 2023","title":"PRG 패턴 (Post/Redirect/Get)","tags":["DesignPattern"]},"rawMarkdownBody":"#### 정의\n\nPRG 패턴은 웹 어플리케이션에서 폼 제출 후의 중복 제출 문제를 해결하기 위한 웹 디자인 패턴입니다. 사용자가 웹 폼을 제출한 후 (Post), 서버가 해당 요청을 처리하고 사용자를 새로운 위치로 리다이렉트 (Redirect) 시킵니다. 이후 사용자는 리다이렉트된 위치의 정보를 (Get) 요청하여 표시합니다.\n\n#### 특징\n\n- 중복 폼 제출 방지: 사용자가 새로고침을 눌렀을 때, 같은 폼 데이터가 다시 제출되는 것을 방지합니다.\n\n- 향상된 사용자 경험: 사용자가 폼 제출 후 브라우저의 뒤로 가기 버튼을 사용할 때, \"폼 데이터 제출 확인\"과 같은 경고 메시지가 나타나지 않습니다.\n\n- 검색 엔진 최적화: 리다이렉트된 URL은 검색 엔진에 의해 색인화될 수 있기 때문에, POST 요청으로 인한 잘못된 URL 색인화를 방지합니다.\n\n#### 장점\n\n- 브라우저 호환성: 대부분의 웹 브라우저에서 지원되므로 호환성 문제가 적습니다.\n\n- 직관적인 URL: 리다이렉트 후의 URL은 대체로 사용자에게 직관적이며, 북마크나 공유하기에 적합합니다.\n\n#### 단점\n\n- 추가적인 서버 응답: 리다이렉트로 인해 추가적인 서버 응답이 필요하므로, 약간의 성능 저하가 발생할 수 있습니다.\n\n- 구현 복잡성: 일부 웹 어플리케이션 프레임워크에서는 PRG 패턴의 구현이 복잡할 수 있습니다.\n\n#### 쇠퇴 이유\n\n- 프레임워크의 발전: 많은 현대 웹 프레임워크와 라이브러리가 이 문제를 내부적으로 처리합니다.\n\n- Single Page Applications (SPA): AJAX와 함께 SPA의 인기가 높아지면서 전통적인 폼 제출 방식이 적게 사용되게 되었습니다.\n\n\n\n"},{"excerpt":"venv란? Python의 표준 라이브러리에 포함된 가상 환경 모듈이다. 가상환경은 프로젝트마다 독립적인 Python 환경을 생성하여 패키지 의존성을 분리하고 관리할 수 있는 기능을 제공한다.  한 시스템 내에서 여러 프로젝트를 개발하거나 실행할 때 각 프로젝트별로 필요한 패키지를 격리된 환경에서 관리할 수 있다.  Python 3.3부터 venv가 표준…","fields":{"slug":"/Python-venv-Windows/"},"frontmatter":{"date":"August 23, 2023","title":"Python venv (Windows)","tags":["Python"]},"rawMarkdownBody":"#### venv란?\n\nPython의 표준 라이브러리에 포함된 가상 환경 모듈이다. 가상환경은 프로젝트마다 독립적인 Python 환경을 생성하여 패키지 의존성을 분리하고 관리할 수 있는 기능을 제공한다. \n\n한 시스템 내에서 여러 프로젝트를 개발하거나 실행할 때 각 프로젝트별로 필요한 패키지를 격리된 환경에서 관리할 수 있다. \n\nPython 3.3부터 venv가 표준 라이브러리에 탑재되었다. \n\n#### venv의 장점\n\n1. 독립성\n\n1. 의존성 분리\n\n1. 가볍고 효율적\n\n1. 파일 분리\n\n#### venv를 이용한 가상환경 생성\n\n1. 가상환경을 생성할 디렉토리를 생성하고 이동한다. \n\n1. 가상환경 생성\n\n    ```bash\n    phthon -m venv venv\n    ```\n\n    마지막 “venv”는 가상환경 이름이다. 이렇게 하면 현재 디렉토리 하에 가상환경의 이름으로 새로운 디렉토리가 생성된다. \n\n1. 가상환경 활성화\n\n    ```bash\n    venv\\Scripts\\activate\n    ```\n\n    첫번째 venv는 가상환경의 이름이다. 가상환경이 활성화되면, 프롬포트에서 가장 앞에 가상 환경의 이름이 나타난다. 이제부터 설치한 패키지는 해당 가상환경에만 적용이 된다. \n\n1. 가상환경 비활성화\n\n    가상환경이 활성화된 상태에서 \n\n    ```bash\n    deactivate\n    ```\n\n"},{"excerpt":"gh-pages (GitHub Pages) 설명: GitHub Pages는 GitHub 저장소를 기반으로 정적 웹사이트를 무료로 호스팅할 수 있는 서비스입니다. 주로 프로젝트 페이지, 개인 포트폴리오, 단순한 블로그 등을 호스팅하는 데 사용됩니다. Jekyll과 같은 정적 사이트 생성 도구와 함께 사용될 수 있습니다. 비슷한 서비스: GitLab Page…","fields":{"slug":"/무료-웹-호스팅-비교/"},"frontmatter":{"date":"August 22, 2023","title":"무료 웹 호스팅 비교","tags":["Hobby"]},"rawMarkdownBody":"1. **gh-pages (GitHub Pages)**\n\n    - **설명**: GitHub Pages는 GitHub 저장소를 기반으로 정적 웹사이트를 무료로 호스팅할 수 있는 서비스입니다. 주로 프로젝트 페이지, 개인 포트폴리오, 단순한 블로그 등을 호스팅하는 데 사용됩니다. Jekyll과 같은 정적 사이트 생성 도구와 함께 사용될 수 있습니다.\n\n    - **비슷한 서비스**: GitLab Pages, Bitbucket Pages.\n\n1. **Heroku**\n\n    - **설명**: Heroku는 클라우드 플랫폼으로 개발자들이 다양한 프로그래밍 언어로 작성된 애플리케이션을 쉽게 배포, 운영, 스케일링 할 수 있게 해줍니다. 기본적으로 무료 티어를 제공하며, 필요에 따라 리소스를 추가적으로 구매할 수 있습니다.\n\n    - **비슷한 서비스**: Google Cloud Run, AWS Elastic Beanstalk, Microsoft Azure App Service.\n\n1. **Vercel**\n\n    - **설명**: Vercel은 주로 프론트엔드와 서버리스 함수를 위한 배포 및 호스팅 플랫폼입니다. Next.js, Gatsby와 같은 프론트엔드 프레임워크에 특화되어 있지만, 다른 프로젝트에도 사용될 수 있습니다. 푸시를 할 때마다 자동으로 배포되는 Continuous Deployment를 지원합니다.\n\n    - **비슷한 서비스**: Netlify, Surge.\n\n### Vercel, Netlify, Surge에 대해 더 알아보자. \n\n1. **Vercel**\n\n    - Next.js의 개발팀에 의해 만들어진 서비스로, 특히 Next.js와의 통합이 매우 간편함.\n\n    - 서버리스 함수의 지원.\n\n    - 자동화된 Continuous Deployment (CD).\n\n    - 무료 플랜과 함께 커스텀 도메인 연결 지원.\n\n    - 최적의 사용 사례: Next.js를 사용한 프로젝트, React 프로젝트, 서버리스 함수가 필요한 프로젝트.\n\n1. **Netlify**\n\n    - 다양한 정적 사이트 생성기와의 통합 (Gatsby, Hugo, Jekyll 등).\n\n    - 서버리스 함수 지원.\n\n    - 자동화된 Continuous Deployment (CD)와 Git 리포지토리 연동.\n\n    - Netlify Identity, Netlify CMS 등의 추가 기능을 제공.\n\n    - 무료 플랜에도 커스텀 도메인 연결 지원.\n\n    - 분할 A/B 테스팅, 인증 및 더 많은 기능 제공.\n\n    - 최적의 사용 사례: 다양한 정적 사이트 생성 도구를 사용하는 프로젝트, 서버리스 함수가 필요한 프로젝트, 웹사이트에 추가 기능이 필요한 경우.\n\n1. **Surge**\n\n    - 매우 간단하고 빠른 정적 웹사이트 배포.\n\n    - CLI를 통한 간단한 배포 경험.\n\n    - 무료 플랜에서 커스텀 도메인 연결 가능.\n\n    - HTTPS 자동 지원.\n\n    - 최적의 사용 사례: 빠르게 정적 사이트를 배포하고자 하는 프로젝트.\n\n### 요약\n\n- Vercel은 특히 Next.js와 함께 사용하기에 최적화된 서비스입니다.\n\n- Netlify는 다양한 정적 사이트 생성 도구와의 호환성과 다양한 추가 기능을 통해 풍부한 개발 경험을 제공합니다.\n\n- Surge는 정적 웹사이트를 빠르고 간단하게 배포하는데 초점을 맞춘 서비스입니다.\n각 프로젝트의 요구 사항에 따라 적합한 플랫폼을 선택할 수 있습니다.\n\n"},{"excerpt":"파일의 확장자를 으로 변경합니다. 예를 들어, 을 으로 변경합니다. ZIP 압축 해제 도구 (예: WinRAR, 7-Zip 등)를 사용하여 변경된  파일을 엽니다.  파일 내에서  폴더나 해당하는 폴더를 찾아  파일을 찾습니다. 해당  파일을 추출합니다. 이렇게 하면 에서 원하는  파일을 얻을 수 있습니다. 이렇게 하면 의존성, 버전 호환성 등의 문제가 …","fields":{"slug":"/Nuget-패키지-dll-추출/"},"frontmatter":{"date":"August 22, 2023","title":"Nuget 패키지 dll 추출","tags":["ASP.Net"]},"rawMarkdownBody":"1. `.nupkg` 파일의 확장자를 `.zip`으로 변경합니다. 예를 들어, `tiktokensharp.1.0.6.nupkg`을 `tiktokensharp.1.0.6.zip`으로 변경합니다.\n\n1. ZIP 압축 해제 도구 (예: WinRAR, 7-Zip 등)를 사용하여 변경된 `.zip` 파일을 엽니다.\n\n1. `.zip` 파일 내에서 `lib` 폴더나 해당하는 폴더를 찾아 `.dll` 파일을 찾습니다.\n\n1. 해당 `.dll` 파일을 추출합니다.\n\n이렇게 하면 `.nupkg`에서 원하는 `.dll` 파일을 얻을 수 있습니다.\n\n\n\n이렇게 하면 의존성, 버전 호환성 등의 문제가 발생할 수 있고 NuGet 패키지 관리 기능도 사용할 수 없기 때문에 권장하지 않는 방법입니다.\n\n"},{"excerpt":"웹 사이트 프로젝트 특징 웹 사이트 프로젝트는 파일 시스템을 기반으로 하는 프로젝트 유형입니다. 웹 애플리케이션의 파일들이 프로젝트 디렉토리 내에 그대로 위치하며, ASP.NET 컴파일러에 의해 실시간으로 컴파일됩니다. 프로젝트 파일이 없고, 각 파일은 개별적으로 관리됩니다. 이로 인해 각 파일의 수정이 간단하며, 특히 작은 프로젝트나 신속한 개발에 유리…","fields":{"slug":"/웹-사이트-프로젝트-vs-웹-응용-프로그램-프로젝트/"},"frontmatter":{"date":"August 21, 2023","title":"웹 사이트 프로젝트 vs 웹 응용 프로그램 프로젝트","tags":["ASP.Net"]},"rawMarkdownBody":"## 웹 사이트 프로젝트\n\n### 특징\n\n1. 웹 사이트 프로젝트는 파일 시스템을 기반으로 하는 프로젝트 유형입니다. 웹 애플리케이션의 파일들이 프로젝트 디렉토리 내에 그대로 위치하며, [ASP.NET](http://asp.net/) 컴파일러에 의해 실시간으로 컴파일됩니다.\n\n1. 프로젝트 파일이 없고, 각 파일은 개별적으로 관리됩니다. 이로 인해 각 파일의 수정이 간단하며, 특히 작은 프로젝트나 신속한 개발에 유리합니다.\n\n1. 런타임 시 컴파일되므로 변경된 파일만 다시 컴파일되고 재시작하지 않아도 됩니다.\n\n1. 소스 코드 파일이 웹 서버에 그대로 배포되므로, 보안 측면에서 주의가 필요합니다.\n\n1. 개발 및 디버깅이 상대적으로 더 간단합니다.\n\n### 장단점\n\n#### 장점\n\n- 신속한 개발과 테스트가 가능합니다.\n\n- 작은 규모의 프로젝트나 간단한 웹 사이트에 적합합니다.\n\n- 파일 단위로 변경 및 배포가 가능하여 유연성이 높습니다.\n\n#### 단점:\n\n- 대규모 프로젝트에서는 파일의 분산 관리 및 성능 문제가 발생할 수 있습니다.\n\n- 보안 취약성이 있을 수 있으며, 코드를 직접 노출시키기 때문에 보안에 더욱 신경을 써야 합니다.\n\n## 웹 응용 프로그램 프로젝트\n\n### 특징\n\n1. 웹 응용 프로그램 프로젝트는 Visual Studio 솔루션 파일에 의해 관리되며, 컴파일된 어셈블리로 배포됩니다.\n\n1. 컴파일된 어셈블리 파일은 웹 서버에 배치되므로, 소스 코드가 직접 노출되지 않습니다.\n\n1. 코드 비하인드 파일과 레이어 분리가 쉽게 가능하며, 따라서 대규모 및 복잡한 프로젝트에 적합합니다.\n\n1. 런타임 이전에 컴파일되므로, 타입 오류 등을 컴파일 단계에서 미리 확인할 수 있습니다.\n\n### 장단점\n\n#### 장점:\n\n- 대규모 및 복잡한 프로젝트에 적합하며, 레이어 구조와 코드 분리를 지원하여 유지 보수가 용이합니다.\n\n- 컴파일된 어셈블리의 배포로 인해 보안 측면에서 더욱 안전합니다.\n\n#### 단점:\n\n- 컴파일 단계가 추가되므로 개발 및 디버깅이 웹 사이트 프로젝트에 비해 상대적으로 복잡할 수 있습니다.\n\n- 프로젝트 설정이 복잡하거나 변경되어야 할 때 번거로울 수 있습니다.\n\n\n\n"},{"excerpt":"NestJS는 MVC 패턴을 지원하지만, 전통적인 Express.js 스타일의 MVC 구조와는 약간 차이가 있다.  NestJS의 기본적인 디렉토리 구조는 다음과 같다.  NestJS에서는 각 기능별로 (예: 사용자, 포스트, 댓글 등) 모듈을 분리하는 것을 권장한다. 따라서 각 기능별로 모듈을 생성하고, 해당 모듈 내에서 MVC 구조를 구성하는 것이 더…","fields":{"slug":"/NestJS의-디렉토리-구조/"},"frontmatter":{"date":"August 20, 2023","title":"NestJS의 디렉토리 구조","tags":["NestJS"]},"rawMarkdownBody":"NestJS는 MVC 패턴을 지원하지만, 전통적인 Express.js 스타일의 MVC 구조와는 약간 차이가 있다. \n\nNestJS의 기본적인 디렉토리 구조는 다음과 같다. \n\n```lua\nsrc/\n|-- app.module.ts         // 앱의 주 모듈\n|-- main.ts               // 앱의 진입점\n|-- controllers/          // 컨트롤러 폴더\n|   |-- app.controller.ts\n|-- services/             // 서비스 폴더\n|   |-- app.service.ts\n```\n\nNestJS에서는 각 기능별로 (예: 사용자, 포스트, 댓글 등) 모듈을 분리하는 것을 권장한다. 따라서 각 기능별로 모듈을 생성하고, 해당 모듈 내에서 MVC 구조를 구성하는 것이 더 일반적이다.\n\n```lua\nsrc/\n|-- app.module.ts\n|-- main.ts\n|-- user/\n|   |-- user.module.ts\n|   |-- user.controller.ts\n|   |-- user.service.ts\n|   |-- user.model.ts (또는 user.entity.ts)\n|-- post/\n|   |-- post.module.ts\n|   |-- post.controller.ts\n|   |-- post.service.ts\n|   |-- post.model.ts (또는 post.entity.ts)\n```\n\n#### MVC 패턴과의 비교\n\n- NestJS의 장점:\n\n    1. 모듈화: NestJS는 기능별로 코드를 모듈로 분리하고 관리하도록 설계되었다. 이로 인해 각 기능이 독립적으로 개발되고 테스트될 수 있으며, 코드 재사용성도 향상된다.\n\n    1. 의존성 주입: NestJS는 내장된 의존성 주입 컨테이너를 제공하므로, 코드의 결합도를 낮추고 유닛 테스트를 쉽게 할 수 있다.\n\n    1. 타입 안전성: TypeScript를 기본 언어로 사용하기 때문에, 타입 검사와 관련된 오류를 컴파일 시점에 발견할 수 있다.\n\n    1. 데코레이터 기반: 데코레이터를 사용하여 메타데이터를 기반으로 하는 다양한 기능을 쉽게 추가할 수 있다.\n\n    1. 플랫폼 독립성: NestJS는 기본적으로 Express.js를 사용하지만, Fastify와 같은 다른 HTTP 서버 라이브러리로 쉽게 전환할 수 있다.\n\n- NestJS의 단점:\n\n    1. 학습 곡선: NestJS에는 다양한 개념과 기능이 있으므로 초기에 학습하기가 다소 어려울 수 있다.\n\n    1. 추가적인 추상화: 모듈화와 의존성 주입과 같은 기능들은 유용하지만, 간단한 애플리케이션에서는 오버헤드로 느껴질 수 있다.\n\n- MVC 패턴의 장점:\n\n    1. 간단함: 전통적인 MVC 패턴은 많은 웹 개발자들에게 잘 알려져 있으므로 학습 곡선이 낮다.\n\n    1. 직관성: 모델, 뷰, 컨트롤러의 구조는 웹 애플리케이션의 데이터 흐름을 이해하기 쉽다.\n\n- MVC 패턴의 단점:\n\n    1. 확장성: 애플리케이션의 복잡도가 증가하면 MVC 구조만으로는 코드의 복잡성을 관리하기 어려울 수 있다.\n\n    1. 타이트 커플링: 모델, 뷰, 컨트롤러 간에 타이트 커플링이 발생할 수 있어, 변경 사항이 여러 컴포넌트에 영향을 줄 수 있다.\n\n"},{"excerpt":"아주 마음에 드는 오픈소스로, 다만 내 취향에 맞게 짜잘짜잘 임의로 몇 가지 부분을 수정했다.  Tags 정렬 등록한 태그들이 블로그 좌측에 나열되는데, 이를 이름 순서대로 나오게 정렬했다.  자동썸네일 기능 테스트 src\\routes\\Detail\\PostDetail\\PostHeader.tsx 에서 썸네일을 가져오고 있고,  src\\pages[slug].…","fields":{"slug":"/MORETHAN-LOG-수정/"},"frontmatter":{"date":"August 20, 2023","title":"MORETHAN-LOG 수정","tags":["Blogging","Hobby"]},"rawMarkdownBody":"아주 마음에 드는 오픈소스로, 다만 내 취향에 맞게 짜잘짜잘 임의로 몇 가지 부분을 수정했다. \n\n1. Tags 정렬\n\n    등록한 태그들이 블로그 좌측에 나열되는데, 이를 이름 순서대로 나오게 정렬했다. \n\n    ```typescript\n    // itemObj를 item name으로 정렬\n      const sortedItemObj = Object.entries(itemObj)\n        .sort((a, b) => a[0].localeCompare(b[0]))\n        .reduce((acc, [key, val]) => {\n          acc[key] = val\n          return acc\n        }, {} as { [itemName: string]: number })\n    \n      return sortedItemObj\n    ```\n\n    \n\n1. 자동썸네일 기능 테스트\n\n    src\\routes\\Detail\\PostDetail\\PostHeader.tsx 에서 썸네일을 가져오고 있고, \n\n    src\\pages\\[slug].tsx 에서 ogImageGenerateURL을 이용한 부분이 있다. \n\n    다만 ogImageGenerateURL을 이용한 부분은 meta에만 적용되고 있고, 자동 썸네일 용 [https://og-image-korean.vercel.app](https://og-image-korean.vercel.app/) 는 만약 사용을 위해서라면 내 이미지를 활용한것으로 수정해야 할 것 같다. \n\n"},{"excerpt":"모 회사 과제 때문에 NestJS를 설치해 볼 일이 생겨 기록해둔다.  Node.js 설치 NestJS는 Node.js를 기반으로 한다. Node.js에서 맞는 버전을 설치한다.  NestJS 프로젝트 생성 Node.js에 npm이 포함되어있다. NestJS 서버를 구성하기 위해서는 @nestjs/cli 를 설치해야 한다.  원하는 디렉토리로 이동해서 프…","fields":{"slug":"/NestJS-설치/"},"frontmatter":{"date":"August 20, 2023","title":"NestJS 설치","tags":["NestJS"]},"rawMarkdownBody":"모 회사 과제 때문에 NestJS를 설치해 볼 일이 생겨 기록해둔다. \n\n#### Node.js 설치\n\nNestJS는 Node.js를 기반으로 한다. [Node.js](https://nodejs.org/ko/download)에서 맞는 버전을 설치한다. \n\n#### NestJS 프로젝트 생성\n\nNode.js에 npm이 포함되어있다. NestJS 서버를 구성하기 위해서는 @nestjs/cli 를 설치해야 한다. \n\n```bash\nnpm i -g @nestjs/cli\n```\n\n\n\n원하는 디렉토리로 이동해서 프로젝트를 초기화한다. \n\n```bash\nnest new <project-Name>\n```\n\n<project-Name> 에는 원하는 프로젝트 이름을 입력한다. 패키지 매니저를 선택할 수 있는데, npm을 선택했다. \n\n#### NestJS 프로젝트 구동\n\nnpm run start 명령어로 구동할 수 있지만, \n\nNestJS에는 내부적으로 내부적으로 [**webpack**](https://webpack.js.org/)과 함께 작동하는 [`@nestjs/cli`](https://docs.nestjs.com/cli/overview)를 제공한다. \n\n이 CLI 도구는 소스 코드의 변경을 감지하고 자동으로 애플리케이션을 재시작하는 기능을 포함하고 있다. \n\n개발 모드로 애플리케이션을 실행하려면 다음 명령어를 사용하면 된다. \n\n```bash\nnpm run start:dev\n```\n\n\n\n이제 localhost:3000 에서 서버가 실행되었음을 확인할 수 있다. \n\n\n\n"},{"excerpt":"var와 let, 그리고 const는 JavaScript에서 변수를 선언할 때 사용되는 키워드이다. 그러나 둘 사이엔 중요한 차이점들이 있으며, 이를 명확히 이해하고 사용해야 한다.  let과 const는 ES6에서 도입된 키워드로, ES6 이전에 변수를 선언하는 유일한 방법은 var 사용 뿐이었다.  var는  블록범위를 지원하지 않는다.  Hoisti…","fields":{"slug":"/var와-letconst/"},"frontmatter":{"date":"August 19, 2023","title":"var와 let,const","tags":["Javascript"]},"rawMarkdownBody":"var와 let, 그리고 const는 JavaScript에서 변수를 선언할 때 사용되는 키워드이다. 그러나 둘 사이엔 중요한 차이점들이 있으며, 이를 명확히 이해하고 사용해야 한다. \n\n\n\nlet과 const는 ES6에서 도입된 키워드로, ES6 이전에 변수를 선언하는 유일한 방법은 var 사용 뿐이었다. \n\nvar는 \n\n- 블록범위를 지원하지 않는다. \n\n- Hoisting되어 해당 범위의 맨 위로 이동하게 되어 변수 선언 전에 변수를 사용해도 오류가 발생하지 않는다. \n\n- 동일한 범위 내에서 같은 이름의 변수를 여러 번 선언할 수 있다. \n\n이러한 문제는 개발자가 의도하지 않은 오류를 발생시킬 가능성이 높았기 때문에 ES6에서 let과 const이 등장하게 되었다. \n\nlet과 const는 var와 비교해서 아래와 같은 특성을 가진다. \n\n1. Scope\n\n    블록범위를 지원한다. 가장 가까운 중괄호 내에서만 변수가 유효하다. \n\n1. Hoisting\n\n    선언 자체는 호이스팅 되지만, 선언 전에 접근하려면 오류가 발생한다. 이를 일시적 죽은 영역(Temporal Dead Zone, TDZ)이라고 한다. \n\n1. 재선언\n\n    동일한 범위 내에서 변수를 재선언 할 수 없다. \n\n    다만, let은 재할당이 가능하지만 const는 재할당도 할 수 없다. (상수 선언)\n\n    따라서 let은 초기 선언 시 초기 값을 제공하지 않을 수 있지만, const는 초기 선언시에 반드시 초기값을 제공해야 한다. \n\n\n\n언뜻 보면 마구 쓰기에 var 선언이 더 편하다고 생각할 수 있다. 하지만 인간은 늘 실수를 하기 때문에, 특히 코드가 길어지고 복잡해질수록 이런 제한 조건이 더욱 필요해지는 것 같다. \n\n원래 브라우저에서만 쓰이려고 태어난 JavaScript 언어가 브라우저의 역할이 늘어나고, 심지어 단순히 브라우저에서만 쓰이는 걸 넘어서 다양한 플랫폼과 환경에서 더 많은 역할을 하게 되면서 이런 것들이 생기는 것 같다. 이러한 맥락에서 ES6 → TypeScript가 생긴게 아닐까?\n\n\n\n"},{"excerpt":"DB  처음에는 단순히 코드 짜는것에만 집중했었는데, 어느 순간 단순한 에러 고침 또는 간단한 로직 수정을 넘어서 더 큰 의미의 유지보수 또는 고도화를 겪으면서 DB 튜닝이 속도에 큰 영향을 미침을 인지하고, 더 빠른 페이지를 만드는것에 집중했다. 그때 아래와 같은 것들을 시도하였다. 인덱싱 : 자주 사용하는, 자주 불려오는 테이블들, 자주 사용되는 컬럼…","fields":{"slug":"/DB-튜닝-경험/"},"frontmatter":{"date":"August 18, 2023","title":"DB 튜닝 경험","tags":["DataBase"]},"rawMarkdownBody":"DB \n\n처음에는 단순히 코드 짜는것에만 집중했었는데, 어느 순간 단순한 에러 고침 또는 간단한 로직 수정을 넘어서 더 큰 의미의 유지보수 또는 고도화를 겪으면서 DB 튜닝이 속도에 큰 영향을 미침을 인지하고, 더 빠른 페이지를 만드는것에 집중했다.\n\n\n\n그때 아래와 같은 것들을 시도하였다.\n\n\n\n- 인덱싱 : 자주 사용하는, 자주 불려오는 테이블들, 자주 사용되는 컬럼에 대해 인덱싱\n\n    - 인덱싱이란?\n\n        - 데이터를 빠르게 조회하기 위한 자료구조(B-tree 등)\n\n        - 색인과 유사한 개념\n\n        - 특정행을 찾기 위한 경로를 미리 만들어 두는 것과 같아 검색 시간을 크게 줄일 수 있다. \n\n    - 인덱싱 사용할 때 주의할 점\n\n        - 추가적인 공간을 사용하므로 무작정 늘려서는 안된다. 가장 자주 조회되는 \n\n        - 복합 인덱스 생성시에는 가장 많이 쿼리되는 열을 인덱스의 앞부분에 둔다. \n\n        - 인덱스는 쓰기 연산(Insert, Update, Delete)시에 유지 비용이 발생(쉽게 생각하면 색인을 새로 만들어야 하므로)하므로 연산이 빈번하게 일어나는 테이블에 대해 많은 인덱스를 만들면 성능 저하가 일어날 수 있음 \n\n- 쿼리 최적화 \n\n    - 불필요한 컬럼 제거(select *(스타) 지양)\n\n    - 서브쿼리 대신 조인 사용\n\n    - 데이터 타입을 최적화 하기 위해 노력(적절한 크기 지정, 숫자형이나 날짜형 형식을 주의해서 용도에 맞게 사용, 예를 들면 idx로 사용할건데 또는 학번에는 숫자만 사용되는데 varchar형식으로 선언되는 경우도 있었는데 그런 경우를 피하기 위해 노력)\n\n- 정규화와 반정규화\n\n    - 회사의 테이블들이 중복된 데이터들이 저장되어있는 경우가 많았다. 따라서 데이터의 무결성을 해치거나 해치지 않더라도 유지보수에 어려움을 야기하는 경우가 자주 발생 → 이런것들을 최대한 피하고 싶어서 직접 설계한 스키마는 이런것들을 최대한 피하고자 했다. 중복 데이터를 피하기 위한 정규화를 진행.\n\n    - 정규화를 빈틈없이 했더니 테이블이 많이 쪼개지고, 중복된 데이터들이 하나도 없어서 조인을 여러 테이블을 해야 하는 경우가 많았다. 정규화를 한 경우에는 반드시 자체 협업툴에 스키마에 대한 문서를 남겼다(정규화가 덜 된 테이블에 비해 상대적으로 알아보기 어려우므로).  또한, 정규화된 테이블들을 이쁘게 조인해서 뷰로 만들어서 문서로 함께 제공했다. \n\n    - 소수의 데이터만 필요한데 조인을 너무 많이 해야 하는 경우나, 특정 집계 결과가 필요한 경우 또는 학생의 역량별 성적같이 조회가 많은데 잦은 계산이 필요한 경우 에는 반정규화를 하기도 했다. 반정규화를 한 경우에는, 테이블 관리(update, insert, edit등)하는 코드를 가능한 한 한군데로 모으고 자체 협업툴에 문서를 자세히 적어서 다른 개발자가 유지보수 시 최대한 알 수 있도록 했다. \n\n    \n\n\n\n"},{"excerpt":"개발자로서 어엿한 블로그 하나는 있어야 하지 않을까, 생각을 항상 하곤 했다.   하지만 블로그라는게 여간 귀찮은 일이 아니다.  공부한 내용을 노션에 적어두기 시작한지 좀 됐는데, 티스토리나 velog나 여러 방법을 시도한적이 있었지만 노션에 적은 내용을 거기로 옮기는게 얼마나 귀찮은지, 시작은 해도 도무지 꾸준히 이어할 수가 없었다.  그냥 언젠가 노…","fields":{"slug":"/MORETHAN-LOG-설치/"},"frontmatter":{"date":"August 18, 2023","title":"MORETHAN-LOG 설치","tags":["Blogging","Hobby"]},"rawMarkdownBody":"개발자로서 어엿한 블로그 하나는 있어야 하지 않을까, 생각을 항상 하곤 했다.  \n\n하지만 블로그라는게 여간 귀찮은 일이 아니다.  공부한 내용을 노션에 적어두기 시작한지 좀 됐는데, 티스토리나 velog나 여러 방법을 시도한적이 있었지만 노션에 적은 내용을 거기로 옮기는게 얼마나 귀찮은지, 시작은 해도 도무지 꾸준히 이어할 수가 없었다. \n\n그냥 언젠가 노션에 쓰기만 하면 블로그가 되는게 없나 그냥 가끔 검색만 하는 정도였는데, 어느샌가 나온 것이다. 그런 천지개벽할 물건이. \n\n\n\n![](image1.png)\n커밋할 필요도, 포스트 할 필요도 없다. 단지 노션에 글을 적으면 바로 포스팅 된다. \n\n게다가 Readme에 설치방법이 10단계로 스크린샷까지 동봉되어서 너무나 친절하게 나와있다. \n\n[https://github.com/morethanmin/morethan-log#readme](https://github.com/morethanmin/morethan-log#readme)\n\n\n\n블로그로 대성하겠다, 이런 마음가짐은 없기 때문에 심플한 구성과 간단한 기능이 너무 마음에 들었다. 누구나 따라할 수 있을 만큼 쉽게 되어있기 때문에 그대로 그대로 따라하는데,\n\n```javascript\nFailed to compile.\n./src/routes/Detail/PostDetail/CommentBox/Utterances.tsx:28:11\nType error: Type '{ repo: string | undefined; \"issue-term\": string; label: string; }' is not assignable to type '{ [key: string]: string; }'.\n  Property 'repo' is incompatible with index signature.\n    Type 'string | undefined' is not assignable to type 'string'.\n      Type 'undefined' is not assignable to type 'string'.\n  26 |     script.setAttribute(\"issue-term\", issueTerm)\n  27 |     script.setAttribute(\"theme\", theme)\n> 28 |     const config: { [key: string]: string } = CONFIG.utterances.config\n     |           ^\n  29 |     Object.keys(config).forEach((key) => {\n  30 |       script.setAttribute(key, config[key])\n  31 |     })\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\nError: Command \"yarn run build\" exited with 1\n```\n\n글쎄 대체 나만 Deploy가 안되는것이다. \n\nISSUE에 나와 같은 오류를 겪은 외국인이 한 명 더 있었는데, 다른 플랫폼으로 갈아탄 모양인지 해결방법은 적혀있지 않았다. \n\n타입스크립트, 리액트는 처음이었기 때문에 정확히 알 수 없었는데, site.config.js 의 utterances-config 의 값을 가져올 때 오류가 나는 것이라고 짐작을 할 수 있었기 때문에 해당 부분을 챗지피티의 도움을 받아 코드를 수정했다. \n\n```typescript\nconst config: { [key: string]: string } = Object.entries(CONFIG.utterances.config)\n  .filter(([_, value]) => value !== undefined)\n  .reduce((acc, [key, value]) => ({ ...acc, [key]: value }), {});\n```\n\n이렇게 하여, 무사히 deploy를 마치고 블로그 포스팅에 성공한것이다. \n\n\n\n문제는 여기서 끝나지 않았는데 노션에 글을 써도 도무지 블로그에 새로운 글이 포스팅되지 않았다. \n\n이것 때문에 잠도 못자고 설쳐가면서 새벽에 새로고침을 해보기도 했는데 자고 일어나도 되어있지 않았다. \n\n\n\n결론부터 이야기하자면, 아무것도 오류는 아니었다. \n\n[https://vercel.com/docs/functions/serverless-functions](https://vercel.com/docs/functions/serverless-functions)\n\nvercel의 Serverless Functions 기능은 모든 plan에서 이용가능하며, 이 기능을 이용해서 Notion API를 활용해서 글 내용을 불러오도록 되어있었다. \n\nsite.config.js를 확인하면 revalidateTime 변수가 있는데 처음에는 이 값이 42시간(!)으로 설정되어있었으므로, 이를 임의로 1시간으로 수정해주었다. \n\n성격이 급해서 1시간 단위로 갱신되게 해두었는데, 지금 와서 생각해보면 굳이 1시간일 필요도 없다는 생각이 든다. \n\n"},{"excerpt":"RDB(관계형 데이터베이스)란? RDB(Relational Database) Oracle, MySQL 둘 다 관계형 데이터베이스이다.  모든 데이터를 2차원의 테이블 형태로 표현한다.  데이터의 독립성이 높고 고수준의 데이터 조작 언어를 사용하여 결합, 제약, 투영등의 관계 조작에 의해 비약적으로 표현 능력을 높일 수 있다.  관계 조작에 의해 자유롭게 …","fields":{"slug":"/RDB관계형-데이터베이스-RDBMS/"},"frontmatter":{"date":"August 18, 2023","title":"RDB(관계형 데이터베이스) + RDBMS","tags":["DataBase"]},"rawMarkdownBody":"1. RDB(관계형 데이터베이스)란?\n\n    - RDB(Relational Database)\n\n    - Oracle, MySQL 둘 다 관계형 데이터베이스이다. \n\n    - 모든 데이터를 2차원의 테이블 형태로 표현한다. \n\n    - 데이터의 독립성이 높고 고수준의 데이터 조작 언어를 사용하여 결합, 제약, 투영등의 관계 조작에 의해 비약적으로 표현 능력을 높일 수 있다. \n\n    - 관계 조작에 의해 자유롭게 구조를 변경할 수 있다. \n\n1. RDBMS란?\n\n    - RDBMS(Relational Database Management System)\n\n        - R\n\n            관계형은 DBMS의 특정한 종류를 의미하고, 여러개의 테이블을 조합해 원하는 데이터를 찾아올 수 있게한다. 보통 한 개의 테이블로 답을 얻을 수 없는 상황에서 이 관계성을 사용해 더 복잡한 요구를 실현할 수 있다. \n\n            관계형을 지원하기 위해 트랜잭션, ACID 등의 개념이 도입되었다. \n\n        - DB\n\n            일종의 저장소(Storage)\n\n            정보를 단순하고 규칙적인 모양새로 구성한 저장소\n\n        - MS\n\n            관리 시스템은은 레코드들을 삽입, 탐색, 수정, 삭제 할 수 있도록 해주는 소프트웨어를 지칭\n\n            데이터를 처리할 수 있는 기능을 의미함\n\n    - 관계형 데이터베이스를 생성하고 수정/관리할 수 있는 소프트웨어\n\n    - 모든 데이터를 2차원 테이블로 표현\n\n    - 테이블은 Row, Column으로 이루어진 기본 데이터 저장 단위\n\n    - 상호 관련성을 가진 테이블의 집합\n\n    - 만들거나 이용하기도 비교적 쉽지만, 확장이 가장 용이함\n\n    - 데이터베이스의 설계도를 ER(Entity Relationship) 모델로 나타낼 수 있다. \n\n    - ER모델에 따라 데이터베이스가 만들어지며, 데이터베이스는 하나 이상의 테이블로 구성된다. \n\n    - ER 모델에서 엔티티를 기반으로 테이블이 만들어진다. \n\n1. JOIN\n\n    - INNER JOIN\n\n        교집합\n\n    - OUTER JOIN\n\n        특정 테이블 기준으로 데이터를 보여줌\n\n        ex) Left Outer Join : 왼쪽 테이블 A의 모든 데이터와 A와 B 테이블의 중복 데이터가 검색됨\n\n"},{"excerpt":"Nest.js란? Node.js 서버측 애플리케이션을 구축하기 위한 프레임워크 TypeScript 기반으로 구축되어 완벽하게 지원 Express를 기본으로 채택하고 그 위에 여러 기능을 미리 구현한 것이 NestJS Nest.js의 특징 확장 가능하며 유지 관리가 쉬운 애플리케이션을 개발할 수 있다 TypeScript, OOP, FP, FRP 요소의 결합…","fields":{"slug":"/NestJS-소개/"},"frontmatter":{"date":"January 25, 2023","title":"NestJS 소개","tags":["NestJS"]},"rawMarkdownBody":"1. Nest.js란?\n\n    Node.js 서버측 애플리케이션을 구축하기 위한 프레임워크\n\n    TypeScript 기반으로 구축되어 완벽하게 지원\n\n    Express를 기본으로 채택하고 그 위에 여러 기능을 미리 구현한 것이 NestJS\n\n1. Nest.js의 특징\n\n    - 확장 가능하며 유지 관리가 쉬운 애플리케이션을 개발할 수 있다\n\n    - TypeScript, OOP, FP, FRP 요소의 결합\n\n    - 타입스크립트 적극 도입\n\n    - 모듈로 감싸는 형태로 테스트 코드의 작성이 용이함\n\n    - 모듈을 사용하여 확장이 용이함\n\n    - 스프링과 유사한 경험을 제공한다\n\n1. Express\n\n    1. Express 특징\n\n        - 전 세계 node.js 프레임워크 1위, 개발 규칙이 강제되어 코드 및 구조의 통일성이 향상됨\n\n        - 레퍼런스가 많다\n\n        - 구조에 대한 자유도가 높다\n\n    1. 차이점\n\n        - NestJS는 타입스크립트를 적극 채용하여 json 파일을 만들고 세팅하는 과정이 복잡하지 않다. \n\n        - NestJS는 아키텍쳐 정의가 프레임워크에서 제공되므로 각 개발자들의 아키텍쳐가 통일되어있다. (장점이자 단점)\n\n        - 라우팅의 차이\n\n            Express는 라우팅 할 때 app.use 처럼 등록해서 사용하지만 NestJS는 모듈별로 나누어서 라우팅한다. \n\n        - 컨트롤러\n\n            NestJS는 클래스별로 컨트롤러에서 메소드에 데코레이터를 사용한다.\n\n        - 서비스\n\n            NestJS는 create 함수 인자로 인터페이스로 정한 값들이 들어오고 return 타입을 공통 coreOutput이라는 인터페이스로 정한 ok, message로 리턴한다. \n\n        \n\n"},{"excerpt":"클로저란? 자신을 포함하고 있는 외부함수보다 내부함수가 더 오래 유지되는 경우, 외부 함수 밖에서 내부함수가 호출되더라도 외부함수의 지역 변수에 접근할 수 있는데 이러한 함수를 클로저라고 부른다.  클로저는 반환된 내부함수가 자신이 선언됐을 때의 환경인 스코프를 기억하여 자신이 선언됐을 때의 환경밖에서 호출되어도 그 환경에 접근할 수 있는 함수를 말한다.…","fields":{"slug":"/JavaScript-클로저/"},"frontmatter":{"date":"January 21, 2023","title":"JavaScript 클로저","tags":["Javascript"]},"rawMarkdownBody":"1. 클로저란?\n\n    자신을 포함하고 있는 외부함수보다 내부함수가 더 오래 유지되는 경우, 외부 함수 밖에서 내부함수가 호출되더라도 외부함수의 지역 변수에 접근할 수 있는데 이러한 함수를 클로저라고 부른다. \n\n    클로저는 반환된 내부함수가 자신이 선언됐을 때의 환경인 스코프를 기억하여 자신이 선언됐을 때의 환경밖에서 호출되어도 그 환경에 접근할 수 있는 함수를 말한다.\n\n    간단히 말하면 클로저는 자신이 생성될 때의 환경을 기억하는 함수 라고 말할 수 있다. \n\n    내부 함수가 클로저이며, 외부함수는 자유변수라고 부른다. 클로저라는 이름은 자유변수에 함수가 닫혀있다는 의미로 의역하면 자유변수에 엮여있는 함수라는 뜻이다. \n\n1. 클로저의 활용\n\n    1. 상태 유지\n\n        클로저가 가장 유용하게 사용되는 상황은 현재 상태를 기억하고 변경된 최신 상태를 유지하는 것이다. \n\n        클로저가 아니고 전역변수를 사용할 수 있지만 전역 변수는 언제든지 누구나 접근할 수 있고 변경할 수 있기 때문에 많은 부작용을 유발하므로 사용하지 않는 것이 좋다. \n\n    1. 전역 변수 사용의 억제\n\n        변수의 값은 누군가에 의해 언제든지 변경될 수 있어 오류 발생의 근본적인 원인이 될 수 있다. 상태 변경이나 가변 데이터를 피하고 불변성을 유지향하는 함수형 프로그래밍에서 사이드 이펙트를 최대한 억제하여 오류를 피하고 프로그램의 안정성을 높이기 위해 클로저를 사용한다. \n\n    1. 정보의 은닉\n\n        자신이 생성됐을 때의 렉시컬 환경인 생성자 함수의 변수에 접근 가능한 클로저의 특징을 사용해 private 키워드를 흉내낼 수 있다. \n\n"},{"excerpt":"타입변환이란? 자바스크립트의 모든 값은 타입이 있다. 값의 타입은 다른 타입으로 개발자에 의해 의도적으로 변환할 수 있다. 또는 자바스크립트 엔진에 의해 암묵적으로 자동 변환될 수 있다. 개발자가 의도적으로 타입을 변경하는 것을 명시적 타입 변환(Explict coercion) 또는 타입 캐스팅(Type casting) 이라고 한다.  자바스크립트 엔진이…","fields":{"slug":"/JavaScript의-타입-변환과-단축-평가/"},"frontmatter":{"date":"January 10, 2023","title":"JavaScript의 타입 변환과 단축 평가","tags":["Javascript"]},"rawMarkdownBody":"1. 타입변환이란?\n\n    자바스크립트의 모든 값은 타입이 있다. 값의 타입은 다른 타입으로 개발자에 의해 의도적으로 변환할 수 있다. 또는 자바스크립트 엔진에 의해 암묵적으로 자동 변환될 수 있다.\n\n    개발자가 의도적으로 타입을 변경하는 것을 명시적 타입 변환(Explict coercion) 또는 타입 캐스팅(Type casting) 이라고 한다. \n\n    자바스크립트 엔진이 타입을 자동 변환 하는 것을 암묵적 타입 변환(Implict coercion) 또는 타입 강제 변환(Type coercion)이라고 한다. \n\n    타입변환이 값을 직접 변경하는 것은 아니다. 변수 값을 변경하려면 재할당을 통해 새로운 메모리 공간을 확보하고 그 곳에 원시 값을 저장한 후 변수명이 재할당된 원시값이 저장된 메모리 공간의 주소를 기억하도록 해야 한다. \n\n    압묵적 타입 변환은 변수 값을 재할당하는 것이 아니라 자바스크립트 엔진이 표현식을 에러없이 평가하기 위해 기존 값을 바탕으로 새로운 타입의 값을 만들어 단 한 번 사용하고 버린다. \n\n1. 암묵적 타입 변환\n\n    자바스크립트 엔진은 표현식을 평가할 때에 컨텍스트를 고려하여 가급적 에러를 발생시키지 않도록 암묵적으로 타입 변환을 실시한다. \n\n    - boolean 타입으로 변환\n\n        자바스크립트 엔진은 다음과 같은 값을 false로 평가한다. 아래의 값을 제외한 값은 모두 true로 판단한다. \n\n        - false\n\n        - undefined\n\n        - null\n\n        - 0, -0\n\n        - NaN\n\n        - ‘’(빈 문자열)\n\n1. 명시적 타입 변환\n\n    암묵적 타입 변환을 이용해서 명시적 타입 변환을 할 수도 있다. \n\n1. 단축평가\n\n    논리곱(&&) 논리합(||) 연산자\n\n    단축평가는 다음과 같은 상황에서 유리하다. \n\n    - 객체가 null 인지 확인하고 프로퍼티를 참조할 때에\n\n    - 함수의 인수를 초기화할 때\n\n        함수를 호출할 때에 인수를 전달하지 않으면 매개변수는 undefined를 갖는다. 이때 단축 평가를 사용하여 매개변수의 기본값을 설정하면 undefined로 인해 발생할 수 있는 에러를 방지할 수 있다. \n\n        \n\n"},{"excerpt":"변수 호이스팅 (Variable Hoisting) 자바스크립트에서는 모든 변수는 호이스팅 된다. 호이스팅이란 var 선언문이나 function 선언문 등 모든 선언문이 해당 Scope의 선두로 옮겨진 것처럼 동작하는 특성을 말한다. 즉 자바스크립트에서는 모든 선언문(var, let, const, function, function*, class)이 선언되기…","fields":{"slug":"/JavaScript의-변수/"},"frontmatter":{"date":"January 07, 2023","title":"JavaScript의 변수","tags":["Javascript"]},"rawMarkdownBody":"- 변수 호이스팅 (Variable Hoisting)\n\n    자바스크립트에서는 모든 변수는 호이스팅 된다. 호이스팅이란 var 선언문이나 function 선언문 등 모든 선언문이 해당 Scope의 선두로 옮겨진 것처럼 동작하는 특성을 말한다. 즉 자바스크립트에서는 모든 선언문(var, let, const, function, function*, class)이 선언되기 이전에도 참조가 가능하다. \n\n- var 키워드로 생성된 변수의 문제점\n\n    ES5에서는 오직 var 키워드로만 변수의 선언이 가능하다. 이는 다음과 같은 특징을 가지며 주의를 기울이지 않으면 심각한 문제를 발생시킨다. \n\n    1. 함수 레벨 스코프\n\n        - 전역변수의 남발\n\n        - for문에서 초기화식에 사용한 변수를 for문 외부 또는 전역에서 참조할 수 있다. \n\n    1. var 키워드 생략 허용\n\n        - 의도하지 않은 변수의 전역화\n\n    1. 중복 선언 허용\n\n        - 의도하지 않은 변수값 변경\n\n    1. 변수 호이스팅\n\n    ES6는 이런 단점을 보완하기 위해 let, const 키워드를 도입하였다. \n\n"},{"excerpt":"변수(Variable) 메모리상의 주소(위치)를 기억하는 저장소. 즉 메모리 주소에 접근하기 위해 사람이 이해할 수 있는 언어로 지정한 식별자 값 데이터 타입 - 프로그래밍 언어에서 사용할 수 있는 값의 종료 변수 - 값이 저장된 메모리를 가리키는 식별자 리터럴 - 소스코드 안에 직접 만들어낸 상수 값 자체. 값의 최소 단위 값은 프로그램에 의해 조작될 …","fields":{"slug":"/JavaScript-기본-문법/"},"frontmatter":{"date":"January 06, 2023","title":"JavaScript 기본 문법","tags":["Javascript"]},"rawMarkdownBody":"1. 변수(Variable)\n\n    메모리상의 주소(위치)를 기억하는 저장소. 즉 메모리 주소에 접근하기 위해 사람이 이해할 수 있는 언어로 지정한 식별자\n\n1. 값\n\n    1. 데이터 타입 - 프로그래밍 언어에서 사용할 수 있는 값의 종료\n\n    1. 변수 - 값이 저장된 메모리를 가리키는 식별자\n\n    1. 리터럴 - 소스코드 안에 직접 만들어낸 상수 값 자체. 값의 최소 단위\n\n    값은 프로그램에 의해 조작될 수 있는 대상을 말한다. \n\n    자바스크립트의 모든 값은 자바스크립트가 제공하는 7가지 데이터 타입 중 하나를 갖는다. \n\n    - 데이터타입\n\n        - 원시타입(Privitive data type)\n\n            - number\n\n            - string\n\n            - boolean\n\n            - null\n\n            - undefined\n\n            - symbol(ES6)\n\n        - 객체 타입(Object data type)\n\n            - object\n\n    자바스크립트는 데이터 타입을 미리 지정하지 않는다. 값의 타입에 의해 동적으로 변수의 타입이 결정되며, 이를 동적 타이핑이라고 한다. \n\n1. 연산자\n\n    피연산자의 타입이 반드시 일치할 필요가 없다. 암묵적 타입 강제 변환을 통해 연산을 수행한다. \n\n1. 키워드\n\n    키워드는 수행할 동작을 규정한다. 예를 들어 var 는 변수를 생성할 것을 지시한다. \n\n1. 주석\n\n    // 로 쓴다. 또는 /* */ 사이에 쓴다. \n\n    주석은 해석기가 무시하며 실행되지 않는다. \n\n1. 문\n\n    브라우저에 의해 단계별로 수행될 명령들의 집합. 문은 리터럴, 연산자, 표현식, 키워드 등으로 구성되며 세미콜론으로 끝나야 한다. \n\n    문은 코드 블록으로 그룹화 할 수 있다. \n\n    자바스크립트는 블록 유효범위 (Block-Level Scope)를 생성하지 않는다. 함수 단위의 유효범위 (Function-Level Scope)만 생성된다.\n\n1. 표현식\n\n    표현식은 하나의 값으로 평가된다.\n\n1. 문과 표현식의 비교\n\n    문은 표현식을 포함한다. 표현식은 그 자체로 하나의 문이 될 수도 있다. \n\n    표현식은 평가되어 값을 만들지만 그 이상의 행위는 할 수 없다. \n\n    표현식을 통해 평가한 값을 통해 실제로 컴퓨터에게 명령을 하여 무언가를 하는 것은 문이다.\n\n1. 함수\n\n    어떤 작업을 수행하기 위해 필요한 문들의 집함. 이름과 매개변수를 가진다. \n\n1. 객체\n\n    자바스크립트는 객체(object) 기반의 스크립트 언어로 자바스크립트를 이루는 거의 모든 것이 객체이다. \n\n    원시 타입을 제외한 나머지 값들은 모두 객체이다. (함수, 배열, 정규표현식 등)\n\n    자바스크립트 객체는 키(이름)와 값으로 구성된 프로퍼티(property)의 집합이다. 프로퍼티의 값으로 자바스크립트에서 사용할 수 있는 모든 값을 사용할 수 있다. 자바스크립트의 함수는 일급 객체이므로 값으로 취급할 수 있다. \n\n    따라서 프로퍼티 값으로 함수를 사용할수도 있으며, 프로퍼티 값이 함수일 경우 일반 함수와 구분하기 위헤 메소드라 부른다. \n\n    객체는 데이터를 의미하는 프로퍼티와 데이터를 참조하고 조작할 수 있는 동작을 의미하는 메소드로 구성된 집합이다. 객체는 데이터(프로퍼티)와 그 데이터에 관련되는 동작(메소드)을 모두 포함할 수 있기 때문에 데이터와 동작을 하나의 단위로 구조화할 수 있어 유용하다. \n\n    자바스크립트의 객체는 객체지향의 상속을 구현하기 위해 “프로토타입”이라고 불리는 객체의 프로퍼티와 메소드를 상속받을 수 있다. \n\n1. 배열\n\n    자바스크립트의 배열은 객체이다. 유용한 내장 메소드를 포함하고 있다. \n\n"},{"excerpt":"자바스크립트의 특징 웹브라우저에서 동작하는 유일한 프로그래밍 언어 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어  명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어 강력한 프로토타입 기반의 객체지향 언어 브라우저 동작 원리 브라우저의 핵심 기능은 사용자가 참조하고자 하는 페이지를 서버에 요청(Request…","fields":{"slug":"/JavaScript의-특징-브라우저-동작-원리/"},"frontmatter":{"date":"January 05, 2023","title":"JavaScript의 특징, 브라우저 동작 원리","tags":["Javascript"]},"rawMarkdownBody":"1. 자바스크립트의 특징\n\n    - 웹브라우저에서 동작하는 유일한 프로그래밍 언어\n\n    - 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어 \n\n    - 명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어\n\n    - 강력한 프로토타입 기반의 객체지향 언어\n\n1. 브라우저 동작 원리\n\n    - 브라우저의 핵심 기능은 사용자가 참조하고자 하는 페이지를 서버에 요청(Request) 하고 서버의 응답(Response)를 받아 브라우저에 표시하는 것. 브라우저는 서버로부터 HTML, CSS, JavaScript, 이미지 파일 등을 응답받는다. HTML, CSS 파일은 렌더링 엔진의 HTML 파서와 CSS 파서에 의해 파싱되어 DOM, CSSOM 트리로 변환되고 렌더 트리로 결합된다. 렌더 트리를 기반으로 브라우저는 웹페이지를 표시한다. \n\n    - 자바스크립트는 렌더링 엔진이 아닌 자바스크립트 엔진이 처리한다. HTML 파서는 script 태그를 만나면 자바스크립트 코드를 실행하기 위해 DOM 생성 프로세스를 중지하고 자바스크립트 엔진으로 제어 권한을 넘긴다. 제어 권한을 넘겨 받은 자바스크립트 엔진은 script 태그 내의 자바스크립트 코드 또는 script 태그의 src 속성에 정의된 자바스크립트 파일을 로드하고 파싱하여 실행한다. 자바스크립트의 실행이 완료되면 다시 HTML 파서가 제어 권한을 받아 DOM 생성을 재개한다. \n\n    - 이처럼 브라우저는 동기적으로 HTML, CSS, JAVASCRIPT를 처리한다. 이것은 script 위치에 따라 블로킹이 발생하여 DOM 구조 생성이 지연될 수 있음을 의미한다. 따라서 script 태그의 위치는 중요한 의미를 갖는다. \n\n    - body 요소의 가장 아래에 자바스크립트를 위치시키는 것은 좋은 아이디어다. \n\n        - HTML 요소들이 지연없이 렌더링 되므로 페이지 로딩 시간이 단축된다. \n\n        - DOM이 완성되지 않은 상태에서 자바스크립트가 DOM을 조작한다면 에러가 발생한다. \n\n"},{"excerpt":"Stun 서버와 Turn 서버를 위한 Coturn Server도 함께 설치 환경구성 Ubuntu 18.04 bionic Python 3.7 설치 명령어 정리 파이썬 및 기본 소프트웨어 설치 Janus 설치 https://ourcodeworld.com/articles/read/1197/how-to-install-janus-gateway-in-ubuntu-s…","fields":{"slug":"/화상상담을-위한-Janus-구성/"},"frontmatter":{"date":"January 03, 2023","title":"화상상담을 위한 Janus 구성","tags":["WebRTC","Work"]},"rawMarkdownBody":"\n        Janus를 우분투에 설치하면서 사용한 명령어 정리\nStun 서버와 Turn 서버를 위한 Coturn Server도 함께 설치\n\n### 환경구성\n\n- Ubuntu 18.04 bionic\n\n- Python 3.7\n\n### 설치 명령어 정리\n\n1. 파이썬 및 기본 소프트웨어 설치\n\n1. Janus 설치\n\n    [https://ourcodeworld.com/articles/read/1197/how-to-install-janus-gateway-in-ubuntu-server-18-04](https://ourcodeworld.com/articles/read/1197/how-to-install-janus-gateway-in-ubuntu-server-18-04)\n\n    - Janus 설치\n\n        ```bash\n        packagelist=( \n        git \n        libmicrohttpd-dev \n        libjansson-dev \n        libssl-dev \n        libsrtp-dev \n        libsofia-sip-ua-dev \n        libglib2.0-dev \n        libopus-dev \n        libogg-dev \n        libcurl4-openssl-dev \n        liblua5.3-dev \n        libconfig-dev \n        pkg-config \n        gengetopt \n        libtool \n        automake \n        gtk-doc-tools \n        cmake \n        ) \n        apt-get install ${packagelist[@]}\n        ```\n\n    - libnice 설치\n\n        *※ 최소 파이썬 3.7을 요구한다.* \n\n        ```bash\n        pip3 install meson==0.61.5 \n        ln -s /usr/local/bin/meson /usr/bin/ \n        wget https://github.com/ninja-build/ninja/releases/download/v1.10.1/ninja-linux.zip \n        unzip ninja-linux.zip \n        cp ninja /usr/bin/ \n        git clone https://gitlab.freedesktop.org/libnice/libnice.git\n        cd libnice \n        meson --prefix=/usr build \n        ninja -C build \n        ninja -C build install\n        ```\n\n    - libstrp 설치\n\n        ```bash\n        wget https://github.com/cisco/libsrtp/archive/v2.2.0.tar.gz \n        tar xfv v2.2.0.tar.gz \n        cd libsrtp-2.2.0 \n        ./configure —prefix=/usr —enable-openssl \n        make shared_library && make install\n        ```\n\n    - usrctp 설치\n\n        ```bash\n        git clone https://github.com/sctplab/usrsctp \n        cd usrsctp \n        ./bootstrap \n        ./configure --prefix=/usr && make && make install\n        ```\n\n    - libwebsockets 설치\n\n        ```bash\n        git clone https://github.com/warmcat/libwebsockets.git \n        cd libwebsockets \n        mkdir build \n        cd build \n        cmake -DLWS_MAX_SMP=1 -LWS_IPV6=ON -DCMAKE_INSTALL_PREFIX:PATH=/usr -DCMAKE_C_FLAGS=\"-fpic\" .. \n        make && make install\n        ```\n\n    - mqtt 설치\n\n        ```bash\n        git clone https://github.com/eclipse/paho.mqtt.c.git \n        cd paho.mqtt.c \n        prefix=/usr make install\n        ```\n\n    - NanoMSG 설치\n\n        ```bash\n        apt-get install libnanomsg-dev -y\n        ```\n\n    - RabbitMQ C AMQP 설치\n\n        ```bash\n        git clone https://github.com/alanxz/rabbitmq-c \n        cd rabbitmq-c \n        git submodule init \n        git submodule update \n        mkdir build && cd build \n        cmake -DCMAKE_INSTALL_PREFIX=/usr ..\n        make && make install\n        ```\n\n    - janus 컴파일링\n\n        ```bash\n        git clone https://github.com/meetecho/janus-gateway.git\n        cd janus-gateway\n        sh autogen.sh\n        ./configure —prefix=/opt/janus\n        make && make install\n        make configs\n        ```\n\n1. Janus 설정\n\n    설정 파일 위치 : /opt/janus/etc/janus\n\n    - janus.jcfg\n\n        ```bash\n        - log_to_file : 주석해제 \n        - admin_secret : 변경 \"PW\" \n        - rtp_port_range : 주석해제 및 변경 \"20000-60000\" \n        - stun_server : 변경 \"Stun Server Domain\" \n        - stun_port : 주석해제 \n        - nat_1_1_mapping : 변경 \"Server IP\" \n        - turn_server : 변경 \"Turn Server Domain\" \n        - turn_port : 주석해제 \n        - turn_type : 주석해제 \n        - turn_user : 주석해제 및 변경 \"Turn ID\" \n        - turn_pwd : 주석해제 및 변경 \" Turn PW\" \n        - ipv6 : 주석해제 및 변경 \"true\"\n        ```\n\n    - janus.plugin.videoroom.jcfg\n\n        ```bash\n        general{ \n        admin_key : 주석해제 및 변경 \"비밀번호\" \n        publishers : 생성 값 \"10\" \n        }\n        ```\n\n    - janus.transport.http.jcfg\n\n        ```bash\n        https : 값 변경 \"true\" \n        secure_port : 주석 해제 \n        admin_https : 주석처리 값 변경 \"true\" \n        cert_pem : 값 변경 \"crt.pem 위치\" \n        cert_key : 값 변경 \"key.pem 위치\" \n        cert_pwd : 값 변경 \"인증서 비밀번호\"\n        ```\n\n1. Coturn 설치\n\n    [http://john-home.iptime.org:8085/xe/index.php?mid=board_sKSz42&document_srl=1546](http://john-home.iptime.org:8085/xe/index.php?mid=board_sKSz42&document_srl=1546)\n\n    \n\n"}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}